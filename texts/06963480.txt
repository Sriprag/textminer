1132

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

Multimodal Neuroimaging Feature Learning
for Multiclass Diagnosis of Alzheimer’s Disease
Siqi Liu∗ , Student Member, IEEE, Sidong Liu, Student Member, IEEE, Weidong Cai, Member, IEEE, Hangyu Che,
Sonia Pujol, Ron Kikinis, Dagan Feng, Fellow, IEEE, Michael J. Fulham, and ADNI

Abstract—The accurate diagnosis of Alzheimer’s disease (AD)
is essential for patient care and will be increasingly important as
disease modifying agents become available, early in the course of
the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the
diagnostic performance was shown in previous methods, due to
the lacking of efficient strategies for representing neuroimaging
biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD.
This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities.
Compared to the previous state-of-the-art workflows, our method is
capable of fusing multimodal neuroimaging features in one setting
and has the potential to require less labeled data. A performance
gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed
framework are discussed.
Index Terms—Alzheimer’s disease (AD), classification, deep
Learning, MRI, neuroimaging, positron emission tomography
(PET).

I. INTRODUCTION
LZHEIMER’S disease (AD) is a degenerative brain disorder that is characterized by a progressive dementia that is
charactered by the degeneration of specific nerve cells, presence
of neuritic plaques, and neurofibrillary tangles [1]. A decline in
memory and other cognitive functions are the usual early syndromes. AD will be a global burden over the coming decades,
due to the increasing age of societies. It was reported that in
2006, there were 26.6 million AD cases in the world, including
about 56% of the cases that are at the early stage. In 2050, the

A

Manuscript received January 31, 2014; revised November 2, 2014; accepted
November 7, 2014. Date of publication November 20, 2014; date of current
version March 17, 2015. This work was supported in part by the ARC, AADRF,
NA-MIC (NIH U54EB005149), and NAC (NIH P41EB015902). Asterisk indicates corresponding author.
∗ S. Q. Liu is with the Biomedical and Multimedia Information Technology
Research Group, School of Information Technologies, University of Sydney,
Sydney, N.S.W. 2006, Australia (e-mail: sliu4512@uni.sydney.edu.au).
S. Liu and W. Cai are with the Biomedical and Multimedia Information
Technology Research Group, School of Information Technologies, University
of Sydney, and also with the Surgical Planning Laboratory, Department of
Radiology, Brigham and Womens Hospital, Harvard Medical School.
H. Che is with the Biomedical and Multimedia Information Technology
Research Group, School of Information Technologies, University of Sydney.
S. Pujol and R. Kikinis are with the Surgical Planning Laboratory, Department
of Radiology, Brigham and Womens Hospital, Harvard Medical School.
D. Feng is with the Biomedical and Multimedia Information Technology
Research Group, School of Information Technologies, University of Sydney,
and also with the Med-X Research Institute, Shanghai Jiao Tong University.
M. J. Fulham is with the Department of PET and Nuclear Medicine, Royal
Prince Alfred Hospital, and the Sydney Medical School, University of Sydney.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2372011

population of the AD patients is predicted to grow fourfold to
106.8 million [2]. The precise diagnosis of AD was considered
as a difficult clinical task with insufficient specificity because
the evaluation of the mental status cannot be made when the
consciousness is impaired. Another difficulty is caused by the
confusion of other non-AD dementia syndromes. Mild Cognitive Impairment (MCI), a prodromal stage of AD, has drawn
attention of researchers recently because it is useful for clinical
trials. Though MCI does not notably interfere with daily activities, it has been constantly proven that MCI patients are at a high
risk of AD progression [3]. To conduct prediction of transition
risk of MCI, MCI subjects can be further categorized as MCI
converters (cMCI) and MCI nonconverters (ncMCI). It is essential to detect the early stages as well as across the full spectrum
of AD progression; therefore, patients are allowed to control
the risk factors, for example, isolated systolic hypertension [4],
[5], before irreversible brain damage develops. Neuroimaging
techniques, such as magnetic resonance imaging (MRI) [6]–[11]
and positron emission tomography (PET) [12]–[18], have been
widely used in the assessment of AD, along with many other
nonimaging biomarkers [6], [19], [20].
Machine learning methods have been proposed to aid the diagnosis of AD. Precomputed medical descriptors were widely
used to represent biomedical images. Approximate measurements, such as the volume [21] and the cerebral metabolic rate
of glucose (CMRGlc) [22], were normally computed from segmented 3-D brain regions of interest (ROI) and were used for
AD classification with support vector machine (SVM) [23],
Bayesian method [24], or other methods [25], [26]. However,
there are several constraints in such workflows. The methods
based on these conventional machine learners often work well
in binary classification, such as categorizing AD subjects from
normal control (NC) subjects, but it is difficult to extend them
to multiclasses [27]. As a result, although the diagnosis of AD
should be naturally modeled as a multiclass classification problem, it was normally simplified as a set of binary classification tasks [23, 28] that distinguish AD or MCI subjects from
NC subjects. Another constraint is the embedding of clinical
prior knowledge. A method based on the graph cut algorithm
was proposed recently by Liu et al. [10]. This workflow adjusted the graph cut algorithm with parameters corresponding
to the relationships between different stages of AD. Though
such customization tends to yield promising classification results, the workflow can be sensitive to changes in the dataset
and can be difficult to extend to a large scale. Another challenge
of AD diagnosis is to represent the original biomarkers in an
unsupervised approach. Some frameworks reduces the dimensionality of each type of biomarker in a supervised way and

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

LIU et al.: MULTIMODAL NEUROIMAGING FEATURE LEARNING FOR MULTICLASS DIAGNOSIS OF ALZHEIMER’S DISEASE

then fuses the feature modalities to form a new feature space
[29]–[31]. Such workflows depend heavily on the quantity of
the labeled samples, which are difficult to achieve. Separating
the dimensionality reduction and the data fusion may also result
in losing complementary information.
We believe that the previous workflows can be optimized by
designing a new framework to efficiently represent the multimodal biomarkers and effectively characterize the multiple
stages of AD. The conventional feature engineering workflows
with shallow structures and affine data transformation often
simply result in feature repetition or dimensionality selection.
As shown in many recent studies, deep data representation can
be more efficient than the shallow architectures in multiclass
classification by disentangling complex patterns in the inputs
[32]–[36]. Deep learning architectures extract high-level features progressively via several layers of feature representations
[37]. The high-level features tend to be more separable in classification problems due to the sequential transformations of the
feature space.
Brosch and Tam, using MR, reported that the multilayered
learning structure was effective in capturing shape variations of
the brain regions that correlate with demographic and disease
information, such as the ventricle size [38]. In the framework
proposed by Suk et al. [39], one setting of stacked autoencoders
(SAEs) was trained for each image modality; then, the learnt
high-level features were further fused with a multikernel support
vector machine (MKSVM). In such workflows, the single-modal
high-level features were learnt regardless of the other modalities,
which may ignore the synergy between different modalities in
the feature learning.
In this study, we propose a novel framework of multiclass
AD diagnosis with deep learning architecture embedded which
benefits from the synergy between multimodal neuroimaging
features. The framework is constructed with an SAE and a softmax logistic regressor. The autoencoders represent the data in
an unsupervised way which can be extended to use unlabeled
data in practice. The proposed framework is capable of data
fusion when multimodal neuroimaging image data are available. Following the concepts of denoising autoencoder [40],
we applied the zero-mask strategy on bimodal deep learning
tasks to extract the synergy between different image modalities. By randomly hiding one modality of the training set, the
hidden layers of the neural network tend to be able to reconstruct the missing modality with corrupted inputs by inferring
the correlations between multimodal features. With the softmax regression embedded in the deep learning architecture, our
framework is capable of classifying AD patients into four AD
stages.
The rest of this paper is organized as follows. We introduce
the proposed learning framework and the training strategies
in Section II. The experiments and results of this study are
presented in Section III. We discuss the proposed framework
and conclusions of the paper in Sections IV and V.

neuroimaging modalities. All collected brain images are first
preprocessed and segmented into 83 functional ROI, and a set
of descriptors are computed from each ROI. The dataset is divided into a training set and a testing set. We perform Elastic Net
[9], [41], [42] only on the training samples to select the discriminative subset of the feature parameters. A multilayered neural
network consisting of several autoencoders is then trained using
the selected feature subset in the training dataset. Each layer of
the network obtains a higher level of abstraction of the previous layer with nonlinear transformation [43]–[45]. The softmax
layer is added on the top of the SAEs for classification. The
trained network is then evaluated with the labeled testing samples.
A. Data Acquisition and Feature Extraction
The neuroimaging data used in this study were obtained
from the Alzheimer’s Disease Neuroimaging Initiative (ADNI)
database1 [46]. This database was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical
Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies, and nonprofit organizations as a five-year public partnership. The primary purpose
of ADNI project was to study the effects of combining multiple
biomarkers, such as MRI, PET, and CSF data accompanied with
neuropsychological assessments, to predict the progression of
MCI and early AD. Around 200 normal instances and 400 MCI
instances were followed for three years; 200 AD patients were
followed within two years. Determining the sensitive biomarkers to the progression of AD might also aid the clinicians to
discover new treatments, as well as other possible biomedical
exploration.
We obtained two datasets from ADNI. For the dataset with
only MR images, 816 age and sex matched subjects were recruited from the ADNI repository and a T1-weighted MR image
was acquired from each subject. We excluded 20 subjects with
multiple conversions or reversions as well as 21 MCI subjects
whose data were incomplete. We labeled the MCI subjects that
converted to AD from 0.5 to 3 years from the first scan as cMCI,
otherwise the MCI subjects were labeled as ncMCI. The normal
subjects and the AD patients were labeled as NC and AD [10].
All raw MR images were corrected following the ADNI MR image protocol and were nonlinearly registered to the ICBM_152
template [47] using the Image Registration Toolkit [48]. Only
17 images were excluded because of the intolerable distortion.
Finally, 758 MR subjects were reserved for the experiments
conducted in this study, including 180 AD subjects, 160 cMCI
subjects, 214 ncMCI subjects, and 204 normal ageing control
subjects.
For the dataset with multimodal data fusion, 331 age and
sex matched subjects were selected from the baseline cohort,
including 77 NC-, 102 ncMCI-, 67 cMCI-, 85 AD-subjects with
both MR and PET data available. Each instance was associated
with T1-weighted volumes and FDG-PET images. All the 3-D
images were preprocessed with the similar workflow described

II. METHODOLOGY
The pipeline of the proposed framework is illustrated in
Fig. 1. In this study, MR and PET data are used as two input

1133

1 The

database is available at adni.loni.ucla.edu

1134

Fig. 1.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

Proposed diagnostic framework of AD with deep learning architecture embedded.

earlier for MR images. The PET images were aligned to the
corresponding MR image using FSL FLIRT [49].
For each registered 3-D image, 83 brain regions were mapped
in the template space using multiatlas propagation with enhanced registration approach [50]. The gray matter volumes
were extracted from MR images, same as in [9] and [10]. For
PET images, we extracted the regional average CMRGlc feature same as in [22] and [51]. We then normalized features to be
between 0 and 1 to support the sigmoidal decoders by shifting
the negative values and rescaling.
B. Learning Framework
1) Pretraining SAEs: We applied SAEs [45], [52], [53] to
learn the high-level features in an unsupervised way as shown in
Fig. 2. Each autoencoder framework encodes an input vector x
into a hidden representation y with an affine mapping followed
by nonlinear sigmoidal distortion
y = σ(W x + b)

(1)

where σ is set as a sigmoid function σ(x) = 1+e1 −x , W is a
weight matrix, and b is a vector of bias terms. y is the encodings
that represent the original input x. The ideal case is that we
can maximally reconstruct x with only knowing y. The decoder
reconstructs the input vector from the hidden representation by




x∗ = τ (W y + b )

(2)


where τ is another sigmoidal filter and W is the decoding
weights. The number of the hidden neurons determines the dimensionality of the encodings at each layer. By controlling the
number of hidden units, we can either perform dimensionality
reduction or learn overcomplete features. The decoding results
in a reconstruction of input vector x with high probability of
P (x∗ |x). Therefore, the reconstruction loss can be minimized
by optimizing the log likelihood
L(x, x∗ ) ∝ −logP (x∗ |x).

(3)

Since the features extracted from MR images were real valued
and were normalized to a domain x ∈ [0, 1], we used the mean
squared error to measure the reconstruction loss L(x, x∗ ). To
prevent the autoencoder from learning merely an identity function, the objective function is regularized by adding a weight

decay, e.g.,
L(W, b, x, x∗ ) = min L(x, x∗ ) + λW 22
W,b

(4)

where W 22 is the weight decay that controls overfitting.
Though the objective function is not convex, the gradients
of the objective function in (4) can be exactly computed by
error back-propagation algorithm. In this study, we applied the
nonlinear conjugate gradient algorithm to optimize (4) [52].
Following the greedy layer-wised training strategy, rather
than training all the hidden layers of the unsupervised network
altogether, we train one autoencoder with a single hidden layer
at a time [43]. When an autoencoder is trained with the features obtained from the previously trained hidden layers, the
hidden layer of the current autoencoder is then stacked on the
trained network. After training all the autoencoders, the final
high-level features are obtained by feedforwarding the activation signals through the stacked sigmoidal filters. When unlabeled subjects are available, the unsupervised feature learning
can be performed with a mixture of the labeled and the unlabeled
samples.
2) Multimodal Data Fusion: When more than one image
modality are used for model training, modality fusion methods
are required to discover the synergy between different modalities. Shared representation can be obtained by jointly training
the autoencoders with the concatenated MR and PET inputs.
The first shared hidden layer is used to model the correlations
between different data modalities. However, the simple feature
concatenation strategy often results in hidden neurons that are
only activated by one single modality, because the correlations
of MR and PET are highly nonlinear. Inspired by Ngiam et al.
[54], we applied the pretraining method with a proportion of
corrupted inputs which had only one modality presented, following the denoising concepts of training deep architecture. One
of the modalities is randomly hidden by replacing these inputs
with 0; the rest of the training samples are presented with both
modalities. The hidden layer of the first autoencoder is trained
to reconstruct all of the original inputs from the inputs that are
mixed with hidden modalities. The original inputs and the corrupted inputs are propagated to the higher layers of the neural
network independently to obtain both the clean representation

LIU et al.: MULTIMODAL NEUROIMAGING FEATURE LEARNING FOR MULTICLASS DIAGNOSIS OF ALZHEIMER’S DISEASE

1135

The softmax filter is defined as
(s )

(s )

eW i a+b i
P (Y = i|x) = 
(s )
(s )
W i a+b i
ie

(5)

where Y is the possible stages of AD progression, a is the
feature representation obtained from the last hidden layer of
(s)
(s)
the pretrained network, and Wi and bi are the weight and
bias for the ith possible decision. For example, P (Y = ‘cMCI’
|x(l) ) is the probability that the patient is diagnosed as a MCI
converter. The label with the highest probability is selected as
the final diagnosis. Optimizing softmax layer is similar to the
unsupervised network. The objective function of fine-tuning the
network with softmax layer is defined as
L(W, b, X, Y ) = min J(X, Y ) + λ(s) W (s) 22
W,b

Fig. 2. Illustration of the single-modal and multimodal architectures of the
proposed framework. (a) Single-modal inputs. (b) Multimodal data fusion with
corrupted inputs.

and the noisy representation using the same neural network.
Each higher layer is then trained progressively to reconstruct
the clean high-level representation from the propagated noisy
representation. Thus, some of the hidden neurons are expected
to infer the correlations between different neuroimaging modalities.
3) Fine-Tuning for AD Classification: For the AD diagnosis, we modeled the task as a four-class classification problem
containing four predefined labels: NC, cMCI, ncMCI, and AD.
Although the features learnt by the unsupervised network can
also be transferred to a conventional classifier, such as SVM,
softmax logistic regression enables us to jointly optimize the
entire network via fine-tuning.
The features extracted by the unsupervised network are fed
to an output layer with softmax regression [55]. The softmax
layer uses a different activation function, which might have
nonlinearity different from the one applied in previous layers.

(6)

where W and b are the weights and bias of the entire network,
including the pretrained SAE and the softmax regression layer,
J(X, Y ) is the logistic regression cost between the diagnosis
generated with on the input features X and the prelabeled results
Y , and λ(s) is the relative weight of the weight decay on softmax
layer, which can be tuned to control the overfitting problem. To
fine-tune the pretrained structure, the softmax layer is then connected to the last hidden layer of the unsupervised network. We
then propagate the activation signals through the entire neural
network and optimize all the parameters according to the classification loss as a supervised neural network [43], [56] as shown
in Fig. 2(a). When more than one modality are used in training
the supervised network, a small proportion of the single-modal
inputs are dropped out in a similar way described in Section
II-B2. The hidden neurons are trained to make diagnosis even
when one modality is absent. This strategy is supposed to make
some of the hidden neurons at the first hidden layer easy to be
activated by the incoming weights from both modalities [54].
C. Feature Examination
Hidden neurons at the first layer of our network are trained to
catch different patterns of input subjects. In deep learning tasks
with general images, the hidden neurons can be visualized as
(l)

Wij
xij =
W 2




(7)

where xij is the input pattern that maximally activates the ith
hidden neuron.
Unlike pixels, the patterns in biomarkers are brain ROI measurements that may be nontrivial to be visualized. We examined the representation quality by mapping the input patterns
produced by (7) back to a masked 3-D MR image, with 83

segmented ROIs. Each input xij corresponds to the brain ROI
where it was extracted. By splitting the pattern x into m features
(m )
(volumes, CMLGLc, etc.), we compute the variance Dj of
all of the same ROI, measuring how the ROIs activate different
(m )
hidden neurons. When Dj is low, the biomarkers from region
j are more effective for AD diagnosis than the other regions.
The overall feature stability Sj of the jth ROI can be computed

1136

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

not denoted to be totally trivial, but carrying less predictive
information.
B. Performance Evaluation

Fig. 3. Image was generated using 3-D Slicer (V4.3) [57]. The stability map
denotes that the darker ROIs tend to be more affected by AD, hence more
sensitive to the AD progression than the brighter regions. The features extracted
from the darker areas are also considered as more stable predictors for our deep
learning model and tend to be beneficial to all of the hidden neurons in the
neural network.

as
Sj =




j

(m )

Dj

(m )

m

.

(8)

Dj

S can be convolved with a Gaussian filter to enlarge the distinctions between different ROIs. The brain ROIs with relatively
high stability score are considered as more effective to the AD
progression. These mappings on the MR image can be examined
with the clinical prior knowledge to monitor the performance of
the feature learning network in the context of AD diagnosis.
III. EXPERIMENTS AND RESULTS
A. Visualization of High-Level Biomarkers
With the feature examination method described in Section
II-C, we calculated the stability score of each brain ROI and
mapped the stability score to a masked 3-D MR image (83 ROIs)
of an NC subject as shown in Fig. 3. The distinctions between
various ROIs were clearly visualized. The darker regions tend
to be more sensitive to the progression of AD and MCI than
the lighter ROIs, since features extracted from these ROIs tend
to benefit all the hidden neurons equally. The light regions are

We compared the proposed framework with the widely applied methods using the single-kernel SVM and the MKSVM
[23], [28]. To evaluate the proposed data fusion method, we
compared the zero-mask method to the architecture proposed
in [39] that trains two SAEs independently and then fuses the
high-level features with MKSVM after each SAE is fine-tuned.
All of the experiments were evaluated with the same features
extracted from MR images and PET images as described in
Section II-A.
The proposed framework was implemented on MATLAB
2013a. The SVM-based experiments were performed using
LIBSVM [58]. The MKSVM was implemented by using precomputed kernels and fusing the multiple kernels with relative
weights.
The evaluation was conducted by using tenfold crossvalidation. In experiments including multiple modalities, we
compared the performance with only single-modal data, MR or
PET, and the data fusion methods with both modalities. To avoid
the “lucky trials,” we randomly sampled the training and testing
instances from each class to ensure they have similar distributions as the original dataset. The entire network was trained and
fine-tuned with the 90% of data and then tested with the rest
of samples in each validation trial. The hyperparameters of all
compared methods were chosen in each validation trial using
the approx search in log-domain to obtain the best performed
model [59]. Two hidden layers were used in all neural-networkbased experiments because adding additional hidden layers did
not show further improvements on AD classification. It is reasonable to assume that two nonlinear transformations could be
ideal to represent the neuroimaging features for AD classification. The number of neurons at hidden layers were chosen
between 30 and 200 according to the classification performance
in each fold. In each neural network, hidden layers shared the
same number of hidden neurons [60]. The MKSVM was trained
with the training samples. Following the workflow in [23], the
relative weights of each kernel in MKSVM were chosen through
a coarse-grid search with a step size of 0.1. In the experiments
that used MKSVM for fusing two SAE networks, each SAE was
first pretrained and fine-tuned with the training data, and then,
the high-level features obtained from each network were fused
with MKSVM with the procedure stated before.
1) Experiments on MR (758 Subjects): We first evaluated
the proposed framework with 758 3-D MR images. Since only
one modality is presented, no modality fusion strategy was used
in both SVM and the proposed method.
The performance of binary classification (NC versus AD and
NC versus MCI) is displayed in Table I. The first two columns
are precisions on individual classes, and the following three
columns are the overall performance including accuracy, sensitivity, and specificity. The proposed method (SAE) outperformed SVM in classifying AD subjects from the NC subjects

LIU et al.: MULTIMODAL NEUROIMAGING FEATURE LEARNING FOR MULTICLASS DIAGNOSIS OF ALZHEIMER’S DISEASE

1137

TABLE I
PERFORMANCE (%) OF BINARY AD CLASSIFICATION WITH MR-ONLY SAMPLES
NC versus AD
Methods
SVM
SAE

NC
82.95 ± 8.57
82.23 ± 6.54

Methods
SVM
SAE

NC
62.39 ± 4.87
67.44 ± 13.14

AD
ACC
80.16 ± 6.46
81.04 ± 6.28
84.31 ± 7.36
82.59 ± 5.33
NC versus MCI
MCI
74.81 ± 4.34
75.80 ± 3.58

ACC
71.27 ± 3.26
71.98 ± 5.48

by leading the overall accuracy (82.59%) and overall sensitivity
(86.83%). The overall specificities between these two methods
were very closed (78.89% and 77.78%). The proposed method
outperformed SVM in all overall performance measurements of
classifying NC from MCI. The proposed method achieved 5%
higher precision on classifying the NC subjects.
The performance of multiclass classification is displayed in
Table II. The first four columns are the precisions of the individual classes and the following three are the overall performance. The proposed method performed better precisions than
SVM in three classes (52.40% on NC, 38.71% on cMCI, and
46.89% on AD). The proposed method leads in the overall accuracy (46.30%) and overall specificity (77.78%). SVM achieved
higher sensitivity (75.00%). In summary, our proposed method
outperformed the state-of-the-art SVM-based methods in most
of the performance measurements in both binary and multiclass
AD classification problems when only MR data are presented.
2) Experiments on MR and PET (331 Subjects): There are
totally 331 subjects with both MR and PET data available. We
first evaluated the performance of SVM and the proposed SAE
based method with only MR images (SVM-MR, SAE-MR) or
PET images (SVM-PET, SAE-PET). The performance of fusing modalities with multikernel SVM is shown as MKSVM.
For deep learning methods, we compared the proposed zeromasking training strategy (SAE-ZEROMASK) to the simple
feature concatenation (SAE-CONCAT).
The binary classification performance is displayed in
Table III. It can be observed that the experiments with both
modalities (MKSVM, SAE-CONCAT, and SAE-ZEROMASK)
yielded better performance than those with only single modality
in both binary classification tasks. SAE-CONCAT outperformed
MKSVM slightly in the overall accuracy (90.15% − 90.11%
and 77.65% − 76.88%). It can be observed that when the proposed SAE-ZEROMASK method is used, the performance is
enhanced in all measurements comparing to SAE-CONCAT.
MKSVM performed slightly higher specificity in classifying
NC and AD comparing to ZERO-MASK. Though SVM-MR
achieved slightly higher precision (83.92%) on MCI, it is reasonable to assume this performance may be due to an unbalanced
decision making (only 67% on NC). Among all the methods,
SAE-ZEROMASK achieved the most balanced performance
in the classification between NC and MCI (81.95% on NC and
83.88% on AD), which is relatively difficult when MCI occupies
a big proportion of the dataset (169 out of 246). The proposed

SEN
82.83 ± 6.02
86.83 ± 6.83

SPE
78.89 ± 14.66
77.78 ± 10.83

SEN
47.02 ± 12.37
49.52 ± 13.68

SPE
84.50 ± 4.25
84.31 ± 13.15

data fusion method SAE-ZEROMASK with only one neural network achieved comparable performance with 2SAE-MKSVM,
which fuses two high-level feature matrices from two independently trained networks. The accuracy of 2SAE-MKSVM was
not obviously higher than that of simple feature concatenation
(77.90% to 77.65%), because it was observed in the experiments
the MKSVM added for feature fusion only preserved the higher
accuracy achieved by a single-modal network in some of the
validation trials.
The performance of the multiclass classification is shown in
Table IV. The proposed framework with the corrupted inputs
(SAE-ZEROMASK) leads the overall accuracy and specificity
(53.79% and 86.98%). Deep learning-based methods (SAECONCAT and SAE-ZEROMASK) lead the precision on NC,
cMCI, and AD. The precision of cMCI was constrained by the
quantity of cMCI instances (67 out of 331) and was effected by
its sibling class ncMCI with 102 instances. For ncMCI, the precision achieved by SAE-ZEROMASK and MKSVM were very
closed. Compared to the simple feature concatenation (SAECONCAT), SAE-ZEROMASK increased the overall accuracy
by about 5%. SAE-ZEROMASK also outperformed the other
data fusion option 2SAE-MKSVM in the overall accuracy and
specificity. SVM-based methods tend to have better sensitivity.
IV. DISCUSSION
A. Model Designing and Training
Studies have shown that learning architecture with multilayered nonlinear representations of the original data would yield
meaningful features for classification [56], [61]–[63]. For accurate diagnosis in AD subjects, we investigated the use of multilayered representations of neuroimaging biomarkers on AD
classification. Our results showed that the multilayered structure
can be used to distinguish MR and PET subjects along the spectrum of AD progression with higher accuracy than conventional
shallow architectures. The performance of classification primarily benefited from the depth (a notion derived from complexity
theory) of the learning architecture, which can be illustrated as
a sequence of non-linear transformations of the feature space.
During fine-tuning, the neuroimaging feature space is distorted
and folded to minimize the classification loss on the training
data. Thus, after several layers of transformations, the inseparable samples would become separable in the learnt high-level
feature space. Compared to traditional methods, the proposed

1138

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

TABLE II
PERFORMANCE (%) OF MULTICLASS AD CLASSIFICATION WITH MR-ONLY SAMPLES
Methods
SVM
SAE

NC

ncMCI

cMCI

AD

ACC

SEN

SPE

46.96 ± 3.95
52.40 ± 8.43

42.95 ± 10.80
41.25 ± 7.16

37.88 ± 10.18
38.71 ± 23.18

44.62 ± 8.04
46.89 ± 4.40

44.45 ± 3.07
46.30 ± 4.24

75.00 ± 9.04
66.14 ± 10.57

68.59 ± 5.13
77.78 ± 4.48

TABLE III
PERFORMANCE (%) OF THE BINARY AD CLASSIFICATION WITH MR AND PET
NC versus AD
Methods
SVM-MR
SVM-PET
MKSVM
SAE-MR
SAE-PET
SAE-CONCAT
2SAE-MKSVM
SAE-ZEROMASK

NC
87.83 ± 11.81
84.58 ± 7.02
89.68 ± 5.67
88.28 ± 11.68
83.27 ± 12.44
88.67 ± 12.84
91.35 ± 8.15
90.38 ± 7.36

Methods
SVM-MR
SVM-PET
MKSVM
SAE-MR
SAE-PET
SAE-CONCAT
2SAE-MKSVM
SAE-ZEROMASK

NC
67.52 ± 14.15
62.66 ± 22.67
70.85 ± 17.69
65.56 ± 24.61
50.69 ± 22.53
73.56 ± 16.55
90.42 ± 11.46
81.95 ± 14.99

AD
ACC
84.06 ± 10.42
84.67 ± 8.45
87.57 ± 10.12
84.60 ± 4.05
91.50 ± 9.37
90.11 ± 5.57
88.74 ± 10.82
87.79 ± 9.12
85.91 ± 10.73
83.53 ± 9.80
92.56 ± 8.35
90.15 ± 9.54
91.40 ± 6.82
92.42 ± 8.81
92.89 ± 6.17
91.40 ± 5.56
NC versus MCI
MCI
83.92 ± 7.56
77.17 ± 4.78
80.16 ± 3.40
76.86 ± 5.59
77.12 ± 7.51
80.00 ± 4.94
73.85 ± 10.01
83.88 ± 4.99

ACC
77.70 ± 5.27
72.35 ± 8.67
76.88 ± 5.83
74.02 ± 7.58
70.00 ± 9.33
77.65 ± 5.18
77.90 ± 5.18
82.10 ± 4.91

SEN
80.54 ± 11.59
84.11 ± 12.86
89.64 ± 11.43
87.32 ± 11.19
82.86 ± 15.59
92.14 ± 9.08
90.89 ± 10.40
92.32 ± 6.29

SPE
88.33 ± 12.19
84.58 ± 9.07
90.56 ± 4.76
88.47 ± 12.19
83.75 ± 12.41
88.19 ± 12.72
91.67 ± 7.40
90.42 ± 6.93

SEN
62.50 ± 20.11
45.54 ± 10.23
52.14 ± 8.56
40.36 ± 15.63
46.96 ± 19.12
49.46 ± 14.35
61.43 ± 18.99
60.00 ± 13.93

SPE
84.60 ± 6.60
84.60 ± 10.94
88.16 ± 8.73
89.26 ± 7.61
80.44 ± 8.42
90.51 ± 7.09
92.92 ± 7.64
92.32 ± 8.74

TABLE IV
PERFORMANCE (%) OF MULTICLASS AD CLASSIFICATION WITH MR AND PET
Methods
SVM-MR
SVM-PET
MKSVM
SAE-MR
SAE-PET
SAE-CONCAT
2SAE-MKSVM
SAE-ZEROMASK

NC

ncMCI

cMCI

AD

ACC

SEN

SPE

49.74 ± 8.79
30.30 ± 20.15
47.71 ± 12.73
47.80 ± 17.97
41.79 ± 11.76
49.21 ± 14.74
53.86 ± 11.47
59.07 ± 19.74

44.58 ± 14.91
36.90 ± 11.63
52.76 ± 19.33
40.39 ± 9.46
35.17 ± 10.10
43.54 ± 9.43
52.08 ± 18.65
52.21 ± 11.84

46.45 ± 31.63
45.79 ± 27.08
38.17 ± 31.94
45.08 ± 24.95
41.06 ± 10.06
49.62 ± 9.66
53.17 ± 26.63
40.17 ± 14.42

53.74 ± 10.20
50.30 ± 7.00
53.81 ± 6.81
56.33 ± 14.03
54.25 ± 11.79
56.35 ± 14.21
55.58 ± 13.06
64.07 ± 15.24

47.74 ± 1.82
42.60 ± 2.90
48.65 ± 4.29
45.61 ± 8.31
42.91 ± 6.63
48.96 ± 5.32
51.39 ± 5.64
53.79 ± 4.76

66.43 ± 14.46
35.36 ± 23.00
61.07 ± 18.95
48.04 ± 14.97
43.04 ± 17.45
46.61 ± 22.04
66.25 ± 18.34
52.14 ± 11.81

78.78 ± 8.13
79.95 ± 8.33
79.86 ± 6.43
82.69 ± 7.88
82.26 ± 5.36
84.63 ± 8.51
82.66 ± 6.16
86.98 ± 9.62

framework is more powerful in extracting the complex correlations between neuroimaging ROI-based biomarkers as well as
different feature modalities. Another motivation of using multilayered structure for AD diagnosis is to reuse the high-level
features for semisupervised learning [64]. Besides the supervised data fusion or dimensionality reduction [29], the proposed
workflow can be easily extended to use unlabeled neuroimaging
data.
We combined different data modalities with the proposed
zero-mask fusion strategy by propagating noisy signals with
one modality randomly hidden. The autoencoders were trained
to reconstruct the original incoming signals with the corrupted
incoming signals. We also tried to avoid training separate neural networks on different data modalities, because this may ignore the complementary information during feature learning.
The training subjects with one hidden modality tend to force

some neurons to be sensitive to MR and PET inputs, which
makes the zero-mask fusion network different as it has two
independent feature learning networks. It was noticeable that
2SAE+MKSVM also achieved an overall classification accuracy of 91.4% and a higher specificity of 91.67% in the binary
classification of NC and AD. It may indicate that when relatively
larger margins exist between different feature clusters, the binary decision boundaries might be similar between both feature
fusion methods. Observing the experimental results with nonconvertible and convertible MCI subjects involved, we assume
that the proposed zero-mask method may have more advantages
when subtler differences and more outliers are included in a
noisy training set.
Instead of using raw image patches for the medical feature
learning, we applied the feature engineering pipeline to extract
the initial ROI measurements of MR and PET images as inputs.

LIU et al.: MULTIMODAL NEUROIMAGING FEATURE LEARNING FOR MULTICLASS DIAGNOSIS OF ALZHEIMER’S DISEASE

The differences between 3-D medical images of AD-related patients tend to be subtle and the variance tends to be large. From
this perspective, the hidden neurons of the network decision
system can also be interpreted as automatically encoded inferences of diagnostic rules [65]. Our experiments showed that
when using ROI precomputed features, the unsupervised network achieved the best performance with two hidden layers in
pretraining. This means that relatively shallower architectures
are practically required when using the approximately measured
imaging features, compared with the learning tasks which use
raw images as inputs [38]. The networks with the same number
of neurons in all hidden layers often performed better in our
experiments. We found that both overcompleted manifolds or
low-dimensional manifolds yielded effective features for AD
classification. The number of hidden neurons was chosen according to different training sets.
The feature selection, using Elastic Net, enhanced the performance of all examined methods. It helped control the overfitting
caused by the noisy and redundant feature parameters. Notably,
the majority of the selected feature parameters were consistently
chosen by Elastic Net. The validation trials, with fewer chosen
feature parameters, tended to have higher generalization errors,
which might be due to the biased outliers that were included in
the training set.
Although the extracted features can be used by some other
conventional classifiers, such as SVM, we connected an output layer with softmax regression to the unsupervised network.
With a different nonlinearity from the one used in other layers, softmax regression corresponded to multinomial log-output
variables. As a result, it is capable of classifying samples among
several AD stages; it also simplified the fine-tuning phase of
training because the softmax layer can be jointly optimized with
the hidden layers. We also investigated the framework designs
of transferring the fine-tuned features to popular classifiers other
than the embedded softmax regressor. It was interesting to see
that, taking as input the same high-level features learnt by our
deep learning network, all of the investigated classifiers tended
to make highly consistent decisions.
B. Limitations and Future Work
Considering the limited quantity of the available neuroimaging data, we assume that the synergy between different biomarkers can be further extracted with more training samples which
may have smaller variance. The proposed data fusion strategy
follows the denoising fashion of training autoencoders, which
theoretically increased the difficulty of feature learning, but controlled the overfitting. Although the predicted probability distribution of the four-class AD classification may be of more
practical use in a decision-making system, the performance that
we achieved with the available dataset should be improved before multiclass classification frameworks are applied to clinical
use. All the methods that we compared our methodology to
tended to overfit but had high accuracy on the training set and
low accuracy on the testing set. Since the multimodal learning
architectures with neural networks (2SAE-MKSVM and SAEZEROMASK) are parametric models, we assume that they may

1139

have the potential to achieve better diagnostic accuracy on multiclass AD diagnosis when larger datasets are available. This
will allow better extraction of subject-independent features with
lower variance.
V. CONCLUSION
We propose a novel framework for the diagnosis of AD with
deep learning embedded. The framework can distinguish four
stages of AD progression with less clinical prior knowledge
required. Since the unsupervised feature representation is embedded in this workflow, it has potential to be extended to more
unlabeled data for feature engineering in practice. In the unsupervised pretraining stage, we used SAEs to obtain high-level
features. When more than one neuroimaging modality was used,
we applied a zero-masking strategy to extract the synergy between different modalities following a denoising fashion. After the unsupervised feature engineering, a softmax regression
was used. We used a novel method of visualising high-level
brain biomarkers to analyze the high-level features that were
extracted.
The proposed framework was evaluated with AD classification between stage two and four. Based on MR and PET ADNI
data repository, our framework outperformed the state-of-theart SVM-based method and other deep learning frameworks. We
argue, therefore, that the proposed method can be a powerful
means to represent multimodal neuroimaging biomarkers.
REFERENCES
[1] G. McKhann et al., “Clinical diagnosis of Alzheimer’s disease report of
the NINCDSADRDA Work Group under the auspices of Department of
Health and Human Services Task Force on Alzheimer’s disease,” Neurology, vol. 34, no. 7, pp. 939–939, 1984.
[2] R. Brookmeyer et al., “Forecasting the global burden of Alzheimer’s
disease,” Alzheimer’s Dementia, vol. 3, no. 3, pp. 186–191, 2007.
[3] B. Dubois et al., “Research criteria for the diagnosis of Alzheimer’s
disease: Revising the NINCDSADRDA criteria,” Lancet Neurol., vol. 6,
no. 8, pp. 734–746, 2007.
[4] S. Gauthier et al., “Mild cognitive impairment,” The Lancet, vol. 367, no.
9518, pp. 1262–1270, 2006.
[5] C. DeCarli, “Mild cognitive impairment: Prevalence, prognosis, aetiology,
and treatment,” Lancet Neurol., vol. 2, no. 1, pp. 15–21, 2003.
[6] C. Davatzikos et al., “Prediction of MCI to AD conversion, via MRI,
CSF biomarkers, and pattern classification,” Neurobiol. Aging, vol. 32,
no. 12, pp. 2322.e19–2322.e27, 2011.
[7] R. Cuingnet et al., “Automatic classification of patients with Alzheimer’s
disease from structural MRI: A comparison of ten methods using the
ADNI database,” Neuroimage, vol. 56, no. 2, pp. 766–781, 2011.
[8] Y. Fan et al., “Structural and functional biomarkers of prodromal
Alzheimer’s disease: A high-dimensional pattern classification study,”
Neuroimage, vol. 41, no. 2, pp. 277–285, 2008.
[9] S. Liu et al., “Multi-channel brain atrophy pattern analysis in neuroimaging retrieval,” in Proc. IEEE Int. Symp. Biomed. Imaging: From Nano to
Macro, 2013, pp. 206–209.
[10] S. Liu et al., “Neuroimaging biomarker based prediction of Alzheimer’s
disease severity with optimized graph construction,” in Proc. IEEE Int.
Symp. Biomed. Imaging: From Nano to Macro, 2013, pp. 1324–1327.
[11] W. Cai et al., “A 3D difference of Gaussian based lesion detector for brain
PET,” in Proc. IEEE Int. Symp. Biomed. Imaging: From Nano to Macro,
2014, pp. 677–680.
[12] G. Chetelat et al., “Mild cognitive impairment can FDG-PET predict
who is to rapidly convert to Alzheimer’s disease?” Neurology, vol. 60,
no. 8, pp. 1374–1377, 2003.
[13] R. Higdon et al., “A comparison of classification methods for differentiating frontotemporal dementia from Alzheimer’s disease using FDG-PET
imaging,” Statist. Med., vol. 23, no. 2, pp. 315–326, 2004.

1140

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 4, APRIL 2015

[14] N. L. Foster et al., “FDG-PET improves accuracy in distinguishing
frontotemporal Dementia and Alzheimer’s disease,” Brain, vol. 130,
no. 10, pp. 2616–2635, 2007.
[15] S. Liu et al., “A robust volumetric feature extraction approach for 3D
neuroimaging retrieval,” in Proc. Annu. Int. Conf. IEEE Eng. Med. Biol.
Soc., 2010, pp. 5657–5660.
[16] S. Liu et al., “Localized functional neuroimaging retrieval using 3D
discrete curvelet transform,” in Proc. IEEE Int. Symp. Biomed. Imaging:
From Nano to Macro, 2011, pp. 1877–1880.
[17] S. Liu et al., “Multiscale and multiorientation feature extraction with
degenerative patterns for 3D neuroimaging retrieval,” in Proc. 19th IEEE
Int. Conf. Image Process., 2012, pp. 1249–1252.
[18] S. Liu et al.,, “Multi-channel neurodegenerative pattern analysis and
its application in Alzheimer’s disease characterization,” Comput. Med.
Imaging Graph., vol. 38, no. 4, pp. 436–444, 2014.
[19] F. H. Bouwman et al., “CSF biomarkers and medial temporal lobe atrophy predict dementia in mild cognitive impairment,” Neurobiol. Aging,
vol. 28, no. 7, pp. 1070–1074, 2007.
[20] S. Q. Liu et al., “Multi-phase feature representation learning for neurodegenerative disease diagnosis,” in Proc. Australian Conf. Artif. Life
Comput. Intell., pp. 350–359, 2015.
[21] S. L. Risacher et al., “Baseline MRI predictors of conversion from MCI
to probable AD in the ADNI cohort,” Current Alzheimer’s Res., vol. 6,
no. 4, pp. 347–361, 2009.
[22] W. Cai et al., “3D neurological image retrieval with localized pathologycentric CMRGlc patterns,” in Proc. 17th IEEE Int. Conf. Image Process.,
2010, pp. 3201–3204.
[23] D. Zhang et al., “Multimodal classification of Alzheimer’s disease and
mild cognitive impairment,” NeuroImage, vol. 55, no. 3, pp. 856–867,
2011.
[24] S. Liu et al., “Multifold Bayesian kernelization in Alzheimers diagnosis,”
in Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention,
2013, pp. 303–310.
[25] S. Liu et al., “Semantic-word-based image retrieval for neurodegenerative
disorders,” J. Nucl. Med., vol. 53, no. Supplement 1, p. 2309, 2012.
[26] F. Zhang et al., “Semantic association for neuroimaging classification of
PET images,” J. Nucl. Med., vol. 55, no. Supplement 1, p. 2029, 2014.
[27] J. Liu et al., “Distance-informed metric learning for Alzheimer’s disease staging,” in Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., 2014,
pp. 934–937.
[28] S. Klöppel et al., “Automatic classification of MR scans in Alzheimer’s
disease,” Brain, vol. 131, no. 3, pp. 681–689, 2008.
[29] N. Singh et al., “Genetic, Structural and functional imaging biomarkers
for early detection of conversion from MCI to AD,” in Proc. Int. Conf.
Med. Image Comput. Comput.-Assisted Intervention, 2012, pp. 132–140.
[30] S. Liu et al., “A supervised multiview spectral embedding method for neuroimaging classification,” in Proc. 20th IEEE Int. Conf. Image Process.,
2013, pp. 601–605.
[31] H. Che et al., “Co-neighbor multi-view spectral embedding for medical
content-based retrieval,” in Proc. IEEE Int. Symp. Biomed. Imaging: From
Nano to Macro, 2014, pp. 911–914.
[32] J. Hastad, “Almost optimal lower bounds for small depth circuits,” in Proc.
18th Annu. ACM Symp. Theory Comput., 1986, pp. 6–20.
[33] J. Hastad and M. Goldmann, “On the power of small-depth threshold
circuits,” Comput. Complexity, vol. 1, no. 2, pp. 113–129, 1991.
[34] S. Q. Liu et al., “Early diagnosis of Alzheimer’s disease with deep
learning,” in Proc. IEEE Int. Symp. Biomed. Imaging: From Nano to
Macro, 2014, pp. 1015–1018.
[35] S. Liu et al., “Sparse auto-encoded hypo-metabolism patterns in
Alzheimer’s disease and mild cognitive impairment,” J. Nucl. Med.,
vol. 54, no. Supplement 2, p. 1807, 2013.
[36] S. Q. Liu et al., “High-level feature based PET image retrieval with deep
learning architecture,” J. Nucl. Med., vol. 55, no. Supplement 1, p. 2018,
2014.
[37] Y. Bengio et al., “Representation learning: A review and new perspectives,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 8,
pp. 1798–1828, Aug. 2013.
[38] T. Brosch, and R. Tam, “Manifold learning of brain MRIs by deep learning,” in Proc. Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, 2013, pp. 633–640.
[39] H.-I. Suk et al., “Latent feature representation with stacked auto-encoder
for AD/MCI diagnosis,” Brain Struct. Funct., pp. 1–19, 2013.

[40] P. Vincent et al., “Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion,” J. Mach.
Learn. Res., vol. 11, pp. 3371–3408, 2010.
[41] H. Zou and T. Hastie, “Regularization and variable selection via the elastic
net,” J. Roy. Stat. Soc. Ser. B: Stat. Methodol., vol. 67, no. 2, pp. 301–320,
2005.
[42] L. Shen et al., “Identifying neuroimaging and proteomic biomarkers for
MCI and AD via the elastic net,” in Proc. 1st Int. Conf. Multimodal Brain
Image Analysis, 2011, pp. 27–34.
[43] Y. Bengio et al., “Greedy layer-wise training of deep networks,” Adv.
Neural Inf. Process. Syst., vol. 19, pp. 153–160, 2007.
[44] G. E. Hinton et al., “A fast learning algorithm for deep belief nets,”
Neural Comput., vol. 18, no. 7, pp. 1527–1554, 2006.
[45] C. Poultney et al., “Efficient learning of sparse representations with
an energy-based model,” in Adv. Neural Inform. Process. Syst., 2006,
pp. 1137–1144.
[46] C. R. Jack et al., “The Alzheimer’s disease neuroimaging initiative
(ADNI): MRI methods,” J. Magn. Reson., vol. 27, no. 4, pp. 685–691,
2008.
[47] J. Mazziotta et al., “A probabilistic Atlas and reference system for
the human brain: International consortium for brain mapping (ICBM),”
Philosoph. Trans. Roy. Soc. London. Ser. B: Biol. Sci., vol. 356, no. 1412,
pp. 1293–1322, 2001.
[48] J. A. Schnabel et al., “A generic framework for non-rigid registration based on non-uniform multi-level free-form deformations,” in Proc.
Int. Conf. Med. Image Comput. Comput.-Assisted Intervention, 2001,
pp. 573–581.
[49] M. Jenkinson et al. “Improved optimization for the robust and accurate
linear registration and motion correction of brain images,” Neuroimage,
vol. 17, no. 2, pp. 825–841, 2002.
[50] R. A. Heckemann et al., “Automatic morphometry in Alzheimer’s
disease and mild cognitive impairment,” Neuroimage, vol. 56, no. 4,
pp. 2024–2037, 2011.
[51] S. Liu et al., “Generalized regional disorder-sensitive-weighting scheme
for 3D neuroimaging retrieval,” in Proc. Annu. Int. Conf. IEEE Eng. Med.
Biol., 2011, pp. 7009–7012.
[52] J. Ngiam et al., “On optimization methods for deep learning,” in Proc.
28th Int. Conf. Mach. Learn., 2011, pp. 265–272.
[53] W. Y. Zou et al., “Unsupervised learning of visual invariance with temporal coherence,” in Proc. NIPS Workshop Deep Learn. Unsupervised
Feature Learn., 2011.
[54] J. Ngiam et al., “Multimodal deep learning,” in Proc. 28th Int. Conf.
Mach. Learn., 2011, pp. 689–696.
[55] Y. Bengio, “Learning deep architectures for AI,” Found. Trends Mach.
Learn., vol. 2, no. 1, pp. 1–127, 2009.
[56] G. E. Hinton and R. R. Salakhutdinov, “Reducing the dimensionality of
data with neural networks,” Science, vol. 313, no. 5786, pp. 504–507,
2006.
[57] A. Fedorov et al., “3D slicer as an image computing platform for
the quantitative imaging network,” Magn. Reson. Imaging, vol. 30,
pp. 1323–1341, 2012.
[58] C. C. Chang and C. J. Lin, “LIBSVM: A library for support vector machines,” ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 27:1–27:27,
May 2011.
[59] J. Bergstra and Y. Bengio, “Random search for hyper-parameter optimization,” J. Mach. Learn. Res., vol. 13, pp. 281–305, 2012.
[60] Y. Bengio, Practical Recommendations for Gradient-Based Training of
Deep Architectures. New York, NY, USA: Springer, 2012, pp. 437–478.
[61] Y. Bengio and Y. LeCun, “Scaling learning algorithms towards AI,” LargeScale Kernel Mach., vol. 34, art. no. 5, 2007.
[62] Y. Bengio et al., “The curse of highly variable functions for local Kernel
machines,” in Proc. Adv. Neural Inf. Process. Syst., 2005, pp. 107–114.
[63] Y. Bengio and O. Delalleau, “On the expressive power of deep architectures,” in Proc. 12th Int. Conf. Algorithmic Learning Theory, 2011,
pp. 18–36.
[64] J. Weston et al., Deep Learning via Semi-Supervised Embedding. New
York, NY, USA: Springer, 2012, pp. 639–655.
[65] S. I. Gallant, Neural Network Learning and Expert Systems. Cambridge,
MA, USA: MIT Press, 1993.
Authors’ photographs and biographies not available at the time of publication.

