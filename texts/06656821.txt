756

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 3, MARCH 2014

Prostate MRI Segmentation Using Learned Semantic
Knowledge and Graph Cuts
Dwarikanath Mahapatra∗ and Joachim M. Buhmann

Abstract—We propose a fully automated method for prostate
segmentation using random forests (RFs) and graph cuts. A volume of interest (VOI) is automatically selected using supervoxel
segmentation, and its subsequent classification using image features and RF classifiers. The VOIs probability map is generated
using image and context features, and a second set of RF classifiers. The negative log-likelihood of the probability maps acts as
the penalty cost in a second-order Markov random field cost function. Semantic information from the second set of RF classifiers
is an important measure of each feature to the classification task,
which contributes to formulating the smoothness cost. The cost
function is optimized using graph cuts to get the final segmentation
of the prostate. With average dice metric (DM) > 0.91 (on the
training set) and DM > 0.81 (on the test set), our experimental
results show that inclusion of the context and semantic information
contributes to higher segmentation accuracy than other methods.
Index Terms—Graph cuts, Markov random field (MRI), prostate
segmentation, random forests (RFs), semantic information.

I. INTRODUCTION
CCORDING to the American Cancer society, prostate
cancer is the second leading cause of cancer death in
American men [1].
Accurate segmentation of the prostate gland gives the prostate
volume (PV) which is clinically important for a number of reasons: First, PV in conjunction with other parameters can help
predict the pathological stage of prostate cancer, offer insight
into prognosis of the disease, and predict treatment response.
PV also facilitates the calculation of prostate-specific antigen
levels, a widely-used biomarker for prostate cancer. Second, information on the size (or PV), shape, and location of the prostate
relative to adjacent organs is an essential part of treatment planning. The high-spatial resolution and soft-tissue contrast offered
by magnetic resonance imaging (MRI) makes it the most accurate method available for obtaining this kind of information.
Thus, recent years have seen increased clinical adoption of MRI
for treatment planning.
Manual segmentation of the prostate is extremely time consuming, and is prone to inter- and intraexpert variability. This

A

Manuscript received March 23, 2013; revised October 7, 2013; accepted
October 29, 2013. Date of publication November 6, 2013; date of current version
February 14, 2014. Asterisk indicates corresponding author.
*D. Mahapatra is with the Department of Computer Science, ETH Zurich
8092, Switzerland (e-mail: dwarikanath.mahapatra@inf.ethz.ch).
J. M. Buhmann is with the Department of Computer Science, ETH Zurich
8092, Switzerland (e-mail: jbuhmann@inf.ethz.ch).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2289306

necessitates the design of (semi)automated segmentation algorithms. A reliable algorithm needs to overcome challenges
like: 1) variability of prostate size and shape between subjects;
2) variable image appearance and intensity ranges from various
MR scanning protocols; and 3) lack of clear prostate boundaries
due to similar intensity profiles of surrounding tissues. In this
study, we propose a fully automatic method for prostate segmentation from MRI using machine-learning techniques and
graph-cut segmentation.
A. Related Work on MRI Prostate Segmentation
Many recent works have proposed automatic segmentation
algorithms for prostate MRI. Klein et al. [2] used multiatlas
matching and localized mutual information while Martin et al.
in [3] used an atlas of training images with a statistical shape
model for deformable segmentation. Pasquier et al. [4] adopted
active shape models (ASMs) that look for strong gradients to
identify the prostate boundary. Makni et al. [5] placed a manual
region of interest (ROI) and voxels within the ROI are clustered
to identify the prostate region. Toth and Madabhushi in [6] used
active appearance models (AAMs) with level sets to overcome
the difficulty of setting landmarks in ASMs. Li et al. in [7]
adopted a machine-learning approach using auto context and
level sets to segment the prostate from CT images.
The growing importance of prostate MRI segmentation has
led to the recent prostate segmentation challenge in MICCAI
2012. Since we use the same dataset, we review some related
works from the challenge. With the help of marginal space
learning [8], a training image set is compactly represented with
discriminative classifiers and used to align a statistical mesh
model onto the test image. The classifiers aggregate and choose
the best image features from a large feature pool. Gao et al.
in [9] applied multiatlas segmentation using local appearance
specific atlases which is robust to intersubject variation. The
selected atlases are nonrigidly aligned to the target image and
weighted using a patch-based local voxel selection strategy.
Ghose et al. [10] used appearance features from training images
to calculate voxel-wise probabilities. The probability map is
segmented using level sets to produce a binary segmentation
of the prostate. Other works on the same dataset use multiple
atlases [11], [12], ASMs [13], convex relaxation using star shape
priors [14], and AAMs [15], [16].
Graph-based techniques have also been used for segmenting
prostate and other organs [17]–[19]. Freedman and Zhang [20]
used zero level sets of training images as shape priors to segment the prostate and bladder from CT images. The method
in [21] combines graph cut and random forest (RF) classifiers for
prostate segmentation. Zouqi and Samarabandu in [22] used a

0018-9294 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

MAHAPATRA AND BUHMANN: PROSTATE MRI SEGMENTATION USING LEARNED SEMANTIC KNOWLEDGE AND GRAPH CUTS

757

B. RF Classifiers

domain-knowledge-based fuzzy inference system for the graphcut-based prostate segmentation.
B. Our Contribution
In this paper, we propose a method for fully automatic segmentation of the prostate from MR images. The problem has
two main challenges. First, automatic identification of a volume
of interest (VOI) containing the prostate and second, prostate
segmentation from the VOI. They are challenging due to similar
intensities of the surrounding tissues. Automatic VOI identification is achieved by segmenting the test volume into supervoxels,
and classifying each supervoxel for presence of prostate tissue.
Supervoxels containing parts of the prostate constitute the VOI.
Subsequently, RF classifiers are used to generate probability
maps for VOI voxels giving their likelihood of being prostate or
nonprostate. The maps are then used for graph-cut segmentation.
The primary contribution of this paper is the use of semantic information from trained RF classifiers to design a novel
smoothness cost for the graph-cut segmentation. None of the
previous works have exploited this aspect of RF classifiers. Semantic information quantifies the importance of different image
features in classification, which are used to weigh features in the
smoothness cost function. This leads to higher segmentation accuracy than conventional methods using image features. We also
develop a hierarchical framework using supervoxel segmentation and trained RF classifiers to automatically detect a VOI
containing the prostate. Our method is described in Section II
followed by a discussion on experimental results in Section III.
Section IV concludes our paper and outlines possible future
work.
II. METHODS
A. Method Overview
An overview of our method is given in Algorithm 1. A VOI
containing the prostate is automatically determined by supervoxel segmentation and their classification by RF classifiers.
Probability values of each voxel within the VOI are calculated
using a second set of RF classifiers. The intensity information is
not used directly as regions in the prostate’s neighborhood have
similar intensity. The negative log likelihood of the probability
map is used as the penalty cost in a second-order Markov random field (MRF) cost function. Semantic information is used
to design a novel smoothness cost. The final class labels are
obtained by graph-cut optimization of the cost function.

RF classifiers [23] are computationally efficient for large
training data, can handle multiclass classification, and the
learned knowledge extracted and interpreted to get a deeper
insight into the training procedure. RFs are used: 1) to extract
the semantic information after the training step in the form of
relative importance of different features to the classification task.
This is important in the formulation of segmentation smoothness cost. 2) RFs also allow for a probabilistic interpretation of
the classification which aids in designing an appropriate penalty
cost function for segmentation.
A RF is an ensemble of decision trees. Each tree is typically
trained with a different subset of the training set (“bagging”),
which improves the generalization ability of the classifier. Training amounts to identifying the set of tests in each decision tree
that best separates the data into different classes. A test sample
is pushed down different trees by applying the tests according to
the path from the root node to the leaf it traverses. When a leaf
node is reached, the tree casts a vote corresponding to the class
assigned to this node in the training stage. The final decision for
a test sample is obtained by selecting the class with the majority
of votes. The fraction of votes for a class also gives its probability value. Random classification and regressionforests are being
used increasingly by many medical applications like classification, segmentation, [24], [25], assessing bowel diseases [26],
detecting and segmenting Crohn’s disease [27]–[29], anatomy
localization [30], and abnormalities in mammograms [31].

C. Image Features
Feature extraction is required at two stages: supervoxel classification and probability map generation. In the first case, we
calculate features from every supervoxel, while in the second
instance we calculate features around a voxel’s 31 × 31 × 3
neighborhood. Since, for an untrained eye, it is difficult to visually determine the boundary of the prostate, we propose to
investigate features that are not discernible by the human eye
but may provide discriminating information for prostate segmentation. Psychophysical experiments have established that
the human visual system is sensitive only to image features of
the first and second order (mean and variance) [32]. Therefore,
in addition to mean and variance, we calculate the skewness
and kurtosis of intensity, texture, and two-dimensional (2-D)
curvature. For supervoxel classification, we use only the lowlevel image features (intensity, texture, and curvature), while
for voxel-wise probability map generation we use additional
high-level context features which are a combination of intensity, texture, and curvature values.
Let us denote each subvolume (supervoxel or voxel neighborhood) as Si , the intensity of its jth pixel as Si (j), the
mean intensity by Si , and the variance by σi2 . The skewness
(third-order statistic) of the intensity distribution is given by
⎤
n


3
1
1
Si (j) − Si ⎦ × 3
Sk = ⎣
N j =1
σi
⎡

(1)

758

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 3, MARCH 2014

a “X,” we extract a 10 × 10 × 10mm3 region and calculate the
mean intensity, texture, and curvature values. The texture values
were derived from the texture maps obtained at 90◦ orientation
and scale 1. The “X”s are located at distances of 10,18,30,45
mm from the center, and the angle between consecutive rays
is 45◦ . The values from the 32 regions are concatenated into a
96-dimensional feature vector. Thus for generating probability
maps of the VOI, the feature vector has 120 (96 + 24) values.
Fig. 1. (a) Illustration of sample locations for context information. Probability
maps for (b) prostate; (c) background. Higher values indicate greater likelihood
of belonging to that region. Red indicates maximum probability while blue
indicates zero probability.

where the term within the square brackets is the third-order moment. The skewness is a measure of symmetry of a distribution.
Kurtosis (the fourth-order statistic) is a measure of the “peakedness” of the probability distribution and along with skewness
describes its shape. It is given by
⎤
⎡
n


4
1
1
Si (j) − Si ⎦ × 4
(2)
Ku = ⎣
N j =1
σi
where the term within the square brackets is the fourth-order
moment.
Texture is modeled as patterns distinguished by a high concentration of localized spatial frequencies. Oriented Gabor filter
banks at {0◦ , 45◦ , 90◦ , 135◦ } are used to generate texture maps
for each slice. Gabor filters have optimal joint localization in the
spatial and frequency domains, and conforms to the receptive
field profiles of simple cortical cells [33]. They are also robust
to noise. The central frequency of the sinusoid was fixed at 16
and a single scale is used for texture computation.
Shape features in the VOI are characterized by the 2-D curvature of voxels. A detailed explanation for its calculation can
be found in [34]. We give a summary of the different steps. The
normal N(x, y) at image pixel (x, y) is given by
	 

1
fx
(3)
N̂(x, y) = 
1/2 f
2
2
y
fx + fy
where fx and fy are the image gradients along the x and y
directions. The curvature is defined as the divergence of the
normal, and is hence given by
Curv =

fxx fy2 + fy y fx2 − 2fxy fx fy

3/2
fx2 + fy2

(4)

where fxx , fy y indicate second derivatives. The final feature
vector consists of 24 values, i.e., 4 values each from one map of
intensity, four maps of texture, and one map of curvature.
Since the human anatomy is fixed, presence of one organ
leads to a strong cue about the presence of another organ in
medical images. Through appropriately designed features, we
aim to capture the contextual relationship between prostate and
other organs. Since contextual information depends on relative
orientation and distance, we sample regions at fixed positions
from a voxel. Fig. 1 shows an illustration of the sampling scheme
where the circle center is the voxel in question and the sampled
points are identified by a red “X.” At each point corresponding to

D. VOI Identification
VOI identification is an important part of our method because:
1) it reduces the total computation time since we need not classify each voxel; and 2) gives an initial selection of likely prostate
voxels which reduces false positives in subsequent analysis.
Intensity inhomogeneity correction was performed using the
nonparametric nonuniform intensity normalization (N3) method
of [35]. This method performs well without requiring a model
of the tissue classes present. The intensities were normalized
using the method in [36]. First a “standard” intensity histogram
is learnt from a subset of the training images. The training intensity histograms are roughly bimodal and parameters, such
as minimum and maximum percentile intensities (p1 , p2 ), and
the second mode of the histogram (μ) are determined. For a
given test image, the intensities are rescaled using the following
formula:
x − p1
(s2 − s1 )
(5)
x = s1 +
p2 − p1
where x is the new intensity obtained from the original intensity
value x; s1 and s2 are the minimum and maximum intensities
of the test image. This approach leads to good contrast of the
different images.
The normalized images are oversegmented into supervoxels using the simple linear iterative clustering supervoxel algorithm [37]. The desired number of supervoxels is specified,
which is the same as the number of initial supervoxel centers.
The centers are initially equally spaced, and moved to the lowest gradient position. The pixels are clustered based on intensity
similarity and spatial proximity to the nearest supervoxel centers. After every iteration, the cluster centers are updated based
on the voxels assigned to that cluster. The iterations continue
till superpixel centers do not change.
In the original algorithm, the intervals between supervoxels along each spatial dimension are equal such that the initial
supervoxels are cubes. This choice does not work well in our
application since the spatial extent in the z direction is short, has
low resolution, and the structure of organs also changes significantly along the z-axis. If a supervoxel has too many slices, it
may contain more than one organ, which complicates the further
analysis. For better spatial coherence, we restrict the number of
slices which supervoxels may cover. Instead of applying the
same step size along three spatial axes, the average number of
supervoxels on the xy-plane and along the z-axis is specified
separately so that the step sizes are calculated independently.
Supervoxel classification for VOI identification needs to be
fast and accurate. From the training images, we identify supervoxels that contain prostate and background voxels, and extract

MAHAPATRA AND BUHMANN: PROSTATE MRI SEGMENTATION USING LEARNED SEMANTIC KNOWLEDGE AND GRAPH CUTS

relevant features from them. For every supervoxel class, we train
RF classifiers with 50 trees. Each supervoxel of the test image is
labeled by the RF classifier as “prostate” or “background” with
“prostate” supervoxels making up the VOI. In Section III-B, we
discuss the removal of false positives.
E. Probability Maps and Graph-Cut Segmentation
Probability maps are generated for all VOI voxels using a
second set of RF classifiers. Intensity, texture, curvature, and
context features derived from these samples were used to train a
RF classifier (different from the one trained on supervoxel features). The features were extracted from a 31 × 31 × 3 neighborhood of each voxel. The training set varies with each round
of cross validation.
The trained classifier is used to generate probability maps for
every voxel within the identified VOI. Each voxel has 2 probability values corresponding to the prostate and background. The
probability maps serve as penalty costs in a second-order MRF
cost function. Fig. 1(b)–(c) shows the probability maps of the
prostate region for one slice of a patient.
The final segmentation is obtained by optimizing a secondorder MRF energy function which is written as
E(L) =


s∈P

D(Ls ) + λ



V (Ls , Lt )

(6)

(s,t)∈N

where P denotes the set of pixels, N is the set of neighboring
pixels for pixel s, Ls is the label of s, and L is the set of all labels.
The cost function is optimized using 2-D graph cuts [38]. λ is a
weight that determines the relative contribution of penalty cost
(D) and smoothness cost (V ). D(Ls ) is given by
D(Ls ) = − log (P r(Ls ) + )

(7)

where P r is the likelihood (from probability maps) previously
obtained using RF classifiers and  = 0.00001 is a very small
value to ensure that the cost is a real number. Higher the probability for a class, lower the corresponding data penalty for that
class.
V ensures a smooth solution by penalizing spatial discontinuities. The RF classifier returns a measure of the importance
of each dimension in the feature vector to the classification task.
In spite of the multiple-dimensional feature vector, the features
can be classified into three types—intensity, texture, and curvature. The context information is a combination of the three. By
aggregating the importance values of each feature category and
normalizing them, we obtain the relative importance of each
feature in the classification task. This provides the necessary
semantic information by quantifying the importance of each
feature in classifying a voxel into different categories. Let the
weight of the different features be wI (intensity), wT (texture),
and wC (curvature), where wI + wT + wC = 1. The smoothness cost V is given by

V (Ls , Lt ) =

wI VI + wT VT + wC VC ,
0

Ls =
 Lt
Ls = Lt .


(8)

759

TABLE I
EXAMPLE ILLUSTRATION FOR CALCULATING NORMALIZED IMPORTANCE
MEASURES USING SEMANTIC INFORMATION

where VI , VT , VC are the individual contributions to the smoothness by intensity, texture, and curvature. VI is defined as
VI (Ls , Lt ) = e−

( I s −I t ) 2
2σ 2

·

1
.
s − t

(9)

I is the intensity. VT and VC are similarly defined using
texture and curvature. Note that the weights (or importance
measures) depend upon the training set. Since we use different
volumes for training (as in a cross-validation setting), we obtain
different weight values. However, after training the weights take
the following values wI = 0.19 − 0.22, wT = 0.3 − 0.33, and
wC = 0.43 − 0.48. This indicates that the relative importance
of the different features is the same in all cases.
With the help of Table I, we illustrate how the weights are derived from semantic information. Table I shows the importance
measures of different features after training. For convenience,
we have grouped the measures under two categories—“lowlevel” and “context” features. Low-level features indicate intensity, texture, and curvature features, while context features
are a combination of the three values sampled at fixed points
from a voxel. Note that there are four columns for texture under
“low-level” features corresponding to the four orientations of
Gabor filters, while “context” has only one value for texture.
The importance measures are not normalized. First, we sum
up all the values under respective feature categories namely intensity, texture, and curvature. Therefore, the importance measure of intensity is 343 (Col 1 + Col 7). Similarly, the importance measure of texture (T ) is 488 (Col 2 + Col 3 + Col 4 +
Col 5 + Col 8) and for Curv is 701 (Col 6 + Col 9). The sum
of all the importance measures is 1532. Dividing the individual importance measures with the sum gives the final
normalized importance measures as follows: Int − 0.22(wI ),
T ex − 0.32(wT ), and Curv − 0.46(wC ). For the particular
round of cross validation, these are the values for the weights.
III. EXPERIMENTS AND RESULTS
The MICCAI 2012 prostate segmentation challenge
(http://promise12.grand-challenge.org/) has 50 training and 30
test datasets of transversal T2-weighted MR images. The
datasets are acquired under different clinical settings. They are
multicenter and multivendor, and have different acquisition protocols (e.g., differences in slice thickness, with/without endorectal coil). The volume dimensions and voxel resolutions are different for different images. Each slice of the different volumes
is of size 512 × 512 (voxel resolution of 0.4 × 0.4 × 3.3) or
320 × 320 (resolution 0.6 × 0.6 × 3.6). The set is selected such
that there is a spread in prostate sizes and appearance. Reference
segmentations are available for each dataset. We employ a tenfold cross validation (on the training data) where training (for
VOI and probability maps) is done on 45 datasets and tested on 5

760

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 3, MARCH 2014

TABLE II
CHANGE IN SEGMENTATION ACCURACY WITH DIFFERENT VALUES OF λ (6)

TABLE III
QUANTITATIVE MEASURES FOR SUPERVOXEL CLASSIFICATION UNDER
DIFFERENT FEATURE COMBINATIONS

datasets. We report average test results over all patients. Results
are also reported for the test data. Each patient was segmented
by our algorithm only once. N3 intensity nonuniformity correction was applied to reduce intensity inhomogeneity and the
image intensities are normalized using the method in [36]. Our
whole pipeline was implemented in MATLAB on a 2.66-GHz
quad core CPU running Windows 7; however, our code was not
optimized to take advantage of parallel processing. The quality
of our segmentation results with respect manual segmentations
was evaluated using two measures: 1) dice metric (DM) and 2)
Hausdorff distance (HD).
A. MRF Regularization Strength λ (6)
To choose the MRF regularization strength λ, we adopt the
following steps. We choose a separate group of seven patient
volumes, and perform segmentation using our method but with
λ taking different values from 10 to 0.001. The results are summarized in Table II. The maximum average DM was obtained
for λ = 0.02 and this parameter value was chosen for the subsequent parameter λ = 0.02. Note that these seven datasets were
not part of the test dataset used for evaluating our algorithm.
B. VOI Identification
Errors in VOI identification lead to inaccurate prostate segmentation. VOI detection is a classification problem where we
need to identify those supervoxels that contain prostate regions.
If a supervoxel has even one prostate voxel it is denoted as
prostate, while background supervoxels have all background
voxels. Table III summarizes the supervoxel classification performance for different feature combinations using five-fold
cross validation. Over the 50 patients, we have 478 prostate supervoxels and 443 background supervoxels. AllF eatures indicate the combination of intensity, texture, and curvature features.
Note that context features were not derived from supervoxels.
Sen is the sensitivity and is the percentage of correctly classified prostate supervoxels. We require a higher Sen even at the
expense of lower Spe (specificity—the percentage of correctly
classified background supervoxels).

Fig. 2. VOI detection. First column shows the supervoxels (green) and manually annoted diseased region (red). Second column shows the identified prostate
supervoxels (in yellow), and third column shows the final VOI after neighboring
supervoxels of column 2 are changed labels. Rows show results of consecutive
slices.

As expected, the accuracy for the individual features is lower
than their combinations. The combination of texture and curvature features produces results closest to AllF eat. However,
this does not indicate that intensity information is unimportant. Conducting a t-test on the values for T ex + Curv and
AllF eat gives p < 0.017 which indicates statistically different
results. Furthermore, we also conduct t-tests for features T ex
versus T ex − Int, and Curv versus Curv − Int. In all cases,
we find that p < 0.025, thus clearly showing that inclusion of intensity statistics improves classification accuracy without extra
computational cost.
In any classification scheme, it is difficult to get 100% classification/detection accuracy. Misclassification of prostate supervoxels occurs when the number of prostate voxels in a supervoxel is very low. Hence, the extracted features are more representative of the background. To overcome this shortcoming, we
adopt the following strategy. After the supervoxels have been
classified, we assign corresponding labels to the voxels within
and change the labels of all supervoxels adjoining the classified
“prostate” supervoxels (irrespective of their originally assigned
labels). This allows us to include some “prostate” supervoxels
that may have been missed in the initial classification. To reduce false positives, we reject any supervoxels that are outside
a 0.5 L × 0.5 L window (values are set by observation) centered
around each slice of the volume. L is in pixels and is the larger
image dimension (row or column).
Fig. 2 shows an example where this strategy is particularly
effective. The first column shows consecutive slices from a volume where the prostate segmentation is shown in red and the
supervoxels are shown in green. The second column shows

MAHAPATRA AND BUHMANN: PROSTATE MRI SEGMENTATION USING LEARNED SEMANTIC KNOWLEDGE AND GRAPH CUTS

TABLE IV
QUANTITATIVE MEASURES FOR DIFFERENT SUPERVOXEL SIZES

the detected ROI in each slice (shown by the yellow supervoxels) overlaid on the manually annotated prostate (in red).
Small prostate regions are missed by the supervoxel classification scheme. However, when we include all the neighboring
supervoxels, the ROI encompasses all possible prostate voxels
(third column).
Table III (AllF eat) gives quantitative measures for supervoxel classification before changing the labels of neighboring
supervoxels. With the change of labels of neighboring supervoxels, the following values are obtained: Sen = 98.5%, Spe =
82.4%, and Acc = 92.5%. The labeling of neighboring supervoxels increases “false positives” (background supervoxels labeled prostate). Correspondingly, it also reduces the classification accuracy of background supervoxels (or “specificity”).
However, it also reduces the false negatives (prostate supervoxels labeled background) and hence the “sensitivity” (prostate
labeled as prostate) also increases. The overall accuracy increases due to a higher improvement in Sen than decrease in
Spe.
C. Effect of Supervoxel Size
Smaller supervoxels are more homogeneous and the extracted
features are representative of a single class. However, they may
not always provide sufficient number of voxels to calculate stable features. Larger supervoxels contain more voxels to calculate
features but may contain voxels from more than one class. Consequently, the extracted features may not be representative of
one class. Table IV summarizes the performance for different supervoxel sizes in terms of Sen, Spe, and Acc. Our experiments
show that a good tradeoff between accuracy and homogeneous
samples is achieved when the the number of voxels in a supervoxel is in the range 1800–2100. Depending upon the volume
dimensions, we set supervoxel parameters to get an appropriate
number of voxels in a supervoxel.
D. Segmentation Results on Training Data
We present segmentation results for RFSem (our proposed
method using RF classifiers and semantic information) and the
following methods on the same dataset: [13]—use ASM; [8]—
marginal space learning; [10]—RF classifiers and level sets;
[15]—3-D AAM; [16]—AAM; [14]—convex optimization; and
multiatlas segmentation methods of [9], [12], and [11].
Table V summarizes the performance of all the previous methods in terms of DM, HD, and computation time. DM is available
for all methods, while HD and T ime are not available for all

761

TABLE V
QUANTITATIVE MEASURES FOR SEGMENTATION ACCURACY ON TRAINING DATA

TABLE VI
QUANTITATIVE MEASURES FOR TRAINING DATA

methods. RFSem gives higher DM than all other methods and
lower HD values for all methods for which HD values are available, except [16]. It is to be kept in mind that the reported
computation time for some methods is based on GPU implementation and/or multiple cores. Since we did not have access
to the segmentations of the individual methods, we are unable
to perform a detailed statistical comparison. The consistently
better performance of RFSem can be attributed to two factors:
1) use of machine-learning techniques to identify most discriminant features; and 2) incorporating semantic information into
the smoothness cost.
The importance of semantic information is further examined
by different experiments, and subsequent statistical analysis.
Table VI summarizes the performance of our method under different conditions: 1) RFn C –RFSem without context information
to train the RF classifier; 2) RFn V I –RFSem without intensity information in V , i.e., wI = 0 and wT , wC are normalized by the
sum of their values (wT + wC = 1); 3) RFn V T –RFSem without texture information in V , i.e., wT = 0 and wI + wC = 1;
4) RFn V C –RFSem without using curvature information in V ,
i.e., wC = 0 and wI + wT = 1; 5) RFn V –RFSem without semantic information in V , i.e., wI = wT = wC = 0.33—all the
features are weighted equally in the smoothness cost without
considering their importance in classification. The results are
tabulated in consecutive columns in Table VI.
Context information has the greatest contribution to segmentation accuracy, and a t-test between the values of RFSem and
RFn C gives pRF S e m –RFn C < 0.0002. From intensity, texture,
and curvature, curvature is found to be most important for our
method (Table VI RFn V C ), followed by texture and intensity.
Although intensity has a lower impact, it is still significant
(p < 0.008) and cannot be neglected in the final pipeline. Exclusion of semantic information gives the closest segmentation
accuracy to RFSem . However, it is still statistically significant
with p < 0.01. Therefore, we continue to use semantic information to weigh different features in the smoothness cost. With
these set of results, we infer that although different features and
semantic information have varying degrees of influence over the

762

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 3, MARCH 2014

Fig. 4. Inaccurate segmentation results for test data Patient 13 using our
proposed method. Since manual segmentations are not available to us, we cannot
show the reference segmentations.
TABLE VIII
QUANTITATIVE MEASURES FOR SEGMENTATION ACCURACY OF RF S e m UNDER
DIFFERENT DEGREES OF NOISE

Fig. 3. Segmentation results for Patient 38: (a) RF S e m; (b) RF n C ; (c) RF n V .
The manual segmentations are in red, while the algorithm segmentations are in
green.
TABLE VII
QUANTITATIVE MEASURES FROM TEST DATASET

classification results, each of them is significant enough not to
be excluded from the final framework.
Fig. 3 shows segmentation results for Patient 38 obtained
by RFSem , RFn C , and RFn V . The results reflect the values in
Table VI with RFn C giving the worst results. The low DM
values for RFn C highlights the fact that context plays a very
important role in our method. Although all the low-level features are used, without context information it is very difficult to discriminate between the prostate and background. For
RFn V I , RFn V T , RFn V C although one low-level feature is excluded, context information provides greater discrimination.

rules of the PROMISE 2012 challenge (as each participant can
submit test set segmentations only once). Sampling at distances
in millimeters is better than pixel distances as it covers equal
physical areas of the organ and not just the image area. Other
possible factors are noise levels that could not be handled by our
classifier, and complex structures in the image which the classifier did not encounter previously. The average DM value for the
test data as obtained by our method is 81.2 ± 4.5 which is quite
low compared to the training set. A closer analysis reveals that
6 out of the 30 test datasets had DM values less than 75. These
volumes contributed toward a lower average DM. However, if
we exclude these datasets, the average DM value of the remaining 24 datasets is 84.3 ± 3.6. Although it is still lower than the
results of the training data, it is nevertheless indicative of good
performance.
Of the remaining 24 datasets, 6 had 89 ≤ DM < 91 (maximum DM = 90.8), 7 had 85 ≤ DM < 89, 8 had 80 ≤ DM <
85, and the remaining 3 had 75 ≤ DM < 80. This breakdown
suggests that our classifiers are fairly accurate in learning the
appropriate information from the training data. Section III-F
discusses our algorithm’s robustness to different levels of noise.
Fig. 4 shows segmentation output using RFSem for test data patient 13 for which DM = 73. The images show noise and indistinct prostrate boundaries that affects our method’s performance.
Since we did not have access to the manual segmentations of
the test data, we are unable to provide further analysis.
F. Robustness to Noise

E. Segmentation Results on Test Data
The classifiers from the training data are directly applied to
the test data. Table VII summarizes the performance of all the
methods (from Table V) in terms of DM and HD. Many of the
other methods perform better than ours on the test data. With the
exception of [8] and [16], our performance is close to most other
methods which is encouraging. The poor performance may be
attributed to a design flaw in the template for context information. Although the modified template improved results on the
training set, we cannot check our results on the test set under the

Zero mean Gaussian noise of different variance (σ) was added
to the original images after intensity normalization, and the
prostate regions were segmented using the previously described
steps of VOI detection, probability map generation, and segmentation. The trained classifiers were the same as used before,
i.e., new classifiers were not trained on images with simulated
noise. The segmentation performance of RFSem under different
noise levels is summarized in Table VIII.
A modified context feature template significantly improves
our method’s robustness to noise. Sampling at fixed distances

MAHAPATRA AND BUHMANN: PROSTATE MRI SEGMENTATION USING LEARNED SEMANTIC KNOWLEDGE AND GRAPH CUTS

TABLE IX
EFFECT OF NUMBER OF TREES IN RF CLASSIFIERS (N T ) ON SEGMENTATION
ACCURACY AND TRAINING TIME

in millimeters instead of pixels is very helpful in capturing
contextual information from surrounding organs in the actual
physical space. Since the image intensities were normalized to
[0, 1], the variance of added noise was varied from 0.001 − 0.3.
The resulting segmentation accuracy was above 87 for σ < 0.01.
Although this is significantly less than the values in Table V, it
is still high. With increase in σ, DM decreases to 80 for σ = 0.1.
Considering that the maximum intensity is 1, σ = 0.1 indicates
good robustness to added noise.
G. Computational Cost
Our method basically consists of the following steps: supervoxel segmentation and classification to get the VOI, analyzing
every voxel within the VOI to identify prostate voxels by generating probability maps, and graph-cut segmentation. Supervoxel segmentation of a single volume takes about 30 s. Subsequent classification of VOI identification requires between 6
and 11 min. Probability map generation is more time consuming as features have to be extracted for each voxel. Depending
upon the size of the VOI, a further 11–16 m are required to get
the probability maps. Thus, on an average the entire method
from start to finish takes 20–25 min. In future, we aim to explore other possibilities of reducing analysis time through more
efficient feature extraction or deploying parallel processing.
H. Influence of Number of Trees
We examine the effect of varying number of trees (NT ) in the
second RF classifier (used for generating probability maps) on
overall performance. The results for RFSem are summarized in
Table IX in terms of DM values. When NT < 10, DM< 0.71.
With increasing NT , DM increases along with the time taken for
training. Table IX shows the training time (TTr ) for different NT
as a multiple of the training time for NT = 50. For NT > 50,
there is no significant increase in DM (p > 0.1) but the training
time increases significantly. The best tradeoff between NT and
DM is achieved for 50 trees and is the reason we have 50 trees
for our RF ensemble.

763

from the trained RF classifiers in prostate segmentation. RFs
measure the importance of each feature in the classification task
(also known as importance measures) which are used to design a
novel smoothness cost of a second-order MRF energy function.
The smoothness cost combines the effect of intensity, texture,
and curvature. Our experiments show that context information
has the greatest influence on the final classification and subsequent segmentation. Semantic information of the importance of
different features indicates that curvature is the most important
feature followed by texture and intensity. This is also reflected
in the supervoxel classification accuracy in Table III. Excluding
semantic information implies equal weighting of each feature in
the smoothness cost, which lowers the segmentation accuracy.
This degradation of performance is less than when excluding
context or curvature information, but is significant as is evident
from the results of t-tests.
Context information proves quite robust to added noise levels
because of the sampling scheme. Sampling at fixed distances
(in millimeters) allows us to capture the contextual information
for every pixel. This arrangement improves our method’s accuracy for the training dataset under different conditions of added
noise. In a separate set of experiments, we observe inferior performance if the sampling unit is pixels (instead of millimeters)
as the actual physical distance varies for different images. These
features do not capture the desired information for the robust
performance.
An unexpected outcome is the low segmentation accuracy on
the test dataset. Compared to other methods, our results on the
test set are very different from results on the training set. Possible explanations could be the presence of different structural
information that is absent in the training data, or noise levels
which our classifier cannot handle. However, it is interesting
to note that other methods achieve consistent performance over
the training and test data. The method in [10] also uses RF
classifiers and obtains similar performance for test and training
data although they are lower than that obtained by our method.
We explored different possibilities for the significant disparity
for our method’s performance on test and training data. One
possible factor could be the template for the context information. Since we sampled points at fixed pixel distances (instead
of millimeters), we obtained a lower segmentation accuracy on
the test set. However, a modified template is expected to improve segmentation accuracy as was the case for training data.
Since the rules of the segmentation challenge allow only for a
single submission, we are unable to test the performance of our
modified method on the test data.

IV. DISCUSSION AND CONCLUSION
We have presented a method for segmenting the human
prostate gland from MR images from the PROMISE 2012 segmentation challenge. Our method has two stages: 1) VOI detection through supervoxel segmentation and classification; and
2) analyzing each VOI voxel for prostate segmentation. Context
information is designed as a combination of intensity, texture,
and curvature features using a fixed sampling template. An important novelty of our method is the use of semantic information
(the contribution of individual features to the classification task)

REFERENCES
[1] “Cancer society atlanta,” (2011). [Online]. Avialable: http://www.cancer.
org
[2] S. Klein, U. van der Heide, I. Lipps, M. Vulpen, M. Staring, and J. Pluim,
“Automatic segmentation of the prostate in 3-D MR images by atlas
matching using localized mutual information,” Med. Phys., vol. 35, no. 4,
pp. 1407–1417, 2008.
[3] S. Martin, V. Daanen, and J. Troccaz, “Automated segmentation of the
prostate 3-D MR images using a probabilistic atlas and a spatially constrained deformable model,” Med. Phys., vol. 37, no. 4, pp. 1579–1590,
2010.

764

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 3, MARCH 2014

[4] D. Pasquier, T. Lacornerie, M. Vermandel, J. Rousseau, E. lartigau, and
N. Betrouni, “Automatic segmentation of pelvic structures from magnetic
resonance images for prostate cancer radiotherapy,” Int. J. Radiat. Oncol.
Biol. Phys., vol. 68, no. 2, pp. 592–600, 2007.
[5] N. Makni, P. Puech, R. Lopes, and A. Dewalle, “Combining a deformable
model and a probabilistic framework for an automatic 3-D segmentation
of prostate on MRI,” Int. J. Comput. Assisted. Radiol. Surg., vol. 4, no. 2,
pp. 181–188, 2009.
[6] R. Toth and A. Madabhushi, “Multifeature landmark-free active appearance models: Application to prostate MRI segmentation,” IEEE Trans.
Med. Imag, vol. 31, no. 8, pp. 1638–1650, Aug. 2012.
[7] W. Li, S. Liao, Q. Feng, W. Chen, and D. Shen, “Learning image context
for segmentation of prostate in CT-guided radiotherapy,” in Proc. Med.
Image Comput. Comput.-Assisted Intervention Conf., 2011, pp. 570–578.
[8] N. Birkbeck, J. Zhang, and S. Zhou, “Region-specific hierarchical segmentation of MR prostate using discriminative learning,” in Proc. Med.
Image Comput. Comput.-Assisted Intervention Conf. Prostate Segment.
Challenge, 2012, pp. 4–11.
[9] Q. Gao, D. Rueckert, and P. Edwards, “An automatic multi-atlas based
prostate segmentation using local appearance-specific atlases and patchbased voxel weighting,” in Proc. Med. Image Comput. Comput.-Assisted
Intervention Conf. Prostate Segment. Challenge, 2012, pp. 12–19.
[10] S. Ghose, J. Mitra, A. Oliver, R. Marti, X. Llado, J. Freixenet, J. Vilanova,
D. Sidibe, and F. Meriaudeau, “A random forest based classification approach to prostate segmentation in MRI,” in Proc. Med. Image Comput.
Comput.-Assisted Intervention Conf. Prostate Segment. Challenge, 2012,
pp. 20–27.
[11] G. Litjens, N. Karssemeijer, and H. Huisman, “A multi-atlas approach
for prostate segmentation in MR images,” in Proc. Med. Image Comput.
Comput.-Assisted Intervention Conf. Prostate Segment. Challenge, 2012,
pp. 36–43.
[12] Y. Ou, J. Doshi, G. Erus, and C. Davatzikos, “Multi-atlas segmentation
of the prostate: A zooming process with robust registration and atlas
selection,” in Proc. Med. Image Comput. Comput.-Assisted Intervention
Conf. Prostate Segment. Challenge, 2012, pp. 60–66.
[13] M. Kirschner, F. Jung, and S. Wesarg, “Automatic prostate segmentation
in MR images with a probabilistic active shape model,” in Proc. Med.
Image Comput. Comput.-Assisted Intervention Conf. Prostate Segment.
Challenge, 2012, pp. 28–35.
[14] J. Yuan, W. Qiu, E. Ukwatta, M. Rajchl, Y. Sun, and A. Fenster, “An
efficient convex optimization approach to 3-D prostate MRI segmentation
with generic star shape prior,” in Proc. Med. Image Comput. Comput.Assisted Intervention Conf. Prostate Segment. Challenge, 2012, pp. 82–
89.
[15] B. Maan and F. van der Heijden, “Prostate mr image segmentation using
3-D active appearance models,” in Proc. Med. Image Comput. Comput.Assisted Intervention Conf. Prostate Segment. Challenge, 2012, pp. 44–51.
[16] G. Vincent, G. Guillard, and M. Bowes, “Fully automatic segmentation of
the prostate using active appearance models,” in Proc. Med. Image Comput. Comput.-Assisted Intervention Conf. Prostate Segment. Challenge,
2012, pp. 75–81.
[17] D. Mahapatra and Y. Sun, “MRF based intensity invariant elastic registration of cardiac perfusion images using saliency information,” IEEE Trans.
Biomed. Eng., vol. 58, no. 4, pp. 991–1000, Apr. 2011.
[18] D. Mahapatra and Y. Sun, “Orientation histograms as shape priors for left
ventricle segmentation using graph cuts,” in Proc. Med. Image Comput.
Comput.-Assisted Intervention Conf., 2011, pp. 420–427.
[19] D. Mahapatra and Y. Sun, “Integrating segmentation information for improved MRF-based elastic image registration,” IEEE Trans. Imag. Process., vol. 21, no. 1, pp. 170–183, Jan. 2012.
[20] D. Freedman and T. Zhang, “Interactive graph cut based segmentation
with shape priors,” in Proc. Comput. Vis. Pattern Recog. Conf., 2005,
pp. 755–762.

[21] E. Moschidis and J. Graham, “Automatic differential segmentation of the
prostate in 3-D MRI using random forest classification and graph cuts
optimization,” in Proc. IEEE Int. Symp. Biomed. Imag., 2012, pp. 1727–
1730.
[22] M. Zouqi and J. Samarabandu, “Prostate segmentation from 2-D ultrasound images using graph cuts and domain knowledge,” in Proc. Comput.
Robot Vis. Conf., 2008, pp. 359–362.
[23] L. Breiman, “Random forests,” Mach. Learn., vol. 45, no. 1, pp. 5–32,
2001.
[24] A. Montillo, J. Shotton, J. Winn, J. Iglesias, D. Metaxas, and A. Criminisi,
“Entangled decision forests and their application for semantic segmentation of ct images,” in Med. Image Comput. Comput.-Assisted Intervention
Conf., 2011, pp. 184–196.
[25] D. Mahapatra, P. J. Schüffler, J. Tielbeek, J. M. Buhmann, and F. M. Vos.,
“A supervised learning based approach to detect Crohn’s disease in abdominal MR volumes,” in Proc. Med. Image Comput. Comput.-Assisted
Intervention Conf. Worksh. Comput. Clin. Appl. Abdominal Imag., 2012,
pp. 97–106.
[26] F. M. Vos, J. Tielbeek, R. Naziroglu, Z. Li, P. Schüffler, D. Mahapatra,
A. Wiebel, C. Lavini, J. Buhmann, H. Hege, J. Stoker, and L. van Vliet,
“Computational modeling for assessment of IBD: To be or not to be?” in
Proc. IEEE Eng. Med. Biol. Soc., 2012, pp. 3974–3977.
[27] D. Mahapatra, P. J. Schüffler, J. Tielbeek, J. Buhmann, and F. M. Vos.,
“Localizing and segmenting crohn’s disease affected regions in abdominal MRI using novel context features,” in Proc. SPIE 8669,
Med. Imaging 2013; Image Process., 86693K (Mar. 13, 2013).
DOI:10.1117/12.2006698.
[28] D. Mahapatra, P. Schüffler, J. Tielbeek, J. Makanyanga, J. Stoker,
S. Taylor, F. Vos, and J. Buhmann, “Automatic detection and segmentation of Crohn’s disease tissues from abdominal MRI,” IEEE Trans. Med.
Imag., [Online]: DOI: 10.1109/TMI.2013.2282124.
[29] D. Mahapatra, P. Schüffler, J. Tielbeek, F. Vos, and J. B. , “Crohn’s disease
tissue segmentation from abdominal MRI using semantic information and
graph cuts,” in Proc. IEEE Int. Symp. Biomed. Imag., 2013, pp. 358–361.
[30] A. Criminisi, D. Robertson, E. Konukgolu, J. Shotton, S. Pathak, S. White,
and K. Siddiqui, “Regression forests for efficient for efficient anatomy
detection and localization in computed tomography scans,” Med. Image
Anal., vol. 17, no. 8, pp. 1293–1303, Dec. 2013.
[31] M. Berks, Z. Chen, S. Astley, and C. Taylor, “Detecting and classifying
linear structures in mammograms using random forests,” in Inf. Process.
Med. Imag., 2011, pp. 510–524.
[32] B. Julesz, E. Gilbert, L. Shepp, and H. Frisch, “Inability of humans to
discriminate between visual textures that agree in second-order statisticsrevisited,” Perception, vol. 2, no. 4, pp. 391–405, 1973.
[33] R. L. D. Valois, D. G. Albrecht, and L. G. Thorell, “Spatial-frequency
selectivity of cells in macaque visual cortex,” Vis. Res., vol. 22, no. 5,
pp. 545–559, 1982.
[34] (2009). [Online]. Available: http://www.cs.ucl.ac.uk/staff/S.Arridge/
teaching/ndsp/
[35] J. Sled, A. Zijdenbos, and A. Evans, “A nonparametric method for automatic correction of intensity nonuniformity in MRI data,” IEEE Trans.
Med. Imag., vol. 17, no. 1, pp. 87–97, Feb. 1998.
[36] L. Nyúl and J. Udupa, “On standardizing the MR image intensity scale,”
Magn. Reson. Med., vol. 42, no. 6, pp. 1072–1081, 1999.
[37] R. Achanta, A. Shaji, K. Smith, A. Lucchi, P. Fua, and S. Süsstrunk,
“Slic superpixels compared to state-of-the-art superpixel methods,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 34, no. 11, pp. 2274–2282, Nov.
2012.
[38] Y. Boykov and O. Veksler, “Fast approximate energy minimization via
graph cuts,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 23, no. 11,
pp. 1222–1239, Nov. 2001.
Authors’ photographs and biographies not available at the time of publication.

