316

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Software-Based Real-Time Acquisition and
Processing of PET Detector Raw Data
Benjamin Goldschmidt∗ , David Schug, Christoph W. Lerche, André Salomon, Pierre Gebhardt, Bjoern Weissler,
Jakob Wehner, Peter M. Dueppenbecker, Fabian Kiessling, and Volkmar Schulz

Abstract—In modern positron emission tomography (PET)
readout architectures, the position and energy estimation of scintillation events (singles) and the detection of coincident events
(coincidences) are typically carried out on highly integrated,
programmable printed circuit boards. The implementation of
advanced singles and coincidence processing (SCP) algorithms
for these architectures is often limited by the strict constraints
of hardware-based data processing. In this paper, we present
a software-based data acquisition and processing architecture
(DAPA) that offers a high degree of flexibility for advanced SCP
algorithms through relaxed real-time constraints and an easily extendible data processing framework. The DAPA is designed to
acquire detector raw data from independent (but synchronized)
detector modules and process the data for singles and coincidences in real-time using a center-of-gravity (COG)-based, a leastsquares (LS)-based, or a maximum-likelihood (ML)-based crystal
position and energy estimation approach (CPEEA). To test the
DAPA, we adapted it to a preclinical PET detector that outputs
detector raw data from 60 independent digital silicon photomultiplier (dSiPM)-based detector stacks and evaluated it with a [1 8 F]fluorodeoxyglucose-filled hot-rod phantom. The DAPA is highly
reliable with less than 0.1% of all detector raw data lost or corrupted. For high validation thresholds (37.1 ± 12.8 photons per
pixel) of the dSiPM detector tiles, the DAPA is real time capable
up to 55 MBq for the COG-based CPEEA, up to 31 MBq for the
LS-based CPEEA, and up to 28 MBq for the ML-based CPEEA.
Compared to the COG-based CPEEA, the rods in the image reconstruction of the hot-rod phantom are only slightly better separable
and less blurred for the LS- and ML-based CPEEA. While the coincidence time resolution (∼550 ps) and energy resolution (∼12.3%)

Manuscript received September 27, 2014; revised January 27, 2015, April
3, 2015, May 6, 2015, and June 25, 2015; accepted July 9, 2015. Date of
publication July 14, 2015; date of current version January 16, 2016. This work
was supported in parts by the EU FP7 project SUBLIMA under Grant N°241711,
the Centre of Excellence in Medical Engineering funded by the Wellcome Trust
and EPSRC under Grant WT 088641/Z/09/Z, and the ForSaTum project which
is cofunded by the European Union (European Regional Development Fund—
Investing in your future) and the German federal state North Rhine-Westphalia
(NRW). Asterisk indicates corresponding author.
* B. Goldschmidt is with the Department of Physics of Molecular Imaging Systems, RWTH Aachen University, 52062 Aachen, Germany (e-mail:
goldschmidt@pmi.rwth-aachen.de).
D. Schug, J. Wehner, and P. M. Dueppenbecker are with the Department of
Physics of Molecular Imaging Systems.
C. W. Lerche is with the Forschungszentrum Juelich GmbH, Institute of
Neuroscience and Medicine (INM-4).
A. Salomon is with the Department for Oncology Solutions, Philips Research.
P. Gebhardt is with the Division of Imaging Sciences and Biomedical Engineering at Kings College London.
B. Weissler is with the Clinical Application Research Department, Philips
Research.
F. Kiessling is with the Department of Experimental Molecular Imaging.
V. Schulz is with the Department of Physics of Molecular Imaging Systems,
and with the Clinical Application Research Department, Philips Research.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2015.2456640

are comparable for all three CPEEA, the system sensitivity is up
to 2.5× higher for the LS- and ML-based CPEEA.
Index Terms—Data acquisition, parallel processing, positron
emission tomography (PET), real time, singles and coincidence
processing.

I. INTRODUCTION
ODERN readout architectures for positron emission tomography (PET) detectors are typically custom-built
hardware solutions based on field programmable gate arrays
(FPGAs) or digital signal processors (DSPs) [1]–[11]. These
systems either directly output list-mode coincidences data to
an image reconstruction server, or in some cases, singles data
[12]–[17], which then requires an offline coincidence processing
before an image can be reconstructed.
Although current FPGA generations allow the realization
of complex singles-position and -energy estimation schemes
[7], [18]–[22], their implementation is often limited by the
strict constraints of hardware-based data processing [23]. Here,
software-based data processing would allow for softer real-time
constraints through large and easily extendible random access
memory (RAM) and would generally offer a broader toolset for
real-time data processing.
While transferring the entire detector raw data to a computing server for software-based data processing is currently not
viable for large clinical PET scanners and/or high count rate
measurements due to the very high data rates in these scenarios,
for preclinical PET scanners, such an approach can be considered feasible for most PET applications especially when photodetectors that require continuous readout like digital silicon
photomultipliers (dSiPMs) [24], [25] are used. Here, a softwarebased approach would reduce the design constraints for the (e.g.,
FPGA-based) readout modules: instead of designing the readout
modules and firmware to be capable of real-time processing, it
would be sufficient for the readout modules to output the raw
data to a processing server.
As it is usually much simpler to port a data processing
framework developed in a high-level programming language
than a custom-developed firmware (with all its timing and
placement requirements/restrictions), existing software-based
solutions could be reused more easily. Furthermore, with a
software-based data processing approach, the data processing
capabilities of the readout architecture would only be limited by
the interconnect speed, as increases in computing performance
could be incorporated easier.
The flexibility gained through relaxed real-time constraints
could, e.g., be used in simultaneous PET/magnet resonance

M

0018-9294 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

imaging (MRI) measurements to dynamically incorporate MRI
trigger information in the PET data processing to acquire
cardiac- and respiratory-gated PET/MRI motion-capturing images [26].
In a previous publication [27], we showed the feasibility of
processing detector raw data from our first generation preclinical
PET/MRI insert [28] in software in real time using previously
acquired detector raw data. Building on our findings, in this
paper, we propose a fully integrated, software-based data acquisition and processing architecture (DAPA). The DAPA receives
detector raw data and either calculates singles and coincidences
in real time or stores the data on hard disk drives (HDDs) for
offline processing.
We introduce a novel fixed mapping scheme that makes it
possible to run over 50 data acquisition and processing threads
in parallel in real-time without the need for explicit synchronization. Furthermore, we explain how work can be distributed
fairly among worker threads and how dynamic process management can be used to reconfigure the DAPA on the fly to calculate
singles using a center-of-gravity (COG)-based, a least-squares
(LS)-based, or a maximum-likelihood (ML)-based crystal position and energy estimation approach (CPEEA). Additionally,
we introduce measures to minimize data loss and corruption
and detail how we had to improve and adapt the data processing to handle the increased number of detector modules and the
resulting significantly increased input data rates, the additional
two detector rings, and the integration of the newly designed
dSiPM-based detector modules.
To evaluate the DAPA, we fitted it to our second generation
preclinical PET/MRI insert [26], [29] and performed a series
of performance measurements with a [18 F]-fluorodeoxyglucose
(FDG)-filled hot-rod phantom and sodium (22 Na) point-sources
in the isocentre. Here, next to the real-time capabilities of the
DAPA, we also investigated the PET performance during realtime processing to evaluate possible influences of the DAPA.
II. HYPERION IID PET DETECTOR
The Hyperion IID preclinical PET/MRI detector insert consists of ten circularly arranged singles detection modules
(SDMs) [30], [31] resulting in a cylindrical field-of-view (FOV)
of 96.6 mm in length and 160 mm in diameter. The SDMs are
connected via User Datagram Protocol (UDP) over an optical
gigabit Ethernet (GbE) link to a data acquisition and processing
server (DAPS) and feature six digital silicon photomultiplier
(dSiPM)-based detector stacks [32].
The detector stacks are arranged in three adjacent rings of 20
stacks and consist of an array of 30×30 lutetium–yttrium oxyorthosilicate (LYSO) 0.932 ×12 mm3 crystals that is mounted
on top of a 2.0-mm thick lightguide that in turn is attached to a
32.6 mm ×32.6 mm dSiPM-based detector tile. The detector tile
features 4×4 detector dies where each die holds 2×2 pixels that
consist of 3200 digital single photon avalanche diodes (SPADs)
each [24], [25].
To synchronize the PET data acquisition, the SDMs are connected individually to a synchronization module (backbone) that
sends synchronization pulses to the SDMs at the start of a mea-

317

Fig. 1. Communication architecture of the Hyperion IID detector. The ten
SDMs and the backbone are connected via separate GbE links to the DAPS.
Each SDM houses six PET detector stacks and is connected to the backbone
for synchronization. The DAPS acquires detector raw data from the SDMs and
routes status and control information between the detector and the control PC
(adapted from [27], [33]).

surement. The backbone is also connected to the DAPS over an
individual optical GbE link (see Fig. 1).
The detector insert and the DAPS are controlled by a separate
control personal computer (PC) that is connected to the DAPS
via transmission control protocol (TCP) over a GbE link and
relays control information to the detector insert via the DAPS.
III. DATA ACQUISITION AND PROCESSING ARCHITECTURE
The DAPA is fully software-based and built on dynamic process management of the message passing interface (MPI) standard [34]. The main component is the coordination and monitoring module that acts as daemon on the DAPS, and which is
controlled by a separate control PC. Depending on the configuration, different data acquisition and processing modules can be
executed: in addition to storing of the entire detector raw data
stream on hard disk drives (HDDs), data can also be processed
for singles and coincidences in real time using different crystal
position and energy estimation approaches (CPEEA).
To exchange data between the software modules, detector
raw data and status and control information are separated: Detector raw data are exchanged via shared memory, while status
and control information (including statistics) are exchanged via
MPI. Therefore, a fixed amount of shared memory (typically
120 GB) is allocated by the coordination and monitoring module
before a measurement. The shared memory acts as ring buffer
that is continuously filled by the data acquisition modules and
emptied by the data processing modules.
In the following paragraphs, the DAPA and how it was specifically fitted to the Hyperion IID detector (see Fig. 2) are explained in detail.
The communication between the SDMs of the Hyperion IID
detector and the data acquisition modules on the DAPS is based
on so-called HyperDAC messages (see Fig. 3). Here, each data
acquisition module controls the UDP-based GbE connection
to one SDM. Incoming HyperDAC messages are checked for
preamble, header cyclic redundancy check (CRC), sequence

318

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Fig. 4. Illustration of the fixed mapping scheme for detector raw data for one
stack. The ring buffer for each stack is separated into W work packets and a
header layer is used to store and access detector raw data messages of different
size continuously.

The exact position Ms,w for each detector raw data message
p can then be calculated using its frame counter tp
Ms,w = (tp mod (W · F )) /F  .

Fig. 2. The DAPA fitted to the Hyperion IID detector: Controlled by the
Hyperion control software on the control PC, the coordination and monitoring
module dynamically launches and controls different data acquisition (DAQ)
and/or data processing modules.

Fig. 3. Structure of a HyperDAC message. Transmission errors can be identified by a missing message preamble, a failed header CRC or a missing message trailer. Missing messages can be detected with the sequence number field
(seq. no.).

number and trailer. Valid messages are either stored in shared
memory (for detector raw data) or routed to the control PC (for
status and control data). The data acquisition modules are also
responsible to route control data from the control PC to the
SDMs (via the coordination and monitoring module).
The stacks send HyperDAC messages to the DAPS every
327 μs, where each message contains the detector raw data for
the last time frame (frame). To align the messages on a global
timeline, each message contains a so-called frame counter. The
frame counters in the stacks are started synchronously at the start
of a measurement through a signal from the backbone, and therefore, can be used to time align the detector raw data of all stacks.
Due to a variety of effects (e.g., buffering, scheduling, etc.),
the synchronicity of the detector stacks is lost to a degree when
the data are transferred to the DAPS. In order to store the detector raw data synchronized in shared memory, a fixed mapping
scheme is used (see Fig. 4). Based on the frame counter of each
HyperDAC raw data message, the detector raw data are stored
in a specific, predefined memory area, where it can then be
accessed by the data processing modules. This allows the separation of the detector raw data into independent time intervals
that can be processed effectively in parallel [27].
Therefore, the shared memory ring buffer M is separated
into S areas Ms (one for each stack s). These areas Ms are
then separated into W so-called work packets Ms,w , where
each work packet w features a previously defined number of F
frames.

(1)

Using this fixed mapping scheme, each data acquisition module can store detector raw data from each stack without the
need for explicit synchronization with the other data acquisition
modules. Furthermore, each data processing module can access
the entire detector raw data for a time interval without explicitly
polling the data acquisition modules.
Since the amount of detector raw data in a frame has a wide
range due to the significant changes of the input data rate during
the measurement of a fast decaying radioactive source, a fixed
allocation scheme would result in a poor utilization of the ring
buffer. Therefore, an additional header layer is introduced to
store and access the detector raw data continuously (see Fig. 4).
To guarantee data consistency and take wraparounds of the
ring buffer into account, progress flags for each data acquisition
and processing module are stored in shared memory. To prevent
unprocessed measurement data from being overwritten, the data
acquisition modules monitor the progress of the data processing
modules. In case of a full ring buffer, the data acquisition is
stopped and the user notified.
At the same time, the data processing modules monitor the
progress of each data acquisition module and start the processing
of the detector raw data of a work packet w only when all data
acquisition modules have finished storing the data for w.
As processing modules running in parallel can have varying
data processing rates (e.g., due to the scheduling of the operating
system, hyper-threading, etc.), a ticket lock [35] to distribute
work packets fairly is employed. Here, each idle processing
module acquires a ticket and waits for its turn. The module with
the current ticket selects an unprocessed work packet and checks
if all data are available. When the work packet is ready to be
processed, the module increases the ticket counter and starts
processing.
By increasing the ticket counter, the processing module with
the next ticket can start selecting a fitting work packet and so
on. To prevent race conditions between processing modules, the
ticket generator has to be protected by a lock. All other flags
can be stored in shared memory and accessed lock free.
The highest priority of the DAPA is the seamless acquisition
of the entire detector raw data stream. Therefore, to minimize
the potentially negative interference of the operating system

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

Fig. 5. Data flow of the SCP. The clusters of all stacks are combined and sorted
at the end of the clustering stage to create a global cluster timeline (adapted from
[27] and [33]).

(Windows or Linux) and the data processing modules on the
data acquisition modules, each software module is pinned to a
specific central processing unit (CPU) core with the data acquisition modules specifically isolated from the data processing
modules.
IV. SINGLES AND COINCIDENCE PROCESSING
While the general processing flow of the singles and coincidence processing (SCP) for the Hyperion IID detector is similar
to the SCP for the Hyperion I detector [27], [36], significant
changes were nevertheless necessary: Next to the move from
offline to online processing, also the new dSiPM-based detector
stacks and two more detector rings had to be incorporated. For
this reason, the SCP is described here briefly (see Fig. 5).
As the Hyperion I/IID detectors are mainly used for hybrid
PET/MR measurements, randoms and scatters are corrected
based on emission data only [37]. Therefore, no separate random
estimation approach is implemented in the DAPA. However, as
the entire detector raw data stream is available on the DAPS,
other random estimation techniques [38], [39] could also be
implemented.
The SCP starts with the selection of a work packet as explained earlier. The data in a work packet represents the entire
detector raw data for a configurable time interval (typically 100
.
frames = 0.03 s). Due to a small dead time at the end of a frame
and the synchronous system clock, no coincidences are lost by
splitting the detector raw data streams into time intervals based
on frames.
Similar to the SCP for the Hyperion I detector [27], the SCP
for the Hyperion IID detector can be separated into four stages,
from decoding the detector raw data to identifying coincidences,
that are explained in detail in the following sections.
A. Decoding and Corrections
After selecting a work packet w, the dSiPM detector data
are decoded for each stack and frame inside the work packet
individually. The dSiPM detector data consists of a header (9
Bytes) containing timing information and so-called die hits (14
Bytes each). In contrast to the hits from the Hyperion I detector,
die hits contain the photon counts of four pixels.

319

A die hit is registered when the photon count of one or more of
the four pixels on a detector die surpasses the trigger threshold
and subsequently the validation threshold [40]. A die hit contains
the photon counts of all four pixels (even when only one pixel
was triggered), timing information and status flags.
After extracting the header and the die hits, mainly through
bit shifts, each die hit is checked for validity and a picosecond
timestamp is calculated with the frame counter of the HyperDAC
message and the timing information of the die hits. The resulting
timestamp is a 64-bit integer that represents the die hit on the
global detector timeline. Using previously acquired calibration
data [41], the timestamps are corrected for offsets between the
different detector dies and stacks while the photon counts are
corrected for saturation and gain. To speed up calculations, the
log() function in the saturation correction is realized as a lookup
table.
The corrected die hits are stored in continuous memory segments (one segment per stack), and then, sorted with respect to
their timestamp on stack level. As the die hits from the Hyperion IID detector do not possess the same, high degree of sortedness as the hits from the Hyperion I detector [27], the insertion
sort-based approach was discarded in favor of a comparisonbased sorting implementation (std::sort [42]). Here, to prevent
large memory transfers, arrays of pointers instead of arrays of
(full-sized) data structures are used.
B. Singles Clustering
Due to the light guide between the dSiPM detector tile and
the LYSO crystals array on the stacks, each scintillation event
(single) typically results in multiple die hits. Analogously to
the SCP for the Hyperion I detector [27], in order to estimate
the energy, position, and timestamp of the these events, die hits
are clustered (bundled) with respect to their timestamp for each
stack individually. Here, the timestamp of the first die hit of a
cluster is selected as the timestamp of the cluster.
Before storing, clusters are filtered based on the combined
photon sum of the die hits that are associated with them to
remove events that are unlikely to result in valid singles. Here,
the combined photon sum of valid clusters is expected to be
within a configurable interval.
In contrast to the SCP for the Hyperion I detector, where the
valid clusters for each stack are stored separately, valid clusters
are stored in a continuous memory segment. After time sorting (std::sort), this segment represents a global cluster timeline
for all stacks for the current work packet w. These changes
were necessary to account for the increased number of detector
stacks, and therefore, significantly increased number of potentially coincident stack combinations, as the coincidence search
presented in [27] does not scale well for large numbers of detector stacks.
C. Position and Energy Estimation
In order to estimate the position and energy of clusters, three
algorithms with different complexities are evaluated: A centerof-gravity (COG)-, a least-squares (LS)-, and a maximumlikelihood (ML)-based CPEEA. Akin to the SCP for the

320

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Fig. 6. Selection of hits for the COG-based CPEEA: For the position estimation, only the hit with the highest photon count (red) off the die with the highest
photon count (max. die) and its direct neighbors (blue) are considered. For the
energy estimation, also die-neighbor hits (green) are used.

Hyperion I detector, the cluster position pC ∈ N is determined
as a discrete integer crystal identifier (ID).
After the position and energy estimation, clusters are filtered
based on their energy. Valid clusters (singles) are stored in a continuous memory segment. Since singles timestamps can change
due to crystal-specific time-offsets that are applied after the position and energy estimation, singles have to be time sorted to
establish a valid timeline. However, as only minor changes to
the timestamps are performed the majority of singles is already
in the correct order. Therefore, an insertion sort is used to sort
the singles upon storing.
1) COG-Based CPEEA: To achieve a stable calibration, improve positioning, and reduce the influence of noise hits far from
the position of the scintillation [43] only a subset A of hits in
a cluster C (A ⊆ C) is used to estimate the cluster position pC
for the COG-based CPEEA [44]. Here, the hit with the highest
photon count of the die with the highest combined photon count
in C and its direct neighbor hits are selected for positioning (see
Fig. 6)
 

(2)
Eh · ph
Eh
pC = v
h∈A

h∈A

where v : R → N is a crystal ID lookup table that is generated
during calibration, Eh the corrected photon sum and ph ∈ R2
the position of a hit that was selected for positioning. It is important to note that clusters with missing neighbor hits will be
discarded, and thus, not positioned.
For the energy estimation a different subset of hits B (B ⊆ C)
is used. It contains all hits in subset A and also die-neighbor hits
(see Fig. 6). Using the subset B, the cluster energy estimation
EC is calculated as

EC =
Eh .
(3)
2

h∈B

Finally, the cluster energy EC is corrected with a crystalspecific and a stack-specific energy gain (in keV) that are also
determined during calibration.
2) LS- and ML-Based CPEEA: Based on a light distribution
map l that is acquired during calibration, the position of a cluster C can also be estimated by using an LS- or an ML-based
approach [27], [45]. The light distribution map l contains the

Fig. 7. Crystal position estimation strategy for the LS- and ML-based CPEEA.
Starting with the COG-based crystal position (red), the LSSs/likelihoods of all
direct neighbor crystals (purple) are calculated (1). When the center crystal has
the lowest LSS/highest likelihood, it is selected as the cluster position (green)
(4). Otherwise, the neighbor crystal with the lowest LSS/highest likelihood
becomes the new center crystal (blue) and so on (2, 3).

average corrected photon count li,h for each dSiPM pixel h (64
values in total) in case of a scintillation event in crystal i for each
of the 900 crystals on a stack. Using this information, the measured light distribution lC of a cluster C can be compared to the
expected light distribution li for each crystal i. Here, the crystal
pC whose light distribution lp C best fits lC can be determined
by using an LS- or ML-based scheme.
Similar to the ML-based positioning for the Hyperion I detector, a Poisson model is used for the ML-based positioning.
However, instead of calculating one energy estimation for each
iteration, a crystal-specific energy estimation that is updated
continuously is used so that the iterative approach proposed in
[45] is not required.
In contrast to the COG-based CPEEA, all die hits in a cluster
C are used for the position and energy estimation.
To reduce the energy dependence of the light distribution
map and to adjust for missing pixels, the following crystalspecific energy estimation Ê(i) is used for the LS- and MLbased CPEEA:
 
Eh
li,h .
(4)
Ê(i) =
h∈C

h∈C

The cluster position pC can be found by either minimizing
the sum of LS

2
Eh − li,h · Ê(i)
pC = argmin
(5)
i

h∈C

or by maximising the log-likelihood [27], [45]





pC = argmax
Eh · log li,h · Ê(i) − 1 .
i

(6)

h∈C

In order to speed up the position calculation, the least-squares
sums (LSS)/likelihoods are only calculated for a subset of
crystals (see Fig. 7). Here, starting with the COG-based crystal estimation (calculated for all die hits) as the center crystal, the LSS/likelihood for all direct neighbor crystals will be

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

calculated. If none of the neighbor crystals has a smaller
LSS/higher likelihood, the center crystal is selected as the cluster position. Otherwise the neighbor crystal with the smallest
LSS/biggest likelihood is selected as the new center crystal,
for which the LSS/likelihood for all neighbor crystals will
be calculated and so on. In order to prevent calculating the
LSS/likelihood of a crystal more than once, a map with already
calculated crystals is kept.
In the unlikely case of two crystals having the same
LSS/likelihood, the crystal that was calculated first will be selected. The LSSs/likelihoods are calculated in single precision
as it was found, that calculating in double precision does not
lead to different results but impacts the processing throughput
negatively.
When the cluster position pC is found, the cluster energy
EC (= Ê(pC )) is corrected with a stack-specific energy gain
(in keV) that is determined before each measurement series
to recalibrate for changes induced from small environmental
changes (voltages, temperatures, . . .).
D. Coincidence Processing
Analogously to previous work [27], the coincidence processing is separated into two stages: coincident clustering and main
coincidence processing.
1) Coincident Clustering: Without compromising data, the
introduction of coincident clustering leads to a significant increase of the data processing throughput as the workload for
the following position and energy estimation and coincidence
processing stages is reduced [27].
Based on the global cluster timeline that is established in the
clustering, a sliding time window is used to identify clusters that
are within a configurable time interval tc of each other.
At this point, energy-based constraints are not taken into consideration, as the final energy estimation of the clusters has not
been determined yet. Therefore, multiple coincident clusters are
not removed, as one or more of the clusters might be removed,
when their final energy estimation is determined.
2) Main Coincidence Processing: Similar to the coincident
clustering stage, the input (in this case, singles) for the main coincidence processing is already time sorted and in a continuous
memory segment. Therefore, in contrast to previous work [27],
where the singles were stored in separate memory segments,
coincidences can be identified with one sliding time window
with configurable length tc .
The resulting coincidences can either be exported directly to
a HDD or stored in a specific memory segment, that is exported
to a HDD at the end of a measurement. In case of more than
one data processing module, the resulting list-mode coincidence
files (one per processing module) have to be merged after the
measurement has concluded.

321

measurements using the COG-, LS-, and ML-based CPEEA
were run for a fixed number of processing threads. The start
activities were chosen so that the measurements start with the
SDMs in (or close to) saturation to study the entire input data rate
range. The SDMs saturate (and start discarding data messages
from the stacks) at a data rate of approximately 95 MB/s per
SDM. Here, data messages are discarded randomly, so that each
stack is affected approximately equally. Therefore, no atypical
behavior of the PET system at high data rates could be registered.
Also, to determine parallelization speedups for the data processing, a third test series (C) with five sodium (22 Na)-filled
point sources (each ∼1.85 MBq) positioned along the transverse
axis of the Hyperion IID detector was administered. Contrary
to test series A and B, the measurements were run for a varying
number of processing threads.
In all test series (A–C), the dSiPM-based detector tiles were
operated at an overvoltage of 2.5 V, with the trigger threshold
set to three photons [40]. The validation time was set to 40 ns
while the validation threshold was set to 37.1 ± 12.8 photons
(27.5 ± 10.3 photons) per pixel for test series A (B, C). The
validation threshold is a statistical quantity that is derived from
the configuration (A: 0x50, B: 0x54) of the validation logic of the
dSiPMs [40]. On the dSiPMs, 20% of the noisiest microcells
were deactivated. The cooling temperature was set to 5 °C,
resulting in detector tile temperatures of ∼12 °C.
The DAPA was run on a Dell Poweredge R910 with four
Intel Xeon X7560 (2.27 GHz, 8 (16 logical) cores) CPUs and
512 GB of DDR3 1066 MHz RAM under Ubuntu Server 12.04.
The DAPA software is written in C++ and uses the MPICH
3.1 library. It was compiled using the GNU GCC 4.8 compiler
collection using −O2 optimization.
For the data acquisition, 11 separate core-mapped processes
(one for the backbone and one for each SDM) were used, while
the data processing was executed using 40 parallel (also core
mapped) processes for test series A and B, and a varying number
of data processing threads for test series C.
For the PET processing, a singles clustering window ts of
40 ns and a coincidence window tc of 1.5 ns (also used for coincident clustering) were used. The size of the singles clustering
window was determined during calibration and selected taking
the decay time of the scintillator (LYSO) into account. The energy window was set to 300–625 keV. The minimum angular
distance for a valid coincidence was set to five stacks difference that equals an acceptance angle of approximately 108° in
azimuth direction.
Images were reconstructed using an OSEM-based iterative reconstruction algorithm (8 subsets, 32 iterations) [37], [46] with
self-normalization [47] on a voxel-grid of 200×200×387 voxels with 0.25 mm pitch. As the measurements were conducted
outside of an MRI, no random/scatter correction was used.

V. MATERIALS AND METHODS
To evaluate the DAPA two test series (A: 60.9 MBq, B: 52.3
MBq) with an FDG-filled Jaszczak hot-rod phantom (rod diameters: 0.6, 0.8, 1.0, 1.4, 1.7, 2.0 mm) in the isocentre of
the Hyperion IID detector were conducted. Here, for each
test series, multiple real-time data acquisition and processing

VI. RESULTS
A. Speedup
Fig. 8 shows the data processing speedups in relation to the
number of threads used for the three different CPEEA for test

322

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Fig. 8. Data processing speedup for the COG- (green), LS- (blue) and MLbased (red) CPEEA in relation to the number of threads used for test series C.
Compared to the theoretical maximum speedup (depicted in purple), the slope
of the measured speedups flattens when more than one processing thread is run
on a CPU core via hyperthreading.

Fig. 10. Energy resolution (top) and CRT (bottom) for the COG- (dots), LS(squares), and ML-based (rhombi) CPEEA for test series A (green) and B (blue)
in relation to source activity.

Fig. 9. Data quality coefficient (green) and missing message ratio (blue) in
relation to source activity for test series A (dots) and B (squares). For clarity,
the missing message ratio marks are connected by dashed lines.

series C. For all CPEEA, the speedups increase with the number
of threads used. The maximum measured speedups are 28.1×
for the COG-, 26.4× for the LS-, and 26.7× for the ML-based
CPEEA for 40 processing threads.

Fig. 11. Cluster (green) and coincident cluster (blue) rates for test series A
(dots) and B (squares) in relation to source activity. Linear fits to the measurement data in the range of 3-45 MBq for test series A and 3-40 MBq for test
series B are depicted as dashed lines.

B. Data Quality
The data quality coefficient and missing message ratio in
relation to source activity for test series A and B are depicted in
Fig. 9. The data quality coefficient QD for each measurement
in each test series is calculated as follows:
QD = DG /(DG + DB )

(7)

where DG is the number of valid incoming bytes and DB is
the number of invalid incoming bytes, which is determined by
protocol errors such as incorrect header CRCs, missing message
preambles or message trailers. For both test series A and B, over
99.99 % of incoming bytes are valid.
The missing message ratio QM for each measurement of each
test series is calculated as follows:
QM = ML /(MN + ML )

(8)

where MN is the number of incoming detector raw data messages and ML is the number of missing messages, which is determined by missing sequence numbers in the input data streams.
Here, less than 0.1 % of all input messages are missing for source
activities of up to approximately 55 MBq for test series A and
up to 47 MBq for test series B. For source activities higher than
55 MBq for test series A and 47 MBq for test series B the lost
message ratio increases with activity.

C. Energy Resolution and Coincidence Resolution Time
Displayed in Fig. 10 are the energy resolution and coincidence resolution time (CRT) for test series A and B in relation
to source activity. For both test series, only minor CRT differences between the different CPEEA were measured. The average energy resolution is approximately equal for all CPEEA for
test series A. For test series B, the average energy resolution for
the LS- and ML-based CPEEA is ∼0.1 percentage points (pp)
and ∼0.05 pp, respectively, better on average than the average
energy resolution for the COG-based CPEEA. Here, the energy
resolution degrades by up to 0.1 pp for the LS- and ML-based
CPEEA for higher source activities.
D. Cluster and Coincident Cluster Rates
The cluster and coincident cluster rates in relation to source
activity for test series A and B are shown in Fig. 11. For test
series A (B), up to a source activity of 45 MBq (40 MBq), both
the cluster and coincident cluster rate increase approximately
linearly by 348 700 (356 200) clusters / (MBq·s) and 91 600
(93 500) coincident clusters / (MBq·s), respectively.
E. Average Size of Accepted Singles
Depicted in Fig. 12 is the average size of accepted singles
(defined as the average number of hits in singles that were used

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

323

Fig. 12. Average size of accepted singles (average number of hits in singles
that were used in coincidences) for the COG- (dots), LS- (squares), and MLbased (rhombi) CPEEA for test series A (green) and B (blue) in relation to
source activity.

Fig. 14. Data processing rates (A: green, B: red) for the COG- (dots), LS(squares), and ML-based (rhombi) CPEEA and input data rates (A: blue, B:
purple) for test series A and B in relation to source activity. For the input data
rates, linear fits (A: 3–45 MBq, B: 3–40 MBq) are depicted as a dashed line.

G. Input Data Rates and Data Processing Rates

Fig. 13. Singles (A: green, B: red) and coincidence (A: blue, B: purple) rates
for the COG- (dots), LS- (squares), and ML-based (rhombi) CPEEA for test
series A and B in relation to source activity. Linear fits to the measurement data
(A: 3–45 MBq, B: 3–40 MBq) are depicted as dashed lines.

in coincidences) for the COG-, LS-, and ML-based CPEEA for
test series A and B in relation to source activity.
In comparison to test series A, the average size of accepted
singles is approximately 14% higher for test series B. For the
COG-based CPEEA the average size of accepted singles is ∼3 %
and ∼4 % bigger on average than for the LS- or ML-based
CPEEA for test series A and B, respectively.
For higher activities, the average size of accepted singles
decreases by up to 3 % for the COG- and 6 % for the LS- and
ML-based CPEEA for both test series.
F. Singles and Coincidence Rates
Displayed in Fig. 13 are the singles and coincidence rates for
the COG-, LS-, and ML-based CPEEA for test series A and B,
respectively, in relation to source activity. For the singles rates,
only singles resulting from coincident clusters were considered.
Up to 45 MBq for test series A and 40 MBq for test series
B, the singles and coincidence rates increase approximately
linearly for each CPEEA. Here, in comparison to the COGbased CPEEA, the singles rate of the LS- and ML-based CPEEA
is 63% (36%) higher and the average coincidence rate is 153%
(78%) higher for test series A (B). In general, both the singles
and coincidence rates for the COG-based CPEEA are higher for
test series B in comparison to test series A for similar activity
levels, while the rates for the LS- and ML-based CPEEA are
almost identical.

Fig. 14 shows the combined input data rates of the Hyperion IID detector and the data processing rates for the COG-,
LS-, and ML-based CPEEA in relation to source activity for
test series A and B, respectively. The data processing rate is defined as the amount of input data (in MB) that can be processed
for singles and coincidences per second. Up to approximately
45 MBq for test series A and 40 MBq for test series B, the input data rate increases approximately linearly with activity (A:
15.7 MByte / (MBq·s), B: 19.1 MByte / (MBq·s)). The data processing rate for the COG-based CPEEA is higher than the input
data rate for all measured activities for both test series, while for
the LS-based CPEEA, the data processing rate is surpassed by
the input data rate at ∼31 MBq (∼29 MBq) for test series A (B).
For the ML-based CPEEA, the data processing rate is surpassed
by the input data rate at ∼28 MBq (∼26 MBq) for test series A
(B). For each CPEEA, the data processing rate increases with
lower source activities for both test series.
H. Data Processing Run Times
Depicted in Fig. 15 are the relative run times of the decoding
and corrections, clustering, position and energy estimation, and
coincidence processing (includes coincident clustering) sections
for the COG-, LS-, and ML-based CPEEA for test series A and
B in relation to source activity. The run times of the different
processing sections for each measurement m are normalized to
the total processing time for m. For measurements where the
sum of the different processing sections is below 100 %, the
rest of the processing time was spent waiting for new input data
(idle time). For test series A (B), up to ∼31 MBq (∼29 MBq)
for the LS-based CPEEA and up to ∼28 MBq (∼26 MBq) for
the ML-based CPEEA, the relative run times of the different
processing sections increase monotonously and the idle times
decrease monotonously with higher activities until no more time
is spent idling. For the COG-based CPEEA, the relative run
times also increase for higher activities, however for all measured activities processing time is spent idling. Compared to the
COG-based CPEEA, the normalized run times of the position

324

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Fig. 15. Normalized run times of the decoding and correction (green), clustering (blue), position and energy estimation (red), and coincidence processing
(purple) sections for the COG- (dots), LS- (squares), and ML-based (rhombi)
CPEEA for test series A (top) (B (bottom)) in relation to source activity. The
run times are normalized to the total processing time for each measurement.
When the sum of the run times is below 100%, the rest of the processing time
was spent idling.

Fig. 17. Image reconstruction and line profiles for the COG- (left), LS- (middle), and ML-based (right) CPEEA for test series A (top) and B (bottom).

both test series, the average number of crystals that is evaluated
per single increases for higher source activities.

J. Image Reconstructions and Line Profiles
Fig. 16. Average number of crystals evaluated per single for the ML-based
(dots) and LS-based (squares) CPEEA for test series A (green) and B (blue)
in relation to source activity. For test series A (B), linear fits in the range of
3–45 MBq (3–40 MBq) are depicted as dashed lines.

and energy estimation are significantly higher for the LS- and
ML-based CPEEA for both test series.
I. Average Number of Crystals Evaluated Per Single
For each cluster that is selected for position and energy estimation, the number of crystals that are evaluated for the MLbased and LS-based CPEEA is stored in a histogram. Using this
histogram, the average number of crystals evaluated per single
for test series A and B in relation to source activity is depicted in
Fig. 16. On average, 11.3 (11.4) crystals are evaluated per single
for the LS-based CPEEA for test series A (B), while 11.2 (11.3)
crystals are evaluated on average for the ML-based CPEEA. For

Fig. 17 shows the image reconstructions of the Jazczak hotrod phantom using 250 million coincidences for each CPEEA
for test series A and B. Further depicted is a line profile for each
reconstructed image.
For test series A, rods smaller than 2.0 mm in the LS- and
ML-based image reconstructions are better separable than the
rods in the COG-based image reconstruction. In the line profiles,
this is exemplified by the 1.4-mm rods, that are clearly separable
for the LS- and ML-based CPEEA but not so for the COG-based
CPEEA. In contrast, the 2.0-mm rods in the line profile for the
COG-based CPEEA have a 30% higher intensity in comparison
to the LS- and ML-based CPEEA.
For test series B, the 2.0 and 1.4-mm rods in the line profiles
for all CPEEA can be clearly separated. Here, especially the LSand ML-based line profiles show only small intensity variations
for rods of equal size. The COG-based image reconstruction is
similar to the LS- and ML-based images for bigger rods, but
rods smaller than 1.7 mm appear more blurred in general.

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

VII. DISCUSSION
A. Speedup
Dividing the PET detector raw data input into independent
time intervals allows for effective parallelization, as can be seen
in the speedups for multiple processing threads (see Fig. 8).
While the processing performance does not scale linearly with
the number of processing threads used, more processing threads
generally lead to higher performance though speedup slope flattens when more than one processing thread is run on a CPU core
via hyperthreading.
B. Data Quality
Concerning data quality, the DAPA can generally be considered very reliable as over 99.1% of all incoming bytes could be
identified as valid and less than 0.1% of all messages were lost
on average. Only when the SDMs saturate and start discarding
data messages from the stacks, the lost message ratio increases
as would be expected. For both data quality and lost message
ratio, no direct relation to the source activity, and thus, the input
data rate could be noted. Until now, lost messages and invalid
bytes, especially for lower data rates, could not be traced back
to the DAPA (or DAPS) and are likely the result of transmission errors (as UDP does not guarantee packet delivery) on the
PET detector side. This also applies to the seemingly structured
variations of the data quality coefficient. Overall, these minor
transmission problems can be considered negligible however.
C. Energy Resolution and CRT
For all CPEEA for test series A, the energy resolution
and CRT can be considered very stable at an average of
∼12.3 ± 0.02 % and ∼548 ± 4.99 ps, respectively. For test
series B, the energy resolution depends on the CPEEA and generally improves for lower source activities, while the CRT can
be considered stable at ∼548 ± 4.65 ps. However, with a maximum deviation of 0.2 pp, the differences in the energy resolution
for test series B can be regarded as negligible.
D. Cluster and Coincident Cluster Rates
As expected, the cluster and coincident cluster rates increase
approximately linearly with higher activities up until the PET
detector saturates due to transfer limitations on SDM level (as
mentioned earlier) and dSiPM dead time.
To offset the transfer limitations on SDM level and enable
measurements with (even) higher count rates, the raw data could
be compressed (losslessly) on the SDMs before transferring it
to the DAPS. It would also be possible to move tasks that are
currently executed on the DAPS to the SDMs instead. Here,
e.g., preclustering hits and/or filtering out noise hits would not
only reduce the data rate significantly, but also the runtime of
the data processing on the DAPS (see Fig. 15).
E. Average Size of Accepted Singles
The average size of accepted singles is higher for the COGbased CPEEA due to the restrictive neighborhood criterion that

325

requires a full neighborhood for clusters to be accepted. The LSand ML-based CPEEA are less restrictive in this respect, and
therefore, also accept smaller clusters with incomplete neighborhoods. The decreasing size of accepted singles for higher
activities can likely be attributed to the rising dead time of the
dSiPM detector tiles for higher activities. The general differences in size of accepted singles between test series A and test
series B, can most likely be attributed to the different validation
thresholds for both test series.
F. Singles and Coincidence Rates
Due to the less restrictive behavior toward missing neighbors
and also the ability to adjust for missing information, the singles
and coincidence rates for the LS- and ML-based CPEEA are
higher than for the COG-based CPEEA. The higher singles and
coincidence rates for the COG-based CPEEA for test series B
(in comparison to test series A) can presumably be attributed
to the lower validation threshold, and thus, bigger cluster sizes
for test series B that increase the likelihood of complete cluster
neighborhoods.
G. Input Data and Data Processing Rates
Similar to the cluster rates, the input data rate increases approximately linearly with activity until the PET detector reaches
the point of saturation (A: 45 MBq, B: 40 MBq). For a lower validation threshold (test series B), this point is reached earlier, as
more noise and also bigger clusters are transferred on average.
The data processing can be considered real time capable for
the COG-based CPEEA for both evaluated validation thresholds
for activities of at least up to 55 MBq for test series A and
50 MBq for test series B. The LS-based CPEEA is real time
capable up to ∼31 MBq for test series A and ∼29 MBq for test
series B, while the ML-based CPEEA is real time capable up to
∼28 MBq for test series A and ∼26 MBq for test series B.
If the input data rate is considerably higher than the data processing rate (so that the shared memory buffer will eventually
overflow), incoming data are stored on HDDs. When the input data rate has decreased to the point where it is lower than
the expected real-time data processing rate, the real-time data
processing is started.
With smaller activities the data processing rates for all
CPEEA increase. This can most likely be attributed to the fact,
that smaller work packets and thus smaller workloads increase
the chance for exclusive CPU time (e.g., without interfering
hyperthreading threads) and a less used memory bus for each
thread. The performance of each processing thread thus gets
closer to the performance one thread has, when running alone
(see Fig. 8). Additionally, the reduced amount of events for lower
activities might also have an impact on the throughput of the
sorting routines that are employed in the decode and clustering
sections.
Although the input data rate is ∼21 % higher on average for
test series B, the data processing rates are similar for both test
series for similar activities. The significantly larger cluster size
in test series B and thus the lower validation threshold, therefore,
only seems to have a minor effect on the data processing rate.

326

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

H. Data Processing Run Times
As shown in Fig. 15, the coincidence processing section only
plays a minor role in terms of total run time of the data processing. In part, this can most likely be attributed to the fact,
that the input (clusters/singles) to the coincidence processing
section is already sorted and in a continuous data array that
allows the CPU to effectively load data into the CPU cache,
and thus, increase the chance of cache hits. This leads to an
isolated maximum throughput of the coincidence processing of
up to ∼500-million coincidences per second for both test series.
However, as the underlying problem statement is much more
complex, the overall performance of the processing is not given
in coincidences per second as is usual for hardware-based coincidence units, but in MB/s and in relation to the input data rate.
The relative run times for LS- and ML-based CPEEA reach a
relatively constant plateau once the workload is high enough so
that no more time is spent idling. The source activity for which
the plateau is reached is consistent with the source activity, when
the input data rate surpasses the data processing rate, as would
be expected. For the COG-based CPEEA, a constant plateau
as described previously is never reached, as the processing rate
is higher than the input rate for all measured activities. The
position and energy estimation requires up to ∼58% of the total
processing time for the LS- and ML-based CPEEA, while it
only requires up to ∼12% of the processing time for the COGbased CPEEA. In contrast, the other processing sections have
very similar data processing rates for all CPEEA for similar
activities.
I. Average Number of Crystals Evaluated Per Single
The average number of crystals that is evaluated per single
(CPS) for the LS- and ML-based CPEEA can be considered
reasonably stable at an average of ∼11.3 ± 0.17. Considering
that for most singles at least nine crystals have to be evaluated,
the selection of the start positions can be regarded as effective.
The reason for the increased CPS for higher activities is difficult
to identify and likely the result of multiple different factors such
as the decreasing cluster size for higher activities that could
result in less precise start positions for the position estimation.
J. Image Reconstructions and Line Profiles
Since the LS- and ML-based CPEEA use the same calibration data, the image reconstruction of the LS- and ML-based
CPEEA are very similar, as would be expected. In general, the
lower validation threshold of test series B leads to a better separability and less blurred rods in the image reconstructions and
line profiles, which can be most likely attributed to the increased
amount of information per cluster. Especially for the COG-based
CPEEA, this leads to a less blurred image and better separability
of the rods. In comparison to the COG-based CPEEA, the LSand ML-based CPEEA lead to slightly less blurred images and
better separability of the rods for both test series.
The minor artifacts outside of the hot-rod phantom contour
in the reconstructed images are likely the result of random
and scattered coincidences (that were not corrected for reasons
explained in Section IV).

For both test series, all evaluated PET performance parameters were within the range that was established with non realtime measurements and no impact of the real-time processing
could be found.
VIII. CONCLUSION
In this paper, we introduced a software-based DAPA and
evaluated it with a digital, preclinical PET/MRI-insert.
We showed, that separating the input data streams into independent work packets, leads to an effective parallelization of the
data processing with maximum speedups of 28.1× for a COG-,
26.4× for an LS-, and 26.7× for an ML-based CPEEA for 40
processing threads.
Using an FDG-filled Jaszczak hot-rod phantom in the
isocentre of the detector to evaluate the real-time capability of the DAPA for two different validation thresholds (A:
37.1 ± 12.8 photons, B: 27.5 ± 10.3 photons), we showed that
the DAPA can be considered very reliable as over 99.9% of all
incoming data were valid and less than 0.1% of all messages
were lost on average.
For the COG-based CPEEA, the incoming data for all measured activities [up to 55 MBq (50 MBq) for validation threshold
A (B)] could be acquired and processed in real time while the
real-time capability of the LS- and ML-based CPEEA was limited to source activities of up to ∼31 MBq (∼29 MBq) and
∼28 MBq (∼26 MBq) for validation threshold A (B), respectively.
The average energy resolution was ∼12.3 % for the COGbased CPEEA for both validation thresholds and ∼12.3 %
(∼12.2 %) for the LS- and ML-based CPEEA for validation
threshold A (B). The average coincidence time resolution was
at approximately 548 ps for both validation thresholds. On average, the system sensitivity using the LS- or ML-based CPEEA
was ∼2.5× higher compared to the system sensitivity for the
COG-based CPEEA for validation threshold A and 1.8× for
validation threshold B.
In comparison to the COG-based CPEEA, the rods in the image reconstruction of the hot-rod phantom were better separable
and less blurred for the LS- and ML-based CPEEA, especially
for validation threshold A.
To our knowledge, the DAPA is the first software-based architecture that allows real-time acquisition and processing of
PET data on this scale.
ACKNOWLEDGMENT
The authors would like to thank S. Kivits of Philips Research,
Eindhoven, Netherlands for the support in preparing the hotrod phantom, the Philips Life Sciences Facilities in Eindhoven,
Netherlands for providing the facilities for PET measurements,
and Y. Berker and P. Hallen from RWTH Aachen University,
Germany for the help with the manuscript.
REFERENCES
[1] B. J. Kemp et al., “NEMA NU 2-2007 performance measurements of the
Siemens Inveon(TM) preclinical small animal PET system,” Phys. Med.
Biol., vol. 54, no. 8, pp. 2359–2376, 2009.

GOLDSCHMIDT et al.: SOFTWARE-BASED REAL-TIME ACQUISITION AND PROCESSING OF PET DETECTOR RAW DATA

[2] Z. Gu et al., “NEMA NU-4 performance evaluation of PETbox4, a
high sensitivity dedicated PET preclinical tomograph,” Phys. Med. Biol.,
vol. 58, no. 11, pp. 3791–3814, 2013.
[3] C. Bruschini et al., “SPADnet: Embedded coincidence in a smart sensor
network for PET applications,” Nucl. Instr. Meth. Phys. Res. A, vol. 734,
no. 1, pp. 122–126, 2014.
[4] R. Fontaine et al., “The hardware and signal processing architecture of
LabPET, a small animal APD-based digital PET scanner,” IEEE Trans.
Nucl. Sci., vol. 56, no. 1, pp. 3–9, Feb. 2009.
[5] E. Albuquerque et al., “The clear-PEM electronics system,” IEEE Trans.
Nucl. Sci., vol. 53, no. 5, pp. 2704–2711, Oct. 2006.
[6] W. W. Moses et al., “OpenPET: A flexible electronics system for radiotracer imaging,” IEEE Trans. Nucl. Sci., vol. 57, no. 5, pp. 2532–2537,
Oct. 2010.
[7] T. K. Lewellen et al., “Evolution of the design of a second generation
FireWire based data acquisition system,” in Proc. IEEE Nuclear Sci. Symp.
Conf. Rec., 2010, pp. 2510–2514.
[8] S. A. Kis et al., “Performance test of the MiniPET-II small animal scanner
according to the NEMA NU-4 standard,” in Proc. IEEE Nucl. Sci. Symp.
Conf. Rec., 2009, pp. 3185–3189.
[9] H. S. Yoon et al., “Initial results of simultaneous PET/MRI experiments
with an MRI-compatible silicon photomultiplier PET scanner,” J. Nucl.
Med., vol. 53, no. 4, pp. 608–614, 2012.
[10] S. Yamamoto et al., “Development of a Si-PM-based high-resolution
PET system for small animals,” Phys. Med. Biol., vol. 55, no. 19,
pp. 5817–5831, 2010.
[11] C. Degenhardt et al., “Performance evaluation of a prototype PET scanner
using digital photon counters (DPC),” in Proc. IEEE Nucl. Sci. Symp. Med.
Imag. Conf., 2012, pp. 2820–2824.
[12] M. Streun et al., “The data acquisition system of ClearPET neuro–A small
animal PET scanner,” IEEE Trans. Nucl. Sci., vol. 53, no. 3, pp. 700–703,
Jun. 2006.
[13] R. R. Raylman et al., “The positron emission mammography/tomography
breast imaging and biopsy system (PEM/PET): Design, construction
and phantom-based measurements,” Phys. Med. Biol., vol. 53, no. 3,
pp. 637–653, 2008.
[14] K. J. Hong et al., “A prototype MR insertable brain PET using tileable
GAPD arrays,” Med. Phys., vol. 40, no. 4, pp. 042 503-1–042 503-12,
2013.
[15] E. Kim et al., “PET DAQ system for compressed sensing detector
modules,” in Proc. IEEE Nuclear Sci. Symp. Med. Imag. Conf., 2012,
pp. 2798–2801.
[16] A. Mann et al., “A sampling ADC data acquisition system for positron
emission tomography,” IEEE Trans. Nucl. Sci., vol. 53, no. 1, pp. 297–303,
Feb. 2006.
[17] P. Vaska et al., “An MRI-compatible PET insert for whole body studies
in rodents at high functional and anatomical resolution,” in Proc. IEEE
Nucl. Sci. Symp. Med. Imag. Conf., 2011, pp. 3169–3172.
[18] D. Dewitt et al., “Design of an FPGA based algorithm for real-time
solutions of statistics-based positioning,” in Proc. IEEE Nucl. Sci. Symp.
Conf. Rec., 2008, pp. 5029–5035.
[19] N. G. Johnson-Williams et al., “Design of a real time FPGA-based
three dimensional positioning algorithm,” IEEE Trans. Nucl. Sci., vol. 58,
no. 1, pp. 26–33, Feb. 2011.
[20] M. Haselman et al., “FPGA-based front-end electronics for positron emission tomography,” in Proc. ACM/SIGDA Int. Symp. Field Programmable
Gate Arrays, 2009, pp. 93–102.
[21] H. T. van Dam et al., “Improved nearest neighbor methods for gamma
photon interaction position determination in monolithic scintillator PET
detectors,” IEEE Trans. Nucl. Sci., vol. 58, no. 5, pp. 2139–2147, Oct.
2011.
[22] R. J. Aliaga et al., “Corrected position estimation in PET detector modules
with multi-anode PMTs using neural networks,” IEEE Trans. Nucl. Sci.,
vol. 53, no. 3, pp. 776–783, Jun. 2006.
[23] W. Yonggang et al., “Self-organizing map neural network based positioning scheme for continuous crystal PET detectors,” in Proc. Nuclear Sci.
Symp. Med. Imag. Conf., 2013, pp. 1–5.
[24] T. Frach et al., “The digital silicon photomultiplier system architecture
and performance evaluation,” in Proc. IEEE Nucl. Sci. Symp. Conf. Rec.,
2010, pp. 1722–1727.

327

[25] C. Degenhardt et al., “Arrays of digital silicon photomultipliers intrinsic
performance and application to scintillator readout,” in Proc. IEEE Nucl.
Sci. Symp. Conf. Rec, 2010, pp. 1954–1956.
[26] B. Weissler et al., “A digital preclinical PET/MRI insert and initial results,” IEEE Trans. Med. Imag., 2015, DOI: 10.1109/TMI.2015.2427993.
[27] B. Goldschmidt et al., “Towards software-based real-time singles and
coincidence processing of digital PET detector raw data,” IEEE Trans.
Nucl. Sci., vol. 60, no. 3, pp. 1550–1559, Jun. 2013.
[28] V. Schulz et al., “SiPM based preclinical PET/MR Insert for a human
3T MR: first imaging experiments,” in Proc. Nucl. Sci. Symp. Med. Imag.
Conf., 2011, pp. 4467–4469.
[29] J. Wehner et al., “PET/MRI insert using digital SiPMs: Investigation
of MR-compatibility,” Nucl. Instr. Meth. Phys. Res. A, vol. 734, no. 1,
pp. 116–121, 2014.
[30] B. Weissler et al., “An MR-compatible singles detection and processing
unit for simultaneous preclinical PET/MR,” in Proc. Nucl. Sci. Symp. Med.
Imag. Conf., 2012, pp. 2759–2761.
[31] P. Gebhardt et al., “FPGA-based singles and coincidences processing
pipeline for integrated digital PET/MR detectors,” in Proc. Nucl. Sci.
Symp. Med. Imag. Conf., 2012, pp. 2479–2482.
[32] P. M. Duppenbecker et al., “Development of an MRI compatible digital
SiPM based PET detector stack for simultaneous preclinical PET/MRI,”
in Proc. Nucl. Sci. Symp. Med. Imag. Conf., 2012, pp. 3481–3483.
[33] B. Goldschmidt et al., “Singles and coincidence processing for a digital
PET/MR system using SiPM detectors,” in Proc. Nucl. Sci. Symp. Med.
Imag. Conf., 2011, pp. 3922–3924.
[34] M. P. I. Forum, MPI: A Message Passing Interface Standard v3.0, (2012).
[Online]. Available: http://www.mpiforum.org.
[35] D. P. Reed and R. K. Kanodia, “Synchronization with eventcounts and
sequencers,” Commun. ACM, vol. 22, no. 2, pp. 115–123, 1979.
[36] B. Weissler et al., “MR compatibility aspects of a silicon photomultiplierbased PET/RF insert with integrated digitisation,” Phys. Med. Biol.,
vol. 59, no. 17, pp. 5119–5139, 2014.
[37] A. Salomon et al., “Simultaneous reconstruction of activity and attenuation for PET/MR,” IEEE Trans. Med. Imag., vol. 30, no. 3, pp. 804–813,
Mar. 2011.
[38] S.-J. Park et al., “Digital coincidence processing for the RatCAP conscious
rat brain PET scanner,” IEEE Trans. Nucl. Sci., vol. 55, no. 1, pp. 510–515,
Feb. 2008.
[39] C. W. Stearns et al., “Random coincidence estimation from single event
rates on the discovery ST PET/CT scanner,” in Proc. Nucl. Sci. Symp.
Conf. Rec., 2003, pp. 3067–3069.
[40] V. Tabacchini et al., “Probabilities of triggering and validation in a digital
silicon photomultiplier,” J. Instrum., vol. 9, no. 6, p. P06016, 2014.
[41] D. Schug et al., “Data processing for a high resolution preclinical PET
detector based on Philips DPC digital SiPMs,” IEEE Trans. Nucl. Sci.,
vol. 62, no. 3, pp. 669–678, Jun. 2015.
[42] ISO, ISO/IEC 14882:2003: Programming languages — C++, International Organization for Standardization, Geneva, Switzerland, 2003.
[43] E. Netter et al., “The tumor resection camera (trecam), a multipixel
imaging probe for radio-guided surgery,” in Proc. IEEE Nucl. Sci. Symp.
Conf. Rec., 2009, pp. 2573–2576.
[44] H. Anger, “Scintillation camera,” Rev. Sci. Instrum., vol. 29, no. 1,
pp. 27–33, 1958.
[45] C. W. Lerche et al., “Maximum likelihood based positioning and energy
correction for pixelated solid state PET detectors,” in Proc. Nucl. Sci.
Symp. Med. Imag. Conf., 2011, pp. 3610–3613.
[46] L. Shepp et al., “Maximum likelihood reconstruction for emission tomography,” IEEE Trans. Med. Imag., vol. 1, no. 2, pp. 113–122, Oct.
1982.
[47] A. Salomon et al., “A self-normalization reconstruction technique for
PET scans using the Positron emission data,” IEEE Trans. Med. Imag.,
vol. 31, no. 12, pp. 2234–2240, Dec. 2012.

Authors’ photographs and biographies not available at the time of publication.

