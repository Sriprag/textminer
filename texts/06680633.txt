IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

1717

DREAM: Diabetic Retinopathy Analysis Using
Machine Learning
Sohini Roychowdhury, Student Member, IEEE, Dara D. Koozekanani, Member, IEEE,
and Keshab K. Parhi, Fellow, IEEE

Abstract—This paper presents a computer-aided screening system (DREAM) that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic
retinopathy (DR) using machine learning. Classifiers such as the
Gaussian Mixture model (GMM), k-nearest neighbor (kNN), support vector machine (SVM), and AdaBoost are analyzed for classifying retinopathy lesions from nonlesions. GMM and kNN classifiers are found to be the best classifiers for bright and red lesion
classification, respectively. A main contribution of this paper is the
reduction in the number of features used for lesion classification by
feature ranking using Adaboost where 30 top features are selected
out of 78. A novel two-step hierarchical classification approach is
proposed where the nonlesions or false positives are rejected in
the first step. In the second step, the bright lesions are classified
as hard exudates and cotton wool spots, and the red lesions are
classified as hemorrhages and micro-aneurysms. This lesion classification problem deals with unbalanced datasets and SVM or
combination classifiers derived from SVM using the Dempster–
Shafer theory are found to incur more classification error than
the GMM and kNN classifiers due to the data imbalance. The DR
severity grading system is tested on 1200 images from the publicly available MESSIDOR dataset. The DREAM system achieves
100% sensitivity, 53.16% specificity, and 0.904 AUC, compared to
the best reported 96% sensitivity, 51% specificity, and 0.875 AUC,
for classifying images as with or without DR. The feature reduction
further reduces the average computation time for DR severity per
image from 59.54 to 3.46 s.
Index Terms—Bright lesions, classification, diabetic retinopathy (DR), fundus image processing, red lesions, segmentation,
severity grade.

I. INTRODUCTION
CCORDING to a study by the American Diabetes Association (ADA), diabetic retinopathy (DR) had affected more
than 4.4 million Americans of age 40 and older during 2005–
2008, with almost 0.7 million (4.4% of those with diabetes)
having advanced DR that could lead to severe vision loss [1].
Early detection and treatment of DR can provably decrease the
risk of severe vision loss by over 90% [2]. Thus, there is a high

A

Manuscript received July 29, 2013; revised October 09, 2013; accepted
November 30, 2013. Date of publication December 11, 2013; date of current
version September 2, 2014. This research was supported in part by a grant from
the Office of Vice President for Research at the University of Minnesota.
S. Roychowdhury and K. K. Parhi are with the Department of Electrical and Computer Engineering, Minneapolis, MN 55455 USA (e-mail:
roych005@umn.edu; parhi@umn.edu).
D. D. Koozekanani is with the Department of Ophthalmology and Visual
Neuroscience, University of Minnesota, Minneapolis, MN 55455 USA (e-mail:
dkoozeka@umn.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2294635

consensus for the need of efficient and cost-effective DR screening systems [3]. Unfortunately almost 50% of diabetic patients
in the United States currently do not undergo any form of documented screening exams in spite of the guidelines established
by the ADA and the American Academy of Ophthalmology [3].
Statistics show that 60% of the patients requiring laser surgery
to prevent blindness do not receive treatment [2]. The major
reasons for this screening and treatment gap include insufficient
referrals, economic hindrances, and insufficient access to proper
eye care. Telemedicine, with distributed remote retinal fundus
imaging and grading at either local primary care offices or centralized grading remotely by eye care specialists, has increased
access to screening and follow-up necessary treatment [4].
Computer-aided screening systems (DREAM) have recently
gained importance for increasing the feasibility of DR screening,
and several algorithms have been developed for automated detection of lesions such as exudates, hemorrhages (HA), and microaneurysms (MA). However, MA are considered to be the primary indicators of DR, and among the noteworthy prior works
that aimed at detecting DR, in [5], color normalization and contrast enhancement were applied in the preprocessing steps on
fundus images followed by fuzzy C-means clustering for image
segmentation, and classification of candidate regions into bright
lesions and nonlesions. In [6], MA were detected using template matching in wavelet subbands followed by thresholding.
Another approach to detect MA based on multiscale correlation
filtering and dynamic thresholding was also developed in [7].
Another method based on the extraction of AM–FM features of
fundus images, followed by dimensionality reduction and hierarchical clustering, and the use of partial least-squares method
to assign DR severity grade has been presented in [8].
The novel contribution of this paper is a fully automated, fast
and accurate DR detection and grading system that can be used
for automated screening for DR and treatment prioritization.
So far an automated DR screening system, Medalytix [9], has
been used for screening normal patients without DR from abnormal patients with DR on a local dataset, with sensitivity in the
range 97.4–99.3% on diabetic patients in Scotland. The screening outcome combined with manual analysis of the images that
are classified as abnormal by the automated system has shown to
reduce the clinical workload by more than 25% in Scotland [9].
Another automated DR screening system grades images from
a local dataset for unacceptable quality, referable, nonreferable
DR with sensitivity 84% and specificity 64% [3] [10]. Both
these automated systems motivate the need for a fast and more
accurate DR screening and prioritization system. The proposed
DR detection system is capable of detecting lesions and generating a severity grade for nonproliferative DR (NPDR) in 6 s with

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1718

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

100% sensitivity and 53% specificity. Such a system will ensure
that no patient with DR is missed for follow-up treatment, and
will be critical in prioritizing eye-care delivery measures for
patients with highest DR severity.
The design of the proposed DREAM system is novel since
it aims at outlining 3 separate DR detection stages, and minimizing the run-time complexity of each stage to ensure a fast
detection system. We propose a feature size reduction followed
by selection of lesion classifiers that have low computational
complexity, such that the total time to detect retinopathy lesions
and to generate a severity grade is less than 6 s for a fundus
image. Additionally, the proposed system contains an initial image enhancement module in Stage 1 that enhances the contrast
and edge sharpness in fundus images so that images are not
rejected based on poor image quality. Existing automated DR
screening systems such as the VA screening system [10] rejects
about 18% of the images, while Medalytix [9] rejects about 5%
of the images based on their bad image quality. The proposed
system has not rejected any image so far on account of poor
image quality and hence, it is more suitable for automated DR
screening and prioritization.
This paper makes two major contributions. The first major
contribution is identification of the top 30 features from a set
of 78 features for classifying bright and red retinopathy lesions.
We implement feature ranking based on the feature weights
generated by AdaBoost [11]. We demonstrate how feature reduction reduces computational complexity of lesion classification by 94–97% in terms of mean processing time per image.
The second contribution is a novel two-step hierarchical binary
classification method that rejects false positives in the first step
and in the second step, bright lesions are classified as CWS
or hard exudates (HE), and red lesions are classified as HA,
and MA, respectively. This hierarchical classification method
reduces the time complexity by 18–24% over a parallel classification method that trains separate classifiers for identifying
CWS, HE, HA, and MA from false positives. While identifying the optimal bright and red lesion classifiers, we analyze a
variety of feature based classifiers such as GMM, kNN, SVM,
AdaBoost, and combinational classifiers, and select an optimal
classifier set that ensures high specificity for bright lesion classification and high sensitivity for red lesion classification, and
low computational time complexity. In our case, we found that
GMM is a preferred choice of classifier for detecting bright
lesions, and kNN is a preferred choice for detecting red lesions.
The algorithms used in Stage 1 for image segmentation, detection of optic disc, blood vasculature, and one-step classification
of red and bright lesions in Stage 2 for one public dataset have
been presented in our earlier work [12]. In this study, we extend
our analysis to construct a complete three-stage automated system that not only detects retinopathy lesions, but also generates a
DR severity grade for every fundus image. This paper introduces
a novel two-step hierarchical classification method in Stage 2, it
analyzes the importance of feature reduction and feature ranking, and it combines the number of lesions in Stage 3 to generate
a DR severity measure, and tests the performance of the proposed automated DR detection system on another public dataset.
The organization of this paper is as follows. In Section II,
the functionalities of each of the three-stages of the proposed

Fig. 1. Three-stage algorithm for grading DR severity using fundus images.
The system flow depicts the functionalities of the three individual stages and
their interconnections.

DR screening system is mathematically defined. Section III describes each of the three stages in separate sections. Results regarding the performance of lesion classification and DR severity grading, time complexity analysis, and comparison of the
proposed system with prior works are presented in Section IV
followed by concluding remarks and discussion in Section V.
II. METHOD AND MATERIALS
The three-stage algorithm to automatically detect and grade
the severity of DR using ophthalmic fundus images is shown in
Fig. 1. For each fundus image in JPEG format, the green plane
(Igreen ) is used for information extraction.
In Stage 1, a minimum-intensity maximum-solidity (MinIMaS) algorithm [12] is invoked to detect the regions corresponding to the optic disc (ROD ) and vasculature (Rvasc ) as the image
background from Igreen . Next, candidate regions corresponding
to red lesions (RRL ) and bright lesions (RBL ) are detected as
foreground (Rfore ). This is mathematically represented as
ROD , Rvasc , Rfore ⊆ Igreen

(1)

Rfore = RBL ∪ RRL .

(2)

In Stage 2, classifiers are used in two hierarchical steps. In
step 1, candidate lesion regions (RBL , RRL ) are classified as
true lesions (RTBL , RTRL ) and nonlesions (RNBL , RNRL ), respectively. In step 2, the true bright lesions are further classified
ˆ S ), while
as hard exudates (RˆHE ) and cotton wool spots (RCW
ˆ A ) and
the true red lesions are classified as microaneurysms (RM
ˆ
hemorrhages (RHA ). This is mathematically represented in (3)–
(5). The classified regions are compared to manually annotated
ground-truth regions (RHE , RCW S , RM A , RHA ).
RBL = RTBL ∪ RNBL , RRL = RTRL ∪ RNRL

(3)

ˆ S
= RˆHE ∪ RCW

(4)

ˆ A ∪ RˆHA .
RTRL = RM

(5)

RTBL

ROYCHOWDHURY et al.: DREAM: DIABETIC RETINOPATHY ANALYSIS USING MACHINE LEARNING

1719

In Stage 3, the number of red lesions and bright lesions are
counted and combined using a combination function (Ψ) defined
in (6)–(8) to generate a DR severity grade per image (G). An
image with G = 0 implies no DR, G = 1 implies mild DR,
G = 2 implies moderate DR and G = 3 implies severe DR,
respectively.
ˆ S (j  )| (6)
HE = | ∪j RˆHE (j)|, CWS = | ∪j  RCW
ˆ A (m)|, HA = | ∪m  RˆHA (m )| (7)
MA = | ∪m RM
∀I, G = Ψ(HE, CWS, MA, HA)
where,

G = {0, 1, 2, 3} .

(8)
(9)

A. Data
We train and test our DR screening system for normal patients
and patients with DR using two public datasets that are described
as follows.
1) DIARETDB1 dataset [13] consists of 89 images with 50◦
FOV. These 89 images have been separated to two groups:
28 training images and 61 test images, by the authors such
that the training and test datasets were independent of one
another. All the 89 images are manually annotated for
lesions as ground truth (RHE , RCW S , RM A , RHA ).
2) MESSIDOR dataset [14] contains 1200 images with 45◦
FOV. Each image is manually assigned a DR severity
grade.
B. Lesion Classification
Once each fundus image is segmented in stage 1, foreground
regions corresponding to bright (RBL ) and red (RRL ) retinopathy lesions can be detected as candidate objects. However, classification of these candidate lesion objects is imperative to reduce
instances of false positives, and to categorize bright lesions as
hard exudates (HE) or cotton wool spots (CWS), and red lesions
as HA or MA, respectively. Thus, to achieve lesion classification, we extract features corresponding to each candidate object,
as explained in Section III-B. Every candidate object can be represented by a feature vector (x) corresponding to the location
of the object in the feature space. While training classifiers, the
feature vectors of the objects from the training set of images are
scaled in [0,1]. If the object is manually annotated as a lesion,
then a class label y = 1 is assigned to the object. Based on the
groundtruth, if the object is not a lesion, then y = 0. Object
vectors from the test set of images (x∗ ) are similarly scaled in
[0,1], and the class labels determined using groundtruth (y ∗ ) are
compared with the class labels, assigned by the classifiers (yˆ∗ ),
which correspond to class 1 or class 0.
The individual classifiers analyzed here can be categorized
into two kinds based on the computational complexity. The first
kind of classifiers incur low computational complexity such as
kNN [15], and GMM [16] with two multivariate Gaussians, one
Gaussian corresponding to class 0 and another to class 1. The
second kind of classifiers with high computational complexity
include kernel space SVM [17] [18] and AdaBoost [19], [11].
The Bayesian combination of these two categories of classifiers
using the Dempster–Shafer theory [20] is also analyzed. Such

Fig. 2. Original blurry images are enhanced by spatial filtering. (a) and
(c) Fundus image (I), (b) and (d) Filtered image with enhanced red lesions
marked in red circles.

a combinational classifier, SVM+GMM combines the independent prior probabilities from the SVM and GMM classifiers, and
SVM+kNN combines the priors from SVM and kNN, respectively. These classifiers are implemented in MATLAB and the
priors for the SVM classifier for the combinational classifiers
are computed using the libSVM toolbox [21].
III. PROPOSED SYSTEM
In the proposed automated system, all images are preprocessed first to eliminate false photographic artifacts and illumination inconsistencies. The preprocessing module proceeds
by histogram equalization and contrast enhancement on Igreen ,
followed by scaling all pixel intensities in the range [0,1] resulting in image Im . Next, since the quality of images vary among
databases, it is necessary to enhance the sharpness and illumination of certain images especially when the images are scanned
film prototypes. Im is filtered using the Laplacian of Gaussian
filter to extract the gradient variations (Ih ). Next, (Im − Ih )
is median filtered to obtain the enhanced images as shown in
Fig. 2. Following the preprocessing module, the three detection
stages are invoked for DR severity detection per image.
A. Stage 1: Image Segmentation
In the first stage, it is imperative to mask out the regions
corresponding to the optic disc (OD) and major portions of the
blood vasculature. This is important since a bright OD may
otherwise be mistaken for a bright lesion, and the vasculature
can be falsely detected as a red lesion in subsequent automated
stages if not masked out in the early stage. For this purpose, a
region-based MinIMaS algorithm [12] is invoked, which detects
the regions that lie on the intersection of the largest red region
and the bright regions in the image. Once all such intersecting

1720

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Fig. 3. Detection of OD, vasculature, and foreground candidate regions. (a) Illumination-corrected preprocessed image. (b) Intersecting red regions and bright
regions (blue), out of which OD is selected as the bright region with highest solidity. (c) OD (R O D ) detected. (d) vasculature (R va sc ) detected. (e) Candidate
bright lesion regions (R B L ) detected. (f) Candidate red lesion regions (R R L ) detected.

bright regions are identified (R), then the region containing the
OD (ROD ⊂ R) is the bright region with minimum pixel intensity sum (Intensity, due to the dark pixels corresponding
to thick blood vessels occurring at the OD region), and maximum compactness (Sol) as represented in (10). If the region
with minimum pixel intensity sum is not the same as the region with maximum solidity (i1 	= i2 ), then the region with
minimum pixel intensity sum is discarded (R = R\Ri 1 ), and
(10) is reevaluated for the remaining regions till convergence is
achieved.
If R = {R1 , R2 , . . . Rr } , i ∈ {1, 2, . . . r}

(10)

Λ(R) = [i1 , i2 : i1 = arg min Intensity(Ri ),
i

i2 = arg max Sol(Ri )].
i

Ri 1 = Ri 2
ROD (Λ(R)) =
ROD (Λ(R = R Ri 1 ))

: i1 = i2
(11)
: i1 	= i2 .

After ROD detection, to detect major portions of the blood
vasculature (Rvasc ), each image is gradient smoothened by median filtering (Ibg ), and subtracted from image Im to obtain the
shade- corrected image (Isc = Im − Ibg ), followed by thresholding and region growing. The largest red region extracted after
region growing corresponds to Rvasc .
Once the OD and blood vessels are detected and masked out
as background regions, foreground candidate regions (Rfore )
that may represent retinopathy lesions are detected, thereby resulting in a segmented image [12]. This process is shown in
Fig. 3. Once the fundus images are segmented and the foreground candidate regions are detected, the second stage of the
algorithm is invoked that classifies the bright candidate regions
(RBL ) and red candidate regions (RRL ) as retinopathy lesions.

B. Stage 2: Lesion Classification
Following foreground detection, for all the six classifiers under analysis, a training dataset was used to obtain the optimal
parameters for classification, and a separate test dataset was
used to analyze their performance.
Lesion classification is performed in two hierarchical binary
(1/0) classification steps. In the first step, the bright candidate
regions (RBL ) are classified as true bright lesions (RTBL , class
1) and nonbright lesions (RNBL , class 0), while the red candidate
regions (RRL ) are classified as true red lesions (RTRL , class 1)
and nonred lesions (RNRL , class 0), respectively. In the second
level, the true bright lesions are reclassified as cotton wool spots
ˆ S , class 1) and hard exudates (RˆHE , class 0), while the
(RCW
ˆ A , class
true red lesions are classified as microaneurysms (RM
ˆ
1) and hemorrhages (RHA , class 0).
Selection of features is an important aspect for lesion classification. Prior works have introduced feature sets comprising of
13 features [22] using the Spencer–Frame system, while extensions of these features up to 68 features for lesion classification
have been presented in [23]. Motivated from these prior works,
in this paper, we analyze a set of 78 features that consists of
most of the features from [22] and [23] along with additional
structural features that are obtained using the “regionprops”
command in MATLAB. Here, 14 features are structure-based
features such as area, convex area, solidity, orientation, etc., and
16 features correspond to the mean, minimum, maximum, and
standard deviation of pixels within each object region in the
green plane (Igreen ), red plane (Ired ), saturation plane (Isat ),
and intensity plane (Iinten ) of the RGB to HSI converted image. The 14 structure-based features are useful primarily for
separating the nonlesion regions from the lesion candidates.
For example, bright lesions (HE and CWS) in general have a
higher distance from the OD region compared to the nonlesions.

ROYCHOWDHURY et al.: DREAM: DIABETIC RETINOPATHY ANALYSIS USING MACHINE LEARNING

TABLE I
FEATURES FOR CLASSIFICATION

1721

Corresponding to each foreground candidate object for bright
and red lesions, 78 features are extracted (x), and each region is
manually annotated as a lesion or nonlesion (y). Next, the Adaboost algorithm in (12) is invoked to generate feature weights
based on the linear discriminating performance of each feature,
followed by feature ranking. The top 30 features were the ones
with nonzero positive feature weights for classifying bright and
red lesions.
While implementing Adaboost for feature selection [11], the
weight of each object (i) is first initialized (W1,i ). Next, we
analyze the F = 78 features over T = 100 iteration rounds.
For each feature “j,” at iteration “t,” a linear classifier classifies sample xi as ht,j (xi ), such that the classification outcome
ht,j (xi ) = 0 or 1. The classification error due to this one-feature
based classifier on all samples is t,j . At any iteration “t,” the
classifier with minimum t,j is chosen as t . This error (t ) is
then used to update the sample weights, such that the misclassified samples are weighted more than the correctly classified
samples. The final resulting classifier is a weighted linear combination of the linear classifiers in each iteration step as shown
in (13). Also, after “T” iterations, the weights of selected discriminating features in each iteration are combined to find the
weight of each feature in the final classifier in (14). Features
are then ranked in the descending order of their contributing
weights.
Initialize : W1,i = 1/N, t = 1 : T

(12)

Def ine : ∀j = 1 : F, ht,j (xi ) ∈ [0, 1]
t,j =

N


Wt,i . |ht,j (xi ) − yi |,

i=1

For the red lesion classification, the nonlesion regions generally
represent fine blood vessels and hence the nonlesions on an average have a smaller distance from the blood vessels than the
red lesions. Also, the blood vessels are elongated structures,
and hence for the nonlesion regions, the ratio of major axis to
minor axis lengths is higher than the HA or MA. For the second
step of hierarchical bright lesion classification, hard exudates
(HE) are compact bright regions with well defined boundaries
as compared to the CWS that are larger in area, and less compact (low solidity). The remaining 48 features correspond to
the mean and standard deviation of six second-order derivative
images corresponding to coefficients of the constant, first- and
second-order horizontal derivative and vertical derivative images ([1, Ix , Iy , Ixy , Ixx , Iy y ]) scaled in [0,1], using a Gaussian
filter with zero mean and σ 2 = [1, 2, 4, 8]. These 48 features are
useful for the second step of hierarchical classification.
In this study, we selected the top 30 features shown in Table I
out of 78 features that were ranked in the decreasing order
of feature weights generated by AdaBoost 12 [19]. For this
feature selection operation, we used the 89 images from the
DIARETDB1 [13] dataset. This process of feature selection
for improving the classification performance and computational
speed of the algorithm can be viewed as analogous to reverse
feature elimination for horizontal pruning in decision tree-based
learning as mentioned in [24].

ht = arg min t,j , t = min t,j
j

j

Stop iterating if t ≥ 0.
Wt,i exp(αt (eri ))
Zt


1 − t
1
αt = ln
, Zt = Normalizer
2
t


αt : t = t,j
−1 : ht (xi ) = yi
eri =
α =
1 : ht (xi ) 	= yi . t,j
0:
t 	= t,j .
⎧
T

⎪
⎨1 :
αt .ht (x∗ ) > 0
∗
F inally : yˆ =
(13)
t=1
⎪
⎩
0 : otherwise.

T
t=1 αt,j
F eature W eight : S(j) = 
F 

.
(14)
T
j
t=1 αt,j
U pdate : Wt+1,i =

Following feature selection, each foreground candidate object is
then classified using the top 30 features described in Table I. The
feature vector (x) corresponding to each object in the feature
space is computed. In the training phase of lesion classification,
the 30-dimensional feature space, with each dimension representing a particular feature, is populated with the objects from
the training set of images. The position of each object in the

1722

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Fig. 4. Two-step hierarchical classification of lesions for an image from DIARETDB1. In Stage 1 of the automated system, background regions corresponding to
vasculature and optic disc are first detected. Candidate regions for bright (R B L ) and red lesions (R R L ) are then detected as the foreground. In Stage 2, hierarchical
two-step classification is performed for identification of the type of lesions. In the first hierarchical classification step, foreground regions for bright lesions are
classified as true lesions (red, R T B L ) and nonlesions (blue, R N B L ), and candidate regions for red lesions are classified as true red lesions (red, R T R L ) and
ˆ E ), and cotton wool spots
nonlesions (blue, R N R L ). In hierarchical second-step classification, true bright lesions are classified into hard exudates (yellow, R H
ˆ A ), and hemorrhages (black, R H
ˆ A ). Corresponding to the 30 features mentioned
(pink, R CˆW S ), while true red lesions are classified as microaneurysms (red, R M
in Table I, the average feature values for all the candidate lesion regions in the sample image is presented in the adjoining table (i.e., f1 corresponds to feature
with rank 1, which is area of the region). The features measuring distance are in terms of pixels, while the mean and variance of intensity values are scaled in [0,
1] range.
TABLE II
THE LESION COMBINATION (ψ) OPERATION PROPOSED FOR MESSIDOR [14]

feature space is determined by the feature vector (x), and the
class labels of the objects belonging to the training dataset (y)
are used to determine the class labels of objects belonging to
the test data (yˆ∗ ) in the testing phase. The two-step hierarchical classification approach is illustrated for an image from the
DIARETDB1 database in Fig. 4.
After the lesions are classified for each image, the next task is
to fuse the information regarding the lesions detected to generate
a DR severity level for each image [3].
C. Stage 3: DR Severity Grading
Once the regions corresponding to the retinopathy lesions
are detected, and the number of HA, MA, HE, and CWS are
computed per image using (6) and (7), lesion combination (ψ)
operation defined in Table II for the MESSIDOR [14] dataset
can be used to generate a DR severity grade per image (G). The
impact of lesion combination on DR severity has been studied
in [25] and [26] to derive the DR severity grading gold standards.
These studies demonstrate that clinically relevant lesion combination operations require more accurate estimation regarding the

number of red lesions than bright lesions. In the case of bright
lesions, over detection, or instances of false positives may imply
macular degeneration or retinal abnormalities other than NPDR.
Thus, the detection of bright lesions must incur less false positives. However, for red lesion detection, failure to detect lesions
will result in false interpretation of the DR severity. Thus, it is
imperative for any automated system to incur low false negatives
for red lesion classification. Hence, the performance criteria for
selecting lesion classifiers are as follows.
1) Bright lesion classifier: It must incur low false positives,
or high specificity.
2) Red lesion classifier: It must incur low false negatives, or
high sensitivity.
IV. RESULTS
We analyze the performance of the three stages of the DR
screening systems individually. In the first image segmentation
stage, the detection of the OD region plays a vital role to ensure
fewer instances of false positive while detecting bright lesions.
This segmentation algorithm has shown to have an accuracy of
99.7% for OD segmentation on public datasets [12].
The metrics used for analyzing the performance of the second
and third stages of the detection system are defined in terms of
true positives (TP), true negatives (TN), false positives (FP), and
false negatives (FN) as follows:
TP ;
1) sensitivity (SEN)= TP+FN
TN ;
2) specificity (SPEC)= TN+FP
3) area under ROC curve (AUC).

ROYCHOWDHURY et al.: DREAM: DIABETIC RETINOPATHY ANALYSIS USING MACHINE LEARNING

1723

TABLE III
SEN/SPEC FOR TWO HIERARCHICAL STEP BRIGHT LESION CLASSIFICATION ON DIARETDB1 DATASET

TABLE IV
SEN/SPEC FOR FOR TWO HIERARCHICAL STEP RED LESION CLASSIFICATION ON DIARETDB1 DATASET

The final goal of our DR detection system is to classify the
fundus images that are free from retinopathy lesions as normal, and to classify the images with bright and red lesions as
abnormal. The receiver operating characteristic (ROC) curves
are obtained by varying the threshold probability in increments
of 0.02 in the range [0,1], and observing the variation in SEN
versus (1-SPEC) each time. The following sections present the
selection of the best set of classifiers for bright and red lesion
classification that have comparatively high SEN, SPEC, and
AUC. The DREAM system applies the classifiers that have a
consistent performance across both datasets.
A. Feature Set and Classifier Evaluation for Dataset 1:
DIARETDB1
Classification of bright lesion and red lesions versus nonlesions for the DIARETDB1 dataset has been analyzed using
kNN, GMM, and SVM classifiers in [12]. However, in this study,
we extend the analysis to two-hierarchical step classification, to
assess the impact of combination classifiers, and to analyze the
incremental importance of the classifying feature set. In lesion
classification analysis, a TP region is one which is classified
as a lesion and that has more than 50% of its pixels manually

marked as a lesion with greater than 50% confidence by the
authors of [13].
In Section III-B, Table I, top 30 features computed using
AdaBoost, for bright and red lesion classification, are ranked
in the decreasing order of their contribution to lesion classification. To analyze the importance of these 30 features for
lesion classification, we selected the top “F” features, where
F= {5, 10, 15, 20, 25, 30}, and observed the lesion classification performance on the test set of images from the DIARETDB1 dataset. In the first hierarchical classification step,
candidate regions for bright and red lesions are separated from
false positives. The SEN/SPEC of each classifier with varying “F” is shown in Tables III and IV for bright lesion and
red lesion classification, respectively. The threshold probabilities obtained from the ROC curves for evaluating a candidate
region as a bright or red lesion in hierarchical Step 1 classification for {GMM, kNN, SVM+GMM,SVM+kNN} are approximately {0.4, 0.3, 0.45, 0.35}, respectively. For Step 2 classification, threshold probability is about 0.45 for all probabilistic
classifiers. For the SVM and AdaBoost classifiers, a single operating point is observed since the final classifier is based on the
sign of the decision principle. Also, we observe that AdaBoost
generates more FNs compared to SVM, which is undesired for

1724

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

lesion classification. It is noteworthy that prior to this lesion
classification step, the top 30 features were selected using AdaBoost on all the 89 images from the DIARETDB1 dataset to
obtain a better fitted feature selection strategy than using the
training set of 28 images.
To compare the performance of SVM and AdaBoost with
other classifiers for Step 1 bright lesion classification, using
the 30 feature set, we observe that at SEN = 0.98, GMM,
kNN, SVM+GMM, SVM+kNN have SPEC = {0.68, 0.45,
0.1, 0.49}, respectively. At SEN = 0.92, GMM, kNN,
SVM+GMM, SVM+kNN have SPEC = {0.82, 0.67, 0.41,
0.71}, respectively. Thus, from Table III, we observe that for
bright lesion classification, the GMM classifier has the highest achievable specificity at a given sensitivity for the most
important Step 1 of hierarchical classification. In Step 2 of hierarchical classification, all classifiers demonstrate comparable
performance. Thus, hard exudates can be very well separated
from CWS using the 30 feature set. Also, due to comparable
classification performances in Step 2, the low complexity GMM
classifier is preferable.
To compare the performance of SVM and AdaBoost with the
other classifiers for Step 1 red lesion classification, using the 30
feature set, we observe that at SPEC = 0.99, SEN of GMM,
kNN, SVM+GMM, SVM+kNN is {0.67, 0.68, 0.63, 0.63},
respectively. And at SPEC = 0.97, SEN of GMM, kNN,
SVM+GMM, SVM+kNN is {0.69, 0.72, 0.64, 0.65}, respectively. Also, for Step 1 classification using AdaBoost classifier
with F = {5, 10} feature set, the training process stops after
T = {68, 43} iterations, respectively, instead of completing the
T = 100 iterations for model selection. These correspond to the
entries marked by “[SEN/SPEC]” in Table IV. The reason for
these incomplete training processes is that the weighted error
in classification exceeds random error of 0.5. Once the feature
set exceeds 15 and more features, AdaBoost classification performance improves due to appropriate model selection after the
complete training process.
From Table IV, we conclude that for red lesion classification,
kNN achieves highest SEN for a given SPEC. Also, for Step
2 of hierarchical red lesion classification, all classifiers have
comparable performances implying that MA can be very well
separated from hemorrhages using the 30 feature set. Hence, the
30 feature set is adequate for lesion classification.
B. Classifier Evaluation for Dataset 2: MESSIDOR
For the MESSIDOR dataset, classifiers are trained on 89
images from the DIARETDB1 dataset with 30 features and
tested on 1200 images for detection of hemorrhages and microaneurysms in each image. In this study, we do not analyze
instances of neovascularization or proliferative DR, and we utilize the lesion combination (ψ) operation defined in Table II.
According to the reference standard [14], out of the 1200 images, a total of 546 images are normal (G = 0), and 654 are abnormal images (G > 0) presenting signs of DR. The DR images
comprise of 153 images with retinopathy grade 1 247 images
with retinopathy grade 2, and 254 images with retinopathy grade
3. Using the 89 images from the DIARETDB1 for training, the

TABLE V
RESULTS OF CLASSIFICATION AND LESION COMBINATION ON
MESSIDOR DATASET

best values of k, for a kNN classifier, were found as k = 11 for
Step 1 red lesion classification (RTRL vs RNRL ), and k = 3 for
ˆ A from RˆHA .
Step 2 classification of RM
The performance of classifying normal images (G = 0) from
all other images with nonzero diabetic grades (G = 1, 2, 3) is
depicted in Table V. The number of images with nonzero DR
grade that are falsely graded as G = 0 are represented as #F N
in Table V. The operating point in this analysis is selected
where SEN is greater than 0.9 and SPEC is approximately
0.5 [27], such that the threshold probabilities for Step 1 red lesion classification for GMM, kNN, SVM+GMM, SVM+kNN
are {0.4, 0.3, 0.45, 0.35}, respectively. For Step 2 classification,
the threshold probability is 0.45.
From Table V, we observe that SVM+kNN and kNN have
good SEN/SPEC classification combination for separating normal images from abnormal ones. In such situations, the kNN
classifier is preferable since it has lower computational complexity. Also, we observe the AdaBoost introduces the highest number of FNs (abnormal images incorrectly classified as
normal), which is undesired. Besides, from Table V, it is evident that the lesion combination operation reduces the impact of lesion classification. However, when the number of images being screened for DR becomes significantly large, such
as in the case of the MESSIDOR dataset, slight variations in
classification performances can be significant. For example, in
Table V, for the normal to abnormal image classification, kNN
and SVM+GMM have SEN/SPEC 1/0.5316, 0.9916/0.5492,
respectively. Although, these performances appear similar, the
significant difference is that the SVM+GMM classifier falsely
classifies 10 images with DR as normal images. As the number of patients being screened for DR increases, the number
of missed screenings will grow, which is undesired from an
automated screening system.
Additionally, from Table VI, we observe that combination
classifiers do not show significant improvement over single classifiers in both datasets. Also, combination classifiers introduce

ROYCHOWDHURY et al.: DREAM: DIABETIC RETINOPATHY ANALYSIS USING MACHINE LEARNING

TABLE VI
AUC ASSESSMENT ON 3 DATASETS

TABLE VII
TIMING ANALYSIS PER IMAGE IN SECONDS

computational complexity, thus discouraging their use for lesion
classification.
The important conclusion from the classification performance
analysis of the two datasets is that, GMM may be preferred for
classification of bright lesions, while kNN outperforms GMM
and SVM for red lesion classification. Thus, for our three-stage
screening system (DREAM), we use GMM for classification of
bright lesions and kNN for classification of red lesions.
C. Time complexity Analysis of the DREAM system
The time complexity of the proposed DREAM system for red
lesion and bright lesion detection is analyzed in terms of the time
required to classify the lesions and grade images using MATLAB programs. On a Laptop (2.53-GHz Intel Core i3 and 2 GB
RAM), the times required per image to classify candidate objects corresponding to lesions on the image from DIARETDB1,
and to grade images from the MESSIDOR dataset are shown in
Table VII.
In the DIARETDB1 dataset, for each test image, the time
required to classify bright candidate objects into HE/CWS, and
the red candidate objects into MA/HA after rejecting false positives are analyzed using three classification methods. The first
method (M1) uses 78 features for classifying bright objects using a GMM classifier and red lesions using the kNN classifier in
two hierarchical steps. The second method (M2) uses the top 30
features for classification, but it uses two separate GMM classifiers in parallel for discriminating CWS from nonbright lesions,
HE from nonbright lesions, and two separate kNN classifiers in
parallel for separating MA from nonred lesions and HA from
nonred lesions, respectively. The third method (M3) follows the
two-step hierarchical classification using 30 features as defined
in (3–(5). In the MESSIDOR dataset, all the three classification
methods (M1, M2, M3) are used for red lesion classification
only followed by generating a DR severity grade per image.

1725

TABLE VIII
COMPARISON OF LESION DETECTION PERFORMANCE (%) ON DIARETDB1

Table VII shows a 97% time complexity reduction (from
71.63 to 2.06 s) for images from DIARETDB1, and 94.18%
reduction (from 59.54 to 3.46 s) for images from MESSIDOR
dataset in the average bright and red lesion detection time required per image. This reduction in time complecity is achieved
by reducing the number of features from 78 to 30 and applying
the proposed two-step hierarchical classification method.
Next, we observe that the proposed two-step hierarchical classification method (M3) incurs 18.5% less time than the parallel
classification method M2 (2.06s versus 2.53s) for DIARETDB1
images, and 24.45% less time (3.46s versus 4.58s) for MESSIDOR images. The best SEN/SPEC obtained by M2 and M3 on
the DIARETDB1 and MESSIDOR datasets are 0.82/0.94, and
1/0.53, respectively.
D. Comparison With the Prior Work
Comparative performance analysis of the proposed system
(DREAM) in classifying bright and red lesions to existing works
is shown in Table VIII. The SEN/SPEC of the DREAM system
is chosen from the ROC curves corresponding to the GMM classifier with 30 feature set and threshold probability 0.7 for bright
lesion classification, and kNN classifier with 30 feature set and
threshold probability 0.3 for red lesion classification, respectively. We observe that the DREAM system has a favorable Step
1 classification performance for bright and red lesions compared
to existing methods. Additionally, the proposed system has 95%
sensitivity and 90% specificity for separating microaneurysms
from hemorrhages, and 99% sensitivity, 97% specificity for separating HE from CWS. From Table VIII, we observe that prior
works on lesion classification using the DIARETDB1 dataset
also demonstrated a preference for high SPEC for bright lesion
classification and high SEN for red lesion classification, which
is in accordance with our classifier evaluation criteria.
Comparing the performance of classification and image grading of our system with existing works on the public dataset MESSIDOR [14], the proposed system achieves higher sensitivity,
specificity, and AUC, as shown in Table IX. Another ARVO
2008 version of the method in [30] tested on 400 images from
this dataset achieved an AUC of 0.89 in classifying images with
grade 0 and grade 1 from the images with a higher retinopathy
severity grade. Our system outperforms all existing methods
that classify all the 1200 images from the MESSIDOR dataset.
In Table IX, the method [32] classifies normal from abnormal
images by subsampling from the MESSIDOR image set to train,
validate, and test neural network classifiers. Thus, this method
classifies about 300 images or less out of the 1200 images. Also,

1726

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

TABLE IX
COMPARISON OF DR SEVERITY GRADING PERFORMANCE FOR SEPARATING
NORMAL FROM ABNORMAL IMAGES

the performance of other DR severity grading methods on various local datasets is shown in Table IX. We observe that only
the proposed system achieves 100% sensitivity, which is favorable over existing grading systems since no patient with DR is
missed while screening using the DREAM system.
V. CONCLUSION AND DISCUSSION
In this paper, we have proposed a three-stage computer-aided
screening system for DR (DREAM) that detects and grades
fundus images for the severity of NPDR. In this study, we have
identified a set of best 30 features out of 78 features using feature
scoring by AdaBoost [11], such that the 30 feature set classifies
bright and red lesions with AUC greater than 0.83 using the classifiers such as kNN, GMM, and Bayesian combination of probabilistic classifiers (SVM+GMM, SVM+kNN). Additionally,
time complexity analysis of the proposed two-step hierarchical
classification method using all the 78 features and the reduced
30 feature set demonstrates computational speed enhancement
by more than 94% by this feature reduction operation. This time
complexity analysis is a significant contribution, since to our
best knowledge no prior works have reported the time complexity of the DR detection systems. The final 30 features are
capable of classifying lesions under 6 s per image, and these 30
features are scalable across image datasets with varying fields
of view.
It is noteworthy that the first stage of the DREAM system is
an important step for correctly detecting the presence of DR.
Among all the images analyzed in this paper from the public
dataset of DIARETDB1 and MESSIDOR (which includes 1289
fundus images), the optic disc (OD) and blood vessels have always been correctly segmented. However, it is possible that for
some fundus images with additional retinal pathologies such as
myelination or peripapillary atrophy, the entire OD region is not
completely masked out. This might result in an additional bright
lesion being detected by the DREAM system. Also, since the
minimum distance from the OD region is a feature for lesion
classification in the second stage of the system, erroneous segmentation of the OD might lead to minor inaccuracies in lesion
classification. However, this classification error is significantly
small since the error appears due to 1 out of 30 un-weighted
features, and since the minimum distance from OD is an

Fig. 5. DREAM system on public domain images with severe retinal degeneration. (a) and (d) Original image. (b) and (e) OD region detected with error.
(c) and (f) Vasculature detected in first stage. Both images (a) and (d) are
classified as images with DR by the DREAM system.

important classification feature for mainly bright lesions regions. Since bright lesions are not considered for evaluating the
severity of DR by the proposed system, this false detection of the
OD region will not impact the final DR severity grade produced
by the DREAM system.
In instances when the retina severely degenerates, such as the
examples shown in Fig. 5, where retinal vein occlusions occur,
the DREAM system classifies such images as abnormal, even if
the patients do not suffer from DR. Such fundus images would
result in false positive outcomes by the automated screening
system. However, all false positive outcomes will be further
manually assessed by retinal specialists to determine follow-up
treatment procedures. Hence, a patient with retinal vein occlusion will be detected as abnormal by the DR screening system,
and the patient will be further manually assessed.
The lesion classification problem in the second stage of the
DREAM system deals with unbalanced datasets. Hence, it is
important to discuss the performance of the lesion classifiers on
the unbalanced lesion candidate regions. A trained SVM classifier generally tends to classify each test sample as belonging
to the majority class [17] [18], thereby causing a biased classification outcome. The AdaBoost classifier is accuracy-oriented,
which means that its weighting strategy may be biased toward
the majority class if it contributes more to the overall classification accuracy as mentioned in [39]. To ensure low computational complexity for the proposed DR screening system,
cost-sensitive SVM and AdaBoost classifiers [39] have not been
assessed in this study. Future works will be directed toward analyzing the impact of cost-sensitive SVM and AdaBoost for
lesion classification. The GMM classifier is weighted by the
number of samples belonging to class 1 or class 0 at the end
of the expectation maximization operation, and hence it is quite
robust to handling unbalanced datasets as mentioned in [40].
The kNN classifier has a localized impact while classification
since each data sample looks at its immediate neighborhood to
decide its class label. Existing works [41] have shown that kNN
is robust to imbalance in dataset caused due to a high number
of negative samples. In this study, we observe that for the red
lesion classification problem, where the numbers of negative

ROYCHOWDHURY et al.: DREAM: DIABETIC RETINOPATHY ANALYSIS USING MACHINE LEARNING

samples are 4 times more than the number of positive samples,
kNN is the best classifier, an observation which is in accordance with the existing works [41]. Combination of SVM with
kNN or GMM classifiers using the Dempster–Shafer theory [20]
does not significantly enhance the lesion classification performance since SVM is sensitive to the imbalance in the data, and
hence, the classification performance of the combined classifiers
is dampened. However, other classifier combination strategies
may enhance the lesion classification performance. Future work
will be focused on the assessment of other classifier combination
strategies for the lesion classification problem.
Although the proposed DREAM system outperforms all existing DR screening systems, further improvement in DR classification specificity may be possible by retraining the lesion classifiers for every new test set of images, by using cost-sensitive
SVM or AdaBoost classifiers for lesion classification, or by using other cost-sensitive classifier combination algorithms. Future work will be directed toward further enhancement of the
DR classification performance. However, classifier retraining
or cost-sensitive classifiers may incur additional computational
time complexity. Additional future work can be directed toward the detection of neovascularization and vascular beading caused by proliferative DR, and drusen caused by macular
degeneration.
ACKNOWLEDGMENT
The authors would like to thank the anonymous reviewers and
the Associate Editor for their numerous constructive comments.
DISCLOSURE
Two provisional patent applications have been filed on the
contents of this paper.

[10]

[11]
[12]
[13]

[14]
[15]

[16]
[17]
[18]
[19]
[20]
[21]
[22]

[23]

REFERENCES
[1] American Diabetes Association. (2011, Jan. 26). Data from the 2011
national diabetes fact sheet. [Online]. Available: http://www.diabetes.org/
diabetes-basics/diabetes-statistics/
[2] S. Garg and R. M. Davis, “Diabetic retinopathy screening update,” Clin.
Diabetes, vol. 27, no. 4, pp. 140–145, 2009.
[3] M. D. Abramoff, M. Niemeijer, and S. R. Russell, “Automated detection
of diabetic retinopathy: Barriers to translation into clinical practice,” Expert Rev. Med. Devices, vol. 7, no. 2, pp. 287–296, 2010.
[4] J. Cuadros and G. Bresnick, “Eyepacs: An adaptable telemedicine system
for diabetic retinopathy screening,” J. Diabetes Sci. Technol., vol. 3, no. 3,
p. 509516, May 2009.
[5] A. Osareh, M. Mirmehdi, B. Thomas, and R. Markham, “Classification
and localisation of diabetic-related eye disease,” in Proc. Computer Vision
Eur. Conf. Comput. Vis., 2006, vol. 2353, pp. 325–329.
[6] G. Quellec, M. Lamard, P. Josselin, and G. Cazuguel, “Optimal wavelet
transform for the detection of microaneurysm in retina photographs,”
IEEE Trans. Med. Imag., vol. 27, pp. 1230–1241, Sep. 2008.
[7] B. Zhang, X. Wu, J. You, Q. Li, and F. Karray, “Detection of microaneurysms using multi-scale correlation coefficients,” Pattern Recognit.,
vol. 43, no. 6, pp. 2237–2248, 2010.
[8] C. Agurto, V. Murray, E. Barriga, S. Murillo, M. Pattichis, H. Davis,
S. Russell, M. Abramoff, and P. Soliz, “Multiscale AM-FM methods for
diabetic retinopathy lesion detection,” IEEE Trans. Med. Imag., vol. 29,
no. 2, pp. 502–512, Feb. 2010.
[9] G. S. Scotland, P. McNamee, A. D. Fleming, K. A. Goatman, S. Philip,
G. J. Prescott, P. F. Sharp, G. J. Williams, W. Wykes, G. P. Leese, and
J. A. Olson, “Costs and consequences of automated algorithms versus

[24]
[25]

[26]
[27]

[28]
[29]

[30]

[31]

1727

manual grading for the detection of referable diabetic retinopathy,” Brit.
J. Ophthalmol., vol. 94, no. 6, pp. 712–719, 2010.
M. D. Abramoff, M. Niemeijer, M. S. Suttorp-Schulten, M. A. Viergever,
S. R. Russell, and B. van Ginneken, “Evaluation of a system for automatic detection of diabetic retinopathy from color fundus photographs in
a large population of patients with diabetes,” Diabetes Care, vol. 31, no. 2,
pp. 193–198, Feb. 2008.
L. Shen and L. Bai, “Abstract adaboost gabor feature selection for classification,” in Proc. Image Vis. Comput., 2004, New Zealand, pp. 77–
83.
S. Roychowdhury, D. D. Koozekanani, and K. K. Parhi, “Screening fundus images for diabetic retinopathy,” in Proc. Conf. Record 46th Asilomar
Conf. Signals, Syst. Comput., 2012, pp. 1641–1645.
T. Kauppi, V. Kalesnykiene, J.-K. Kmrinen, L. Lensu, I. Sorr, A. Raninen,
R. Voutilainen, H. Uusitalo, H. Klviinen, and J. Pietil, “Diaretdb1 diabetic
retinopathy database and evaluation protocol,” in Proc. 11th Conf. Med.
Image Understand. Anal., 2007, pp. 61–65.
Methods to evaluate segmentation and indexing techniques in the
field of retinal ophthalmology. (2011, Sep. 23). [Online]. Available:
http://messidor.crihan.fr/download-en.php
M. Niemeijer, B. van Ginneken, J. Staal, M. Suttorp-Schulten, and
M. Abramoff, “Automatic detection of red lesions in digital color fundus photographs,” IEEE Trans. Med. Imag., vol. 24, no. 5, pp. 584–592,
May 2005.
A. Osareh, B. Shadgar, and R. Markham, “Comparative pixel-level exudate recognition in colour retinal images,” Image Anal. Recognit.,
vol. 3656, pp. 894–902, 2005.
L. Xu and S. Luo, “Support vector machine based method for identifying
hard exudates in retinal images,” Proc. IEEE Youth Conf. Inf., Comput.
Telecommun., pp. 138–141, Sep. 2009.
U. R. Acharya, C. K. Chua, E. Y. Ng, W. Yu, and C. Chee, “Application of
higher order spectra for the identification of diabetes retinopathy stages,”
J. Med. Syst., vol. 32, pp. 481–488, 2008.
V. Cherkassky and F. Mullier, Learning From Data. New York, NY,
USA: Wiley, 1998.
G. Shafer, A Mathematical Theory of Evidence. Princeton, NJ, USA:
Princeton Univ. Press, 1976.
C.-C. Chang and C.-J. Lin. (2011). LIBSVM: A library for support vector
machines. ACM Trans. Intell. Syst. Technol. [Online]. 2, pp. 27:1–27:27.
[Online]. Available: http://www.csie.ntu.edu.tw/ cjlin/libsvm
A. Frame, P. Undrill, M. Cree, J. Olson, K. McHardy, P. Sharp, and
J. Forrester, “A comparison of computer based classification methods
applied to the detection of microaneurysms in ophthalmic fluorescein
angiograms,” Comput. Biol. Med., vol. 28, pp. 225–238, 1998.
M. Niemeijer, B. van Ginneken, J. Staal, M. Suttorp-Schulten, and
M. Abramoff, “Automatic detection of red lesions in digital color fundus photographs,” IEEE Trans. Med. Imag., vol. 24, no. 5, pp. 584–592,
May 2005.
L. Talavera, “Feature selection as retrospective pruning in hierarchical
clustering,” Adv. Intell. Data Anal., vol. 1642, pp. 75–86, 1999.
M. G. Lawrence, “The accuracy of digital-video retinal imaging to screen
for diabetic retinopathy: An analysis of two digital-video retinal imaging
systems using standard stereoscopic seven-field photography and dilated
clinical examination as reference standards,” Trans. Amer. Ophthalmol.
Soc., vol. 102, p. 321–340, 2004.
A. D. S. for the Department of Health and Ageing. (2008). “Guidelines for the management of diabetic retinopathy,” [Online]. Available:
http://www.icoph.org/downloads/Diabetic−Retinopathy−Detail.pdf
C. I. Sanchez, M. Niemeijer, A. V. Dumitrescu, M. S. A. Suttorp-Schulten,
M. D. Abramoff, and B. van Ginneken, “Evaluation of a computer-aided
diagnosis system for diabetic retinopathy screening on public data,” Investigat. Ophthalmol. Vis. Sci., vol. 52, no. 7, pp. 4866–4871, 2011.
A. Bhalerao, A. Patanaik, S. Anand, and P. Saravanan in Proc. 6th Indian
Conf. Comput. Vis., Graph. Image Process., Dec. 2008, pp. 520–527.
A. Sopharak, B. Uyyanonvara, S. Barman, and T. H. Williamson, “Automatic detection of diabetic retinopathy exudates from non-dilated retinal
images using mathematical morphology methods,” Comput. Med. Imag.
Graph., vol. 32, no. 8, pp. 720–727, 2008.
T. Walter, J.-C. Klein, P. Massin, and A. Erginay, “A contribution of image
processing to the diagnosis of diabetic retinopathy-detection of exudates
in color fundus images of the human retina,” IEEE Trans. Med. Imag.,
vol. 21, no. 10, pp. 1236–1243, Oct. 2002.
D. Welfer, J. Scharcanski, and D. R. Marinho, “A coarse-to-fine strategy
for automatically detecting exudates in color eye fundus images,” Comput.
Med. Imag. Graph., vol. 34, no. 3, pp. 228–235, 2010.

1728

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

[32] M. Esnaashari, S. A. Monadjemi, and G. Naderian, “A content-based
retinal image retrieval method for diabetes-related eye diseases diagnosis,”
Int. J. Res. Rev. Comput. Sci., vol. 2, no. 6, pp. 1222–1227, 2011.
[33] B. Antal and A. Hajdu, “An ensemble-based system for microaneurysm
detection and diabetic retinopathy grading,” IEEE Trans. Biomed. Eng.,
vol. 59, no. 6, pp. 1720–1726, Jun. 2012.
[34] E. Barriga, V. Murray, C. Agurto, M. Pattichis, W. Bauman, G. Zamora,
and P. Soliz, “Automatic system for diabetic retinopathy screening based
on am-fm, partial least squares, and support vector machines,” in Proc.
IEEE Int. Symp. Biomed. Imag., Nano Macro, Apr. 2010, pp. 1349–1352.
[35] C. Agurto, E. Barriga, V. Murray, S. Nemeth, R. Crammer, W. Bauman,
G. Zamora, M. Pattichis, and P. Soliz, “Automatic detection of diabetic
retinopathy and age-related macular degeneration in digital fundus images,” Invest Ophthalmol. Vis. Sci., vol. 52, no. 8, pp. 5862–5871, Jul. 29,
2011.
[36] R. Acharya U, C. K. Chua, E. Y. Ng, W. Yu, and C. Chee, “Application of
higher order spectra for the identification of diabetes retinopathy stages,”
J. Med. Syst., vol. 32, no. 6, pp. 481–488, Dec. 2008.
[37] U. R. Acharya, C. M. Lim, E. Y. K. Ng, C. Chee, and T. Tamura,
“Computer-based detection of diabetes retinopathy stages using digital
fundus images,” Proc. Inst. Mech. Eng. H: J. Eng. Med., vol. 223, no. 5,
pp. 545–553, 2009.
[38] D. Usher, M. Dumskyj, M. Himaga, T. H. Williamson, S. Nussey, and
J. Boyce, “Automated detection of diabetic retinopathy in digital retinal
images: A tool for diabetic retinopathy screening,” Diabet. Med., vol. 21,
no. 1, pp. 84–90, 2004.
[39] Y. Sun, M. S. Kamel, A. K. Wong, and Y. Wang, “Cost-sensitive boosting
for classification of imbalanced data,” Pattern Recognit., vol. 40, no. 12,
pp. 3358–3378, 2007.
[40] D. Williams, V. Myers, and M. Silvious, “Mine classification with imbalanced data,” IEEE Geosci. Remote Sens. Lett., vol. 6, no. 3, pp. 528–532,
2009.
[41] Y. Yang, “An evaluation of statistical approaches to text categorization,”
J. Inf. Retrieval, vol. 1, pp. 67–88, 1999.

Sohini Roychowdhury (S’xx) received the B.Tech
degree in electronics and communication engineering
from Birla Institute of Technology, Ranchi, India, in
2007, and M.S. in electrical engineering from Kansas
State University Manhattan, KS, USA, in 2010.
She is currently a Doctoral candidate in the Department of Electrical and Computer Engineering, University of Minnesota Twin Cities, Minneapolis, MN,
USA. Her research interests include image processing, signal processing, pattern recognition, machine
learning, and artificial intelligence.

Dara D. Koozekanani (M’ xx) received the Ph.D.
degree in biomedical engineering and the M. D. degree from the Ohio State University, Columbus, OH,
USA, in 2001and 2003, respectively. His research
dissertation involved application of computer vision
techniques to the analysis of optical coherence tomography images.
He completed ophthalmology residency at the
University of Wisconsin in 2007, and completed a
surgical retinal fellowship at the Medical College of
Wisconsin in 2009. He is currently an Assistant Professor of ophthalmology on the clinical faculty at the University of Minnesota,
Minneapolis, MN, USA. He sees patients with a variety of surgical and medical
retinal diseases. His research interests include the application of ophthalmic
imaging technologies and automated analysis of those images.

Keshab K. Parhi (S’85–M’88–SM’91–F’96) received the B.Tech. degree from the Indian Institute of
Technology, Kharagpur, India, in 1982, the M.S.E.E.
degree from the University of Pennsylvania, Philadelphia, PA, USA, in 1984, and the Ph.D. degree from
the University of California, Berkeley, CA, USA, in
1988.
He has been with the University of Minnesota,
Minneapolis, MN, USA, since 1988, where he is currently Distinguished McKnight University Professor
and Edgar F. Johnson Professor with the Department
of Electrical and Computer Engineering. He has published more than 500 papers,
has authored the textbook VLSI Digital Signal Processing Systems (Wiley, 1999)
and coedited the reference book Digital Signal Processing for Multimedia Systems (Marcel Dekker, 1999). His research interests include VLSI architecture
design and implementation of signal processing, communications and biomedical systems, error control coders and cryptography architectures, high-speed
transceivers, and ultrawideband systems. He is also currently working on intelligent classification of biomedical signals and images, for applications such
as seizure prediction and detection, schizophrenia classification, biomarkers for
mental disorder, brain connectivity, and diabetic retinopathy screening.
Dr. Parhi has received numerous awards including the 2013 Distinguished
Alumnus Award from IIT, Kharagpur, India, 2013 Graduate/Professional Teaching Award from the University of Minnesota, 2012 Charles A. Desoer Technical
Achievement award from the IEEE Circuits and Systems Society, the 2004 F.
E. Terman award from the American Society of Engineering Education, the
2003 IEEE Kiyo Tomiyasu Technical Field Award, the 2001 IEEE W. R. G.
Baker prize paper award, and a Golden Jubilee medal from the IEEE Circuits
and Systems Society in 2000. He has served on the editorial boards of the
IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS &#X97;PART I AND PART
II, VLSI SYSTEMS, SIGNAL PROCESSING, SIGNAL PROCESSING LETTERS,AND
SIGNAL PROCESSING MAGAZINE, and served as the Editor-in-Chief of the IEEE
TRANSACTIONS ON CIRCUITS AND SYSTEMS &#X97;PART I (2004–2005 term),
and currently serves on the Editorial Board of the Journal of VLSI Signal Processing. He has served as the Technical Program Cochair of the 1995 IEEE
VLSI Signal Processing workshop and the 1996 ASAP conference, and as the
general chair of the 2002 IEEE Workshop on Signal Processing Systems. He
was a Distinguished Lecturer for the IEEE Circuits and Systems society during
1996–1998. He served as an elected member of the Board of Governors of the
IEEE Circuits and Systems society from 2005 to 2007.

