IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

281

Content-Based Image Retrieval by Metric Learning
From Radiology Reports: Application
to Interstitial Lung Diseases
José Ramos, Thessa T. J. P. Kockelkorn, Isabel Ramos, Rui Ramos, Jan Grutters, Max A. Viergever,
Bram van Ginneken, and Aurélio Campilho

Abstract—Content-based image retrieval (CBIR) is a search
technology that could aid medical diagnosis by retrieving and presenting earlier reported cases that are related to the one being
diagnosed. To retrieve relevant cases, CBIR systems depend on supervised learning to map low-level image contents to high-level diagnostic concepts. However, the annotation by medical doctors for
training and evaluation purposes is a difficult and time-consuming
task, which restricts the supervised learning phase to specific CBIR
problems of well-defined clinical applications. This paper proposes
a new technique that automatically learns the similarity between
the several exams from textual distances extracted from radiology
reports, thereby successfully reducing the number of annotations
needed. Our method first infers the relation between patients by
using information retrieval techniques to determine the textual
distances between patient radiology reports. These distances are
subsequently used to supervise a metric learning algorithm, that
transforms the image space accordingly to textual distances. CBIR
systems with different image descriptions and different levels of
medical annotations were evaluated, with and without supervision
from textual distances, using a database of computer tomography scans of patients with interstitial lung diseases. The proposed
method consistently improves CBIR mean average precision, with
improvements that can reach 38%, and more marked gains for
small annotation sets. Given the overall availability of radiology
reports in picture archiving and communication systems, the proposed approach can be broadly applied to CBIR systems in different medical problems, and may facilitate the introduction of CBIR
in clinical practice.

Manuscript received June 7, 2014; revised October 9, 2014; accepted November 17, 2014. Date of publication November 25, 2014; date of current version December 31, 2015. This work was supported by the Fundação para
a Ciência e Tecnologia (FCT) under the Programa Operacional de Potencial
Humano program under Grant SFRH/BD/40864/2007. This work was also
supported by FEDER funds through the Programa Operacional Factores de
Competitividade—COMPETE and by Portuguese funds through FCT—FCT in
the framework under Project Pest-C/SAU/LA0002/2011.
J. Ramos and A. Campilho are with the INESC TEC—INESC Technology and Science, 4200-465 Porto, Portugal, and also with the Faculdade
de Engenharia da Universidade do Porto, 4200-465 Porto, Portugal (e-mail:
jose.ricardo.ramos@gmail.com; campilho@fe.up.pt).
T. T. J. P. Kockelkorn and M. A. Viergever are with the Image Sciences
Institute, University Medical Center Utrecht, 3584 Utrecht, The Netherlands
(e-mail: thessa@isi.uu.nl; max@isi.uu.nl).
I. Ramos is with Faculdade de Medicina da Universidade do Porto, 4200-319
Porto, Portugal (e-mail: radiologia.hsj@mail.telepac.pt).
R. Ramos is with the INESC TEC—INESC Technology and Science, 4200465 Porto, Portugal (e-mail: rui@ramos.com.pt).
J. Grutters is with the Department of Pulmonology, St. Antonius Ziekenhuis Institute, 3435 Nieuwegein, The Netherlands (e-mail: j.grutters@
antnononiusziekenhuis.nl).
B. van Ginneken is with the Diagnostic Image Analysis Group, Radboud University Medical Center, 6525 Nijmegen, The Netherlands (e-mail:
b.vanginneken@rad.umcn.nl).
Digital Object Identifier 10.1109/JBHI.2014.2375491

Index Terms—Computed tomography, computer-aided diagnosis (CAD), content-based image retrieval (CBIR), interstitial lung
diseases (ILD), metric learning.

I. INTRODUCTION
ONTENT-BASED image retrieval (CBIR) is a search
paradigm that selects examples from an image collection
that have a similar content to a query image [1]. The large number of exams in picture archiving and communication systems
(PACS) motivated the study of CBIR systems for computeraided diagnosis (CAD), where given an undiagnosed exam [illustrated on the left of Fig. 1(a)], CBIR can be used to aid the
diagnostic process by automatically retrieving previously reported relevant exams [on the right of Fig. 1(a)] [2], [3]. This
enables radiologists to compare cases and check fellow radiologists’ conclusions, which may improve the diagnosis, particularly for inexperienced radiologists and complex diagnostic
problems [4]. It should be noted that such a CAD system requires the use of the query by visual example paradigm, where
the query is a medical image without textual information.
To be effective in CAD, CBIR systems must retrieve relevant
cases, e.g., cases with the same diagnosis or similar clinical
presentation. Such medical aspects are often not correlated with
image analysis characteristics, but rather with presence or absence of structures, or with—not always well defined or readily
quantifiable—deviations of normal appearance of structures. In
spite of that early CBIR systems defined similarity of exams
purely on the basis of visual features with no explicit relation to
medical concepts (e.g., image intensity, first- and second-order
filters, classes obtained from unsupervised methods [5]), rendering them inappropriate for most diagnosis problems [2]. This
difference between high-level radiologic concepts and low-level
CBIR visual representations is the so-called semantic gap [1],
which is the main reason why CBIR technologies are not used
in clinical practice [2], [3].
To reduce the semantic gap, CBIR researchers applied supervised learning in order to infer a relation between low-level
features and abstract medical representations, from annotations
provided by medical doctors [1]–[3]. Metric learning is a popular supervised learning method for CBIR, where the objective is
to learn a metric, which organizes medical exams as closely as
possible to subjective ratings on similarity of exams that were
previously collected from medical doctors. As in other forms of
supervision, the performance of metric learning depends on a

C

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

282

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Fig. 1. (a) Example of a CAD interface for ILD within a CBIR framework. On the left the undiagnosed query exam, while on the right the retrieved related
cases. (b) Slices of CT exams from patients with ILD. In the upper and middle slices are two patients with interstitial pulmonary fibrosis, respectively, in an early
and late stage, and on the bottom a patient with a NSIP in an early stage.

representative dataset. The complexity of diagnostic problems,
the expected intra- and inter-user variance, and the varied appearance of medical images, recommend the use of a large set
of annotations. However, because medical annotations are expensive, most research groups only have access to restricted sets
of similar/dissimilar exam pairs provided by similarity ratings
[6], class labels [7], and relevance feedback [8]. The difficulty
in collecting sufficient annotations is the main reason why supervised learning in CBIR has only been applied to a limited
number of radiology problems (e.g., interstitial lung diseases
(ILD) [4], [9], or breast lesions [7]). We hypothesize that the
progress of medical CBIR into a clinical setting will benefit
from the development of methods that have less stringent requirements on the amount of annotations, by capturing medical
knowledge from indirect sources, such as radiology reports or
the medical literature.
We here investigate the use of radiology reports to supervise
medical CBIR systems. Our approach employs information retrieval techniques, by establishing textual distances between
cases, based on the frequency of terms in the patient reports.
These distances are then used to supervise a metric learning
CBIR system, which is subsequently applied in undiagnosed
exams where reports are not yet available.
Since radiology reports are directly produced by medical doctors, and contain a technical description of exam specifications,
findings, and conclusions [10], report retrieval is not affected
by the semantic gap, as is corroborated by its broad use in clinical practice. Moreover, radiology reports are normally present
for all exams in most hospital PACS, guarantying the availability of large and up-to-date datasets in all radiology diagnosis
problems. Consequently, learning from radiology reports can
be integrated in most CBIR systems and, thus, facilitate the
introduction of CBIR methods in clinical practice.

However, information retrieval systems face several challenges when comparing medical reports, including e.g., polysemy, synonyms, abbreviations, negation, and misspelling [10],
[11], and since they only consider term frequency, are a limited representation of exam semantics. Consequently, radiology
reports cannot be considered as reliable as medical annotations.
In this paper, we evaluate if, and to which degree, the use
of textual distances to supervise CBIR systems improves CBIR
performance. For a variety of CBIR configurations, our evaluation compares use versus nonuse of supervision from textual
distances, on a database of computer tomography (CT) scans
from an ILD CAD problem.
ILD are a set of different lung disorders that, despite having
diverse causes, are normally considered together because they
all affect the lung interstitium [12]. The several ILD subtypes
have different treatments and prognoses and, hence, an accurate
subtype identification is important for disease management. CT
plays a major role in ILD subtype identification.
Radiologists analysis of CT exams from ILD patients first
requires the identification of a set of well-described visual patterns that develop in the lung parenchyma [13]. It is from their
appearance and location that radiologists derive their conclusions, by associating a pattern distribution with an ILD subtype.
However, this process is made difficult by the large number of
patterns and the variation of their visual appearance, by the disparity of possible pattern distributions within the same subtype,
and by the large overlap between different subtypes [12], [13].
These complications reach the extent that frequently other clinical elements, such as clinical history or histological exams, are
necessary to produce a reliable diagnosis [12]. Fig. 1(b) shows
three slices of three patients with ILDs. In the upper and middle slices are two patients with interstitial pulmonary fibrosis,
respectively, in an early and late stage, and on the bottom a

RAMOS et al.: CBIR BY METRIC LEARNING FROM RADIOLOGY REPORTS: APPLICATION TO ILD

patient with a nonspecific interstitial pneumonia (NSIP) in an
early stage. It illustrates the variation in visual appearance within
the same subtype (up and middle), and the visual similarity
that can occur between different ILD subtypes (up and bottom
slices).
Consequently, ILD subtype identification is considered a
complex radiological problem, requiring specialized chest radiologists with years of experience. Furthermore, since some
ILD types are rare, residents, or general radiologists in local
hospitals may have not seen enough exams to be able to produce a reliable diagnosis [14]. This motivated the development
of ILD CBIR CAD systems [4], [9] as such they allow radiologists to enhance their experience with the analysis of cases
previously reported by specialized radiologists.
With the objective of identifying and characterizing ILD patterns, CAD systems have been developed for the automatic
analysis of abnormal volumes of interest (VOI) within the lung
parenchyma [4], [9], [15]–[20]. These VOI are either manually
segmented by a radiologist [4], [20], or depending on an automatic segmentation of the lung followed by a division in volumes
[9], [15], [17], [21]. The segmentation of the lung is important
as to separate the lung from other anatomical structures that
are of little relevance to ILD diagnosis [18]. Considering the
patterns are heterogeneous in their appearance, the characterization of each VOI is normally based on a set features, of varied
nature, and across different scales. The feature set normally
includes intensity, important for e.g., for low- (lung cysts, emphysema) and high-attenuation patterns [consolidation, ground
glass (GG)], and texture descriptors, important for patterns that
correspond to a continuous repetition of some abnormal structure, such as honeycombing, emphysema, or crazy paving (CP)
[4], [9], [15]–[20]. Different classes of texture descriptors have
been evaluated for the problem, without a definitive conclusion
regarding its relative merits [22].
Because the feature set normally contains diverse types of
features across several scales, the resulting feature space is normally complex. Consequently, it is frequent to determine the
importance of each region of the feature space to each pattern,
either using unsupervised learning by dividing the feature space
into a codebook [19], [20], or supervised learning from manual annotations produced by radiologists [15], [16], [18]. Since
manual annotations are difficult to collect, the set of training
examples used by supervised systems is normally confined to a
handful of patterns, annotated in a few dozen cases [4], [9], [15],
[16], [18], limiting their effectivity. Descriptions of the entire
scan can be achieved by summing the number [5], [19], [20], or
area [9] of each type of VOI.
II. RELATED WORK
Medical CBIR systems using both text and image exam descriptions have already been explored in the past in the multimodal systems [1], [3]. ImageCLEF has a medical image
retrieval track since 2004, which has both text and image descriptions [3], [23], [24]. Clinical patient information was also
previously used as a feature for CBIR [3], [9], [25]. Such approaches use text as a feature and do not consider the relations

283

between the image and text representations, and can therefore
not be applied when text is not present in the query, which
represents the typical use case for a CBIR system.
Our study is closer to the area of cross-media retrieval, where
the objective is to find a representation for an image in text space
(and vice versa) to allow queries across the two modalities [1],
[26]. Because of the different objective, cross-media retrieval
neither considers the differences in semantic value of the image
and the text representations, nor was the integration between
text and expert annotations ever considered.
A particular example which is close to our approach is the
study described in Slaney et al. [27], where text from opinion
blogs is used to deduce the similarity between musical bands,
later used to train a music retrieval system.
In a previous conference paper [28], we showed that radiology
reports can be used to guide a metric learning scheme, increasing
the performance over a unsupervised CBIR approach. In this
paper, we extend the referred work by studying the integration of
our approach with medical annotations. We have also previously
presented a system that uses radiology reports to infer models
for CT patterns without the use of medical annotations [29].
While based on a similar principle, that study concerned another
medical problem, which posed considerably different challenges
and, hence, led to a methodology that was quite distinct from
what is proposed in this paper.
III. MATERIALS AND METHODS
A. Materials
1) Dataset: The dataset contains CT scans from 265 patients, diagnosed with one of 17 ILD subtypes at varied disease
stages. From these patients, all CT scans and respective reports
were collected over a period of six years (2005–2010). The
exams were performed according to clinical needs, and, consequently they did not follow a predefined protocol, nor had a
specified time interval between them. Scans with no reports or on
which the segmentation process failed (the segmentation method
is described in Section III-B3) were discarded, resulting in a total
of 1093 scans. To ensure the dataset is representative of a typical
ILD collection, no manual exam selection was made regarding
either acquisition parameters, image quality, ILD class, or ILD
stage. Patients were diagnosed by a consensus from experienced
specialists in pathology, radiology, and pulmonology according to a consensus protocol, following international guidelines
[14]. The number of patients and exams per ILD subtype is presented in Table I, and is ordered according to subtype incidence.
As a result of the diverse ILD subtype, incidence of the number of patients per subtype is very different, varying from 116
(sarcoidosis) to one (microlithiasis) patients, with 12 subtypes
with fewer than ten patients. Scans were acquired on a Siemens
SOMATOM Sensation Cardiac 64 or a Siemens EMOTION
DUO (Siemens Healthcare, Erlangen, Germany). Scans have a
slice thickness between 0.6 and 10 mm and a resolution between
0.3 mm and 2.3 mm.
2) Terminology: The methods for text description, described
in Section III-B2, require the definition of a terminology, that is,
the labeling of each term in the collection according to a medical

284

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

TABLE I
DISTRIBUTION OF PATIENTS AND SCANS OVER THE INTERSTITIAL LUNG
DISEASE CLASSES PRESENTED IN THE STUDY DATASET
ILD subtype
Sarcoidosis
Hypersensitivity Pneumonia (HP)
Connective Tissue Disease Associative ILD (CTD-ILD)
Interstitial Pulmonary Fibrosis (IPF)
Silicosis
Langerhans Histiocytosis
Vasculitis
NSIP
Unknow Fibrosis
Eosinophilia
Desquamative Interstitial Pneumonia (DIP)
Cryptogenic Organizing Pneumonia (COP)
Lymphangioleiomyomatosis (LAM)
Pulmonary Alveolar Proteinosis (PAP)
Bronchiolitis
Hemorrhage
Microlithiasis

Patients

Scans

116
34
21
20
14
7
7
7
7
7
6
5
5
3
3
2
1

484
103
106
65
46
33
28
28
44
24
27
22
30
19
16
18
4

class. Our terminology was constructed by a radiologist from
all terms in the reports of the dataset described in the previous
section.
The reports are in Portuguese and have an average size of
153 terms with a total of 3026 distinct terms in the entire collection. The terms in the dataset were classified into six classes:
findings (e.g., “GG,” “nodule,” “honeycombing”), anatomy
(“lung,” “bronchi,” “parenchyma”), diagnosis (“sarcoidosis,”
“lymphangioleiomyomatosis,” “hypersensitivity pneumonia”),
intensifiers (“acute,” “high,” “delimited”), others (“irregular,”
“immunosuppressed,” “septum”), and irrelevant (“artefact,”
“study,” “ct”)1 .
In the same annotation session, the radiologist grouped synonyms, terms inflections, and abbreviations to a single term. A
list with all groupings is used in the synonym filter described in
Section III-B2.
3) Medical Annotation of VOI: A second database, to be
used by the image processing chain (Section III-B3), is composed of 24 CT scans from the same number of patients. They
were acquired on a Philips Mx8000 IDT and a Philips Brilliance
iCT scanner (Philips Medical Systems, Best, The Netherlands).
Using the tool described in [21], an intern radiologist annotated
all VOI for all scans according to tissue patterns: decreased
density, consolidation, honeycombing, GG, CP, NSIP pattern,
nodular pattern, and finally, inhomogeneous when more than
one class of pattern was present. All remaining VOI were considered normal lung tissue.
B. Methods
1) Overview: This paper introduces the use of distances
among radiology reports to supervise a metric learning algorithm in image space. The objective is to compensate for the
absence of expert annotations in medical CBIR systems, by
1 Although the presented examples are, for illustration purposes, in the English
language, the actual terms used by the system are in the Portuguese.

Fig. 2. Schematic of a supervised metric learning process from radiology reports for a CBIR system. In the top layers the training stage, which is subdivided
into three steps. In the bottom layer the testing stage.

automatically collecting the relations between exams from radiology reports. In this section, we detail the methods used in
our approach.
As illustrated in Fig. 2, our methodology involves a training
stage and a testing stage. The training stage has as input all
PACS exams, and outputs a mapping function, later used in the
testing stage. It can be decomposed into three steps: 1) text
distance calculation among patients using a , 2) extraction of
scan representations in image space X for all exams using an
image processing chain, and 3) learning the metric on which the
distances among the image representations are similar to text
distances. It should be noticed that the objective of the described
system is to learn directly from a database in clinical setting,
which does not normally contain manual annotations, and is
heterogeneous in its acquisition parameters and ILD subtype
distribution, the common scenario in a PACS.
We assume that the metric can be decomposed into a mapping, that transforms a point from image space to what we
denominate as semantic space, followed by a Euclidean metric.
During step 3, an optimization method progressively adapts the
mapping parameters, in order to minimize a cost function that
measures the proximity between the distances in the semantic
space and the text distances, previously calculated in step 1.
Testing uses the mapping constructed in the training stage for
each undiagnosed scan to find in the semantic space Y the most
similar set of exams.
The next sections detail each stage shown in Fig. 2. We start by
describing the extraction of text distances (see Section III-B2)
and image representations (see Section III-B3). In both of these
sections, several methods are presented, to be compared in the

RAMOS et al.: CBIR BY METRIC LEARNING FROM RADIOLOGY REPORTS: APPLICATION TO ILD

evaluation phase described in Section IV. Our metric learning
method is described in Section III-B4 followed by the description of the testing stage in Section III-B5. Finally, Section III-C
describes how our system can incorporate expert annotations by
altering the textual distances matrix.
2) Text Distances: In this section, we describe the several
alternative representations used in the text processing chain.
Our text processing chain uses standard techniques from information retrieval, all based on a statistical analysis of terms
frequency, using the cosine distance between the term
frequency-inverse document frequency (tf-idf) score. tf-idf is
a vector representation that scores the importance of a term according to its frequency in the text, weighted by the number
of documents, where the term is present (see [11] for details).
tf-idf attributes higher importance to terms that are frequent in
the text and, therefore, important to describe it, but infrequent in
the collection, lowering the score of common words. It is commonly used in information retrieval to group texts according to
its topics [11].
In our case, the calculation of tf-idf score requires five steps:
1) Concatenation, where all reports from the same patient are
concatenated into a single text; thus, allowing to consider a distance between patients instead of a distance between exams.
This minimizes the bias from follow-up exams and biopsies,
which are common in ILD databases, and have short reports
without a clear characterization of the patient and its condition.
2) Tokenization, where the text is broken into words or multiword expressions, commonly known as tokens. In our case,
the token delimiters are spaces and punctuation (e.g., “strong
presence” to “strong” and “presence”). 3) Lower case, where
all tokens are lowercased (e.g., “Exam” to “exam”). 4) Synonym filter, where synonyms and term variations are grouped
to a common term according to the synonym list described
in Section III-A2 (e.g., “nodules” and “nodular” to “nodule”).
5) Term score, where each term is scored by its tf-idf score.
The distance between patients is obtained through a weighted
cosine distance between the tf-idf vectors
Dij = 1 −

bT
i .bj
| bi || bj |

(1)

where Dij is the textual distance between patient i and j, |.| is
the vector norm, and
bi = Wai

(2)

where ai is the tf-idf vector of patient i and W is a diagonal
matrix, which weighs each term according to the importance of
the term class in the terminology described in Section III-A2.
The matrix W is the same for all patient vectors. We tested five
alternatives for W.
1) text1: all terms are equally weighted.
2) text2: terms marked as irrelevant are removed from the
score (zero weighted) and all others are equally weighted.
3) text3: terms are weighted according to diagnosis (2), findings (1.5), anatomy (1), intensifiers (0.5), others (0.5), and
irrelevant (0).
4) text4: terms are weighted according to diagnosis (2) and
findings (1), and all others (0).

285

5) text5: terms are weighted according to diagnosis (1) and
all others (0).
These alternatives progressively attribute larger relevance to
term classes related to medical concepts, and were chosen as to
allow an evaluation of the importance of each term class to ILD
retrieval.
The output of the text representations is an N × N distance
matrix D, where N is the number of patients.
3) Image Representation: The image processing chain,
which is the basis of the CBIR representation, transforms the raw
hounsfield units (HU) values into relevant medical representations. It consists of five stages: resizing, segmentation, division
in VOI, describing regions, and summarizing information. The
methods and parameters of the first four blocks are based in
previous work by some of the coauthors in the characterization
of ILD lung patterns [17], [21].
In resizing all scans are resized to 128 × 128 × 128 voxels
using a nearest neighbor interpolation. Dimensions of 64, 128,
and 256 were also tested, with small differences in performance
above 128.
The segmentation of the lung is fully automatic and uses the
algorithm described in Sluimer et al. [30]. It starts by extracting the main airways by the region growing from a seed point
identified automatically from the HU, shape, and position properties. The region growing process stops when the size of the
airways grows abruptly, indicating a leak to the interstitium.
Another region growing process, starting from the lowest HU
in the bronchi, segments the parenchyma, having as a stopping
criterion a threshold determined by the Otsu method. The final
step smooths the lung region with hole filling and morphological closing. Our choice for a fully automatic lung segmentation
method is justified by the requisite of our system to work with
no manual intervention from the user.
In the division in VOI phase, the regions segmented as lungs
are divided in VOIs using the algorithm detailed in Kockelkorn
et al. [21]. It uses seed points based on local maxima or minima with a minimum distance of five voxels. A volume growing
algorithm is then applied until volumes collide based on an
acceptance rule that equally takes into account a component
based on the distance to the seed point and a component based
on the difference in HU values to the VOI mean. This method
compares favorably to a fixed block division in terms of the homogeneity of the VOI, although no significant difference in the
performance of the image processing chain was found between
the two systems.
We base our region descriptors of each individual VOI on a
3-D version of the set of features used in Sluimer et al. [17],
which was already proven suitable for lung textures characterization. The features are local mean, standard deviation, skew,
and kurtosis of each VOI for 32 filtered images. The filtered images include Gaussian, Laplacian, and first- and second-order
Gaussian derivatives in the three anatomical planes. The standard deviations of the Gaussian filters are 0.5, 1, 4, and 8.
The standard deviations were selected by experimentation to
cover the different lung structure scales, and are equal for all
anatomical planes. This approach aims at representing each VOI
by multiscale texture filters, a common approach in medical image representation.

286

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

As referred the first four stages are based in previous work
in the analysis of ILD lung patterns. This paper introduces a
final stage which summarizes information by condensing all
VOI representations into a single vector. Four different methods
were evaluated.
1) MEAN—The mean of each feature for all VOIs, which
condenses the scan representation into a total of 128 features.
2) HIST—A ten bins histogram of each feature for all VOIs.
The concatenated vector of all histograms has 1280 features.
3) UNSUP—In this representation, the K-means clustering
algorithm is first applied to all VOIs to produce a 100
elements codebook. Each VOI is subsequently assigned
to the closest codebook and, finally, the scan representation is a histogram of the frequency of each codebook
element for all scan VOIs. The scan representation has
100 dimensions.
4) CLASS —The CLASS approach is based on the system
detailed in [17] using the medical annotations described
in Section III-A3. It uses seven K-nearest neighbors (KNN) classifiers (one per pathological class) in order to
attribute to each VOI the percentage of nearest neighbors
that belong to one of the seven classes (decreased density,
consolidation, honeycombing, GG, CP, NSIP pattern, and
nodular pattern). The VOI output is therefore a seven
wide vector with values between 1 and 0 indicating the
proximity to the respective pathological class. Given the
unbalanced nature of the dataset, a downsample of the negative class was applied to each classifier, as to not favour
classification toward the normal representation. A sequential forward search feature selection was used as a wrapper
around each K-NN classifier to reduce the volume descriptors to a set of features which minimizes the error associated with each abnormality. The sum of the output of all
VOIs outputs gives an estimation of each class volume in
a seven features wide representation per scan. The value
of K is 50, which was obtained from experimentation.
The output of this stage is a set of vectors V =
{v1 , . . . ,vM } ∈ X, where vi is the image representation of
scan i, M is the number of scans, and X is the image space.
4) Metric Learning: In the final training stage, after the textual distance matrix D and the set of image representations V
have been computed, the metric learning method objective is to
find the mapping L that transforms the set of vectors V into a
corresponding set of vectors U = {u1 , . . . , uM } ∈ Y , in such
a way that the distances between the U vectors are as close as
possible to D. Our metric learning, which is adapted from the
nonlinear Sammon projection [31], is illustrated in Fig. 2. It can
be decomposed into four different stages.
1) Mapping: This phase maps every image vector vi onto
its corresponding semantic representation ui = L(vi ). A
popular choice for mapping L is a linear transformation
(e.g., Mahalanobis transform), with the metric learning
method optimizing the transformation matrix. However,
our experiments revealed that nonlinear mapping had a
superior performance. During the training stage, the map-

ping is a simple table, that each vector vi maps onto a
vector ui . The vectors ui are randomly initialized and are
updated during the metric learning process.
2) Metric: The metric stage is responsible for calculating
the distance between all pairs of scan semantic representations in U . We use the Euclidean distance E(U)ij =
 ui − uj  .
3) Cost Function: The cost function evaluates the proximity of the distances in the semantic space E(U) to the
textual distances D. Since D and E(U) are square matrices of different order (N and M , respectively), D has to
be transformed into an M × M matrix by setting the distances of all intrapatient exams to zero, and those of interpatient exams to their respective patient distance. The
cost function C(E(U), D) is
C(E(U), D) =

1  | E(U)ij − Dij |
c
Dij

(3)

i< j

and
c=



Dij .

(4)

i< j

This cost function is the same as the one used in the
Sammon projection [31]. In our implementation, prior to
the calculation of the cost function, a residual value is
added to D, to avoid problems with zero division.
4) Optimization: The optimization step updates the parameters of the mapping L according to the cost function
evolution. The update uses a steepest descent, following
the original Sammon algorithm. The expression for the
gradient can be found in Sammon [31].
The metric learning process iterates the described four steps.
The optimization algorithm stops when the difference between
ui vectors or the cost function gradient stabilize.
5) Testing: In the testing stage, an undiagnosed scan, that
is, a scan without an associated radiology report, is analyzed to
find the most similar exams in the semantic space U .
The procedure starts by transforming the raw scan into an
image representation x ∈ X using the image processing chain.
This block is similar to the one described in Section III-B3.
Subsequently, the image representation x is transformed into
a semantic representation y ∈ Y . The mapping y = L(x) is
obtained from a weighted interpolation of the vectors ui , previously calculated in the training stage. The weight of each ui
is determined from the nearest neighbors of x from the training
representations V , that is, if


P = v1 v2 . . . vk , vi ∈ V
(5)
is the set of k-nearest neighbors of x, then the weight t of each
vector is such that
x = Pt

(6)

The point y in the semantic space is then obtained from
y = Psem t

(7)

RAMOS et al.: CBIR BY METRIC LEARNING FROM RADIOLOGY REPORTS: APPLICATION TO ILD

where t is the same as in (6), and

Psem = u1 u2

...

uk



(8)

where ui = L(vi ), i = 1, . . . , k are the semantic counterparts
of the image vectors in P, previously calculated during the
training stage.
Finally, y is compared to all vectors in U , using the Euclidean
distance metric, and the set of most similar exams is retrieved.
C. Incorporating Expert Annotations
In the previous sections, we described a system that learns a
new metric from a distance matrix and a set of visual representations. Although we focused on a distance matrix extracted from
radiology reports, the described metric learning method can be
used with any distance matrix, including distances directly collected from experts.
Such distance matrices can contain ratings from medical doctors on the exam pair distance [6], or can be derived from sets
of similar/dissimilar cases by attributing a minimum/maximum
distance to exams from the same set. This process relies on
relevance feedback or on a joint annotation effort. In all cases,
expert matrices are an indication of the distances between exams
as perceived by one or more experts.
In our approach, an expert matrix Dexp would replace the text
distance matrix D in the metric learning procedure described
before. Furthermore, since it is likely that the order of Dexp
is much smaller than the order of D (due to the difficulty of
collecting annotations), it is also possible to produce a matrix
Dm ix that uses both distances from D and from Dexp . The
construction of Dm ix allows the use of both expert annotations
and textual distances in the same learning process, allowing the
integration of different sources of annotations.
In this paper, we will test the performance of a matrix Dm ix
constructed by

Dexp , for exam pairs that are contained in Dexp
Dm ix =
D,
for all other exam pairs
(9)
that is Dexp is considered more relevant than D, because it is
collected directly from the expert. We will test the performance
of Dm ix for a matrix Dexp of progressively larger order.
For the purpose of this paper, medical annotations are considered to be according to ILD class, that is, Dexp is 0 for patients
of same ILD class and 1 for patients of different ILD classes
(minimum and maximum, respectively, of the cosine distance).
This choice of Dexp guarantees both matrices to be on the same
range.
IV. EXPERIMENTS AND RESULTS
This section compares our approach with the traditional CBIR
approaches, by describing and presenting the results for two experiments. Experiment 1 compares our approach against the
unsupervised CBIR systems for several image and text representations. Experiment 2 explores the performance of the metric
learning scheme when medical annotations are present. Both

287

experiments are based on the same framework described in the
following section.
A. Experimental Framework
Medical CBIR evaluation involves the definition of the following requirements: 1) user needs (or topics) expressed as
undiagnosed exams; 2) a collection of documents, in this case
PACS exams, from which the CBIR system retrieves the relevant
examples; 3) a relevance assessment, which maps the relevance
of each of the collection samples to each of the query exams [11].
Performance is evaluated by analyzing the retrieved documents
to each topic according to the relevance assessment. Systems
can then be ranked by a performance measure that reflects how
appropriate the retrieval system was for all topics.
In this paper, each exam is a topic, all exams from the remaining patients are the collection and “same ILD class as topic
exam” is the relevance criterion from which relevance assessments are extrapolated. All exams of the patient whose topic
exam is analyzed are removed from the collection. As explained
in Section III-A1, there was no manual selection of exams in
order to create a dataset that is representative of an ILD collection in a clinical environment. Regarding the topics, since each
exam had to be diagnosed before being part of the collection,
we consider that the dataset exams constitute a realistic set of
topics.
Performance measures used are the mean average precision
(MAP) and precision-recall curves (see [11] for details). MAP
is the mean of the average precision for each query, that is


MAP(Q) = 1/N

AP(qj )

(10)

1< j < N

where N is the number of queries, Q = {q1 , q2 , . . . , qN } is the
set of all queries, and AP(qj ) is the average precision, which
can be described as the precision at each recall level, that is
AP(qj ) = 1/mj



Precision(Rj k )

(11)

1< k < m j

where Rj k is the set of ranked retrieval results and mj is the
number of relevant documents for query qj . The MAP is approximately the average area under the precision-recall curve
for a set of queries [11].
These measures were chosen upon considering the information provided, the stability of the measures and the ease of
processing [32].
For the experiments involving metric learning systems, the
dataset was divided into a training set and a test set, with the
former used as collection and the latter as the topics. The training/test dataset division was done either by a leave-one-patientout (LOPO) (experiment 1) or 60/40 patient Cross Validation
(CV) (experiment 2) with the test set randomly picked 20 times.
LOPO was preferred over CV except if the computational requirements of the experiment made it infeasible, as is the case
of experiment 2.

288

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

TABLE II
MAP OF RETRIEVAL BASED ON TEXT (TOP), ON UNSUPERVISED CBIR (MIDDLE), AND ON SUPERVISED CBIR USING METRIC LEARNING FROM RADIOLOGY
REPORTS (BOTTOM)
Text Retrieval
text1
0.34

text1
text2
text3
text4
text5
expert

text2
0.37

text3
0.43

text4
0.45

text5
0.41

MEAN
0.26

Unsupervised CBIR
HIST
0.24

UNSUP
0.26

CLASS
0.25

Expert
1

Supervised CBIR using Metric Learning from Radiology Reports
MEAN
HIST
UNSUP
CLASS
0.29 (11.0%)
0.29 (20.8%)
0.29 (11.0%)
0.24(−4.0%)
0.30 (15.3%)
0.29 (20.8%)
0.30 (15.3%)
0.25 (0.0%)
0.31 (19.2%)
0.32 (33.3%)
0.32 (23.1%)
0.26 (4.0%)
0.34 (30.8%)
0.34 (37.5%)
0.35 (34.6%)
0.26 (4.0%)
0.34 (30.8%)
0.34 (37.5%)
0.34 (30.8%)
0.27 (8.0%)
0.48 (84.6%)
0.49 (104.1%)
0.48 (84.6%)
0.37 (48.0%)

In parentheses, the improvement over the unsupervised CBIR system using the same image representation is given (in percentage of the original performance).

Fig. 3. Results of experiment 1 for the unsupervised CBIR system using UNSUP, the supervised method with text4 for text representation and UNSUP for image
(text4_UNSUP), and the text retrieval system text4. (a) Precision recall graph. (b) MAP per ILD class.

B. Experiment 1: Supervised Metric Learning Using
Radiology Reports Versus Unsupervised CBIR
In this section, we compare our approach (described in
Section III-B) against the unsupervised CBIR systems. The
evaluation used the framework described in Section IV-A with
a LOPO validation approach. The dimension of the semantic
space was chosen to be 10, and the number of neighbors used
in the mapping described in Section III-B5 was also 10. Experiments revealed low sensitivity of the CBIR performance to these
parameters.
Table II contains the MAP for a text retrieval system (top),
a unsupervised CBIR system (middle), and a supervised metric
learning system from radiology reports (bottom), for all image
and text representations previously described in Section III-B.
In parenthesis, the relative improvement over the corresponding
unsupervised CBIR approach is given. An expert description,
representing a matrix with expert annotations, was also included
to compare our performance with a traditional metric learning
system. Since our relevance criterion is limited to class labels,
the expert matrix was derived from the ILD class (distance is

0 for scans from the same class and 1 for different classes). It
should be noted that text systems, while represented in the table,
are not valid for our CAD configuration, since they require a
written report, which is unavailable at the moment of diagnosis.
From the results in Table II, it follows that text retrieval consistently presents a higher MAP than unsupervised CBIR systems, which perform poorly for all four image representations.
The supervised system from radiology reports presents a performance improvement over the unsupervised CBIR approaches
for almost all text and image alternatives. The supervised performance improvement is more pronounced for text descriptions
that give a higher weight to diagnosis and findings terms, and
less effective for the CLASS image description. As expected,
systems supervised using expert annotations have a better performance than those using textual distances.
Fig. 3(a) contains the precision-recall curves for the unsupervised CBIR system using the described UNSUP image
description, our approach using text4_UNSUP (method text4
for text representation and UNSUP for image), and finally,
the text image retrieval system using text4 text description.

RAMOS et al.: CBIR BY METRIC LEARNING FROM RADIOLOGY REPORTS: APPLICATION TO ILD

Fig. 4. Three examples of query (left column), the retrieved exam by the unsupervised method with UNSUP image description (middle column), and the
retrieved exam by the supervised method with method text4 for text representation and UNSUP for image (text4_UNSUP). The exams are illustrated by one
representative slice. Below each slice the respective ILD class.

The supervised approach outperforms the unsupervised CBIR
for almost all levels of recall, although its performance is always below that of the text retrieval. The same set of retrieval
systems are present in Fig. 3(b), which contains the MAP per
ILD class. Fig. 3(b) shows an improvement of the supervised
approach across most of the frequent ILD diagnoses, whereas
the behavior is more erratic in the less represented (and therefore
more noisy) classes.
Fig. 4 presents representative slices of three examples, with a
comparison between our approach and an unsupervised retrieval
method.
C. Experiment 2: Metric Learning and Medical Annotations
Although in the previous experiment our approach was compared with a system with expert annotations, the comparison assumed that the number of text annotations and radiology reports
was similar, and did not explore the fusing of expert annotations
with text distances, as described in Section III-C. This section
explores the performance of the metric learning system when
expert annotations are considered as such, or mixed with text or
image distances, for a different number of annotations.
All experiments use the framework described in Section IV-A
with a CV dataset division. Two medical annotation scenarios
commonly used in the literature [6], [8] were simulated.
1) In the first scenario, we simulate an annotation session
where medical doctors rate the similarity of different
cases. The output is a distance matrix, the expert matrix
(Dexp ), that contains the similarity of cases as perceived
by medical doctors. In our case, since relevance is limited
to class labels, Dexp is 1 for exams of the same class
and 0 for exams of different classes. Fig. 5(a) contains
the MAP for the integration of the expert matrix with
text distances text+expert (that is, Dm ix as described in

289

Section III-C by fusing D and Dexp ), the integration
of the expert matrix with image distances image+expert
(Dm ix from E(U) and Dexp ) and the expert matrix expert
(Dexp ), for a different number of annotated patients. The
text+expert distance matrix leads to consistently higher
MAP than using expert and image+expert matrices, with
a more pronounced difference for a small number of annotated patients.
2) In the second annotation scenario, medical doctors provide feedback on the relevance of five randomly retrieved
cases to a given query exam. In our case, this boils down to
replacing, in the image and text distance matrices, the corresponding exam distances, by 0 or 1, respectively, if they
belong to the same or to a different class. Fig. 5(b) contains the MAP for an integration of image distances and
relevance feedback annotations image+expert, and text
distances and relevance feedback annotations text+expert
as a function of the number of feedbacks. The figure shows
that the text+expert curve outperforms the image+expert
curve, although the difference becomes small when the
number of feedbacks exceeds 100 000.
V. DISCUSSION
A. General Considerations
Even for experienced radiologists, ILD diagnosis is a complicated task given the variety of possible presentations for each
ILD subtype. This difficulty is enhanced in the dataset described
in Section III-A1, by the highly unbalanced dataset and the large
variety in acquisition parameters. This explains the low MAP
values obtained for all approaches.
However, as explained in Section IV-A, the dataset was designed as to be representative of an ILD collection in clinical
environment, where image acquisition parameters are altered
according to the objective of the clinical exam, and the great
variety in incidences of the several ILD subtypes create an unbalanced PACS. It can therefore be argued that our results reflect
the CBIR performance in a real-PACS environment.
The use of the ILD subtype as the sole relevance criterion,
although commonly used in the literature [7], [9], [33], [34], can
be considered a coarse approximation to reality. It was assumed
that all systems are equally affected by this approximation, to
enable an unbiased comparison between systems. Such assumptions are common in information retrieval evaluations [35]. The
same assumption was made regarding the parameters of the text,
image, and metric learning algorithms. Although parameters are
not guaranteed to be optimal, they are considered to influence
all approaches similarly.
B. Supervised Metric Learning Using Radiology Reports
Versus Unsupervised CBIR
In experiment 1, we assessed whether using text distances
extracted from radiology reports to supervise a CBIR system improves the system performance over unsupervised CBIR
approaches. To compare the two systems, we evaluated their

290

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Fig. 5. (a) MAP as a function of the number of annotated patients using medical annotations (expert), an integration of text and medical annotations (text+expert),
and an integration of image and medical annotations (image+expert). (b) MAP for an integration of image and relevance feedback annotations (image+expert),
and an integration of text and relevance feedback annotations (text+expert) as a function of the number of feedbacks.

performance on the same dataset for a variety of image and text
descriptions.
The results presented in Table II show a consistent improvement over unsupervised CBIR for almost all image and text descriptions, in terms of the system MAP. Since MAP represents
an average performance across all levels of recall, such results
indicate that our approach is globally beneficial for ILD CBIR,
that is, the use of text distances to supervise CBIR systems
improves performance as compared with unsupervised CBIR.
This experiment is more thoroughly analyzed in the precisionrecall graph of Fig. 3(a), and the MAP per class in Fig. 3(b). The
precision-recall graph, which presents the performance of the
CBIR system across all levels of retrieval, shows a consistent
improvement of our approach for almost all levels of recall. The
MAP per class shows that the increase in performance is consistent across most ILD classes. Both Fig. 3(a) and (b) corroborates
that our approach enhances the system performance.
Fig. 4 provides some insight on the causes of this improvement. It is visible that in all examples, the exam retrieved by
the unsupervised method has an appearance similar to the query
exam, as both seem to have a similar distribution of representative VOI. This means that the success of the unsupervised
method is limited to cases, where the relevant exams and the
query belong to the same region in image space, as seems the
case in the first example. In contrast the supervised system,
as is perceivable in the second example, is capable of linking
exams of different regions in image space, provided these regions are mapped to the same region in the semantic space.
This occurs when both image regions contain examples with
small textual distances. This brings supervision from radiology reports closer to the behavior of supervised systems guided
by medical annotations. However, as in all supervised systems,
our supervised method does not dispense the need for appropriate image features, which clearly separate different exams.
This need is apparent in the third example, where the confusion of PAP and sarcoidosis samples in the image space caused
an erroneous mapping to a sarcoidosis region in the semantic
space.

Performance of unsupervised CBIR is generally poor, with
insignificant differences across the different image representations, which confirms the shortcomings of current image representations in adequately representing medical exams. The
improvement in the CLASS image description is much smaller
as compared to the other image descriptions, both for supervised
systems depending on text or depending on expert annotations.
This indicates that the magnitude of the improvement is dependent on the image description and, consequently, care must be
taken on its design. Further studies with other image representations are called for in order to characterize the influence of the
image processing chain in the learning process.
The supervised improvement also seems to be dependent on
the quality of the text descriptions, with the worst text descriptions (text1) leading to smaller gains, and the methods relying on
medical concepts (text4, text5) leading to greater improvements.
This indicates that the impact of our approach will depend on
the evolution of the design of appropriate text distances.
C. Metric Learning and Expert Annotations
While experiment 1 showed that our approach improves over
unsupervised CBIR, it also demonstrated that text distances
yield inferior performance as compared with expert annotations. While it makes intuitively sense that experts are a better
source than text distances, experiment 1 failed to compare the
situation when the number of text distances is much larger than
expert annotations, which is typical for most CBIR applications.
Experiment 2 improved this analysis by comparing the MAP of
the two sources for a progressively larger number of expert annotations. Beyond their one-to-one comparison, we evaluated
the performance of their fusion into a single description.
An important observation on experiment 2 is the volume of
medical annotations required to reach the MAP of the best supervised system in experiment 1 (0.34), which is around 120
annotated patients in Fig. 5(a), or 5000 relevance feedbacks in
Fig. 5(b). In a realistic scenario, where medical doctors contradict each other, more annotations could be required. The large

RAMOS et al.: CBIR BY METRIC LEARNING FROM RADIOLOGY REPORTS: APPLICATION TO ILD

number of annotations required substantiates our initial assumption that, although expert annotations are a valuable source of
information to a CBIR system, the annotation effort reduces its
applicability and, hence, justifies the motivation for the exploration of alternative sources of knowledge.
More interesting than the superiority of text over expert annotations for a small number of patients, is that both Fig. 5(a) and
(b) presents a higher MAP for the fusion of expert and text descriptions, in comparison with both the exclusive use of expert
annotations, and the exclusive use of text. This boosting in performance suggests that text must be regarded as a supplement,
rather than an alternative, to expert distances, and that their
simultaneous use should be considered. Consequently, experiment 2 shows that our approach is beneficial for CBIR systems
even if expert annotations are available.

[5]

[6]

[7]

[8]

[9]

VI. CONCLUSION
In an effort to improve CBIR medical representations, this paper studied the use of radiology reports to supervise CBIR systems. The presented method uses text distances between exam
reports to supervise a metric learning algorithm in the exam
image space. We compared our approach with traditional CBIR
systems, based on visual representations and on expert annotations using a database of ILD CT. In both cases, our approach
consistently increased CBIR performance for the tested image
descriptions. Since radiology reports are normally available in
all hospital PACS, and since results suggest it is beneficial for
a broad range of CBIR configurations and image descriptions,
our approach can be applied to a variety of image retrieval applications and, hence, contribute to the introduction of CBIR
technology into a clinical context.
While these conclusions only have been evaluated for specific image and text descriptions associated with the retrieval of
ILD CT diagnosis, it is reasonable to expect that learning by
radiology reports could be helpful in other CAD CBIR systems,
and using other text and image descriptions. Moreover this approach has an inherent scalability as the increase in complexity
by the introduction of new CAD problems is accompanied by
a richer text distance. Future works should include the study of
this approach to other CAD problems.
ACKNOWLEDGMENT
The authors would like to thank Dr. A. Morais for the help
provided.

[10]

[11]
[12]
[13]
[14]

[15]

[16]
[17]

[18]
[19]

[20]

REFERENCES
[1] R. Datta, D. Joshi, J. Li, and J. Z. Wang, “Image retrieval: Ideas, influences,
and trends of the new age,” ACM Comput. Surveys, vol. 40, no. 2, pp. 1–60,
2008.
[2] C. B. Akgul, D. L. Rubin, S. Napel, C. F. Beaulieu, H. Greenspan, and
B. Acar, “Content-based image retrieval in radiology: Current status and
future directions,” J. Digital Imag., vol. 24, no. 2, pp. 208–222, Apr. 2010.
[3] A. Kumar, J. Kim, W. Cai, M. Fulham, and D. Feng, “Content-based
medical image retrieval: A survey of applications to multidimensional
and multimodality data,” J. Digital Imag., vol. 26, no. 6, pp. 1025–1039,
Dec. 2013.
[4] A. M. Aisen, L. S. Broderick, H. Winer-Muram, C. E. Brodley, A. C. Kak,
C. Pavlopoulou, J. Dy, C.-R. Shyu, and A. Marchiori, “Automated storage

[21]

[22]

[23]

291

and retrieval of thin-section CT images to assist diagnosis: System
description and preliminary assessment,” Radiology, vol. 228, no. 1,
pp. 265–270, Jul. 2003.
J. G. Dy, C. E. Brodley, A. Kak, L. S. Broderick, and A. M. Aisen,
“Unsupervised feature selection applied to content-based retrieval of lung
images,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 25, no. 3, pp. 373–
378, Mar. 2003.
B. Andre, T. Vercauteren, A. M. Buchner, M. B. Wallace, and N. Ayache,
“Learning semantic and visual similarity for endomicroscopy video retrieval,” IEEE Trans. Med. Imag., vol. 31, no. 6, pp. 1276–1288, Jun.
2012.
L. Yang, R. Jin, L. Mummert, R. Sukthankar, A. Goode, B. Zheng,
S. C. Hoi, and M. Satyanarayanan, “ A boosting framework for visualitypreserving distance metric learning and its application to medical image retrieval,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 1,
pp. 30–44, Jan. 2010.
M. Rahman, P. Bhattacharya, and B. C. Desai, “A framework for medical
image retrieval using machine learning and statistical similarity matching
techniques with relevance feedback,” IEEE Trans. Inf. Technol. Biomed.,
vol. 11, no. 1, pp. 58–69, Jan. 2007.
A. Depeursinge, D. Racoceanu, J. Iavindrasana, G. Cohen, A. Platon,
P.-A. Poletti, and H. Muller, “Fusing visual and clinical information for
lung tissue classification in high-resolution computed tomography,” Artif.
Intell. Med., vol. 50, no. 1, pp. 13–21, Sep. 2010.
K. J. Dreyer, M. K. Kalra, M. M. Maher, A. M. Hurier, B. A. Asfaw,
T. Schultz, E. F. Halpern, and J. H. Thrall, “Application of recently developed computer algorithm for automatic classification of unstructured
radiology reports: Validation study,” Radiology, vol. 234, no. 2, pp. 323–
329, Feb. 2005.
C. D. Manning, R. Prabhakar, and S. Hinrich, Introduction to Information Retrieval, 1st ed. New York, NY, USA: Cambridge Univ. Press, Jul.
2008.
S. J. Bourke, “Interstitial lung disease: Progress and problems,” Postgraduate Med. J., vol. 82, no. 970, pp. 494–499, Aug. 2006.
W. R. W. Webb, N. L. Muller, and D. P. Naidich, High-Resolution CT of
the Lung, 4th ed. Philadelphia, PA, USA: Lippincott Williams & Wilkins,
Nov. 2008.
American Thoracic Society and European Respiratory Society, “Idiopathic pulmonary fibrosis: Diagnosis and treatment, international consensus statement,” Amer. J. Respir Crit. Care Med., vol. 161, no. 2,
pp. 646–664, Feb. 2000.
A. Depeursinge, D. Van de Ville, A. Platon, A. Geissbuhler, P.-A. Poletti,
and H. Mller, “ Near-affine-invariant texture learning for lung tissue analysis using isotropic wavelet frames,” IEEE Trans. Inf. Technol. Biomed.,
vol. 16, no. 4, pp. 665–675, Jul. 2012.
Y. Song, W. Cai, Y. Zhou, and D. D. Feng, “Feature-based image patch
approximation for lung tissue classification,” IEEE Trans. Med. Imag.,
vol. 32, no. 4, pp. 797–808, Apr. 2013.
I. C. Sluimer, M. Prokop, I. Hartmann, and B. van Ginneken, “Automated
classification of hyperlucency, fibrosis, ground glass, solid, and focal lesions in high-resolution CT of the lung,” Med. Phys., vol. 33, no. 7,
pp. 2610–2620, Jul. 2006.
I. Sluimer, A. Schilham, M. Prokop, and B. van Ginneken, “Computer
analysis of computed tomography scans of the lung: A survey,” IEEE
Trans. Med. Imag., vol. 25, no. 4, pp. 385–405, Apr. 2006.
A. Foncubierta-Rodriguez, A. Depeursinge, and H. Muller, “Using multiscale visual words for lung texture classification and retrieval,” in Proc.
2nd MICCAI Int. Conf. Med. Content-Based Retrieval Clin. Decision Support, Sep. 2012, pp. 69–79.
A. Burner, R. Donner, M. Mayerhoefer, M. Holzer, F. Kainberger, and
G. Langs, “Texture bags: Anomaly retrieval in medical images based
on local 3d-texture similarity,” in Proc. 2nd MICCAI Int. Conf. Med.
Content-Based Retrieval Clin. Decision Support, Jan. 2012, pp. 116–127.
T. T. J. P. Kockelkorn, P. A. de Jong, H. A. Gietema, J. C. Grutters,
M. Prokop, and B. van Ginneken, “Interactive annotation of textures in
thoracic CT scans,” SPIE Med. Imag.,vol. 7624, pp. 76240X–76248X,
Mar. 2010.
A. Depeursinge, A. Foncubierta-Rodriguez, D. Van De Ville, and
H. Muller, “Three-dimensional solid texture analysis in biomedical imaging: Review and opportunities,” Med. Image Anal., vol. 18, no. 1,
pp. 176–196, Jan. 2014.
W. R. Hersh, H. Muller, J. R. Jensen, J. Yang, P. N. Gorman, and
P. Ruch, “Advancing biomedical image retrieval: Development and analysis of a test collection,” J. Amer. Med. Informat. Assoc., vol. 13, no. 5,
pp. 488–496, Oct. 2006.

292

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

[24] J. Kalpathy-Cramer, H. Muller, S. Bedricks, I. Eggel, A. Seco de Herrera,
and T. Tsikrika, “Overview of the CLEF 2011 medical image classification and retrieval tasks,” in Proc. Conf. Cross-Language Eval. Forum,
Amsterdam, The Netherlands, 2011, pp. 10–20.
[25] Y. Kawata, N. Niki, H. Ohmatsu, M. Kusumoto, R. Kakinuma, K. Mori,
H. Nishiyama, K. Eguchi, M. Kaneko, and N. Moriyama, “Searching similar images for classification of pulmonary nodules in three-dimensional
CT images,” in Proc. IEEE Int. Symp. Biomed. Imag., Washington, DC,
USA, 2002, pp. 189–192.
[26] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, G. R. Lanckriet,
R. Levy, and N. Vasconcelos, “A new approach to cross-modal multimedia
retrieval,” in Proc. Int. Conf. Multimedia, 2010, pp. 251–260.
[27] M. Slaney, K. Weinberger, and W. White, “Learning a metric for music
similarity,” in Proc. 9th Int. Soc. Music Inf. Retrieval Conf., Miami, FL,
USA, 2008, pp. 313–318.
[28] J. Ramos, T. Kockelkorn, B. van Ginneken, M. A. Viergever, R. Ramos,
and A. Campilho, “Supervised content based image retrieval using radiology reports,” in Proc. 9th Int. Conf. Image Analysis Recog., 2012,
pp. 249–258.
[29] J. Ramos, T. Kockelkorn, B. van Ginneken, M. A. Viergever, J. Grutters,
R. Ramos, and A. Campilho, “Learning interstitial lung diseases CT patterns from reports keywords,” in Proc. 5th Int. Workshop Pulm. Image
Anal., Nagoya, Japan, Sep. 2013, pp. 21–32.

[30] I. Sluimer, M. Prokop, and B. van Ginneken, “Toward automated segmentation of the pathological lung in CT,” IEEE Trans. Med. Imag., vol. 24,
no. 8, pp. 1025–1038, Aug. 2005.
[31] J. W. Sammon, “ A nonlinear mapping for data structure analysis,” IEEE
Trans. Comput., vol. C-18, no. 5, pp. 401–409, May 1969.
[32] C. Buckley and E. M. Voorhees, “Evaluating evaluation measure stability,”
in Proc. 23rd Annu. Int. ACM SIGIR Conf. Res. Develop. Inf. Retrieval,
Athens, Greece, 2000, pp. 33–40.
[33] C.-R. Shyu, C. E. Brodley, A. C. Kak, A. Kosaka, A. M. Aisen, and
L. S. Broderick, “ASSERT: A physician-in-the-loop content-based retrieval system for HRCT image databases,” Comput. Vis. Image Understanding, vol. 75, nos. 1/2, pp. 111–132, 1999.
[34] C.-R. Shyu, C. Pavlopoulou, A. C. Kak, C. E. Brodley, and L. S. Broderick,
“Using human perceptual categories for content-based retrieval from a
medical image database,” Comput. Vis. Image Understanding, vol. 88,
no. 3, pp. 119–151, 2002.
[35] E. Voorhees, “The philosophy of information retrieval evaluation,” in
Proc. Conf. Cross-Language Evaluation Forum, 2001, pp. 143–170.

Authors’ photographs and biographies not available at the time of publication.

