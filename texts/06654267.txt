IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

929

Local Mesh Patterns Versus Local Binary Patterns:
Biomedical Image Indexing and Retrieval
Subrahmanyam Murala and Q. M. Jonathan Wu, Senior Member, IEEE

Abstract—In this paper, a new image indexing and retrieval algorithm using local mesh patterns are proposed for biomedical image
retrieval application. The standard local binary pattern encodes
the relationship between the referenced pixel and its surrounding
neighbors, whereas the proposed method encodes the relationship
among the surrounding neighbors for a given referenced pixel in
an image. The possible relationships among the surrounding neighbors are depending on the number of neighbors, P . In addition,
the effectiveness of our algorithm is confirmed by combining it
with the Gabor transform. To prove the effectiveness of our algorithm, three experiments have been carried out on three different
biomedical image databases. Out of which two are meant for computer tomography (CT) and one for magnetic resonance (MR)
image retrieval. It is further mentioned that the database considered for three experiments are OASIS-MRI database, NEMA-CT
database, and VIA/I–ELCAP database which includes region of
interest CT images. The results after being investigated show a
significant improvement in terms of their evaluation measures as
compared to LBP, LBP with Gabor transform, and other spatial
and transform domain methods.
Index Terms—Biomedical image retrieval (CBIR), Gabor transform (GT), local binary pattern (LBP), local mesh patterns (LMeP),
texture.

I. INTRODUCTION
A. Motivation
HERE has been a radical expansion of biomedical images
in medical institutions and hospitals for patient diagnosis.
Database for patient diagnosis exists in different formats such
as computer tomography (CT), magnetic resonance imaging
(MRI), ultrasound (US), X-ray, etc. However, one cannot make
use of this data unless it is organized to allow efficient access,
search, and retrieval. To address this problem, content-based
biomedical image retrieval came into existence. The contentbased image retrieval (CBIR) utilizes visual contents of an image
such as color, texture, shape, faces, spatial layout etc., to represent and index the image database. The previously available
biomedical image retrieval systems are presented in [1]–[6].
The feature extraction in CBIR is a prominent step whose
effectiveness depends upon the method adopted for extracting

T

Manuscript received August 13, 2012; revised February 8, 2013 and June 21,
2013; accepted October 24, 2013. Date of publication November 4, 2013; date
of current version May 1, 2014. This work was supported by the Canada Research Chair program, the Natural Sciences and Engineering Research Council
of Canada (NSERC) Discovery Grant.
The authors are with the Computer Vision and Sensing Systems Lab, Department of Electrical and Computer Engineering, University of Windsor, Windsor,
ON N9B 3P4, Canada (e-mail: muralasu@uwindsor.ca; jwu@uwindsor.ca).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2288522

features from given images. Comprehensive and extensive literature survey on CBIR is presented in [7]–[11].
The visual content descriptors are either global or local. A
global descriptor represents the visual features of the whole image, whereas a local descriptor represents the visual features of
regions or objects to describe the image. Furthermore, these are
subdivided into two categories, spatial and transform domainbased features. The first approach makes use of pixels (or a
group of adjacent pixels) gray value and the second approach
makes use of transformed data of the gray image for feature
extraction.
Texture-based biomedical image retrieval is a branch of texture analysis particularly well suited for the identification of
disease region, and then retrieval of related documents in the
database is making it a star attraction from medical perspective. Cai et al. [12] have used the physiological kinetic feature
which reduces the image storage memory for positron emission tomography (PET) image retrieval. Scott and Shyu have
designed the biomedical media retrieval system [1], where they
utilize the entropy balanced statistical (EBS) k-d tree for feature extraction. The index utilizes statistical properties inherent in large-scale biomedical media databases for efficient and
accurate searches. Rahman et al. [13] have designed the relevance feedback-based biomedical image retrieval system. They
have proposed the query-specific adaptive linear combination of
similarity-matching approach by relying on the image classification and feedback information from users. Nakayama et al. [14]
investigated four objective similarity measures as an image retrieval tool for selecting lesions similar to unknown lesions on
mammograms. Classification of benign and malignant breast
masses based on shape and texture features in sonography images is proposed in [15]. The mass regions were extracted from
the region of interest (ROI) subimage by implementing a hybrid
segmentation approach based on level set algorithms. In [16],
a boosting framework for visuality-preserving distance metric
learning is proposed for medical image retrieval. The mammographic images and dataset from ImageCLEF are used for performance evaluation. Quellec et al. [17] proposed the optimized
wavelet transform for medical image retrieval by adapting the
wavelet basis, within the lifting scheme framework for wavelet
decomposition. The weights are assigned between wavelet subbands. They used the diabetic retinopathy and mammographic
databases for medical image retrieval. The wavelet transformbased brain image retrieval is presented in [18]. The cooccurrence matrix-based retrieval of medical CT and MRI images in
different tissues is can be seen in [19]. Furthermore, the image retrieval of different body parts is proposed in [20], which
employ color quantization and wavelet transform.

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

930

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

B. Related Work
The features, k−d tree [1], cooccurrence matrix [19] etc., are
computationally more expansive. To address this computational
complexity, the local binary pattern (LBP) [21] is proposed. The
LBP operator was introduced by Ojala et al. [21] for texture classification. Success in terms of speed (no need to tune any parameters) and performance is reported in many research areas such
as texture classification [21]–[25], face recognition [26]–[28],
object tracking [29], [30], image retrieval [31]–[37], and interest
point detection [38]. Peng et al. proposed the texture feature extraction based on a uniformity estimation method in brightness
and structure in chest CT images [35]. They used the extended
rotational invariant LBP and the gradient orientation difference
to represent brightness and structure in the image. Unay et al.
proposed the local structure-based region-of-interest retrieval
in brain MR images [36]. Quantitative analysis of pulmonary
emphysema using LBP is presented in [37]. They improved
the quantitative measures of emphysema in CT images of the
lungs by using joint LBP and intensity histograms. Li and Meng
have proposed the automatic recognition of tumor for wireless
capsule endoscopy (WCE) images [39]. The candidate color
texture feature that integrates uniform LBP and wavelet is proposed to characterize WCE images. Furthermore, the detection
of bleeding regions for capsule endoscopy images using LBP is
presented in [40]. Facial paralysis video retrieval system using
LBP is proposed in [41]. The symmetry of facial movements
is measured by the resistor-average distance between LBP features extracted from the two sides of the face. Support vector
machine is applied to provide quantitative evaluation of facial
paralysis.

Fig. 1.

Example of obtaining LBP and LMeP for the 3 × 3 pattern.

Fig. 2.

Circular neighborhood sets for different (P, R).

C. Main Contribution
The pattern-based features available in the literature are encoding the relationship between the center pixel and its surrounding neighbors. This observation has motivated us to propose the local mesh pattern (LMeP), which encodes the relationship among the surrounding neighbors for a given center
pixel in an image. In addition, the effectiveness of our algorithm
is confirmed by combining it with Gabor transform. The performance of the proposed method is tested by conducting three
experiments on three different biomedical databases.
The organization of the paper is as follows. In section I, a brief
review of biomedical image retrieval and related work are given.
Section II presents a concise review of local patterns (LBP and
LMeP). Section III presents the concept of multiscale feature
extraction, proposed system framework and the query matching.
Experimental results and discussions are presented in Section
IV and finally in Section V, we conclude with the summary of
work.
II. LOCAL PATTERNS
A. Local Binary Patterns
The LBP operator was introduced by Ojala et al. [16] for
texture classification. Given a center pixel in the image, LBP
value is computed by comparing its gray value with its neighbors

as shown in Fig. 1, based on (1).
LBPP ,R =

P


2(i−1) × f1 ( gi |R − gc )

(1)

i=1


f1 (x) =

1

x≥0

0

else

(2)

where gc is the gray value of the center pixel, gi |R is the gray
value of neighbor at radius R from the center pixel (gc ), P is
the number of neighbors at a distance (radius) R form the center
pixel (gc ) in an image.
An examples of circular neighbor sets for different configurations of (P, R) can be seen in Fig. 2.
B. Local Mesh Patterns (LMeP)
The idea of the LBP has motivated us to propose the LMeP
for biomedical image retrieval. The LMeP value is computed
based on the relationship among the surrounding neighbors for
a given center pixel in an image [see in (3)]. Fig. 1 illustrates

MURALA AND WU: LOCAL MESH PATTERNS VERSUS LOCAL BINARY PATTERNS

Fig. 3.

LBP and the first three LMeP calculations for a given (P, R).

Fig. 4. Example of LBP and LMeP feature maps: sample image, LBP feature
map, first LMeP feature map, second LMeP feature map and third LMeP feature
map.

the LMeP values calculated for a given 3 × 3 pattern.
LMePjP , R =

P


image. The experimental results demonstrate that the proposed
LMeP shows better performance as compared to LBP, indicating that it can capture more edge information than LBP for
biomedical image retrieval.

2(i−1) × f1 ( gα |R − gi |R )

i=1

α = 1 + mod((i + P + j − 1), P )
∀j = 1, 2, .., (P /2)

(3)

where j represents the LMeP index and mod (x, y) returns the
reminder for x/y operation.
From (3), it can be observed that the possible LMeP patterns
for P neighbors are P /2. In this paper, we consider only first
three LMeP patterns [j = 1, 2, 3 in (3)] for experimentation as
shown in Figs. 1 and 3. Fig. 3 illustrates the LBP and the first
three LMeP calculations for a given (P, R). In this paper, (8,1),
(16, 2) and (24, 3) combinations are considered for experimentation.
For the local pattern with P neighboring pixels, there are 2P
(0 to 2P –1) possible values for both LBP and LMeP, resulting
in a feature vector of length 2P . A high computational cost
is involved in extracting such a feature vector. Thus, uniform
patterns [23] are considered to reduce the computational cost. A
uniform pattern refers to a circular binary representation having
limited discontinuities. In this paper, patterns with two or less
discontinuities in the circular binary representation are termed
as uniform, while rest of the patterns are termed as nonuniform.
Thus, the distinct uniform patterns for a given query image
would be P (P − 1) + 2. The possible uniform patterns for P
= 8 can be seen in [23].
After identifying the local pattern, PTN (the LBP or the first
three LMePs), the whole image is represented by building a
histogram using (4)
HS (l) =

1
N1 × N 2


f2 (x, y) =

N1 
N2


f2 (PTN(j, k), l);

j =1 k =1

l ∈ [0, P (P − 1) + 2]
1 if x = y
0 else

931

(4)
(5)

where N1 × N2 represents the size of an input image.
Fig. 4 illustrates the feature maps obtained by applying the
LBP and the first three LMePs operators on the referenced MR

III. MULTISCALE FEATURE EXTRACTION
In the literature, it is observed that the pattern-based features
are analyzed by combining it with the Gabor transform. Hence,
in this paper, we also confirm the effectiveness of our algorithm
by combining it with Gabor transform (GT).
A. Gabor Transform
Subrahmanyam et al. [42] have given the spatial implementation of GT. A 2-D Gabor function is a Gaussian modulated
by a complex sinusoid. It can be specified by the frequency of
the sinusoid ω and the standard deviations σx and σy of the
Gaussian envelope as follows:
ψ(x, y) =

1
2
2
2
2
e[−(1/2)(x /σ x +y /σ y )+2π j ω x]
2πσx σy

(6)

The Gabor wavelets are obtained by dilation and rotation of
the generating function ψ(x, y)as follows:
ψm n (x, y) = a−m ψ(x , y  )

(7)

where x = a−m (x cos θ + y sin θ)
y  = a−m (−x sin θ + y cos θ)
θ = nπ/K

(8)

where m ∈ {0, . . . , S − 1} and n ∈ {0, . . . , K − 1} represent
scale and orientation, respectively; K and S are the number of
desired orientations and scales, respectively.
The variables in the (6)–(8) are defined as follows:
a = (Uh /Ul )−1/s−1 , ωm ,n = Uh
√
(a + 1) 2 ln 2
σx,m ,n =
2πam (a − 1)Ul
σy ,m ,n =
2π tan



π
2K



1
U h2
2 ln 2

− ( 2π σ x1, m , n )2

(9)
(10)
(11)

932

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

where Uh and Ul are the upper and lower bound of the designing
frequency band, respectively. In this implementation, we used
the following constants as used in the literature: Ul = 0.05,
Uh = 0.49.
The response of Gabor filter is the convolution of Gabor
window with image I, and is given as

∗
Gm n (x, y) =
I((x − s, y − t)ψm
(12)
n (s, t).
s

t

Fig. 5.

Feature vector generation based on LBP and LMeP.

Fig. 6.

Sample images from OASIS database (one image per category).

B. Gabor Local Mesh Patterns (GLMeP)
In this paper, the real part of Gabor responses are utilized for
GLMeP feature maps calculation. The performance of GLMeP
is analyzed with P/2 directions and different scales (see in Section IV-A). From experiments, it is clear that three scales and P/2
directions of GT are showing better performance as compared
to the other scales of GT.
For a given center pixel at (x, y), the GLMeP operators for
P = 8 and R = 1 are calculated as follows:


GLMeP1P , R 
P =8,R =1

⎧
⎫
f1 (G45 ◦ (x − R, y + R) − G0 ◦ (x, y + R)), ⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦ (x − R, y) − G45 ◦ (x − R, y + R)) ⎪
f
(G
1
90
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
−
R,
y
−
R)
−
G
(x
−
R,
y))
90
⎪ 1 135
⎪
⎪
⎪
⎪
⎨ f (G ◦ (x, y − R) − G ◦ (x − R, y − R)) ⎪
⎬
1
0
135
=
⎪
f1 (G45 ◦ (x + R, y − R) − G0 ◦ (x, y − R)) ⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
+
R,
y)
−
G
(x
+
R,
y
−
R))
1
90
45
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
+
R,
y
+
R)
−
G
(x
+
R,
y))
1
135
90
⎪
⎪
⎪
⎪
⎩
⎭
f1 (G0 ◦ (x, y + R) − G135 ◦ (x + R, y + R))


GLMeP2P , R 

TABLE I
PERFORMANCE OF GLMEP WITH DIFFERENT SCALES OF GT IN TERMS OF
ARP ON OASIS-MRI DATABASE

(13)
Similarly, GLMePs are calculated for (16, 2) and (24, 3).
Eventually, the given image is converted to GLMeP images
having values ranging from 0 to P (P −1)+2 (for uniform two
patterns). After calculation of GLMePs, the whole image is
represented by building a histograms supported by (4).

P =8,R =1

⎧
⎫
f1 (G90 ◦ (x − R, y) − G0 ◦ (x, y + R)),
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
−
R,
y
−
R)
−
G
(x
−
R,
y
+
R))
1
135
45
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x,
y
−
R)
−
G
(x
−
R,
y))
1
0
90
⎪
⎪
⎪
⎪
⎪
⎨ f (G ◦ (x + R, y − R) − G ◦ (x − R, y − R))⎪
⎬
1
45
135
(14)
=
⎪
⎪
f1 (G90 ◦ (x + R, y) − G0 ◦ (x, y − R))
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
f1 (G135 ◦ (x + R, y + R) − G45 ◦ (x + R, y − R))⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x,
y
+
R)
−
G
(x
+
R,
y))
1
0
90
⎪
⎪
⎪
⎪
⎩
⎭
f1 (G45 ◦ (x − R, y + R) − G135 ◦ (x + R, y + R))


GLMeP3P , R 
P =8,R =1

⎧
⎫
f1 (G135 ◦ (x − R, y − R) − G0 ◦ (x, y + R)), ⎪
⎪
⎪
⎪
⎪
⎪ f (G ◦ (x, y − R) − G ◦ (x − R, y + R)) ⎪
⎪
⎪
⎪
1
0
45
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
+
R,
y
−
R)
−
G
(x
−
R,
y))
1
45
90
⎪
⎪
⎪
⎪
⎪
⎨ f (G ◦ (x + R, y) − G ◦ (x − R, y − R)) ⎪
⎬
1
90
135
=
⎪
f1 (G135 ◦ (x + R, y + R) − G0 ◦ (x, y − R)) ⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x,
y
+
R)
−
G
(x
+
R,
y
−
R))
1
0
45
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
◦
◦
f
(G
(x
−
R,
y
+
R)
−
G
(x
+
R,
y))
1
45
90
⎪
⎪
⎪
⎪
⎩
⎭
f1 (G90 ◦ (x − R, y) − G135 ◦ (x + R, y + R))
where Gα (x, y) is the response of GT in α direction.

C. Feature Extraction
Fig. 5 illustrates the feature extraction for the LBP and the
proposed method (LMeP). The algorithm for LMeP is given as
follows.
Algorithm: Input: Image; Output: Feature Vector
1) Load the gray scale image (if it is RGB, convert into gray
scale).
2) Generate the first three LMeP operators for each center
pixel.
3) Calculate the local differences among the neighbor pixels.
4) Calculate the binary patterns.
5) Construct the histograms based on uniform two patterns.
6) Form a feature vector by concatenating three histograms.
D. Similarity Measurement

(15)

The feature vector for query image Q represented as fQ =
(fQ 1 , fQ 2 , . . . , fQ L g ), is obtained from feature extraction. Similarly each image in the database is represented with feature vector fDB j = (fDB j 1 , fDB j 2 , . . . , fDB j L g ); j = 1, 2, . . . , |DB|.
The goal is to select the n best images that resemble the query
image. This involves selection of n top matched images by
measuring the distance between query image and images in the
database |DB|. In order to match the images, we use d1 similarity

MURALA AND WU: LOCAL MESH PATTERNS VERSUS LOCAL BINARY PATTERNS

933

TABLE II
GROUPWISE PERFORMANCE OF VARIOUS METHODS IN TERMS OF ARP ON
OASIS-MRI DATABASE

Fig. 8.

Query results of LMeP on OASIS database.

Fig. 9.

Sample images from NEMA database (one image per category).

distance metric [32] computed using (16).
D(Qi , DBj i ) =

Lg 

 fDB j i − fQ i

 1 + fDB + fQ
i=1

ji

i






(16)

where fDB j i is ith feature of jth image in the database |DB|.
IV. EXPERIMENTAL RESULTS AND DISCUSSIONS

Fig. 7 Comparison of proposed method with other existing methods as function of number of top matches on OASIS database. (a) P = 8, R = 1; (b) P =
16, R = 2; and (c) P = 24, R = 3.

In order to analyze the performance of our algorithm for
biomedical image retrieval three experiments were conducted on
three different medical databases. Results obtained are discussed
in the following sections.
Given below are the abbreviations used in the analysis of
result:
LBP: Local binary patterns [21]
GLBP: LBP with GT [21]
DBWP: Directional binary wavelet patterns [32]
LMeP: Local mesh patterns
GLMeP: LMeP with GT

934

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

Fig. 11.

Query results of LMeP on NEMA-CT database.

Fig. 12.

Sample images from VIA/I-ELCAP—CT image database.

images X = (x1 , x2 , . . . , xn ), with the shortest image matching distance is given by (16). If xi ; i = 1, 2, . . . , n belong to
the same category of the query image, we say the system has
correctly matched the desired.
The average retrieval precision (ARP) and average retrieval
rate (ARR) judge the performance of the proposed method those
are calculated by (17)–(20).
For the query imageIq , the precision (P ) and recall (R) are
defined as follows:
Fig. 10. Comparison of proposed method with other existing methods as
function of number of top matches on NEMA-CT database. (a) P = 8, R = 1;
(b) P = 16, R = 2; and (c) P = 24, R = 3.

INTH: Intensity histogram [37]
GLCM1: Gray level cooccurrence matrix Type 1 (Autocorrelation) [37]]
GLCM2: Gray level cooccurrence Matrix Type 2 (Correlation) [37]
GFB: First four central moments of a Gaussian filter bank
with four scales [37]
LBP_P_Ru2: LBP features collected from the uniform two
pattern size (P,R); similar representation is applicable
for all patterns.
In all experiments, each image in the database is used as the
query image. For each query, the system collects n database

Precision: P (Iq ) =

Number of Relevant Images Retrieved
Total Number of Images Retrieved



|DB|


1
ARP =
P (Ii )
|DB| i=1


(17)
(18)
n ≤10

Recall: R(Iq )
=

Number of Relevant Images Retrieved
Total Number of Relevant Images in the Database


|DB|

1 
ARR =
R(Ii )
|DB| i=1


(19)
(20)
n ≥10

MURALA AND WU: LOCAL MESH PATTERNS VERSUS LOCAL BINARY PATTERNS

935

Fig. 13. Comparison of the LMeP/GLMeP with other existing spatial and transform domain methods in terms of: (a)–(c) ARP and (d)–(f) ARR on VIA/IELCAP–CT database.

A. Experiment 1
The Open Access Series of Imaging Studies (OASIS) [43]
is a series of magnetic resonance imaging (MRI) dataset that
is publicly available for study and analysis. This dataset consists of a cross-sectional collection of 421 subjects aged between 18 to 96 years. The MRI acquisition details are available
in [43].
For image retrieval purpose, we grouped these 421 images
into four categories (124, 102, 89, and 106 images) based on
the shape of ventricular in the images. Fig. 6 depicts the sample
images of OASIS database (one image from each category).

Table I illustrates the performance of GLMeP with different scales and P/2 directions on OASIS-MRI database. From
Table I, it is clear that three scales and P/2 directions are outperforming the other scales of GLMeP.
Table II illustrates the group wise performance of various
method with and without GT in terms of ARP on OASIS-MRI
database. From Table II, the following inference is drawn for
the performance of the proposed method with other methods in
terms of ARP at n = 10.
1) ARP of LMePu2_8_1, LMePu2_16_2, and LMePu2_
24_3 is more as compared to LBPu2_8_1, LBPu2_16_2,
and LBPu2_24_3, respectively.

936

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

2) ARP of GLMePu2_8_1, GLMePu2_16_2, and GLMePu
2_24_3 is more as compared to LBPu2_8_1,
LBPu2_16_2, GLBPu2_24_3, DBWPu2_8_1, DBWPu2_16_2, and DBWPu2_24_3, respectively on corresponding (P, R).
Fig. 7(a)–(c) shows the graphs depicting the retrieval performance of the proposed method and other existing methods as
a function of number of top matches. From Fig. 7 and above
observations, it is evident that the proposed method outperforms
the other existing methods. Fig. 8 illustrates two query results
of the proposed method by considering five top matches.
B. Experiment 2
The digital imaging and communications in medicine (DICOM) standard was created by the National Electrical Manufacturers Association (NEMA) [44] to aid the distribution and
viewing of medical images, such as computer tomography (CT)
scans, MRIs, and ultrasound. For this experiment, we have collected 681 CT scans of different parts of human body and these
are grouped into 13 categories (45, 59, 46, 29, 36, 18, 37, 14,
139, 46, 143, 33, and 36 images). Fig. 9 depicts the sample
images of NEMA database (one image from each category).
The retrieval performance of the proposed method (LMePu2/
GLMePu2) and other existing methods (LBPu2/GLBPu2 and
DBWPu2) as a function of number of top matches are given
in Fig. 10(a)–(c). In this experiment, DBWPu2_16_2 and
DBWP_24_3 are showing some similar performance to the
proposed methods LMeP_16_2/ GLMeP_16_2 and LMeP_
24_3/GLMeP_24_3, respectively, because binary wavelet patterns also extracts good directional information from this
database. However, the feature vector length of DBWP is very
high as compared to the proposed method (see in Section IV-D),
which is an important requirement for online applications. From
Fig. 10, it is concluded that the LMeP/GLMeP outperforms other
existing methods. Fig. 11 illustrates two query results of the proposed method by considering five top matches on NEMA-CT
database.
C. Experiment 3
Vision and image analysis (VIA) group and international early
lung cancer action program (I-ELCAP) created a computer tomography (CT) dataset [45] for performance evaluation of different computer-aided detection systems. These images are in
DICOM (digital imaging and communications in medicine) format. The CT scans were obtained in a single breath hold with a
1.25-mm slice thickness. The locations of nodules detected by
the radiologist are also provided. For experiments, we have selected 10 scans. Each scan has 100 images with resolution 512 ×
512. Furthermore, ROIs were annotated manually to construct
the ROI CT image database. Fig. 12 depicts the sample images
of VIA/I-ELCAP database (one image from each category).
Fig. 13 illustrates the retrieval performance of the proposed method (LMeP/GLMeP) and other existing methods
(LBP/GLBP, INTH, GLCM1, GLCM2 and GFB) in terms of
ARP and ARR. Fig. 14 illustrates the individual group performances of the LMeP/GLMeP and other existing methods in

Fig. 14. GroupWise performance of LMeP/GLMeP and other existing methods in terms of : (a) precision and (b) recall on VIA/I-ELCAP—CT database.
TABLE III
FEATURE VECTOR LENGTH OF QUERY IMAGE USING VARIOUS METHODS

terms of precision and recall. From Figs. 13 and 14, it is clear
that the proposed method (LMeP/GLMeP) outperforms other
existing methods in terms of precision, recall, ARP, and ARR
on VIA/I-ELCAP—CT database.
D. Feature Vector Length V/S Performance
Table III shows the feature vector length and execution time
for a given query image of size 256 × 256 using LBP, GLBP,
DBWP, LMeP, and GLMeP. The experimentation is carried out
on core2Duo computer with 2.66 GHz and all methods are
implemented on the MATLAB 7.6 software. From the Table III,
it is clear that the feature vector length of GLMeP is 8.8 times
less as compared to DBWP and is outperforming the DBWP
and other existing methods in terms of ARP and ARR on three
different biomedical databases. The feature vector length and
execution time of the LMeP is more as compared to the LBP, as
it outperforms.

MURALA AND WU: LOCAL MESH PATTERNS VERSUS LOCAL BINARY PATTERNS

1) The LBP by 13.9%, 2.4%, and 9.2% in terms of ARP on
OASIS-MRI, NEMA-CT and VIA/I-ELCAP-CT image
databases, respectively.
2) The LBP by 3.12% in terms of ARR on VIA/I-ELCAP-CT
image database.
V. CONCLUSION
In this paper, a new scheme, LMeP, is presented for biomedical image retrieval application. The LMeP is different from the
existing LBP in a manner that it encodes the relationship among
the surrounding neighbors, whereas LBP encodes the relationship between the reference pixel and its surrounding neighbors
in an image. The performance of the proposed method is tested
on three different biomedical databases. The result after being
investigated show a significant improvement in terms of precision, recall, ARR, and ARP as compared to LBP and other
existing methods on respective databases.

937

[14]
[15]
[16]

[17]
[18]

[19]

[20]

ACKNOWLEDGMENT
The authors would like to thank the associate editor and
anonymous reviewers for insightful comments and helpful suggestions to improve the quality, which have been incorporated
in this manuscript.

[21]
[22]
[23]

REFERENCES
[1] G. Scott and C.-R. Shyu, “Knowledge-Driven multidimensional indexing
structure for biomedical media database retrieval,” IEEE Trans. Inf. Tech.
Biomed., vol. 11, no. 3, pp. 320–331, May 2007.
[2] H. C. Akakin and M. N. Gurcan, “Content-Based microscopic image retrieval system for multi-image queries,” IEEE Trans. Inf. Tech. Biomed.,
vol. 16, no. 4, pp. 758–769, Jul. 2012.
[3] A. Quddus and O. Basir, “Semantic image retrieval in magnetic resonance
brain volumes,” IEEE Trans. Inf. Tech. Biomed., vol. 16, no. 3, pp. 348–
355, May 2012.
[4] L. Zheng, A. W. Wetzel, J. Gilbertson, and M. J. Becich, “Design and analysis of a content-based pathology image retrieval system,” IEEE Trans.
Inf. Tech. Biomed., vol. 7, no. 4, pp. 249–255, Dec. 2003.
[5] X. Xu, D.-J. Lee, S. Antani, and L. R. Long, “A Spine X-Ray image
retrieval system using partial shape matching,” IEEE Trans. Inf. Tech.
Biomed., vol. 12, no. 1, pp. 100–108, Jan. 2008.
[6] B. André, T. Vercauteren, A. M. Buchner, M. B. Wallace, and N. Ayache,
“Learning semantic and visual similarity for endomicroscopy video retrieval,” IEEE Trans. Med. Imag., vol. 31, no. 6, pp. 1276–1288, Jun.
2012.
[7] Y. Rui and T. S. Huang, “Image retrieval: Current techniques, promising
directions and open issues,” J. Vis. Commun. Image Represent., vol. 10,
pp. 39–62, 1999.
[8] A. W. M. Smeulders, M. Worring, S. Santini, A. Gupta, and R. Jain,
“Content-based image retrieval at the end of the early years,” IEEE Trans.
Pattern Anal. Mach. Intell., vol. 22, no. 12, pp. 1349–1380, Dec. 2000.
[9] M. Kokare, B. N. Chatterji, and P. K. Biswas, “A survey on current content
based image retrieval methods,” IETE J. Res., vol. 48, no. 3 and 4, pp. 261–
271, 2002.
[10] Y. Liu, D. Zhang, G. Lu, and W.-Y. Ma, “A survey of content-based image
retrieval with high-level semantics,” Pattern Recog., vol. 40, pp. 262–282,
2007.
[11] H. Muller, N. Michoux, D. Bandon, and A. Geisbuhler, “A review of
content-based image retrieval systems in medical applications–Clinical
benefits and future directions,” J. Med. Inf., vol. 73, no. 1, pp. 1–23, 2004.
[12] W. Cai, D. D. Feng, and R. Fulton, “Content-based retrieval of dynamic
PET functional images,” IEEE Trans. Inf. Tech. Biomed., vol. 4, no. 2,
pp. 152–158, Jun. 2000.
[13] M. M. Rahman, S. K. Antani, and G. R. Thoma, “A learning-based similarity fusion and filtering approach for biomedical image retrieval us-

[24]
[25]
[26]
[27]

[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]

ing SVM classification and relevance feedback,” IEEE Trans. Inf. Tech.
Biomed., vol. 15, no. 4, pp. 640–646, Jul. 2011.
R. Nakayama, H. Abe, J. Shiraishi, and K. Doil, “Evaluation of objective similarity measures for selecting similar images of mammographic
lesions,” J. Digital Imag., vol. 24, no. 1, pp. 75–85, 2011.
F. S. Zakeri, H. Behnam, and N. Ahmadinejad, “Classification of benign
and malignant breast masses based on shape and texture features in sonography images,” J. Med. Syst., vol. 36, no. 3, pp. 1621–1627, 2012.
L. Yang, Student, R. Jin, L. Mummert, R. Sukthankar, A. Goode,
B. Zheng, S. C. H. Hoi, and M. Satyanarayanan, “A boosting framework
for visuality-preserving distance metric learning and its application to
medical image retrieval,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32,
no. 1, pp. 33–44, Jan. 2010.
G. Quellec, M. Lamard, G. Cazuguel, B. Cochener, and C. Roux, “Wavelet
optimization for content-based image retrieval in medical databases,” J.
Med. Image Anal., vol. 14, pp. 227–241, 2010.
A. Traina, C. Castanon, and C. Traina Jr., “Multiwavemed: a system for
medical image retrieval through wavelets transformations,” in Proc. 16th
IEEE Symp. Comput.-Based Med. Syst., New York, NY, USA, 2003,
pp. 150–155.
J. C. Felipe, A. J. M. Traina, and C. Traina Jr., “Retrieval by content of
medical images using texture for tissue identification,” in Proc. 16th IEEE
Symp. Comput.-Based Med. Syst., New York, NY, USA, 2003, pp. 175–
180.
H. Muller, A. Rosset, J.-P. Vall´et, and A. Geisbuhler, “Comparing feature
sets for content-based image retrieval in a medical case database,” in Proc.
SPIE Med. Imag., PACS Imag. Inf., San Diego, USA, 2004, pp. 99–109.
T. Ojala, M. Pietikainen, and D. Harwood, “A comparative study of texture measures with classification based on feature distributions,” Pattern
Recog., vol. 29, no. 1, pp. 51–59, 1996.
T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and
rotation invariant texture classification with local binary patterns,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, Jul. 2002.
Z. Guo, L. Zhang, and D. Zhang, “Rotation invariant texture classification using LBP variance with global matching,” Pattern Recog., vol. 43,
pp. 706–716, 2010.
S. Liao, M. W. K. Law, and A. C. S. Chung, “Dominant local binary patterns for texture classification,” IEEE Tans. Image Proc., vol. 18, no. 5,
pp. 1107–1118, May 2009.
Z. Guo, L. Zhang, and D. Zhang, “A completed modeling of local binary pattern operator for texture classification,” IEEE Tans. Image Proc.,
vol. 19, no. 6, pp. 1657–1663, Jun. 2010.
T. Ahonen, A. Hadid, and M. Pietikainen, “Face description with local
binary patterns: Applications to face recognition,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 28, no. 12, pp. 2037–2041, Dec. 2006.
B. Zhang, Y. Gao, S. Zhao, and J. Liu, “Local derivative pattern versus
local binary pattern: Face recognition with higher-order local pattern descriptor,” IEEE Trans. Image Process., vol. 19, no. 2, pp. 533–544, Feb.
2010.
X. Tan and B. Triggs, “Enhanced local texture feature sets for face recognition under difficult lighting conditions,” IEEE Trans. Image Process.,
vol. 19, no. 6, pp. 1635–1650, Jun. 2010.
J. Ning, L. Zhang, D. Zhang, and W. Chengke, “Robust object tracking
using joint color-texture histogram,” Int. J. Pattern Recog. Artif. Intell.,
vol. 23, no. 7, pp. 1245–1263, 2009.
S. Murala, R. P. Maheshwari, and R. Balasubramanian, “Local maximum
edge binary patterns: A new descriptor for image retrieval and object
tracking,” Signal Process., vol. 92, pp. 1467–1479, 2012.
S. Murala, R. P. Maheshwari, and R. Balasubramanian, “Directional local
extrema patterns: A new descriptor for content based image retrieval,” Int.
J. Multimedia Inf. Retrieval, vol. 1, pp. 191–203, 2012.
S. Murala, R. P. Maheshwari, and R. Balasubramanian, “Directional binary wavelet patterns for biomedical image indexing and retrieval,” J.
Med. Syst., vol. 36, pp. 2865–2879, 2012.
V. Takala, T. Ahonen, and M. Pietikainen, “Block-based methods for
image retrieval using local binary pat-terns,” in Proc. Scandinavian Conf.
Image Anal., 2005, vol. 3450, pp. 882–891.
C.-H. Yao and S.-Y. Chen, “Retrieval of translated, rotated and scaled
color textures,” Pattern Recog., vol. 36, pp. 913–929, 2003.
S. Peng, D. Kim, S. Lee, and M. Lim, “Texture feature extraction on uniformity estimation for local brightness and structure in chest CT images,”
J. Compt. Biol. Med., vol. 40, pp. 931–942, 2010.
D. Unay, A. Ekin, and R. S. Jasinschi, “Local structure-based region-ofinterest retrieval in brain MR images,” IEEE Trans. Inf. Tech. Biomed.,
vol. 14, no. 4, pp. 897–903, Jul. 2010.

938

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 3, MAY 2014

[37] L. Sørensen, S. B. Shaker, and M. de Bruijne, “Quantitative analysis of
pulmonary emphysema using local binary patterns,” IEEE Trans. Med.
Imag., vol. 29, no. 2, pp. 559–569, Feb. 2010.
[38] M. Heikkil, MattiPietikainen, and C. Schmid, “Description of interest
regions with local binary patterns,” Pattern Recog., vol. 42, pp. 425–436,
2009.
[39] B. Li and M. Q.-H. Meng, “Tumor recognition in wireless capsule endoscopy images using textural features and SVM-Based feature selection,” IEEE Trans. Inf. Tech. Biomed., vol. 16, no. 3, pp. 323–329, May
2012.
[40] B. Li and M. Q.-H. Meng, “Computer-Aided detection of bleeding regions
for capsule endoscopy images,” IEEE Trans. Biomed. Eng., vol. 56, no. 4,
pp. 1032–1039, Apr. 2009.
[41] S. He, J. J. Soraghan, B. F. O’Reilly, and D. Xing, “Quantitative analysis
of facial paralysis using local binary patterns in biomedical videos,” IEEE
Trans. Biomed. Eng., vol. 56, no. 7, pp. 1864–1870, Jul. 2009.
[42] S. Murala, R. P. Maheshwari, and R. Balasubramanian, “Local tetra patterns: A new feature de-scriptor for content based image retrieval,” IEEE
Trans. Image Process., vol. 21, no. 5, pp. 2874–2886, May 2012.
[43] D. S. Marcus, T. H. Wang, J. Parker, J. G. Csernansky, J. C. Morris, and
R. L. Buckner, “Open access series of imaging studies (OASIS): Crosssectional MRI data in young, middle aged, nondemented, and demented
older adults,” J. Cogn. Neurosci., vol. 19, no. 9, pp. 1498–1507, 2007.
[44] NEMA-CT image database. (2012). [Online]. Available: ftp://medical.
nema.org/medical/Dicom/Multiframe/
[45] VIA/I-ELCAP CT Lung Image Dataset, (2012). [Online] Available:
http://www.via.cornell.edu/databases/lungdb.html

Subrahmanyam Murala was born in India in 1985.
He received the M.Tech. and Ph.D. degrees from the
Department of Electrical Engineering, Indian Institute of Technology Roorkee, Roorkee, India, in 2009
and 2012 respectively.
He is currently a Postdoctoral Researcher at Computer Vision and Sensing Systems Lab, Department
of Electrical and Computer Engineering, University
of Windsor, Windsor, ON, Canada.
His major research interests include image retrieval, medical imaging, face recognition, pattern
recognition, and object tracking.

Q. M. Jonathan Wu (M’92–SM’09) received the
Ph.D. degree in electrical engineering from the University of Wales, Swansea, U.K., in 1990.
He was with the National Research Council of
Canada for ten years from 1995, where he became a
Senior Research Officer and a Group Leader. He is
currently a Professor with the Department of Electrical and Computer Engineering, University of Windsor, Windsor, ON, Canada. He has published more
than 250 peer-reviewed papers in computer vision,
image processing, intelligent systems, robotics, and
integrated microsystems. His current research interests include 3-D computer
vision, active video object tracking and extraction, interactive multimedia, sensor analysis and fusion, and visual sensor networks.
Dr. Wu holds the Tier 1 Canada Research Chair in Automotive Sensors and
Information Systems. He is an Associate Editor for the IEEE TRANSACTIONS
ON SYSTEMS, MAN, and CYBERNETICS PART A, and the International Journal
of Robotics and Automation. He has served on technical program committees
and international advisory committees for many prestigious conferences.

