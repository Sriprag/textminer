2882

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 12, DECEMBER 2014

Automated Quantitative Analysis of Capnogram
Shape for COPD–Normal and
COPD–CHF Classification
Rebecca J. Mieloszyk, Student Member, IEEE, George C. Verghese, Fellow, IEEE, Kenneth Deitch, Brendan Cooney,
Abdullah Khalid, Milciades A. Mirre-González, Thomas Heldt, Senior Member, IEEE, and Baruch S. Krauss∗

Abstract—We develop an approach to quantitative analysis of
carbon dioxide concentration in exhaled breath, recorded as a
function of time by capnography. The generated waveform—or
capnogram—is currently used in clinical practice to establish the
presence of respiration as well as determine respiratory rate and
end-tidal CO2 concentration. The capnogram shape also has diagnostic value, but is presently assessed qualitatively, by visual
inspection. Prior approaches to quantitatively characterizing the
capnogram shape have explored the correlation of various geometric parameters with pulmonary function tests. These studies
attempted to characterize the capnogram in normal subjects and
patients with cardiopulmonary disease, but no consistent progress
was made, and no translation into clinical practice was achieved.
We apply automated quantitative analysis to discriminate between
chronic obstructive pulmonary disease (COPD) and congestive
heart failure (CHF), and between COPD and normal. Capnograms
were collected from 30 normal subjects, 56 COPD patients, and
53 CHF patients. We computationally extract four physiologically
based capnogram features. Classification on a hold-out test set was
performed by an ensemble of classifiers employing quadratic discriminant analysis, designed through cross validation on a labeled
training set. Using 80 exhalations of each capnogram record in
the test set, performance analysis with bootstrapping yields areas
under the receiver operating characteristic (ROC) curve of 0.89
Manuscript received March 25, 2014; revised June 3, 2014; accepted June
12, 2014. Date of publication June 24, 2014; date of current version November
18, 2014. The work of R. J. Mieloszyk was supported by an ASEE NDSEG
fellowship. Asterisk indicates corresponding author.
R. J. Mieloszyk and G. C. Verghese are with the Research Laboratory of Electronics and the Department of Electrical Engineering and Computer Science,
Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail:
rjasher@mit.edu; verghese@mit.edu).
K. Deitch is with the Department of Emergency Medicine, Einstein Medical
Center, Philadelphia, PA 19141 USA, and also with Thomas Jefferson University, Philadelphia, PA 19107 USA (e-mail: deitchk@einstein.edu).
B. Cooney is with the Department of Emergency Medicine, Einstein Medical
Center, Philadelphia, PA 19141 USA (e-mail: cooneybr@einstein.edu).
A. Khalid was with the Department of Emergency Medicine, Einstein Medical Center, Philadelphia, PA 19141 USA. He is now with the Department of
Medicine, University of Pittsburgh Medical Center Mercy Hospital, Pittsburgh,
PA 15213 USA (e-mail: khalidab@einstein.edu).
M. A. Mirre-González was with the Department of Emergency Medicine, Einstein Medical Center, Philadelphia, PA 19141 USA. He is now with the School of
Medicine, Universidad Iberoamericana, Santo Domingo, Dominican Republic,
Domingo 10205, Dominican Republic (e-mail: m.mirre@prof.unibe.edu.do).
T. Heldt was with the Research Laboratory of Electronics, Massachusetts
Institute of Technology, Cambridge MA 02139 USA. He is now with the Institute for Medical Engineering and Science, the Research Laboratory of Electronics, and the Department of Electrical Engineering and Computer Science,
Massachusetts Institute of Technology, Cambridge, MA 02139 USA (e-mail:
thomas@mit.edu).
* B. S. Krauss is with the Division of Emergency Medicine, Boston Children’s Hospital, Boston, MA 02115 USA, and also with the Department
of Pediatrics, Harvard Medical School, Boston, MA 02115 USA (e-mail:
baruch.krauss@childrens.harvard.edu).
Digital Object Identifier 10.1109/TBME.2014.2332954

(95% CI: 0.72–0.96) for COPD/CHF classification, and 0.98 (95%
CI: 0.82–1.0) for COPD/normal classification. This classification
performance is obtained with a run time sufficiently fast for realtime monitoring.
Index Terms—Capnography, chronic obstructive pulmonary
disease (COPD), classification, congestive heart failure (CHF), ensemble learning.

I. INTRODUCTION
APNOGRAPHY measures the partial pressure of carbon
dioxide in exhaled breath (PeCO2 ) as a function of time
or exhaled volume. Capnography is an essential element of
modern anesthesia and respiratory care, and is available in every
operating room, intensive care unit, emergency department, and
ambulance system in the United States and Canada. We consider
only time-based capnography, which is the predominant form.
This requires breathing normally with a nasal cannula and is thus
noninvasive and effort-independent. The resulting waveform, or
capnogram, provides a graphic representation of PeCO2 as a
function of time.
The capnogram is currently used to establish the presence
of respiration and to determine respiratory rate and end-tidal
PeCO2 . However, the capnogram also exhibits a characteristic
shape related to ventilation and perfusion characteristics of the
lung [1]–[3]. While visual inspection of capnogram shape can
discern gross changes, it cannot reliably recognize and, therefore, systematically leverage small gradations in shape that may
have diagnostic value.
Computational analysis of the capnogram was considered as
early as the 1950s, but computing resources were not sufficient
to support the endeavor [4]. Later studies largely focused on
isolated evaluation of various numerical parameters and indices
describing capnogram shape [5], [6].
We develop quantitative methods suited to automated realtime capnogram analysis and apply these to distinguishing
two of the most common cardiorespiratory diseases: chronic
obstructive pulmonary disease (COPD) and congestive heart
failure (CHF) [7]. COPD results from resistive obstruction to
airflow on exhalation [8]. CHF is primarily a cardiac disease
that can result in fluid buildup in the lungs, thus restricting lung
volume and movement and impeding gas exchange [9]. Capnograms of obstructive lung disease (e.g., COPD and asthma)
exhibit a characteristic “shark’s fin” shape, marked by significant curvature [5], [10], [11] that is thought to result from the

C

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

MIELOSZYK et al.: AUTOMATED QUANTITATIVE ANALYSIS OF CAPNOGRAM SHAPE

2883

TABLE I
PATIENT NUMBERS AND DEMOGRAPHICS BY CLASS
Class
Normal
COPD
CHF

Number of Records (Training/Test)

Median Age (Years)

Age Range (Years)

% Female

30 (20/10)
56 (33/23)
53 (31/22)

24
58
65

18–59
50–82
45–99

36.7
60.9
57.7

staggered arrival of alveolar gas [3]. Comparatively little is
known about capnogram morphology in spontaneously breathing patients with acute CHF and other restrictive lung diseases.
COPD and CHF present with similar symptoms, such as dyspnea, but have different pathophysiology and, therefore, require
different clinical management [12]. In the clinical context, and
especially in prehospital and emergency settings where determination of CHF and COPD may not be straightforward, a
diagnostic test for rapid and accurate classification is crucial for
initiating effective treatment. Current methods rely on history
and physical examination, a non-point-of-care biomarker [13]
when available, and spirometry, which requires forced exhalation and can be difficult to perform in children and patients with
acute respiratory distress. Also of interest clinically is a noninvasive screening test, to distinguish between COPD and normal
capnograms, for use in primary care and pre-operative testing.
Here, we evaluate capnography as a noninvasive, real-time, and
effort-independent approach to respiratory disease classification
or screening.
Section II of the paper describes our data collection and partitioning procedures. Section III addresses capnogram preprocessing and classifier design using the training set. Performance
results on the hold-out test set are described in Section IV. Finally, Section V concludes with a discussion of our results.
II. DATA COLLECTION AND PARTITIONING

USA), for a single recording of 10–30 min. The capnograph
collects a continuous sample at a flow rate of 50 mL/min and
records the instantaneous PeCO2 every 50 ms.
After informed consent was obtained from the normal subjects at BCH and MIT, the subjects were seated and connected
by nasal cannula to a Capnostream 20 for a single recording
(3 min at BCH; 15 min at MIT).
C. Data Partitioning
The data were collected, de-identified, and made available
for analysis in two stages. Capnograms from the first stage constituted the training set; they were tagged with the appropriate
labels (COPD, CHF, or normal) and used to develop and train
our COPD/CHF and COPD/normal classification algorithms.
Records from the second stage constituted the test set; the labels
on these were initially withheld to permit blinded application
of our algorithms to these records and then revealed to allow
final evaluation of performance. Analysis of the data from each
stage was only undertaken after data collection for that stage
was complete.
The labeled training set comprised 20 normal, 31 CHF, and
33 COPD subjects recorded in the first phase of data collection
(see Table I). The initially unlabeled hold-out test set comprised
10 normal, 22 CHF, and 23 COPD subjects. Four patients who
presented with a mixed COPD/CHF picture were excluded, as
prescribed by our criteria (see the Appendix).

A. Patient Population
Data were collected prospectively on a convenience sample
of patients over a seven-year period (June 2006–April 2013) at
three sites. Data on normal, healthy subjects were collected at
Boston Children’s Hospital (BCH) and Massachusetts Institute
of Technology (MIT), Cambridge, Massachusetts. The data collected at Einstein Medical Center, Philadelphia, Pennsylvania,
were from COPD and CHF patients presenting to the emergency
department (ED) with acute respiratory exacerbation. The study
protocols for each site were approved by the applicable Institutional Review Boards.
B. Data Collection
Adult patients over 18 years old presenting to the Einstein
Medical Center ED with a chief complaint of moderate to severe dyspnea underwent initial clinical assessment, including
history, physical examination, and vital signs. Those who met
the COPD or CHF eligibility criteria detailed in the Appendix
were enrolled, and informed consent was obtained. These patients were seated and connected by nasal cannula to a portable
capnograph (Capnostream 20, Oridion Medical, Needham, MA,

III. PREPROCESSING AND CLASSIFIER DESIGN
A. Preprocessing
The steps involved in quantifying a capnographic record comprised: preprocessing to mark the onset and offset of each exhalation; construction of a template representing the average
exhalation profile in the record; removal of outlier exhalations
based on deviation from the template; and extraction of selected
physiologically based features from each exhalation.
The onset and offset of each exhalation segment in every
record were recognized algorithmically by the start of the positive and negative slope, respectively. A template exhalation was
then constructed by superposing exhalations, each anchored at
a PeCO2 of 15 mmHg (a value that each valid exhalation trace
is expected to cross early in exhalation), and then computing
the mean at each time point. The range of variation from the
template was determined by computing, at each time point, the
standard deviation of the superposed exhalations. An example
template is shown in Fig. 1.
Atypical exhalations were recognized and excluded from consideration by using the capnogram template as an exemplary

2884

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 12, DECEMBER 2014

B. Feature Selection and Classifier Design

Fig. 1. Template exhalation from a COPD capnogram. Capnogram exhalations (blue) are aligned at 15 mmHg PeCO2 and then vertically averaged to
construct the capnogram template (red).

Fig. 2. Representative normal, COPD, and CHF capnograms, with the segment between each detected exhalation onset and offset highlighted in red.

breath. Each exhalation was scored based on an aggregate measure of its deviation from the template exhalation, referenced to
the appropriate standard deviation. Exhalations whose aggregate
deviation exceeded a specified amount were excluded from analysis and classification. Results of preprocessing are illustrated
in Fig. 2, which shows sample capnograms from each class of
subjects. The detected exhalation segment for each breath is
highlighted.
After automated preprocessing of each record to eliminate
outlier exhalations, the first 80 valid exhalations in each record
were used for subsequent analysis. Typically, three to five outlier
exhalations (but sometimes as many as 30) were eliminated from
a record in the course of accumulating these 80 valid exhalations.

A classifier in the setting of this study is an algorithm that
examines selected features of a capnographic record to decide
which of two possible classes the record falls into. Our classification of the capnogram, as COPD versus CHF or as COPD
versus normal, uses an ensemble-based method [14], [15] that
aggregates the individual classifications or “votes” of 50 separate classifiers, referred to as “voters.” Each of these voters
is designed or “trained” using a different subset of the training set. The aggregation of the votes of the 50 individual voters then generates the final classification of each record, via a
threshold comparison of the relative numbers of votes for each
class.
The individual voter’s classification rule was obtained by
quadratic discriminant analysis (QDA) [16], using features extracted from 80 consecutive valid exhalations in each of a designated subset of the records in the training set. QDA generates a
quadric surface in the feature space to separate the two classes.
(In a three-dimensional feature space, for instance, the separating surface would be some form of ellipsoid, paraboloid, or
hyperboloid.)
A voter designed through QDA is determined by its training data and a single design parameter. In a probabilistic setting, where two-class QDA corresponds to minimum-errorprobability classification of samples from two multivariate
Gaussian subpopulations, this parameter would reflect the relative prior probabilities or prevalences of the two classes.
However, we used it only as a convenient tuning parameter.
The four physiologically based features used for our voter
design were selected through the cross-validation analysis described later and comprised the following: exhalation duration; maximum PeCO2 or end-tidal PeCO2 (ETCO2 ); time
spent at ETCO2 ; and end-exhalation slope. Exhalation duration
is the time from exhalation onset to offset. Maximum PeCO2 is
the PeCO2 value at exhalation offset. Time spent at ETCO2 is
the duration for which PeCO2 remains at its maximum value.
The end-exhalation slope was computed as the slope of a straight
line fit to the last five PeCO2 values of the exhalation. These
features are marked in the exhalations in Fig. 3.
The training and voting for each voter take place in the
four-dimensional space defined by the features. We pursued
two approaches to this training and voting. In the breath-bybreath approach, the features of each exhalation in the set of
breaths under consideration contribute a point to the feature
space, so there are as many points as there are exhalations.
In the feature-mean approach, the values of these respective
features are averaged across all the exhalations under consideration in a given record to define a single point in the feature
space.
Classifier performance during the design stage was assessed
using a tenfold cross-validation strategy on the training set,
as summarized in Fig. 4(a). A distinct 10% subset of the full
training data was held out for validation in each fold, and an
ensemble of voters was developed using the remaining 90% of
the training data in the fold. Each of the 50 voters in the fold was
designed using a randomly selected 70% of the available records

MIELOSZYK et al.: AUTOMATED QUANTITATIVE ANALYSIS OF CAPNOGRAM SHAPE

2885

Fig. 3. Four features extracted from the capnogram and used for classification. These comprise exhalation duration, end-tidal CO2 (ETCO2 ), end-exhalation
slope, and time spent at ETCO2 .

Fig. 4.

Cross validation for (a) classifier design and (b) bootstrapping for final classifier evaluation.

in the fold, with the remaining 30% reserved for tuning of the
parameter for that voter, to obtain minimum misclassification.
The classification performance of the ensemble of trained voters
on each fold was then evaluated on the held-out 10% validation
set for the fold, using both the breath-by-breath approach and
the feature-mean approach. The performance statistics over the
tenfold then defined the expected range of performance of our
classification methodology.

The receiver operating characteristic (ROC) curve for
COPD/CHF classification on the training data was obtained by
varying the voting threshold in 5% increments and plotting the
fold-averaged sensitivity for CHF detection (versus COPD) as
a function of the fold-averaged specificity [17]. The results for
the breath-by-breath approach using 80 exhalations are shown
in Fig. 5(a). The indicated vertical and horizontal error bars
show, for each threshold and across all folds, the minimum and

2886

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 12, DECEMBER 2014

Fig. 5. Training-set ROC curves (obtained by varying the voting threshold in 5% increments) using breath-by-breath classification on 80 consecutive valid
exhalations, and an ensemble of 50 voters. (a) COPD/CHF classification (AUROC 0.87) and (b) COPD/normal classification (AUROC 0.98).

maximum sensitivities and specificities, respectively. The area
under the ROC curve (AUROC) is 0.87; the AUROCs across all
cross-validation folds are in the range 0.80–0.94. Similarly, the
ROC curve for COPD/normal classification on the training data
[see Fig. 5(b)] shows sensitivity for COPD detection (versus
normal) as a function of specificity. The AUROC is 0.98, with
a range of 0.95–1.
The computations for preprocessing, feature extraction, tenfold cross validation, and ROC construction across the entire
training set took under 2.7 min on a 2012 MacBook Pro laptop
(Apple, Cupertino, California) with 4 GB RAM and a 2.2 GHz
Intel Core i7 processor running MATLAB 2013a (MathWorks,
Natick, Massachusetts).
C. Final Classifier Design
Having used cross validation to delineate the expected range
of performance of our classifiers, we constructed a final ensemble of 50 voters for classification of the hold-out test set in our
COPD/CHF and COPD/normal applications. This ensemble was
designed by the same process used during the cross-validation
phase. However, now we employed the entire training set, rather
than just a 90% cut associated with a fold [see the left half of
Fig. 4(b)].
The voting thresholds required to complete the specifications
of the final COPD/CHF and COPD/normal classifiers were chosen using the training-set ROC curves (see Fig. 5). To minimize
the dependence of classification accuracy on prevalence of the
respective classes within the test set, we picked a threshold that
yielded approximately equal sensitivity and specificity. The motivation for this is the following expression for the probability
of correct classification:
PC = (Sensitivity · P+ ) + (Specificity · P− )

(1)

where P+ is the prevalence of the positive class and P− that of
the negative class (so P+ + P− = 1). If we choose an operating
point that corresponds to equal sensitivity and specificity, then
PC = Sensitivity · (P+ + P− ) = Sensitivity = Specificity
(2)
regardless of prevalence.

Accordingly, a threshold of 60% (corresponding to a sensitivity and specificity of approximately 0.78) was picked for
COPD/CHF classification; this is the minimum percentage of
votes that need to be CHF in order to classify a record as CHF
(versus COPD). Similarly, a threshold of 50% (corresponding to
a sensitivity and specificity of approximately 0.88) was picked
for COPD/normal classification; this is the minimum percentage of votes that need to be COPD in order to classify a record
as COPD (versus normal).
For comparison and as a benchmark, we also specified classifier thresholds for the ideal case in which we assume knowledge
of the relative prevalences of the two classes within the test set
(i.e., 22 CHF to 23 COPD, and 23 COPD to 10 normal). If
the relative prevalences of the two classes are known, then it
turns out that PC is maximized by choosing an operating point
for which a line of slope P− /P+ is tangent to the ROC curve
[18]. Accordingly, the threshold in this case was chosen so that
a line of appropriate slope (i.e., 23/22 and 10/23, respectively,
for COPD/CHF and COPD/normal classification) was tangent
to the nominal training-set ROC curve. This resulted in a threshold of 60% for COPD/CHF and 40% for COPD/normal.
IV. CLASSIFIER PERFORMANCE
The performance of the final classifier was evaluated on the
hold-out test set. The various results described below are summarized in Tables II and III.
Bootstrapping [19] [see the right side of Fig. 4(b)] was used
to generate ROC curves with associated confidence intervals
to delineate the expected range of classifier performance. Running the classifiers using the breath-by-breath approach with
80 exhalations on 1000 bootstrapped samples of the test set
for 5% increments in threshold generated the ROC curve for
COPD/CHF classification [see Fig. 6(a)], with an AUROC of
0.89 (95% CI: 0.72–0.96). The ROC curve for COPD/normal
classification [see Fig. 6(b)] had an AUROC of 0.98 (95% CI:
0.82–1.0).
When the threshold was picked to yield approximately equal
sensitivity and specificity on the training set (thus minimizing
the dependence on prevalence), accuracy on the test set for

MIELOSZYK et al.: AUTOMATED QUANTITATIVE ANALYSIS OF CAPNOGRAM SHAPE

2887

TABLE II
SUMMARY OF TRAINING-SET AUROC VALUES FOR AND TEST-SET AUROC AND ACCURACY VALUES
Training

Testing

Classification Task

AUROC (Range)

AUROC (95% CI)

Accuracy (95% CI)

Diagnostic (COPD/CHF)
Screening (COPD/Normal)

0.87 (0.80–0.94)
0.98 (0.95–1.0)

0.89 (0.72–0.96)
0.98 (0.82–1.0)

75.6% (62.2%–86.7%)
93.9% (81.8%–100%)

The summarized results were obtained through breath-by-breath classification of 80 consecutive
exhalations.

TABLE III
TEST-SET AUROC VALUES FOR DIFFERENT NUMBERS OF EXHALATIONS
COPD/CHF
Number of Exhalations
10
20
30
40
50
60
70
80

COPD/Normal

Feature-Mean AUROC (95% CI)

Breath-by-Breath AUROC (95% CI)

Feature-Mean AUROC (95% CI)

Breath-by-Breath AUROC (95% CI)

0.75 (0.57–0.87)
0.80 (0.63–0.91)
0.84 (0.68–0.93)
0.84 (0.68–0.93)
0.83 (0.67–0.93)
0.86 (0.72–0.94)
0.80 (0.66–0.90)
0.83 (0.68–0.93)

0.82 (0.65–0.93)
0.88 (0.70–0.95)
0.89 (0.73–0.96)
0.88 (0.72–0.96)
0.88 (0.72–0.95)
0.87 (0.72–0.96)
0.88 (0.71–0.96)
0.89 (0.72–0.96)

0.92 (0.75–0.99)
0.96 (0.84–1.0)
0.94 (0.81–1.0)
0.93 (0.81–0.99)
0.95 (0.85–1.0)
0.95 (0.85–1.0)
0.97 (0.84–1.0)
0.97 (0.87–1.0)

0.95 (0.80–1.0)
0.97 (0.85–1.0)
0.97 (0.85–1.0)
0.98 (0.87–1.0)
0.98 (0.88–1.0)
0.98 (0.88–1.0)
0.98 (0.87–1.0)
0.98 (0.82–1.0)

The same number of exhalations is used in both training and testing, and for both breath-by-breath and feature-mean classification. The computed 95% confidence intervals after
bootstrapping the test set are also listed.

Fig. 6. Test-set ROC curves (obtained by varying the voting threshold in 5% increments) using breath-by-breath classification on 80 consecutive valid exhalations,
and an ensemble of 50 voters. (a) COPD/CHF classification (AUROC 0.89) and (b) COPD/normal classification (AUROC 0.98).

COPD/CHF classification was 75.6% (95% CI: 62.2%–86.7%).
Accuracy for COPD/normal classification was 93.9% (95% CI:
81.8%–100%). These performance metrics were computed over
1000 bootstrap samples of the test set.
For comparison, picking a threshold under the ideal assumption that the relative class prevalences are known resulted in a COPD/CHF classification accuracy of 80.0%
(95% CI: 66.7%–88.9%), while COPD/normal classification accuracy remained at 93.9% (95% CI: 81.8%–100%).
The computation of the final voter ensembles, followed by
feature extraction, preprocessing, bootstrap resampling, and
evaluation across the entire test set to produce the results in

Table II took under 1.5 min using the same computing platform
as for training.
We also explored the variation in performance that resulted
when the classifiers were designed and tested using fewer exhalations and more aggregated (i.e., feature-mean rather than
breath-by-breath) measures of the features (see Table III). The
indicated number of exhalations was used in each case for both
classifier design and testing. Note for example that with just
30 exhalations, and using the feature-mean approach, we obtained an AUROC of 0.84 (95% CI: 0.68–0.93) for COPD/CHF
classification and 0.94 (95% CI: 0.81–1.0) for COPD/normal
classification.

2888

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 12, DECEMBER 2014

V. DISCUSSION AND CONCLUSION
We have developed an automated approach to quantitative
capnogram analysis and classification. Capnograms were characterized through selected physiologically based features, which
were then used by an ensemble of classifiers to distinguish
COPD, CHF, and normal records. This approach exhibited good
classification performance and a run time sufficiently fast for
real-time monitoring.
Although several morphologies of the capnogram have
been observed and correlated with pulmonary function tests
[11], [20], automated preprocessing, analysis, and classification
based on these features have not been developed. While prior
studies have focused on obstructive lung disease, the capnogram
in acute CHF has not previously been quantitatively characterized.
Our approach differs from prior approaches in seeral distinctive ways. A record-specific template for preprocessing contrasts
with the use of fixed, predefined cutoffs on duration and amplitude of the capnogram [5]. The template used in our study
enables adaptation to individual variations and permits a wider
range of capnograms to be analyzed.
Previous work relied on features such as slopes, angles,
and curvature measures localized around the transition between the ascending phase and the alveolar plateau of the
capnogram [5]. In contrast, we used features that capture
different aspects of the entire expiratory phase. In addition
to ETCO2 , our cross-validation analysis extracted three features (end-exhalation slope, exhalation duration, and time spent
at ETCO2 ) from the candidate set, selected for their effectiveness in distinguishing among COPD, CHF, and normal
subjects.
Though we explored several machine learning approaches for
classification, the QDA framework for voter design, combined
with ensemble classification, consistently performed well and
required only a modest amount of training data. Such voting
methods are known to enhance effectiveness over that of a single classifier alone [14], [15]. Bootstrapping allowed the final
performance to be assessed statistically, thus providing bounds
on classification accuracy [19]. There are several potential limitations to this study. In patients presenting with a mixed picture
of CHF and COPD, clinical assessment (medical history, physical examination, and diagnostic testing) was made to determine
whether the current exacerbation was primarily CHF or COPD.
Patients for whom such a determination could not be made were
excluded from the study. Further investigation of such indeterminate presentations will be necessary for potential application
in the clinical setting.
In our database, the age distribution of subjects varied across
classes (see Table I). While COPD and CHF subjects tend to
be older, normal subjects in our database had a median age of
24 years (range 18–59 years). However, the capnography literature makes no reference to age dependence of the normal capnogram in this range or older, for adults with normal lung function
in the absence of cardiopulmonary disease [10], [21], [22].
The analysis framework presented in this paper is aimed at automated real-time discrimination of cardiopulmonary diseases

by capnogram shape. The computational approach allows small
gradations in shape to be recognized and applied to capnogram
characterization and classification. Future work should extend
this approach to enable tracking of changes in disease severity
and response to treatment over time. A further task is to expand
the application of this framework to other cardiorespiratory disorders. Our results suggest that automated capnogram analysis
has the potential to be useful in clinical settings as a noninvasive, real-time, and effort-independent diagnostic and screening
tool.
APPENDIX
PATIENT ENROLLMENT CRITERIA
1) COPD: For COPD enrollment, inclusion was based on
having at least one of the following: history of COPD diagnosis, history of wheezing, chronic cough, chronic sputum production, dyspnea on exertion, long-term bronchodilator use, or
long-term exposure to tobacco smoke or occupational dusts.
Exclusion criteria were intubation, clinical signs of cardiopulmonary instability, major trauma, shock, sepsis, pregnancy, or a
hospital discharge diagnosis that did not include COPD.
We recorded physical examination findings, vital signs at time
0 and 30 min, the Medical Research Council breathlessness scale
for COPD, clinical assessment of severity, medications given
and interventions done prior to and during data collection, and
discharge diagnosis. We also verified that the diagnosis was
supported by inpatient pulmonary function testing.
2) CHF: For CHF enrollment, inclusion criteria were a btype natriuretic peptide (BNP) level exceeding 100 pg/mL, and
meeting two of the major Framingham criteria (or one major
and two minor criteria, provided the minor ones could not be
attributed to another medical condition) [23]. Exclusion criteria
were intubation, clinical signs of cardiopulmonary instability,
echocardiographic changes inconsistent with CHF or left ventricular dysfunction, major trauma, shock, sepsis, pregnancy, or
a hospital discharge diagnosis that did not include CHF.
We recorded demographic data, physical examination findings, vital signs at time 0 and 30 min, the New York Heart Association criteria of heart failure, clinical assessment of severity,
BNP levels, medications given and interventions done prior to
and during data collection, and discharge diagnosis. We also
verified that the diagnosis was supported by inpatient echocardiography and cardiac catheterization, when performed.
In patients presenting a mixed picture of CHF and COPD,
clinical assessment was made as to whether the current exacerbation was primarily CHF or COPD. Patients for whom such a
determination could not be made were excluded from the study.
3) Normals: For the BCH normals study, subjects were enrolled from ED staff at the hospital. Normal, healthy subjects
(as determined by history, physical examination, and peak expiratory flow measurement in most cases) without a history of
asthma or smoking were enrolled, and informed consent obtained. For the MIT normals study, subjects were enrolled from
students at the university. Normal, healthy subjects without a
history of asthma or smoking were enrolled, and informed consent obtained.

MIELOSZYK et al.: AUTOMATED QUANTITATIVE ANALYSIS OF CAPNOGRAM SHAPE

ACKNOWLEDGMENT
R. J. Mieloszyk would like to thank J. V. Guttag, R. G. Mark,
and Y.-C. Kuo of MIT for helpful conversations. G. C. Verghese
and T. Heldt are grateful for sustained support and encouragement by T. Kailath of Stanford University.

REFERENCES
[1] W. Fowler, “Lung function studies III: Uneven pulmonary ventilation in
normal subjects and in patients with pulmonary disease,” J. Appl. Physiol.,
vol. 2, pp. 283–299, 1949.
[2] R. Sikand, P. Cerretelli, and L. E. Farhi, “Effects of Va and Va/Q distribution and of time on the alveolar plateau,” J. Appl. Physiol., vol. 21, no. 4,
pp. 1331–1337, 1966.
[3] A. DuBois, R. Fowler, A. Soffer, and W. Fenn, “Alveolar CO2 measured by expiration into the rapid infrared gas analyzer,” J. Appl. Physiol.,
vol. 4, pp. 526–534, 1952.
[4] J. W. Bellville and J. C. Seed, “Respiratory carbon dioxide response curve
computer,” Science, vol. 130, pp. 1079–1083, 1959.
[5] B. You, R. Peslin, C. Duvivier, V. D. Vu, and J. P. Grilliat, “Expiratory
capnography in asthma: Evaluation of various shape indices,” Eur. Respir.
J., vol. 7, pp. 318–323, 1994.
[6] C. V Egleston, H. Ben Aslam, and M. A. Lambert, “Capnography for monitoring non-intubated spontaneously breathing patients in an emergency
room setting,” J. Accid. Emerg. Med., vol. 14, pp. 222–224, 1997.
[7] J. B. West, Pulmonary Pathophysiology: The Essentials. Philadelphia,
USA: Lippincott Williams & Wilkins, 2011.
[8] P. J. Barnes, “Chronic obstructive pulmonary disease,” N. Engl. J. Med.,
vol. 343, no. 4, pp. 269–280, 2000.
[9] M. Jessup and S. Brozena, “Heart failure,” N. Engl. J. Med., vol. 348,
no. 20, pp. 2007–2018, 2003.
[10] B. Smalhout and Z. Kalenda, An Atlas of Capnography. Zeist, The Netherlands: Kerckebusch Zeist, 1975, p. 111.
[11] J. E. Kelsey, E. C. Oldham, and S. M. Horvath, “Expiratory carbon dioxide
concentration curve a test of pulmonary function,” Chest, vol. 41, no. 5,
pp. 498–503, 1962.
[12] N. M. Hawkins, M. C. Petrie, P. S. Jhund, G. W. Chalmers, F. G. Dunn,
and J. J. V McMurray, “Heart failure and chronic obstructive pulmonary
disease: Diagnostic pitfalls and epidemiology,” Eur. J. Heart Failure,
vol. 11, pp. 130–139, 2009.
[13] A. S. Maisel, P. Krishnaswamy, R. M. Nowak, J. McCord, J. E. Hollander,
P. Duc, T. Omland, A. B. Storrow, W. T. Abraham, A. H. B. Wu, P.
Clopton, P. G. Steg, A. Westheim, C. W. Knudsen, A. Perez, R. Kazanegra,
H. C. Herrmann, and P. A. McCullough, “Rapid measurement of b-type
natriuretic peptide in the emergency diagnosis of heart failure,” N. Engl.
J. Med., vol. 347, pp. 161–167, 2002.
[14] T. G. Dietterich, Ensemble Methods in Machine Learning (Lecture Notes
in Computer Science, vol. 1857). New York, USA: Springer, 2000, pp.
1–15.
[15] R. E. Schapire, Y. Freund, P. Bartlett, and W. S. Lee, “Boosting the margin:
A new explanation for the effectiveness of voting methods,” Ann. Stat.,
vol. 26, no. 5, pp. 1651–1686, Oct. 1998.
[16] C. M. Bishop, Pattern Recognition and Machine Learning. New York,
USA: Springer, 2006, p. 738.
[17] T. Fawcett, “An introduction to ROC analysis,” Pattern Recognit. Lett.,
vol. 27, pp. 861–874, 2006.
[18] T. K. Moon and W. C. Stirling, Mathematical Methods and Algorithms
for Signal Processing. New York, USA: Prentice Hall, 2000, p. 202.
[19] A. C. Davison and D. V Hinkley, Bootstrap Methods and Their Application. Cambridge, U.K.: Cambridge Univ. Press, 1997, p. 216.
[20] B. Krauss, A. Deykin, A. Lam, J. J. Ryoo, D. R. Hampton, P. W. Schmitt,
and J. L. Falk, “Capnogram shape in obstructive lung disease,” Anesth.
Analg., vol. 100, no. 3, pp. 884–888, 2005.
[21] R. D. Miller, Miller’s Anesthesia. Philadelphia, USA: Elsevier Health
Sciences, 2010.
[22] P. G. Barash, B. F. Cullen, and R. K. Stoelting, Clinical Anesthesia.
Philadelphia, USA: Lippincott Williams & Wilkins, 2012.
[23] P. A. McKee, W. P. Castelli, P. M. McNamara, and W. B. Kannel, “The
natural history of congestive heart failure: The Framingham study,” N.
Engl. J. Med., vol. 285, no. 26, pp. 1441–1446, 1971.

2889

Rebecca J. Mieloszyk (S’07–10-S’13) received the
B.S. degree in electrical and computer engineering from Carnegie Mellon University, Pittsburgh,
PA, USA, in 2010, and the S.M. degree in electrical engineering and computer science from the
Massachusetts Institute of Technology (MIT), Cambridge, MA, USA, in 2012.
Since 2011, she has been a Graduate Student Researcher in the Computational Physiology and Clinical Inference Group, MIT. Her current research interests include automated learning from medical data
and physiological modeling.
Ms. Mieloszyk was awarded the National Defense Science and Engineering
Graduate Fellowship in 2010.

George C. Verghese (M’74–SM’78–F’98) received
the B.Tech. degree from the Indian Institute of Technology, Madras, India, in 1974, the M.S. degree from
the State University of New York, Stony Brook, USA,
in 1975, and the Ph.D. degree from Stanford University, Stanford, CA, USA, in 1979, all in electrical
engineering.
Since 1979, he has been with the Massachusetts
Institute of Technology, Cambridge, USA, where he
is currently the Henry Ellis Warren (1894) Professor of Electrical and Biomedical Engineering in the
Department of Electrical Engineering and Computer Science and a MacVicar
Faculty Fellow. He is also a member of MIT’s Research Laboratory of Electronics. He has authored or co-authored a textbook and co-edited a monograph in
the area of power electronics. His current research interests include in the areas
of dynamic systems, modeling, estimation, signal processing, and control, and
their applications in biomedicine and power systems.
Dr. Verghese was an Associate Editor for the IEEE TRANSACTIONS ON
AUTOMATIC CONTROL and the IEEE TRANSACTIONS ON CONTROL SYSTEMS
TECHNOLOGY.

Kenneth Deitch received the B.A. degree in Government from St. Lawrence University, Canton,
NY, USA, in 1992, and the Doctor of Osteopathic
Medicine from the Western University Of Health Science, College of Osteopathic Medicine of the Pacific,
Pomona, CA, USA, in 1999.
In 2003, he completed a residency in Emergency
Medicine at Einstein Medical Center, Philadelphia,
PA, USA. Since 2003, he has been with the Einstein
Medical Center as an Attending Physician. He was
a Research Director from 2010 to 2014 and a Member of the IRB from 2006 to 2013. He is currently an Associate Professor of
Emergency Medicine at Thomas Jefferson University, Philadelphia. His current
research interests include procedural sedation and analgesia in the emergency
department, safety monitoring during sedation, capnography, and clinical adverse event prediction modeling. He was a Reviewer for the Annals of Emergency
Medicine, Academic Emergency medicine, and Pediatrics.

Brendan Cooney received the B.A. degree in biology from La Salle University, Philadelphia, PA, USA,
in 2006, and the M.D. degree from Jefferson Medical
College, Philadelphia, in 2010.
From 2010 to 2013, he was a Lead Research Coordinator with Einstein Medical Center, Philadelphia,
where he is currently in industry with a concentration
on Phase I oncology research.

2890

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 12, DECEMBER 2014

Abdullah Khalid received the Bachelors in Medicine
(MBBS) degree from Nishtar Medical College, Multan, Pakistan.
He was a Research Coordinator at Einstein Medical Center, Philadelphia, PA. He is currently completing his Internal Medicine residency training from
the University of Pittsburgh Medical Center (UPMC)
Mercy Hospital, Pittsburgh, PA.

Milciades A. Mirre-González received the M.D. degree from Universidad Nacional Pedro Henrı́quez
Ureña, Santo Domingo, Dominican Republic, in
2005.
Since 2005, he was involved in primary and tertiary settings, medical education, research, health
information technology, and volunteer settings. For
5 years, he was a Faculty of the medical school at Universidad Iberoamericana (UNIBE), Santo Domingo.
From 2011 to 2013, he was a Research Associate
and Research Coordinator at Einstein Medical Center Philadelphia’s Emergency Medicine Department. He is currently a Faculty
at O&M Medical School (allied with Partner’s Harvard Medical International),
Santo Domingo, and at UNIBE. His current research interests include clinical
practices in emergency and critical care medicine, as well as medical education.

Thomas Heldt (M’06–SM’11) studied physics at
Johannes Gutenberg-Universität, Mainz, Germany,
Yale University, New Haven, CT, and the Massachusetts Institute of Technology (MIT), Cambridge, MA. In 2004, he received the Ph.D. degree in
Medical Physics from the Harvard University-MIT
Division of Health Sciences and Technology, and
commenced postdoctoral training at MIT’s Laboratory for Electromagnetic and Electronic Systems.
He joined the MIT faculty in 2013 as the Hermann
L.F. von Helmholtz Career Development Professor in
the Institute for Medical Engineering and Science and as Assistant Professor
of Electrical and Biomedical Engineering. Prior to joining the MIT faculty, he
was a Principal Research Scientist at MIT’s Research Laboratory of Electronics.
His research interests focus on signal processing, mathematical modeling, and
model identification to support real-time clinical decision making.
Dr. Heldt is currently an Associate Editor of the Cardiovascular & Respiratory
Systems Engineering Theme for the annual IEEE ENGINEERING IN MEDICINE
AND BIOLOGY CONFERENCE and is a member of the Cardiopulmonary Systems
Technical Committee. Baruch.

Baruch S. Krauss received the A.B. degree from
Sarah Lawrence College, Bronxville, NY, USA, the
Ed.M. degree from Harvard Graduate School of Education, Cambridge, MA, USA, and the M.D. degree from the State University of New York at Stony
Brook, NY.
He is a Fellow of the American Academy of Pediatrics and of the American College of Emergency
Physicians. He is currently a pediatrician at Boston
Children’s Hospital and an Associate Professor of
Pediatrics at Harvard Medical School. His current
research interests include procedural sedation in children and diagnostic monitoring with capnography in children and adults.
Dr. Krauss was elected to the American Pediatric Society in 2010.

