508

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Exploiting Prior Knowledge in Compressed
Sensing Wireless ECG Systems
Luisa F. Polanı́a, Student Member, IEEE, Rafael E. Carrillo, Member, IEEE,
Manuel Blanco-Velasco, Senior Member, IEEE, and Kenneth E. Barner, Senior Member, IEEE

Abstract—Recent results in telecardiology show that compressed
sensing (CS) is a promising tool to lower energy consumption in
wireless body area networks for electrocardiogram (ECG) monitoring. However, the performance of current CS-based algorithms,
in terms of compression rate and reconstruction quality of the
ECG, still falls short of the performance attained by state-of-the-art
wavelet-based algorithms. In this paper, we propose to exploit the
structure of the wavelet representation of the ECG signal to boost
the performance of CS-based methods for compression and reconstruction of ECG signals. More precisely, we incorporate prior
information about the wavelet dependencies across scales into the
reconstruction algorithms and exploit the high fraction of common
support of the wavelet coefficients of consecutive ECG segments.
Experimental results utilizing the MIT–BIH Arrhythmia Database
show that significant performance gains, in terms of compression
rate and reconstruction quality, can be obtained by the proposed
algorithms compared to current CS-based methods.
Index Terms—compressed sensing (CS), electrocardiogram
(ECG), wavelet transform, wireless body area networks (WBAN).

I. INTRODUCTION
WIRELESS body area network (WBAN) is a radio
frequency-based wireless networking technology that
connects small nodes with sensor and/or actuator capabilities
in, on, or around a human body. WBANs promise to revolutionize health monitoring by allowing the transition from centralized
health care services to ubiquitous and pervasive health monitoring in every-day life. One of the major challenges in the design
of such systems is energy consumption, as WBANs are battery
powered [1].
The WBAN energy consumption can be divided into three
main processes: sensing, wireless communication, and data processing. However, the process that consumes most of the energy
is the wireless transmission of data [1], which indicates that
some data reduction operation should be performed at the sensor node to reduce the energy cost of the network. In addition,
data reduction can supplement the bandwidth constraints of the

A

Manuscript received January 29, 2014; revised April 1, 2014; accepted May
5, 2014. Date of publication May 16, 2014; date of current version March 2,
2015.
L. F. Polanı́a and K. E. Barner are with the Department of Electrical and
Computer Engineering, University of Delaware, Newark, DE 19711 USA
(e-mail: lfpolani@udel.edu; barner@udel.edu).
R. E. Carrillo is with the Institute of Electrical Engineering, École Polytechnique Fédérale de Lausanne (EPFL), CH-1015 Laussane, Switzerland (e-mail:
rafael.carrillo@epfl.ch).
M. Blanco-Velasco is with the Department of Signal Theory and
Communications, Universidad de Alcalá, Madrid 28805, Spain (e-mail:
manuel.blanco@uah.es).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2325017

WBAN when many sensor nodes are required to measure different physiological signals. Mamaghanian et al. [2] recently
proposed to use CS to lower energy consumption and complexity in WBAN-enabled ECG monitors. Compressed sensing is
an emerging field that exploits the structure of signals to acquire
data at a rate proportional to the information content rather than
the frequency content, therefore allowing subNyquist sampling
[3], [4]. When this sampling strategy is introduced in WBANs,
it gives rise to sensor nodes that efficiently acquire a small group
of random linear measurements that are wirelessly transmitted
to remote terminals. Indeed, sensor nodes can achieve high compression of ECG data with low computational cost when using
a sparse binary sensing matrix [2].
Most state-of-the-art algorithms for ECG compression are
based on wavelet transforms because of their straightforward
implementation and desirable time and frequency localization properties [5]–[7]. The main works in this area are characterized by hierarchical tree structures, such as embedded
zero-tree wavelet (EZW) [6] and set partitioning in hierarchical
tree (SPIHT) [7], which leverage the correlations of the wavelet
coefficients across scales within a hierarchically decomposed
wavelet tree. Even though wavelet-based methods offer good
performance in terms of compression, CS-based methods exhibit superior energy efficiency due to their lower processing
complexity at the encoder [2], [8].
The application of CS in WBAN-enabled ECG monitors is
still at its infancy. Dixon et al. [8] studied several design considerations for CS-based ECG telemonitoring via a WBAN, including the encoder architecture and the design of the measurement matrix. Their results show high compression ratios using a
1-bit Bernoulli measurement matrix. Zhilin et al. [9] introduced
CS for wireless telemonitoring of noninvasive fetal ECG. They
exploited the block-sparsity structure of the signals in the time
domain and used block sparse Bayesian learning (BSBL) for
the reconstruction. In [10], Mamaghanian et al. showed significant gains in compression and robustness attained by exploiting
structural ECG information. In a more recent work, Mamaghanian et al. [11] proposed a new CS architecture based on random
demodulation techniques for ambulatory physiological signal
monitoring in embedded systems. Their results exhibit better
energy efficiency than traditional acquisition systems.
Our previous works in the area include CS-based algorithms
for ECG compression with a focus on algorithms achieving
joint reconstruction of ECG cycles by exploiting correlation between adjacent heartbeats [12]–[14]. We also proposed a novel
CS-based approach to recover ECG signals contaminated with
electromyographic (EMG) noise using fractional lower-order
moments [15].

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

Even though previous works validate the potential of CS
for energy-efficient ECG compression [2], [8], [9], the performance of CS in terms of compression rate and ECG
reconstruction quality is still unsatisfactory when compared to
the results attained by state-of-the-art algorithms based on exploiting magnitude correlation across wavelet subbands [6], [7].
Most previous CS-based ECG compression works exploit only
the sparsity of the signal, ignoring important signal structure
information that can be known a priori and lead to enhanced
reconstruction results.
The main contribution of this paper is the application of a CS
algorithm that enables exploitation of ECG signal structure in
the reconstruction process. Two key signal structure properties
are incorporated into the proposed algorithm. The first property
captures the wavelet domain dependencies across subbands and
the second exploits the significant fraction of common wavelet
coefficient support for consecutive ECG segments. The proposed algorithm falls within the framework of model-based CS
[16]—a new CS framework based on unions of subspaces—that
can enhance signal reconstruction while reducing the number
of measurements. The motivation for using a model-based approach is that it enables the incorporation of structural dependences between the locations of the signal coefficients caused
by R wave events. However, it is worth mentioning that the proposed algorithm differs from traditional model-based recovery
algorithms [10], [16] in two respects. First, it uses prior support
estimate knowledge to leverage the small variation in the support
set of adjacent data sequences. Second, it excludes the selection
of coefficients from the lowest-energy wavelet subband.
The performance of the proposed method is evaluated using the MIT–BIH Arrythmia Database [17]. Given that most
CS ECG application works use the BPDN algorithm [2], [8],
system-level comparisons are provided based on implementations of the BPDN and proposed reconstruction algorithms.
Simulations are also performed with the bound-optimizationbased BSBL algorithm, previously employed by Zhilin et al.
[9] for noninvasive fetal ECG, in order to compare with other
structured sparsity-based CS reconstruction algorithms. Results
indicate that the proposed algorithms outperform both BPDM
and bound-optimization-based BSBL in terms of compression
rate and reconstruction quality.
The organization of the paper is as follows. Section II presents
a brief review of CS and model-based CS [16]. Section III
describes the connected subtree structure encountered in the
wavelet representation of the ECG and studies the support variation across consecutive ECG segments. In Section IV, the proposed method is presented. Numerical results for the proposed
method and comparisons with a benchmark state-of-the-art algorithm for ECG compression, SPIHT, are presented in Section
V. Finally, we conclude in Section VI with closing remarks.
II. BACKGROUND
A. Compressed Sensing
Let x ∈ RN be a signal that is either K-sparse or compressible in an orthogonal basis Ψ. Thus, the signal x can be
approximated by a linear combination of a small number of

vectors from Ψ, i.e., x ≈ K
i=1 si ψi , where K  N . Let Φ be

509

an M × N sensing matrix, M < N . Compressed sensing [3],
[4] addresses the recovery of x from linear measurements of
the form y = Φx = ΦΨs. Defining Θ = ΦΨ, the measurement
vector becomes y = Θs. Compressed sensing results show that
the signal x can be reconstructed from M = O(Klog(N/K))
measurements if the matrix Θ satisfies some special conditions,
e.g., the restricted isometry property [4], the mutual coherence
[18], or the null space property [3]. In real scenarios with noise,
the signal s can be reconstructed from y by solving the convex
optimization problem
1
(1)
min y − Θs22 + λs1
s 2
with λ a regularization parameter that balances sparsity and reconstruction fidelity. The optimization problem in (1) is referred
to as basis pursuit denoising (BPDN) [19].
In addition to convex optimization methods, a family of iterative greedy algorithms [20]–[22] has received significant attention due to the algorithmic simplicity and low complexity.
Two iterative algorithms of importance for the problem at hand
are compressive sampling matching pursuit (CoSaMP) [21] and
iterative hard thresholding (IHT) [23].
1) CoSaMP: CoSaMP is an iterative greedy algorithm that
offers the same theoretic performance guarantees as even the
best convex optimization approaches [21]. At each iteration,
several components of the vector s are selected based on the
largest correlation values between the columns of Θ and the
residual vector. If they are found sufficiently reliable, their indices are added to the current support estimate of s. This is
repeated until all the recoverable portion of the signal is found.
2) Iterative Hard Thresholding: IHT is a powerful method
for sparse recovery that converges to a local minimum of the
problem statement
min y − Θs22 subject to s0 ≤ K
s

(2)

by using the recursion
si+1 = HK (si + ΘT (y − Θsi ))

(3)

where HK is a nonlinear operator that sets to zero all elements
other than the K largest elements (in magnitude). IHT is a very
simple algorithm that recovers sparse and compressible vectors
with a minimal number of observations and with near optimal
accuracy, whenever the matrix Θ has a small restricted isometry
constant [23].
B. Model-Based Compressed Sensing
Model-based compressive sensing is a new paradigm that
aims to capture the interdependence structure in the support of
the large signal coefficients using a union-of-subspaces model
[16]. This model decreases the degrees of freedom of the signal
by allowing only some configurations of support for the largest
coefficients.
Let x be a K-sparse signal. Then, x lies in ΣK ⊂ RN , which
N
is a union of ( K
) subspaces of dimension K. A union-ofsubspaces model allows only some K-dimensional subspaces
in ΣK and leads to representations that incorporate signal structure. Let x|Ω denote the coordinates of x corresponding to the

510

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

set of indices Ω ⊆ 1, . . . , N , and let ΩC represent the complement of Ω. Then, the signal model MK is defined as the union
of mK canonical K-dimensional subspaces
MK =

m
K


Xm , Xm = {x : x |Ω m ∈ RK , x |Ω Cm = 0}

(4)

m =1

where {Ω1 , . . . , Ωm K } is the set of all allowed supports with
|Ωm | = K for each m = 1, . . . , mK . It should be noted that
N
MK ⊆ Σk and that MK contains mK ≤ ( K
) subspaces. Signals from MK are called K-model sparse.
A similar treatment is applied to compressible signals. A
compressible signal x ∈ RN that is nearly K-model sparse can
be approximated to the best model-based approximation in MK .
The 2 error produced by the approximation is given by
σMK = inf x − x̄2 .
x̄∈MK

(5)

The algorithm that provides the best K-term approximation of
the signal x under the model MK is denoted as M(x, K). Thus,
the error σMK can also be written as σMK = x − M(x, K)2 .
A sparsity model M = {M1 , M2 , . . .} produces nested approximations if the support of M(x, K ∗ ) contains the support
of M(x, K) for all K < K ∗ . If the signal model produces
nested approximations, then the support of the difference vector M(x, jK) − M(x, (j − 1)K) lies in a small union of subspaces. These difference vectors form sets of residual subspaces.
The jth set of residual subspaces of size K is defined as
Rj,K (M) = {u ∈ RN s.t. u = M(x, jK) − M(x, (j − 1)K)
for some x ∈ RN }

(6)

for j = 1, . . . , 
N/K. A structured compressible signal x
can be robustly recovered from the compressive measurements
y = Φx if the matrix Φ satisfies the restricted amplification
property (RAmP) [16]. A matrix Φ has the (K , r)-restricted
amplification property for the residual subspaces Rj,K of model
M if
Φu22 ≤ (1 + K )j 2r u22

(7)

for any u ∈ Rj,K and for each 1 ≤ j ≤ 
N/K.
Baraniuk et al. [16] incorporated the union-of-subspaces
models into two well-known CS recovery algorithms, CoSaMP
and IHT, through a single modification in the algorithms. The
modification, in practice, replaces the best K-term approximation with a best K-term model-based approximation.
C. Tree-Structured Sparsity Models
One example of a structured sparsity model is that encountered in signals whose most significant wavelet coefficients are
organized into a tree structure, and where the largest coefficients
cluster along the branches of the tree [16].
Consider an N -dimensional signal x. Given a wavelet function ψ and a scaling function ϕ, the wavelet representation of
x is defined in terms of shifted versions of ϕ and shifted and
dilated versions of ψ
x=

N
L −1
i=0

aL ,i ϕL ,i +

j −1
L N


j =1 i=0

dj,i ψj,i

(8)

where j denotes the scale of analysis and L denotes the
coarsest scale. Nj = N/2j indicates the number of coefficients at scale j ∈ {1, . . . , L} and i represents the position,
0 ≤ i ≤ Nj − 1. The wavelet transform consists of the scaling
coefficients aL ,i and wavelet coefficients dj,i . Using the previous notation, we write x = Ψs, where Ψ is the orthogonal matrix containing the wavelet and scaling functions as columns and
s = [d1,0 . . . d1,N 1 −1 . . . dL ,0 . . . dL ,N L −1 aL ,1 . . . aL ,N L −1 ]T is
the vector of scaling and wavelet coefficients. The vector s can
be decomposed into L + 1 subvectors. The first L subvectors
are denoted by dj , j = 1, . . . , L, and the jth subvector contains
all of the wavelet coefficients for scale j. The last subvector corresponds to the scaling coefficients and is denoted as aL . Thus,
s can also be written as s = [d1 d2 . . . dL aL ]T .
The wavelet atoms form a binary tree structure where the
wavelet coefficient dj,i is the parent of its two children dj +1,2i
and dj +1,2i+1 . This nesting property causes rapid transitions
and other singularities to manifest as chains of large coefficients
along the branches of the wavelet tree [24]. This gives rise to the
concept of a connected subtree, which refers to a connected set
of nodes Ω meeting the condition that whenever a coefficient
dj,i ∈ Ω, then its parent also belongs to Ω. In [16], Baraniuk
et al. assumed NL = 1 for simplicity and defined the structured
sparsity model TK as the union of all K-dimensional subspaces
corresponding to supports Ω that form connected subtrees,
TK =


j −1
L N

x = aL ,0 ϕL ,0 +
dj,i ψj,i : d |Ω C = 0,
j =1 i=0

|Ω| = K, Ω forms a connected subtree


(9)

where ΩC denotes the complement of the set Ω. To find the
best K-term tree-based approximation, Baraniuk et al. used the
condensing sort and select algorithm (CSSA) [25], which solves
for
x∗ = argmin x − x̄2

(10)

x̄∈Tk

by using a two-stage process. The first stage merges the nonmonotonic segments of the tree branches with an iterative
sort-and-average routine. The second stage simply sorts the
wavelet coefficients once they are organized in a monotonic
non-increasing sequence along the branches out from the root.
The principle behind the CSSA algorithm is that the standard
K-term approximation coincides with the subtree approximation when the wavelet data is monotonically non-increasing
along the tree branches.
III. MOTIVATION
In this section, we analyze the structure of the wavelet representation of ECG signals to motivate the incorporation of prior
information in CS-based recovery algorithms for ECG reconstruction. We concentrate on exploiting two key properties of
the ECG wavelet coefficients. The first property is the connected
subtree structure formed by the largest (in magnitude) wavelet
coefficients, and the second property is the high fraction of
common support between adjacent ECG segments.

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

511

Fig. 2. Fraction of common support between consecutive ECG segments. The
results are averaged over the selected dataset from the MIT–BIH Arrhythmia
Database.

Fig. 1. Daubechies-4 coefficients subvectors dj , j = 1 . . . 5, and a 5 for ECG
time series.

A. Connected Subtree Structure of ECG Wavelet Coefficients
Sharp transition regions in the time domain generate large
magnitude wavelet coefficients that persist along the branches
of the wavelet tree, forming connected rooted subtrees [24].
This behavior is also present in the wavelet representation of
ECG signals and is connected with R wave events. This idea is
illustrated in Fig. 1, which shows that coefficients subvectors
dj , j = 1, . . . , 5, and a5 , corresponding to the Daubechies-4
wavelet transform of an ECG time series using a decomposition
level L = 5. The subvectors are plotted as rows stacked on top
of each other. The ECG time series, located at the bottom of
Fig. 1, corresponds to an extract of 11.5 s from record 117 of
the MIT–BIH Arrhythmia Database. Each coefficient vector is
time-shifted so that the tree structure can be clearly identified.
For visualization purposes, the magnitude of the coefficients is
normalized so that the Euclidean norm of each wavelet subband
is unity.
Examining the wavelet representation in Fig. 1, it is noticed
that the large coefficients are aligned and propagate across
scales, forming a connected tree structure. This persistence
property is mainly noticed in subbands d4 , d3 , and d2 . These results suggest that the tree-structured sparsity model described in
Section II-C is an appropriate model to represent the support of
the most significant ECG wavelet coefficients. The tree structure is intrinsically related to the shape of the ECG cycles. When
compared with the ECG time series, it is noted that the large
coefficients are connected with the QRS complexes, which can
be regarded as sharp transition regions of the signal. We propose
to exploit the tree-structured sparse representation as additional
prior information for the CS reconstruction of ECG signals. As
stated by the model-based CS framework [16], knowledge of
the signal support potentially leads to high quality reconstruction using fewer measurements, and thus higher compression
performance.

B. Common Support Between Consecutive ECG Segments
In this section, we study how the support of two consecutive
ECG segments varies. Consider the 10-min long single leads
extracted from records 100, 101, 102, 103, 107, 109, 111, 115,
117, 118, and 119 in the MIT–BIH Arrhythmia Database, which
are sampled at 360 Hz, and form 100 consecutive sequences
of 2048 samples for each record. Let Ωt denote the support
of the K = 225 largest (in magnitude) wavelet coefficients of
sequence st , where t = 1, . . . , 100 denotes the order of the sequences. Consider the fraction of common support between two
consecutive sequences, denoted as σt,t−1 , which is defined as
σt,t−1 =

|Ωt ∩ Ωt−1 |
, t = 2, . . . , 100.
|Ωt |

(11)

The averaged results over the entire ECG dataset are illustrated in Fig. 2, where the data points are connected for visualization purposes only. The fraction of common support is high,
always greater than 0.65, which indicates that we can use the
support of the previous data sequence to improve the estimate
of the support of the current data sequence. In designing the
CS-based reconstruction algorithm, we propose to include support information of previous reconstructed data sequences to
improve the performance. A related work by Wang et al. [26]
shows how to iteratively enhance recovery of a signal by solving
basis pursuit in the first iteration to obtain a support estimate,
solving the problem of basis pursuit with partially known support [27] with this support estimate, and repeating the steps.
Their work attains exact reconstruction guarantees for a single
iteration even if the initial support estimate includes a small
fraction of indices outside the true support.
IV. METHODS
The proposed ECG compression method based on compressive sensing for potential implementation in WBAN-based telemonitoring systems is described in this section. The method
is summarized in Fig. 3. The process initiates with the sampling of the ECG signals through linear random measurements,
followed by a redundancy removal module and a quantization
stage. The quantized samples are entropy coded and transmitted to a remote terminal where the reconstruction is performed.
The presented contribution focuses on the reconstruction algorithm that relies on prior knowledge of ECG wavelet coefficient

512

Fig. 3.

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Block diagram of the proposed ECG compression method.

structure to reduce the required number of measurements and
improve reconstruction quality. A more detailed explanation of
each stage is given below.
A. Sampling and Encoding
The first step addresses the sampling of consecutive ECG
segments of length N . Let x denote an ECG segment. The
information we acquire about x can be described by y = Φx,
where Φ is a M × N matrix. In order to recover the best K-term
model-based approximation of the original signal, the matrix Φ
needs to satisfy the restricted amplification property (RAmP).
It is known that subGaussian matrices meet this condition with
high probability [16]. Here, we build the entries of the matrix Φ
by independently sampling
from a symmetric Bernoulli distri√
bution (P(Φi,j = ±1/ M = 1/2)) in order to facilitate an efficient hardware implementation. The use of Bernoulli matrices,
as compared to other subGaussian matrices, results in simpler
circuit complexity, data storage, and computation requirements
[28].
To realize the analog hardware implementation of CS matrices, Mamaghanian et al. [11] recently proposed the spread spectrum random modulator preintegrator. This architecture starts
with a premodulation block, followed by a random demodulation preintegrator architecture, which is composed of parallel
channels of random demodulators. Each random demodulator
is, in turn, composed of three stages. The first stage refers to
the multiplication of the input signal with a pseudo-random
sequence that takes values ±1 with equal probability. Such sequences are generated with a pseudo-random bit sequence generator. The second stage incorporates a low-pass filter to avoid
aliasing and the final stage corresponds to a standard ADC. Their
implementation achieves 43% energy saving, compared to traditional Nyquist sampling, when applied to EEG and ECG signals.
The use of the described sampling procedure results in similar adjacent measurement vectors. This is caused by the quasiperiodic nature of the ECG signal and the use of a fixed sampling matrix. To further improve the compression and reduce the
amount of redundant information, we implemented the same redundancy removal and entropy coding stages proposed by Mamaghanian et al. [2]. The redundancy removal stage computes the
difference between two consecutive measurement vectors and
only transmits this difference to the quantization module. In [2],
it was shown that the variance of the difference signals between
consecutive measurement vectors is lower than the variance of
the original measurement vectors, which leads to a reduction in
the number of bits for signal representation, from 12 to 9 bits. It
is worth mentioning that the experiments in [2] were applied to
nonaligned ECG segments, which implies that the QRS complexes were located at different locations and, nevertheless, the
corresponding measurements exhibited significant redundancy
to be removed. An 8-bit optimal scalar quantizer designed with

Fig. 4. Wavelet decomposition of level L = 5 for ECG time series using
Daubechies-4.

the Lloyd-Max algorithm is utilized [29] and entropy coding
stage uses Huffman coding to further increase the compression
ratio.
B. Reconstruction Algorithm Using Prior Information
The original samples must be recovered from the transmitted
difference between consecutive vectors. Therefore, at the remote terminal, Huffman decoding followed by recovery of the
original samples is employed. For reconstruction of the ECG
signal, two iterative algorithms that are easily modified to incorporate prior ECG wavelet representation structure information are proposed. The algorithms are CoSaMP and IHT, which
were previously modified by Baraniuk et al. [16] to incorporate
structured sparsity models. Their modification results in replacing the nonlinear sparse approximation step with a structured
sparse approximation. These algorithms are known in the literature as model-based CoSaMP and model-based IHT; both
have provable robust guarantees. One of the main properties of
model-based CoSAMP is that robust signal recovery requires
only a number of measurements that is proportional to the sparsity level of the signal.
The sparsity model employed in this paper corresponds to a
modified version of the tree-structured sparsity model described
in section Section II-C. The modification is based on the fact
that ECG wavelet coefficients at scale j = 1 correspond to coefficients with nearly zero magnitude, as illustrated in Fig. 4, and
should therefore not be included in the best K-term model-based
signal approximation. Thus, we select NL scaling coefficients
and define TK as
TK =


N
j −1
L N
L −1

x=
aL ,i ϕL ,i +
dj,i ψj,i :
i=0

j =1 i=0

supp(d1,i ) ∈ ΩC for all i, d|Ω C = 0, |Ω| = K,

Ω forms a connected subtree .
(12)

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

Algorithm 1 MMB–CoSaMP
Require: Matrices Θ = ΦΨ and Ψ, measurements y,
sparsity level K, support of previously reconstructed ECG
segment Λ, structured sparse approximation algorithm T .
1. Initialize ŝ0 |Λ = Θ†Λ y, ŝ0 |Λ C = 0, r0 = y − Θŝ0 , j = 0.
2. while halting criterion false do
3. j ← j + 1
4. e ← ΘT rj −1
5. Ω ← supp(ΨT T (Ψe, 2K))
6. T ← Ω ∪ supp(ŝj −1 )
7. b|T ← Θ†T y, b|T C ← 0
8. ŝj ← ΨT T (Ψb, K)
9. rj ← y − Θŝj
10. end while
11. return x̂ ← Ψŝj

Algorithm 2 MMB–IHT
Require:Matrices Ψ, Φ, and Θ = ΦΨ, measurements y,
sparsity level K, support of previously reconstructed ECG
segment Λ, structured sparse approximation algorithm T .
1. Initialize ŝ0 |Λ = Θ†Λ y, ŝ0 |Λ C = 0, x̂0 = Ψsˆ0 ,
r0 = y − Φx̂0 , j = 0
2. while halting criterion false do
3. j ← j + 1
4. b ← x̂j −1 + ΦT rj −1
5. x̂j ← T (b, K)
6. rj ← y − Φx̂j
7. end while
8. return x̂ ← x̂j
Denote the algorithm that finds the best K-term tree-based
approximation by T (x, K). That is,
T (x, K) = argmin x − x̄2 .

(13)

x̄∈Tk

The model-based CoSaMP and the model-based IHT, using
the tree-structured sparsity model TK , are summarized in Algorithm 1 and 2, respectively, and are referred to as modified
model-based CoSaMP (MMB–CoSaMP) and modified modelbased IHT (MMB–IHT). The halting criterion for both algorithms can be a fixed number of iterations or a bound on the
residual norm, rj 2 ≤ , for some predetermined  > 0. Note
that in the algorithm tables, the Moore–Penrose pseudo-inverse
of Θ is denoted by Θ† , ΘT denotes the submatrix obtained by
extracting the columns of Θ corresponding to the indexes in
T , b |T represents the entries of b corresponding to the set of
indices T , and the support of s is denoted by supp(s). Also, we
use the condensing sort and select algorithm [25] to solve for
the optimization problem in (13).
As in model-based CoSaMP, MMB–CoSaMP starts the iteration by calculating the correlation values between the columns
of Θ and the residual of the previous iteration rj −1 (step 4).
The correlation values are used to find support of the best
2K-term tree-based approximation (step 5), which is subsequently merged with the support of the previous iteration (step

513

6). The next step refers to solving a least-squares problem to
approximate the target signal on the updated support (step 7).
To enforce a sparse solution, the algorithm finds the best K-term
tree-based approximation of the least squares solution (step 8)
and finalizes with the update of the residual (step 9).
Similarly, MMM–IHT also resembles model-based IHT. The
algorithm iteratively solves (2) by moving in the opposite direction of the gradient of y − Θs22 at each iteration (step 4).
MMM–IHT enforces a sparse solution by selecting the best Kterm tree-based approximation of b (step 5) and finalizes with
an update of the residual (step 6). It is worth emphasizing that
the proposed algorithms exploit the wavelet tree structure in the
steps that refer to the best K-term tree-based approximation
defined in (13). Both MMB–IHT and MMB–CoSaMP differ
from the traditional model-based approaches in the initialization step and in the K-term tree-based approximation of the
signal.
Unlike the traditional model-based approaches, the proposed
algorithms always include the scaling coefficients in the Kterm tree-based approximation of the signal, which is of great
importance given that the scaling coefficients accumulate most
of the ECG signal energy. Indeed, in a previous work [30], we
show that enforcing CS greedy algorithms to select the atoms
corresponding to the scaling coefficients can lead to a significant
reduction in the number of measurements while improving the
accuracy of the ECG reconstruction.
The initialization step is a modification with respect to traditional approaches because it incorporates support information
from the previously reconstructed signal. In Section III-B, it is
noted that a high fraction of support is shared between two consecutive ECG segments. The two consecutive ECG segments in
the wavelet domain are denoted by st and st−1 . With the aim
of exploiting this information, the first signal estimate is determined by solving a least squares problem using the support
of the previously reconstructed ECG segment, Λ = supp(st−1 ).
The contribution of this estimate is subtracted from the measurement vector and iterated on the residual. These steps are
implemented in the initialization of Algorithm 1 and 2. In both
algorithms, MMB–IHT and MMB–CoSaMP, the support set is
refined through iterations by addition of promising new atoms
and deletion of unnecessary atoms. In this way, we expect
the algorithms to preserve the common support (supp(st ) ∩
supp(st−1 )) and to replace the unnecessary atoms (supp(st−1 ) \
supp(st )) with the innovation support of the current ECG sequence, i.e., (supp(st ) \ supp(st−1 )).
V. EXPERIMENTAL RESULTS
To validate the proposed methods, compressed measurements
are generated using a set of records from the MIT-BIH Arrhythmia Database [17] as the original signals. Every file in
the database contains two lead recordings sampled at 360 Hz
with 11 bits per sample of resolution. However, since body area
networks adopt lower sampling frequencies than 360 Hz, each
ECG recording is resampled at 250 Hz. The sampling frequency
of 250 Hz is commonly used for ECG monitoring in body area
networks [31]. Experiments are carried out and averaged over
10-min long single leads extracted from records 100, 101, 102,

514

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

103, 107, 109, 111, 115, 117, 118, and 119. This dataset was
proposed in [7]; it consists of a variety of signals with different
rhythms, wave morphologies, and abnormal heartbeats. Results
are presented for averages of 100 repetitions of each experiment,
with a different realization of the random measurement matrix
at each time.
A. Performance Evaluation
The compression ratio (CR), the percentage root-mean-square
difference (PRD), the normalized version of PRD (PRDN), the
quality score (QS), and the reconstruction SNR (R-SNR) are
used as performance measures. The CR is defined as the number of bits required for the original signal over the number of bits
required for the compressed signal. Here, the original signals refer to the resampled ECG records at 250 Hz. The reconstruction
SNR is defined as
R-SNR = 10log10

x22
x − x̂22

Fig. 5. Sequence of residual energy averaged over the selected set of records
from the MIT–BIH Arrhythmia Database.

(14)

where x and x̂ denote the N -dimensional original and reconstructed signals, respectively. The PRD is defined as PRD =
(x − x̂2 /x2 ) × 100. Let e denote an N -dimensional vector of ones and x̄ be the mean value of x. The PRDN is defined as PRDN = (x − x̂2 /x − x̄e2 ) × 100, and the QS as
QS = CR/PRD [32].
B. Practical Considerations
The length of the ECG segments is set to N = 256, so that
the acquisition time can be sufficiently short (approximately 1 s
at the rate of 250 Hz) for real-time monitoring. The orthogonal
Daubechies-4 wavelets is chosen as the sparsifying transform.
For the reconstruction algorithms, the halting criterion is either
a maximum number of iterations (we selected 70 for our simulations) or a bound on the residual norm, rt 2 ≤ . We selected
 = 1 × 10−3 y2 .
For the sparsity level, a sequence of residual energy is defined.
The elements in the vector of wavelet coefficients s are ordered
according to their square magnitudes, such that
|s(1) |2 ≥ |s(2) |2 ≥ . . . ≥ |s(N −1) |2 ≥ |s(N ) |2 .

(15)

The sequence of residual energy is defined as
N


CK =

|s(j ) |2 −

j =1

K

j =1

N


|s(j )

|s(j ) |2
,

K = 1, . . . , N.

(16)

|2

j =1

From each record of the dataset, 300 ECG segments of length
N = 256 are selected. The sequence of residual energy is averaged over all the selected ECG segments and over all the
different records. The results are plotted in Fig. 5 in logarithmic scale. The sparsity level is selected as the value of K that
satisfies averaged CK = 0.001, which corresponds to K = 34
and is indicated in Fig. 5 with dotted lines. This result indicates
that the most significant 34 wavelet coefficients approximately
accumulate 99.999% of the total signal energy.
Given that the ECG recordings are sampled at 250 Hz, the
sampling interval becomes Δt = 1/250 s. A decomposition

Fig. 6. Reconstruction SNR as a function of the wavelet decomposition level.
Number of measurements m = 3K .

level L = 5 is utilized so that the scaling coefficients vector a5 ,
associated with a physical scale of 25 Δt = 25 /250 = 0.13 s,
approximately isolate the T waves, and thus the detail coefficients capture the QRS complex. In this way, the largest (in
magnitude) detail coefficients are expected to exhibit a connected subtree structure caused by the QRS complexes, as shown
in Fig. 1. The ECG reconstruction quality, using both MMB–
CoSaMP and MMB–IHT algorithms, is evaluated as a function
of the wavelet decomposition level. The results are averaged
over the selected set of records for a number of measurements
m = 3K. The results in Fig. 6 indicate that the selection of
L = 5 is indeed a good choice for the decomposition level. The
data points are connected for visualization purposes only.
Sparse binary matrices have recently been proposed for CS
ECG since they lead to fast computations and low-memory
requirements [2]. An experiment is designed to test the performance of the proposed algorithms when using sparse binary
matrices. Similarly to the work in [2], two types of sparse binary matrices, matrix I and matrix II, are employed. Matrix I has
only q  N nonzero elements in each column. Each nonzero
√
element takes the value 1/ q and its location is chosen randomly. Matrix II has only q  N nonzero elements in each
√
column. Each nonzero element takes the value ±1/ q with
equal probability and its location is chosen randomly. In [2], the
value of q = 0.025 × N  provides a good tradeoff between
execution time and reconstruction quality. The same relation

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

515

TABLE I
PRD OBTAINED BY THE PROPOSED ALGORITHMS FOR DIFFERENT SENSING MATRICES
Reconstruction algorithm

Sampling matrix

CR

Bernoulli
Matrix I
Matrix II
Bernoulli
Matrix I
Matrix II

3.5
3.31
3.42
3.42
2.98
2.99
3.01

MMB–IHT

MMB–CoSaMP

4
3.32
3.42
3.43
2.99
2.99
3.01

4.5
3.32
3.47
3.45
2.99
3.04
3.03

5
3.33
3.7
3.55
3.11
3.12
3.07

5.5
3.35
3.85
3.67
3.24
3.62
3.4

6
3.55
4.23
3.99
3.49
3.92
3.7

6.5
3.81
4.8
4.32
4.02
5.12
4.8

7
4.05
5.71
4.68
4.8
6.9
6.2

7.5
4.43
6.7
5.2
9.74
14.01
12.9

8
4.91
7.79
5.82
25.58
28.15
27.74

between the value of q and N is selected for the proposed
experiment, q = 0.025 × N  = 6. The reconstruction performance of the proposed algorithms using the Bernoulli matrix,
described in Section IV-A, and the sparse binary matrices is
shown in Table I. The results correspond to averaged PRD values over the entire set of selected records. Even though all the
matrices have a similar performance for CR ≤ 6, the performance of the sparse binary matrices deteriorates for CR > 6.
Hence, the dense Bernoulli matrix is selected as the sampling
matrix for the proposed methods. Nevertheless, the minimal
detriment to performance suggests that sparse binary sensing
matrices are a plausible alternative for CS ECG.
C. Evaluation of ECG Reconstruction
Using the Proposed Method
This section presents an experiment to evaluate how the reconstruction of ECG signals changes as the amount of prior
knowledge of the support set varies. The best performance is
achieved with the oracle estimate. Assume an oracle reveals the
support set Ω of the K most significant wavelet coefficients
of the signal of interest. The oracle estimate corresponds to
the least squares projection onto the subspace spanned by the
columns of Θ with indices in Ω. This experiment also considers
the case where no prior information is known about the support set Ω, and the signal is reconstructed using the traditional
CoSaMP [21] and the traditional IHT [23]. The performance of
all these methods is compared with the performance attained by
the proposed algorithms, MMB–CoSaMP and MMB–IHT.
The results are averaged over the set of selected records and
illustrated in Fig. 7. Given that the objective of this first experiment is only to evaluate the reconstruction of the proposed
scheme, we restrict our method to the sampling and reconstruction of the signals and exclude the redundancy removal,
quantization, and entropy coding stages. For this experiment,
the reconstruction SNR is used to evaluate the quality of the
recovered signals as a function of the oversampling ratio M/K.
As shown in Fig. 7, the proposed algorithms outperform the
traditional CoSaMP and IHT, indicating that exploiting the connected subtree structure of the most significant wavelet coefficients, as well as the common support between consecutive ECG
segments, results in a large reduction of the required number of
measurements to achieve successful recovery of ECG signals. It
is also noted from Fig. 7 that the algorithms based on IHT require
fewer measurements than the algorithms based on CoSaMP to
attain good reconstruction. The results of the proposed methods

Fig. 7. Comparison of MMB–CoSaMP and MMB–IHT with CoSaMP, IHT,
and the oracle estimate. Reconstruction SNR averaged over all the records of
the selected dataset for different number of measurements.

are the closest to the best achievable performance obtained by
the oracle estimate.
D. Evaluation of Compression Performance
For the second experiment, the reconstruction algorithm
in the compression scheme, Fig. 3, is varied. The proposed
MMB–CoSaMP and MMB–IHT are first used as reconstruction algorithms, and their results are compared with BPDN, the
bound-optimization-based BSBL algorithm, and an overcomplete dictionary-based reconstruction algorithm. The results are
also compared with SPIHT, a state-of-the-art algorithm for ECG
compression. The same entropy coding stage of the proposed
method is added to SPIHT to ensure fairness in the comparison.
The results of the PRD as a function of the compression ratio
are illustrated in Fig. 8. The results are averaged over the entire
set of selected records.
Basis pursuit denoising is the reconstruction algorithm selected by Mamaghanian et al. [2] and Dixon et al. [8] for the
recovery of ECG signals. However, the results in Fig. 8 indicate that the proposed compression scheme works significantly
better when MMB–IHT and MMB–CoSaMP are used as reconstruction algorithms than when BPDN is employed. These
results are expected, as the proposed reconstruction algorithms

516

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

TABLE II
PERFORMANCE OF THE PROPOSED ALGORITHMS FOR CR=6.4
Record

100
101
102
103
107
109
111
115
117
118
119

Fig. 8. Compression performance evaluation of the compression scheme using
the proposed algorithms (MMB–CoSaMP, MMB–IHT), the BSBL–BO, the
MS–BPDN, and the BPDN algorithms. The results of SPIHT are included as a
baseline for comparisons.

exploit prior knowledge of the signal structure, unlike BPDN,
which only leverages the sparsity of the signals.
Unlike BPDN, the bound-optimization-based BSBL algorithm, denoted as BSBL–BO, provides flexibility to exploit the
block structure and intrablock correlation of the signal sparsity
pattern. Even though it was previously employed to reconstruct
noninvasive fetal ECG [9], it is also successfully applied in the
recovery of adult ECG in the wavelet domain. The reason is the
clustering property of the ECG wavelet coefficients that suggests the use of a block-sparsity model. This property refers to
the tendency of the large coefficients to cluster together into
blocks, as it is suggested by Fig. 1. For the implementation of
the BSBL-BO algorithm, the block partition was set to h = 15
and the maximum number of iterations to 30. Even though the
BSBL–BO algorithm offers performance superior to BPDN, it is
outperformed by MMB–IHT and MMB–CoSaMP. This result
suggests that the connected subtree structured sparsity model
may be more appropriate to represent the largest (in magnitude) ECG wavelet coefficients than is the block sparsity model.
In addition, the incorporation of prior support knowledge also
contributes to the superior performance attained by the proposed
methods.
It is of interest to compare the performance of the proposed
algorithms with overcomplete dictionary-based reconstruction
methods. In a previous work [33], we propose the use of a multiscale dictionary D ∈ RN ×J , J > N , for CS ECG, with the aim
of combining the advantages of multiscale representations using
wavelets with the benefits of dictionary learning. The dictionary
D is divided into subdictionaries according to the corresponding wavelet subband and each subdictionary is learned separately. The idea behind this approach is to exploit correlations
within each wavelet subband. The multiscale dictionary-based
algorithm, denoted as MS–BPDN, aims to solve problem (1)
with Θ = ΦΨD, instead of Θ = ΦΨ, by using basis pursuit
denoising. According to the results in Fig. 8, the proposed algo-

MMB–HT

MMB–CoSaMP

PRD

PRDN

QS

PRD

PRDN

QS

3.65
5.86
4.84
3.94
3.56
3.91
5.91
2.74
2.11
2.54
2.29

7.73
7.79
8.34
4.8
3.68
4.51
6.99
6.62
7.84
4.61
4.5

1.75
1.09
1.32
1.62
1.79
1.63
1.08
2.33
3.03
2.51
2.78

3.86
5.83
5.21
4.17
3.99
4.13
6.21
2.57
2.11
2.86
2.61

8.18
7.76
8.98
5.08
4.13
4.75
7.35
6.19
7.86
5.2
5.12

1.66
1.09
1.22
1.53
1.6
1.54
1.02
2.49
3.02
2.23
2.45

rithms outperform MS–BPDN because they exploit additional
signal structure information. However, it is also noticed that
MS–BPDN outperforms the traditional wavelet based-BPDN
algorithm, which suggests the promising application of adaptive
overcomplete dictionaries to CS ECG. The design of more efficient overcomplete dictionary-based reconstruction algorithms,
that exploit dependences among dictionary atoms, is left for
future work.
These results validate the potential of CS-based methods in
achieving high compression rates while offering important advantages, as described in previous works in the area [2], [8],
such as higher energy and hardware efficiency at the encoder.
The results of SPIHT are illustrated in Fig. 8, with the aim
of evaluating how the proposed compression scheme compares
with current ECG compression methods. SPIHT is a benchmark
state-of-the-art algorithm for ECG compression. SPIHT utilizes
an optimized embedded coding of the wavelet coefficients that
increases the compression rates and makes it outperform the
results of the proposed methods, at the expense of requiring
more computations. The time complexity of the SPIHT encoder is O(N logN ) [34], while the matrix-vector multiplication of the CS encoder, Φx, requires only O(N ) operations
when Φ is a random symmetric Bernoulli matrix [35]. Therefore, the use of a CS encoder offers a substantial complexity
reduction.
In CS-based methods, there is no access to the rich structure of the wavelet coefficients at the encoder, but instead,
there is access to the random measurements, which contain
fewer structure properties to be exploited at the coding stage.
This difference in compression performance between wavelet
transform-based methods and CS-based methods was previously
noted by Mamaghanian et al. [2], who emphasized the substantial lower power consumption offered by CS-based methods. Even though the proposed algorithm does not outperform
SPIHT in terms of compression ratio, it offers better compression and reconstruction performance than previously proposed
CS-based methods for ECG compression, such as BPDN and
BSBL–BO.
There is significant variability among the set of ECG records,
and therefore, it is instructive to calculate the reconstruction performance of the proposed algorithms for each ECG recording.

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

517

Fig. 9. Visual evaluation of the reconstruction of record 119 using MMB–CoSaMP and MMB–IHT. C R = 6.4. PRD = 2.61 (MMB-CoSaMP), PRD = 2.29
(MMB–IHT). Left: Recovery results for a 5-s sequence. Right: Magnified views of the dashed boxes located on the left (a) Original MIT-BIH record: 119
(5 seconds) (b) Reconstructed signal using modified model-based CoSaMP (c) Error signal obtained with modified model-based CoSaMP (d) Reconstructed signal
using modified model-based IHT (e) Error signal obtained with modified model-based IHT (f) Magnified segment (0.6 seconds) of the original signal (g) Magnified
reconstructed signal using modified model-based CoSaMP (h) Magnified reconstructed signal using modified model-based IHT.

The results are illustrated in Table II. For almost all the records,
MMB–IHT provides better performance than MMB–CoSaMP.

E. Visual Evaluation
Finally, visual study of the reconstructed and error signals
using the proposed compression scheme is also presented. Two
records with different clinical characteristics are selected for
the study: 118 and 119. Record 119 contains ventricular ectopic
heartbeats while the record 118 contains right bundle branch
block heartbeats. The results for a CR = 6.4 are shown in Fig. 9
for record 119 and in Fig. 10 for record 118. Magnified views of
the original and reconstructed signals are included in the figures.
For the two records, the recovered signals are a good estimate
of the original signals and they preserve detailed information
for clinical diagnosis. It should be noted that the reconstructed
signals using MMB–IHT exhibit less artifacts than the reconstructed signals using MMB–CoSaMP.
VI. CONCLUSIONS
The universal applicability of compressed sensing to sparse
signals lies in the fact that no specific prior information about
the signals is assumed, apart from the sparsity condition. However, for a particular application, some prior information about
the signals is typically available. In this paper, we showed that

for the specific application of compressed sensing to ECG compression, the appropriate incorporation of prior information into
the reconstruction procedures leads to more accurate reconstruction and higher compression rates. More precisely, we exploit
prior information on the connected subtree structure formed by
largest (in magnitude) wavelet coefficients and the common support of the wavelet representation of consecutive ECG segments
are exploited. The model-based CoSaMP and model-based IHT
algorithms are modified to incorporate support knowledge from
the previously reconstructed data sequence. The tree-structured
sparsity model is also modified to exclude the selection of atoms
from the lowest-energy wavelet subband. We justified the application of a model-based compressed sensing approach with
the fact that R wave events cause a connected subtree structure
of large magnitude wavelet coefficients. In addition, we also
selected an appropriate wavelet decomposition level to enable
the formation of such structure.
The proposed scheme was evaluated for the compression
of a set of eleven ECG records from the MIT–BIH Arrhythmia Database encompassing a variety of signals with
different rhythms, wave morphologies, and abnormal heartbeats. The experimental results were evaluated in terms of
PRD and compression ratio. The proposed method outperformed the results of previously proposed CS-based methods for
ECG compression while still maintaining the low-complexity
and energy-efficient implementation inherent to CS-based
approaches.

518

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Fig. 10. Visual evaluation of the reconstruction of record 118 using MMB–CoSaMP and MMB–IHT. C R = 6.4. PRD = 2.86 (MMB–CoSaMP), PRD = 2.54
(MMB–IHT). Left: Recovery results for a 5-s sequence. Right: Magnified views of the dashed boxes located on the left (a) Original MIT-BIH record:118 (5
seconds) (b) Reconstructed signal using modified model-based CoSaMP (c) Error signal obtained with modified model-based CoSaMP (d) Reconstructed signal
using modified model-based IHT (e) Error signal obtained with modified model-based IHT (f) Magnified segment (0.6 seconds) of the original signal (g) Magnified
reconstructed signal using modified model-based CoSaMP (h) Magnified reconstructed signal using modified model-based IHT.

REFERENCES
[1] H. Cao, V. Leung, C. Chow, and H. Chan, “Enabling technologies for
wireless body area networks: A survey and outlook,” IEEE Commun.
Mag., vol. 47, no. 12, pp. 84–93, Dec. 2009.
[2] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, “Compressed sensing for real-time energy-efficient ECG compression on wireless body sensor nodes,” IEEE Trans. Biomed. Eng., vol. 58, no. 9, pp.
2456–2466, Sep. 2011.
[3] D. Donoho, “Compressed sensing,” IEEE Trans. Inform. Theory, vol. 52,
no. 4, pp. 1289–1306, Sep. 2006.
[4] E. J. Candès and M. B. Wakin, “An introduction to compressive sampling,”
IEEE Signal Process. Mag., vol. 25, no. 2, pp. 21–30, Mar. 2008.
[5] R. S. H. Istepanian, L. J. Hadjileontiadis, and S. M. Panas, “ECG data
compression using wavelets and higher order statistics methods,” IEEE
Trans. Inform. Technol. Biomed., vol. 5, no. 2, pp. 108–115, Jun. 2001.
[6] M. L. Hilton, “Wavelet and wavelet packet compression of electrocardiograms,” IEEE Trans. Biomed. Eng., vol. 44, no. 5, pp. 394–402, May
1997.
[7] Z. Lu, D. Y. Kim, and W. A. Pearlman, “Wavelet compression of ECG
signals by the set partitioning in hierarchical trees algorithm,” IEEE Trans.
Biomed. Eng., vol. 47, no. 7, pp. 849–856, Jul. 2000.
[8] A. M. R. Dixon, E. G. Allstot, D. Gangopadhyay, and D. J. Allstot,
“Compressed sensing system considerations for ECG and EMG wireless
biosensors,” IEEE Trans. Biomed. Circuits Syst., vol. 6, no. 2, pp. 156–166,
Apr. 2012.
[9] Z. Zhang, T. Jung, S. Makeig, and B. D. Rao, “Compressed sensing
for energy-efficient wireless telemonitoring of noninvasive fetal ECG via
block sparse bayesian learning,” IEEE Trans. Biomed. Eng., vol. 60, no.
2, pp. 300–309, Feb. 2013.
[10] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, “Structured sparsity models for compressively sensed electrocardiogram signals:
A comparative study,” in Proc. 2011 IEEE Biomed. Circuits Syst. Conf.,
Nov. 2011, pp. 125–128.
[11] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, “Design
and exploration of low-power analog to information conversion based on
compressed sensing,” IEEE J. Emerging Sel. Topics Circuits Syst., vol. 2,
no. 3, pp. 493–501, Sep. 2012.

[12] L. F. Polania, R. E. Carrillo, M. Blanco-Velasco, and K. E. Barner, “Compressed sensing based method for ECG compression,” in Proc. IEEE Int.
Conf. Acoust., Speech Signal Process., May 2011, pp. 761–764.
[13] L. F. Polania, R. E. Carrillo, M. Blanco-Velasco, and K. E. Barner,
“Compressive sensing exploiting wavelet-domain dependencies for ECG
compression,” in Proc. SPIE Defense, Security, Sensing, Apr. 2012, pp.
83650E–83650E.
[14] L. F. Polania, R. E. Carrillo, M. Blanco-Velasco, and K. E. Barner, “On exploiting interbeat correlation in compressive sensing-based ECG compression,” in Proc. SPIE Defense, Security, Sensing, Apr. 2012, pp. 83650D–
83650D.
[15] L. F. Polania, R. E. Carrillo, M. Blanco-Velasco, and K. E. Barner, “Compressive sensing for ECG signals in the presence of electromyographic
noise,” in Proc. 38th Annu. Northeast Bioengineering Conf., Mar. 2012,
pp. 295–296.
[16] R. G. Baraniuk, V. Cevher, M. F. Duarte, and C. Hegde, “Model-based
compressive sensing,” IEEE Trans. Inf. Theory, vol. 56, no. 4, pp. 1982–
2001, Apr. 2010.
[17] A. L. Goldberger et al., “Physiobank, Physiotoolkit, and Physionet: Components of a new research resource for complex physiologic signals,”
Circulation, vol. 101, no. 23, pp. e215–e220, 2000.
[18] A. M. Bruckstein, D. L. Donoho, and M. Elad, “From sparse solutions
of systems of equations to sparse modeling of signals and images,” SIAM
Rev., vol. 51, no. 1, pp. 34–81, Feb. 2009.
[19] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM J. Sci. Comput., vol. 20, no. 1, pp. 33–61, Dec.
1998.
[20] J. A. Tropp and A. C. Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53,
no. 12, pp. 4655–4666, Dec. 2007.
[21] D. Needell and J. A. Tropp, “CoSaMP: Iterative signal recovery from
incomplete and inaccurate samples,” Appl. Comput. Harmonic Anal., vol.
26, no. 3, pp. 301–321, May. 2009.
[22] D. Needell and R. Vershynin, “Uniform uncertainty principle and signal recovery via regularized orthogonal matching pursuit,” Foundations
Comput. Math., vol. 9, no. 3, pp. 317–334, Apr. 2009.

POLANÍA et al.: EXPLOITING PRIOR KNOWLEDGE IN COMPRESSED SENSING WIRELESS ECG SYSTEMS

[23] T. Blumensath and M. Davies, “Iterative hard thresholding for compressed
sensing,” Appl. Comput. Harmonic Anal., vol. 27, no. 3, Nov. 2009.
[24] M. S. Crouse, R. D. Nowak, and R. G. Baraniuk, “Wavelet-based statistical signal processing using hidden markov models,” IEEE Trans. Signal
Process., vol. 46, no. 4, pp. 886–902, Apr. 1998.
[25] R. G. Baraniuk, R. A. DeVore, G. Kyriazis, and X. M. Yu, “Near best tree
approximation,” Advances Comput. Math., vol. 16, no. 4, pp. 357–373,
May 2002.
[26] L. Wang, and W. Yin, “Sparse signal reconstruction via iterative support
detection,” SIAM J. Imag. Sci., vol. 3, no. 3, pp. 462–491, Jul. 2010.
[27] N. Vaswani and L. Wei, “Modified-CS: Modifying compressive sensing
for problems with partially known support,” IEEE Trans. Signal Process.,
vol. 58, no. 9, pp. 4595–4607, Sep. 2010.
[28] F. Chen, A. P. Chandrakasan, and V. M. Stojanovic, “Design and analysis of
a hardware-efficient compressed sensing architecture for data compression
in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3, pp. 744–
756, Mar. 2012.
[29] A. Gersho and R. M. Gray, Vector Quantization and Signal Compression.
New York, NY, USA: Springer, vol. 159, 1992.
[30] R. E. Carrillo, L. F. Polania, and K. E. Barner, “Iterative algorithms for
compressed sensing with partially known support,” in Proc., IEEE Int.
Conf. Acoust., Speech Signal Process., Dallas, TX, USA, Mar. 2010, pp.
3654–3657.
[31] R. Paradiso, G. Loriga, and N. Taccini, “A wearable health care system
based on knitted integrated sensors,” IEEE Trans. Inf. Technol. Biomed.,
vol. 9, no. 3, pp. 337–344, Sep. 2005.
[32] C. M. Fira and L. Goras, “An ECG signals compression method and its
validation using NNs,” IEEE Trans. Biomed. Eng., vol. 55, no. 4, pp.
1319–1326, Apr. 2008.
[33] L. F. Polania and K. E. Barner, “Multi-scale dictionary learning for compressive sensing ECG,” in Proc. 2013 IEEE Digital Signal Process. Signal
Process. Education Meeting, Aug. 2013, pp. 36–41.
[34] Y. Park, “Time complexity analysis of SPIHT (set partitioning in hierarchy
trees) image coding algorithm,” J. Inst. Signal Process. Syst., vol. 4, no.
1, pp. 36–40, 2003.
[35] E. Liberty and S. W. Zucker, “The Mailman algorithm: A note on matrixvector multiplication,” Inf. Process. Lett., vol. 109, no. 3, pp. 179–182,
2009.
Luisa F. Polanı́a (S’12) received the B.S.E.E. degree (with honors) from the National University of
Colombia, Bogotá, Colombia, in 2009. She is currently working toward the Ph.D. degree in the Department of Electrical and Computer Engineering,
University of Delaware, Newark, DE, USA.
Her research interests include signal and image
processing, compressive sensing, low-dimensional
modeling, and biomedical signal processing.
Miss Polanı́a was the recipient of the University
of Delaware Graduate Student Fellowship in 2013.
Rafael E. Carrillo (S’07–M’12) received the B.S.
and the M.S. degrees (with honors) in electronics engineering from the Pontificia Universidad Javeriana,
Bogotá, Colombia, in 2003 and 2006, respectively,
and the Ph.D. degree in electrical engineering from
the University of Delaware, Newark, DE, USA, in
2011.
He was a Lecturer from 2003 to 2006 at the Pontificia Universidad Javeriana and a Research Assistant
at the University of Delaware from 2006 to 2011.
Since 2011, he has been a Postdoctoral Researcher at
the Institute of Electrical Engineering, Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland. His research interests include signal and image
processing, compressive sensing, inverse problems, and robust, nonlinear and
statistical signal processing.
Dr. Carrillo was the recipient of the “Mejor trabajo de grado” award, given
to outstanding master thesis at the Pontificia Universidad Javeriana, in 2006,
the University of Delaware Graduate Student Fellowship in 2007 and the Signal
Processing and Communications Graduate Faculty Award from the University
of Delaware in 2010.

519

Manuel Blanco-Velasco (S’00–M’05–SM’10) received the engineering degree from the Universidad
de Alcalá, Madrid, Spain in 1990, the M.Sc. degree
in communications engineering from the Universidad Politécnica de Madrid, Madrid, in 1999, and the
Ph.D. degree from the Universidad de Alcalá in 2004.
From 1992 to 2002, he was with the Department
of Circuits and Systems, Universidad Politécnica de
Madrid as an Assistant Professor. In April 2002, he
joined the Signal Theory and Communications Department of the Universidad de Alcalá where he is
currently working as an Associate Professor. His main research interests include biomedical signal processing and communications.

Kenneth E. Barner (S’84–M’92–SM’00) received
the B.S.E.E. degree (magna cum laude) from Lehigh
University, Bethlehem, PA, USA, in 1987. He received the M.S.E.E. and Ph.D. degrees from the University of Delaware, Newark, DE, USA, in 1989 and
1992, respectively.
He was the duPont Teaching Fellow and a Visiting Lecturer with the University of Delaware, in 1991
and 1992, respectively. From 1993 to 1997, he was
an Assistant Research Professor with the Department
of Electrical and Computer Engineering, University
of Delaware, and a Research Engineer with the duPont Hospital for Children.
He is currently Professor and Chairman with the Department of Electrical and
Computer Engineering, University of Delaware. He is coeditor of the book
Nonlinear Signal and Image Processing: Theory, Methods, and Applications
(Boca Raton, FL: CRC), 2004. His research interests include signal and image
processing, robust signal processing, nonlinear systems, sensor networks and
consensus systems, compressive sensing, human–computer interaction, haptic
and tactile methods, and universal access.
Dr. Barner is the recipient of a 1999 NSF CAREER award. He was the
Co-Chair of the 2001 IEEE-EURASIP Nonlinear Signal and Image Processing
(NSIP) Workshop and a Guest Editor for a Special Issue of the EURASIP Journal
of Applied Signal Processing on Nonlinear Signal and Image Processing. He
is a member of the Nonlinear Signal and Image Processing Board. He was the
Technical Program Co-Chair for ICASSP 2005 and and previously served on
the IEEE Signal Processing Theory and Methods (SPTM) Technical Committee
and the IEEE Bio-Imaging and Signal Processing (BISP) Technical Committee. He is currently a member of the IEEE Delaware Bay Section Executive
Committee. He has served as an Associate Editor of the IEEE Transactions on
Signal Processing, the IEEE Transaction on Neural Systems and Rehabilitation
Engineering, the IEEE Signal Processing Magazine, the IEEE Signal Processing Letters, and the EURASIP Journal of Applied Signal Processing. He was
the Founding Editor-in-Chief of the journal Advances in Human–Computer Interaction. For his dissertation “Permutation Filters: A Group Theoretic Class of
Non-Linear Filters,” Dr. Barner received the Allan P. Colburn Prize in Mathematical Sciences and Engineering for the most outstanding doctoral dissertation
in the engineering and mathematical disciplines. He is a member of Tau Beta
Pi, Eta Kappa Nu, and Phi Sigma Kappa.

