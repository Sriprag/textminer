624

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 2, MARCH 2016

Bleeding Frame and Region Detection in the
Wireless Capsule Endoscopy Video
Yixuan Yuan, Baopu Li∗ , and Max Q.-H. Meng, Fellow, IEEE

Abstract—Wireless capsule endoscopy (WCE) enables noninvasive and painless direct visual inspection of a patient’s whole digestive tract, but at the price of long time reviewing large amount
of images by clinicians. Thus, an automatic computer-aided technique to reduce the burden of physicians is highly demanded. In
this paper, we propose a novel color feature extraction method to
discriminate the bleeding frames from the normal ones, with further localization of the bleeding regions. Our proposal is based on
a twofold system. First, we make full use of the color information
of WCE images and utilize K-means clustering method on the pixel
represented images to obtain the cluster centers, with which we
characterize WCE images as words-based color histograms. Then,
we judge the status of a WCE frame by applying the support vector
machine (SVM) and K-nearest neighbor methods. Comprehensive
experimental results reveal that the best classification performance
is obtained with YCbCr color space, cluster number 80 and the
SVM. The achieved classification performance reaches 95.75% in
accuracy, 0.9771 for AUC, validating that the proposed scheme provides an exciting performance for bleeding classification. Second,
we propose a two-stage saliency map extraction method to highlight bleeding regions, where the first-stage saliency map is created
by means of different color channels mixer and the second-stage
saliency map is obtained from the visual contrast. Followed by an
appropriate fusion strategy and threshold, we localize the bleeding areas. Quantitative as well as qualitative results show that our
methods could differentiate the bleeding areas from neighborhoods
correctly.
Index Terms—Bleeding classification and region detection,
words-based color histograms, wireless capsule endoscopy (WCE).

I. INTRODUCTION
LEEDING in the gastrointestinal (GI) tract result from a
number of etiologies, including vascular lesions, vascular tumors, ulcers, and inflammatory lesions [1]. The general
approach to diagnose the bleedings is to directly view the GI
tract by different manners. However, the traditional imaging
techniques such as push enteroscopy and sonde enteroscopy
are not only painful and invasive, but also technically difficult to reach the small intestines [2], [3]. The advancement
of miniature technology promoted the emergence of wireless

B

Manuscript received September 9, 2014; revised December 17, 2014; accepted January 28, 2015. Date of publication February 6, 2015; date of current version March 3, 2016. This project is partially supported by RGC GRF
#CUHK415613 and National Natural Science Foundation of China (61305099).
Y. Yuan and M. Q.-H. Meng are with the Department of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong (Co-Corresponding
author, e-mail: max@ee.cuhk.edu.hk).
B. Li is with the Department of Biomedical Engineering, Shenzhen University, Shenzhen 518060, China (∗ Corresponding author, e-mail:
bpli@szu.edu.cn).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2015.2399502

capsule endoscopy (WCE) [4], which is a revolutionary imaging device that provides direct, noninvasive visualization of the
small bowel. The WCE was first introduced by the Given Imaging in 2000 and granted approval by U.S. Food and Drug Administration in 2001. It is a capsule-shaped device with a dimension
of 26 mm in length by 11 mm in diameter, which consists of
an optical dome, an illuminator, an imaging sensor, a battery,
and a RF transmitter. In the examination procedure, the WCE
is swallowed by the patient, and then, pushed by peristalsis to
slowly travel through the small intestine. Equipped with built-in
lighting and camera, a WCE takes pictures of the entire GI tract
with 2–4 images per second for about 8 h before its battery
exhausted. Finally, these images are transmitted wirelessly to a
data-recording device for doctors to examine the images off-line
later to make diagnostic decisions [5].
Although the WCE has shown significant advantages, there
are still challenges associated with this technology. One problem
is that the WCE produces about 55 000 images for each patient’s
examination, which makes it hard for physicians to go through
all these images frame by frame to locate the bleedings [6] in the
GI tract. Furthermore, WCE images capturing the abnormalities
of the GI tract occupy only a small percentage of the complete
WCE images collected [7]. In addition, although the software
developed by the Given Imaging, named the red blood identification system, can detect bleeding frames automatically, it
achieves low sensitivity (21.5%) and specificity (41.6%) [8]. All
these problems motivate us to turn to computer-aided systems.
In the paper [9], the authors put forward a new method for
rapid bleeding detection. They grouped pixels through the superpixel segmentation procedure and used the red color ratio in
the RGB color space to represent the features of these superpixels. The authors in [10] utilized six color features (mean and
variance of H, S, and V value) in the HSI color space to discriminate between bleeding and normal status. Lv et al. [11] introduced a new descriptor, that is, pyramid of hue histograms to
characterize the bleeding WCE images, incorporating color and
spatial information by combining illumination invariant color
histograms and the spatial pyramids method. Although these
methods can detect the bleeding frames from the normal ones
in some degree, majority of them extract the complete color
features from a WCE image, ignoring the specific color range
of WCE images.
In this paper, we propose words-based color histogram for
bleeding detection in WCE images. Our method is an extension
of Bag of Words method [12], [13]. In order to make the most
of the color information of the bleeding images, we calculate
the color words by applying K-means clustering procedure to
the pixel represented WCE images in the specific color space.

2168-2194 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

YUAN et al.: BLEEDING FRAME AND REGION DETECTION IN THE WIRELESS CAPSULE ENDOSCOPY VIDEO

Fig. 2.
Fig. 1.

625

Illustration of ROI extraction.

Workflow of the proposed method.

Then each WCE image is characterized as histogram of the cluster centers (named words-based color histogram) to represent
the feature vector. Finally, support vector machine (SVM) [14]
and K-nearest neighbor (KNN) [15] are utilized as classifiers
to detect bleeding frames. In the second stage, we focus on localization of the bleeding areas in the bleeding frames. Since
the components of various color spaces possess different information, we inspect the bleeding images under different color
spaces like RGB, HSI/HSV, CMYK, CIELAB, YUV, XYZ [16]
and select the components that highlight the bleeding areas.
Then, we create the first-stage saliency map by combing these
components together to strengthen the suspicious regions. In
addition, natural saliency lies in the visual contrast [17], [18].
Thus, we extract the second-stage saliency map from the prior
that if the color information of the region shows large similarity
to the red color, then this region should possess high saliency
value. Finally applying an appropriate fusion strategy of these
two conspicuity maps and automatic threshold, we are able to
localize the bleeding area.
Our main contributions can be summarized in the following
two aspects.
1) We propose the words-based color histogram to represent
the images. That is, we make use of middle-level features
rather than low-level features that are generally used in
the literatures [7], [9]–[11]. Moreover, we study carefully
the influence of the color spaces, cluster centers, and different classification methods in terms of the classification
performance.
2) We propose a two-stage saliency extraction method to localize the bleeding areas in WCE images. Since these twostage saliency maps highlight the bleeding regions and
separate bleeding mucosa from the uninformative parts,
we could obtain the bleeding area candidates successfully.
The remainder of this paper is organized as follows. Section II outlines the proposed method. Section III introduces
the bleeding frame classification method, while Section IV discusses the localization of the bleeding areas. The experimental
results validating the proposed intelligent system are presented
and discussed in Section V. Finally, we draw some conclusions
in Section VI.
II. METHOD OVERVIEW
The flowchart of the proposed method is depicted in Fig. 1
and it comprises two major steps. In the first step of bleeding

frame classification, we propose words-based color histograms
to represent WCE images. Then, we focus on the bleeding area
localization. We calculate the first-stage saliency map based on
the observation of the bleeding areas in the different color spaces
and the second-stage saliency map based on the natural property
of bleeding areas in the RGB color space. Finally, we localize
the bleeding areas through the fusion strategy of the saliency
maps and an appropriate threshold.
III. BLEEDING FRAME DETECTION
A. Region of Interest Extraction
WCE images are often obscured by the large black background and obvious bounders as shown in Fig. 2(a), (b), thus
the image features extracted from the entire image will reflect
the visual contamination presented in the image. To address this
factor, we outline the maximum square inscribed in the circular
image as region of interest (ROI) without loss of the major image
information. The size of the obtained ROI is 180 × 180 from
the original image with the size of 256 × 256. It can be found
that the extracted ROIs are satisfactory since they demonstrate
the major image features and provide a good characterization
and description of a WCE image. The ROI images replace the
original ones for the following processing, making the feature
extraction procedure much easier.
B. Color Feature Extraction
Since clinicians discriminate bleeding frames from the whole
WCE images mainly based on the color information, thus choosing a suitable color space is crucial. It is not obvious in advance
which color space is the most useful one to expose the abnormality of bleeding. Thus, we investigate the proposed feature
extraction methods in the commonly used color spaces including RGB, HSV, YCbCr, and LAB [19], [20] and make a best
choice through the experiment results.
After choosing the reference color space, we extract suitable
color features to describe bleeding frames in the endoscopic
images. Actually many existing approaches on feature extraction
focus on extracting histograms in a complete color range and
statistic characteristics of the histograms. However, the WCE
videos may not include some colors like blue or violet [19].
Furthermore, most of the observed colors in WCE images are
concentrated in a small region of the color space [20]. To address
these issues, we proposed a novel color feature to characterize
the WCE images.

626

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 2, MARCH 2016

on the foundation of statistical learning. The basic idea of the
SVM is to find the optimal hyperplane that separates the points
of different classes. For the implementation of the SVM, we
referred to the work of Chang and Lin in [22] to carry out the
experiments. In our method, the radial basis function is chosen
as the SVM kernel.
KNN is a kind of simple and intuitive method widely used
in pattern recognition and data mining. This classifier makes
a decision on comparing a new testing data with the training
data. In general, we calculate the Euclidean distance from the
new test image to all the training samples in the feature space
and classify this image based on the K-closest neighbors from
the training set. The label that comprises the majority of the
K-nearest training samples is assigned to the test sample. In this
experiment, we set K to 10 based on the optimal performance
to conduct the experiment.
Fig. 3.

Illustration of word-based color histogram extraction method.

In our proposed color feature extraction method, as shown
in Fig. 3, in order to obtain the specific color range of the
WCE images, we randomly select 10% bleeding images and
10% normal images from the datasets and calculate the corresponding cluster centers independently by inputting the pixel
represented image vectors in the color space to the K-means
clustering procedure [21]. This procedure utilizes the color information of the WCE frames and reduces the dimension of
the color feature. The concatenated cluster centers from the
bleeding dataset and the normal WCE dataset serve as a vocabulary of visual words. Then, we map 3-D color data of each
point in a WCE image to the nearest visual words and calculate
the number of each visual word, yielding a histogram (w, d),
where wi denotes the ith visual word in the K-size color clusters
and di counts the frequency of occurrence of it. Utilizing this
method, we characterize the WCE images as the words-based
color histograms. An important decision in the construction of
the new feature is the selection of the vocabulary size K. To
evaluate how the vocabulary size influences the classification
performance with our approach, we gradually increase K from
10 to 100 and evaluate the classification results under each
setting.
As discussed earlier, the proposed color features are based
on the histogram, thus it preserves the robustness to the rotation
and translation of the WCE image contents. In addition, the
words-based color histograms make the best of the range of the
color information in the WCE images and represent the color
distribution of the WCE images in a middle level with smaller
dimensions than the traditional color features. Therefore, the
proposed novel feature could provide an accurate description of
the WCE images effectively.
C. Bleeding Classification Method
To verify the performance of the proposed feature, we deploy
two common classification methods: SVM and KNN to evaluate
their powers in differentiating normal images and bleeding images. SVM [14] is a supervised machine learning method based

D. Criteria for Bleeding Classification
The bleeding frame classification performance in the WCE
images is measured by accuracy, specificity, and sensitivity,
which are widely used to evaluate classification performance.
The sensitivity shows capability of detecting bleeding images
while specificity means the ability to avoid false detection. Accuracy is used to assess the overall performance of the algorithm, which reflects sensitivity and specificity in relation to
each other. In addition, we also make use of AUC (Area under
the ROC curve) to gauge the performance.
IV. BLEEDING LOCATION DETECTION
A. First-Stage Saliency
After obtaining the bleeding frames, we focus on the bleeding area localization in this part. Since color is the main cue
used to distinguish the bleeding areas from the normal mucosa
and it is not obvious in advance which color component contains the most useful information to expose the abnormality of
bleeding, therefore, we inspect the bleeding images under different color components of various color spaces such as RGB,
CIELAB, CIEXYZ, YCbCr, CMYK, HSV, and HSI. As shown
in Fig. 4, interestingly, we have observed that the second component of the transformed WCE images in the CIELAB [23]
and CMYK [24] color spaces highlights the bleeding regions
and separates bleeding mucosa tissues from the uninformative
parts.
In order to emphasize the bleeding mucosa, a saliency map
is created by assigning different weights to the aforementioned
two planes that highlight the bleeding areas with the following
equation:
Sstage1 (x, y) = α ∗ A(x, y) + β ∗ M (x, y)

(1)

where A(x, y) represents the “A” channel of the CIELAB color
space [23], while M (x, y) is the “M” channel of the CMYK
color space [24]. α, β are predefined constants as α = β = 0.5
in our experiments. In addition, since the range of the “A” plane
in CIELAB space and the “M” plane in the CMYK plane is
different, we normalize these two color planes before applying
the formula (1).

YUAN et al.: BLEEDING FRAME AND REGION DETECTION IN THE WIRELESS CAPSULE ENDOSCOPY VIDEO

627

saliency maps:
Sstage2 (x, y) =

1
× (SR + SG + SB ).
3

(5)

C. Fusion Scheme of Two Saliency Maps
The aforementioned two saliency maps localize the bleeding
areas in the WCE images from different aspects, one is from the
color space transformation and the other is from the view of the
clinician. Thus, we could derive our proposed saliency map by
fusing them together by the following formula:
Sﬁnal = w1 ∗ Sstage1 + w2 ∗ Sstage2

(6)

where Sﬁnal is the corresponding fused saliency map and
w1 + w2 = 1. We adjust w1 from 0.1 to 1 with 0.1 increments
each time to evaluate the different weights to the final bleeding localization. Once these two-stage saliency maps are fused
together, the bleeding regions are likely to be strengthened. Finally, after applying a thresholding algorithm, we could further
localize the true bleeding regions; the saliency map will be further processed into a binary mask image.
Fig. 4. (a) Original frames with bleeding regions, (b) ROIs, (c) second channel
of CIELAB color space, (d) second channel of CMYK color space.

B. Second-Stage Saliency
When clinicians view the WCE images, they may be attracted
by the bleeding areas based on red color in RGB space instead
of the transformations in the different color spaces. Therefore,
we define the second salient regions as those regions with large
similarity to the red color values, which means that if a pixel
has a greater R value and smaller G, B values, it would seem
reddish and should be assigned higher saliency value. Hence,
we derive the second-stage saliency map based on the following
three steps. First, we utilize a 5∗5 Gaussian filter on the original
image to eliminate fine texture details as well as noise and coding
artifacts. Then, the saliency maps for R, G, and B color channels
are calculated by


VR2
(2)
SR (x, y) = 1 − exp −
σR


V2
SG (x, y) = exp − G
σG

(3)



VB2
SB (x, y) = exp −
σB

(4)

D. Criteria for Bleeding Localization
To quantitatively assess the localization performance, a pixelbased comparison between the localized bleeding regions and
the ground truth labeled by the clinicians is performed. The three
metrics used in this experiment are precision, the false positive
ratio (FPR), and the false negative ratio (FNR) [25], [26], which
are calculated according to the following formulas:
Precision =

TP
TP + FP

(7)

FPR =

FP
FP + TN

(8)

FNR =

FN
FN + TP

(9)

where true positive (TP) cases are the bleeding areas that are
correctly labeled as the bleedings, while false positive (FP) are
the ones incorrectly labeled as the bleedings. False negative (FN)
represent the regions which are not labeled as the bleedings but
should have been.
V. EXPERIMENT RESULTS
A. Image Acquisition and Experimental Setup

where VR , VG , and VB are the corresponding values in the
RGB space. The parameters σR , σG , and σB are chosen as
σR = 0.32, σG = σB = 0.45 to adjust the three color saliency
map to [0, 1]. It can be found that in the formula (2)–(4), the
region with larger VR , smaller VG and VB will have higher SR ,
SG , and SB .
Finally, the second-stage saliency map Sstage2 for an image
I could be formulated as the combination of the three channel

In our experiments, we conducted experiments on 2400 WCE
images that consist of 400 bleeding frames and 2000 normal
frames. These images were extracted from ten different patients’ videos and manually annotated by gastroenterologists.
The effective resolution of these images is 256∗256.
B. Experiment Results for Bleeding Frames Detection
1) Parameters Selection: The first experiment was designed
to evaluate the discriminative power of the proposed feature and

628

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 2, MARCH 2016

Fig. 5. Classification performance with different color spaces and classification methods. (a)–(c) Classification performance with SVM. (d)–(f) Classification
performance with KNN.

choose the best parameters in the classification of the bleeding
WCE images.
The most important two parameters in the construction of the
word-based color histogram are the selection of the vocabulary
size K and the color space. To evaluate how these two parameters
influence the classification performance, we conduct the coupled
experiments. We tune K from 10 to 100 with 10 increments in
the experiment and the results for the different classification
methods SVM and KNN in different color spaces are showed
in the Fig. 5. To be mentioned here, we apply a tenfold cross
validation method to validate our proposed method [27].
We first analyzed the classification performance with different
classification algorithms. It can be found that the SVM method
[see Fig. 5(a)–(c)] is much more suitable than the KNN method
[see Fig. 5(d)–(f)] for bleeding classification in the WCE images because the SVM method shows relative higher accuracy,
specificity, and sensitivity in all of the color spaces. We then
studied the classification performance with the SVM classifier.
As illustrated in Fig. 5(a)–(c), there are some differences among
the four color spaces. Overall the YCbCr color space exhibits
the best classification performance, while the HSV color space
is not suitable for bleeding detection with lowest accuracy compared with other color spaces.
In addition, the cluster center size does affect the classification performance. In the YcbCr color space, the classification
performance increases first, and then, decreases when the cluster size increases. The best classification result is achieved with
cluster centers 80 and the corresponding accuracy and sensitivity reach 95.75% and 92.00%, respectively. The AUC of the
classification with the best parameter setting is 0.9771, validating the proposed feature has a good capability to classify
bleeding images.
2) Feature Analysis: To show the effectiveness of our features, we plotted two examples of the feature histogram with
cluster number 80 for the normal image and bleeding image in
the Fig. 6. The cluster centers 1–40 are obtained by the training

Fig. 6. Feature analysis. (a) Normal image and the corresponding feature
histogram. (b) Bleeding image and the corresponding feature histogram.

bleeding images, while the cluster centers 41–80 are obtained
from the training normal images. We could find that the bleeding
image and the normal image show significant different wordsbased color histograms.
3) Comparison to the Other Methods: To further evaluate
the performance of the proposed new feature, we compared it
with the six state-of-the-art color features. The first two comparison methods directly used the HSV histogram and YCbCr
histogram to describe images. These histograms are produced
by dividing the colors in the image into specific number of bins
and counting the number of image pixels in each bin. For the
HSV histogram, it quantizes the whole color space into 72 main
colors (dividing H into eight parts, S and V into three parts)
while for the YCbCr histogram, 125 bins are used to represent image features (dividing Y, Cb, and Cr into five parts).
The method in [10] and [11] has already been mentioned in
the introduction part and these two papers are the latest related
papers focused on the bleeding detection. We also compared the

YUAN et al.: BLEEDING FRAME AND REGION DETECTION IN THE WIRELESS CAPSULE ENDOSCOPY VIDEO

TABLE I
PERFORMANCE COMPARISON OF DIFFERENT FEATURES (%)

Acc.
Sen.
Spec.
Time(s)

HSV
histogram

YCbCr
histogram

[10]

92.37
84.75
93.90
75.93

86.25
91.50
85.20
422.27

90.00
81.75
91.65
35.76

[11]

83.00
78.00
84.00
530.43

[28]

91.63
94.75
91.00
4171.5

[29]

76.63
91.25
62.00
28737

629

TABLE II
PERFORMANCE COMPARISON OF DIFFERENT w 1 FOR BLEEDING
LOCALIZATION (%)
Our
method
95.75
92.00
96.50
293.43

proposed method with latest feature coding method: sparse coding [28] and locality-constrained linear coding (LLC) [29]. In
these two methods, after obtaining the cluster centers, instead of
representing images by vector quantization, they encode each
point in the image by the sparse coding and LLC method to
obtain the final histogram. We implemented these algorithms
on our datasets and the corresponding comparison results are
shown in Table I.
It can be found in Table I, the proposed method illustrates
better performance over those of the HSV histogram method
with an improvement 3.38% and 7.25% of in accuracy and sensitivity, respectively, while over those of YCbCr histogram with
an improvement 9.5% and 0.5%. This result demonstrates that
the words-based color histogram is superior to the traditional
color histograms to classify the bleeding frames. In addition,
our method gets higher accuracy, sensitivity, and specificity than
the existed method [10] and [11] in the bleeding frame classification task. Compared with the latest feature coding method
[28] and [29], our method not only shows better accuracy and
sensitivity, but also is much faster than sparse coding and LLC,
which demonstrates that our method is more effective in clinical
situation.
C. Experiment Results for Bleeding Localization
1) Quantitative Analysis: After obtaining the bleeding
frames, we focused on the localization of the bleeding areas
in this part. Our model calculates two-stage saliency maps and
fuses them together to detect the bleeding areas. Evaluation of
the fusion strategy is an important step in the assessment of the
model performance. We tested w1 in (6) for the values {0, 0.2,
0.4, 0.6, 0.8, 1} to obtain the final saliency maps, and then, applied the Otsu’s [30] threshold method on the calculated saliency
maps to obtain the bleeding areas.
To evaluate the localization performance obtained by using
different weighting models, three criteria: Precision, FPR, and
FNR were calculated and the corresponding results are shown
in the Table II. The best bleeding localization result with the
precision of 95.24% was achieved with weight of 0.8 on the
first-stage saliency map. This result is inspiring, illustrating that
our proposed saliency method could detect the bleeding area
accurately. Moreover, when the weight of the saliency region
is set to 0 or 1, the best localization accuracy is not obtained.
This result indicates that these two-stage saliency calculation
methods complement each other to provide useful information
for bleeding localization. Furthermore, the algorithm achieved
a good FPR of 0.86% with w1 = 0.8.

W1

Precision

FPR

FNR

W1

Precision

FPR

FNR

0
0.2
0.4

66.27
78.89
91.05

10.71
5.27
1.82

34.09
34.21
34.99

0.6
0.8
1

94.70
95.24
93.97

0.99
0.86
1.17

36.79
38.97
33.72

Fig. 7. Procedure of localization for bleeding area. (a) First-stage saliency
map. (b) Second-stage saliency map. (c) Fused saliency map. (d) Localized
bleeding area.

2) Qualitative Analysis: We fixed w1 = 0.8 to illustrate our
proposed saliency methods visually by the first example image
given in Fig. 4. Fig. 7(a) is the saliency map generated through
means of channel mixer while Fig. 7(b) is obtained by prior
knowledge that the region with red color should assign large
saliency value. It is obvious that these two saliency maps provide good ability to identify bleeding regions. After the fusion
strategy, the final saliency map is shown in Fig. 7(c). In this figure, the bleeding mucosa appears brighter than normal mucosa
region, so it can be easily extracted by an appropriate threshold.
Fig. 7(d) is the binary bleeding mask image obtained through
the Otsu’s threshold procedure.
In Fig. 8, qualitative results for the bleeding area localization
based on the saliency maps are presented using three different
WCE ROI images containing bleedings. The first column is
the ROI images, while the second column shows the bleeding
saliency map, and the third column shows the detected bleeding
region. The final column represents the ground truth for the
bleeding area labeled by the clinicians. As can be observed,
we are able to differentiate the bleeding areas accurately by the
proposed bleeding localization method.
VI. CONCLUSION
In this paper, we have proposed a novel method for bleeding
frame detection and region localization in WCE images. The
extensive experiments demonstrate that the best classification
performance could be obtained with SVM classifier, YCbCr
color space, and cluster number of 80. The proposed features
could obtain accuracy 95.75%, sensitivity 92% and specificity
96.5%, and the corresponding AUC is 0.9771. In the second
step, we extracted two-stage saliency maps to locate the final

630

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 2, MARCH 2016

Fig. 8. Three examples of localization for bleeding areas. (a) ROI images. (b)
Final saliency maps. (c) Localized bleeding areas. (d) Ground truth.

bleeding areas. The quantitative result indicates the best bleeding localization performance could be obtained by the weight
0.8 for the first-stage saliency map and 0.2 for the second-stage
saliency map. The corresponding localization precision archives
95.24%. In the future, we will find more suitable features for the
bleeding frames classification in order to decrease the mistakes
in the localization tasks.
REFERENCES
[1] D. O. Faigel and D. R. Cave, Capsule Endoscopy: Saunders. New York,
NY, USA: Elsevier, 2008.
[2] M. Yu, “M2a (tm) capsule endoscopy: A breakthrough diagnostic tool
for small intestine imaging,” Gastroenterol. Nursing, vol. 25, pp. 24–27,
2002.
[3] G. Gay, M. Delvaux, and J.-F. Rey, “The role of video capsule endoscopy
in the diagnosis of digestive diseases: A review of current possibilities,”
Endoscopy, vol. 36, pp. 913–920, 2004.
[4] G. Iddan, G. Meron, A. Glukhovsky, and P. Swain, “Wireless capsule
endoscopy,” Nature, vol. 405, pp. 417–418, 2000.
[5] N. M. Lee and G. M. Eisen, “10 years of capsule endoscopy: An update,”
Expert Rev. Gastroenterol. Hepatol., vol. 4, pp. 503–512, 2010.
[6] M. Pennazio, “Capsule endoscopy: Where are we after 6 years of clinical
use?” Digestive Liver Disease, vol. 38, pp. 867–878, 2006.
[7] B. Li and M.-H. Meng, “Computer-aided detection of bleeding regions
for capsule endoscopy images,” IEEE Trans. Biomed. Eng., vol. 56, no. 4,
pp. 1032–1039, Apr. 2009.
[8] R. Francis, “Sensitivity and specificity of the red blood identification
(RBIS) in video capsule endoscopy,” presented at the 3rd Int. Conf. Capsule Endoscopy, Miami, FL, USA, 2004.
[9] Y. Fu, W. Zhang, M. Mandal, and M.-H. Meng, “Computer-aided bleeding
detection in WCE video,” IEEE J. Biomed. Health Inform., vol. 18, no. 2,
pp. 636–642, Mar. 2014.
[10] L. Cui, C. Hu, Y. Zou, and M.-H. Meng, “Bleeding detection in wireless
capsule endoscopy images by support vector classifier,” in Proc. IEEE Int.
Conf. Inform. Autom., 2010, pp. 1746–1751.

[11] G. Lv, G. Yan, and Z. Wang, “Bleeding detection in wireless capsule
endoscopy images based on color invariants and spatial pyramids using
support vector machines,” in Proc. IEEE Annu. Int. Conf. Eng. Med. Biol.
Soc., 2011, pp. 6643–6646.
[12] J. Sivic and A. Zisserman, “Video Google: A text retrieval approach to
object matching in videos,” in Proc. 9th IEEE Int. Conf. Comput. Vision,
2003, pp. 1470–1477.
[13] G. Csurka, C. Dance, L. Fan, J. Willamowski, and C. Bray, “Visual categorization with bags of keypoints,” in Proc. ECCV Workshop Statistical
Learn. Comp. Vision, 2004, pp. 1–2.
[14] C. Cortes and V. Vapnik, “Support-vector networks,” Mach. Learn., vol.
20, pp. 273–297, 1995.
[15] S. A. Dudani, “The distance-weighted k-nearest-neighbor rule,” IEEE
Trans. Syst., Man Cybern., vol. SMC-6, no. 4, pp. 325–327, Apr. 1976.
[16] G. Sharma and H. J. Trussell, “Digital color imaging,” IEEE Trans. Image
Process., vol. 6, no. 7, pp. 901–932, Jul. 1997.
[17] R. Achanta, F. Estrada, P. Wils, and S. Süsstrunk, “Salient region detection
and segmentation,” in Computer Vision Systems. New York, NY, USA:
Springer, 2008, pp. 66–75.
[18] S. Goferman, L. Zelnik-Manor, and A. Tal, “Context-aware saliency detection,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 34, no. 10, pp.
1915–1926, Oct. 2012.
[19] S. Segui, M. Drozdzal, F. Vilarino, C. Malagelada, F. Azpiroz, P. Radeva,
and J. Vitria, “Categorization and segmentation of intestinal content frames
for wireless capsule endoscopy,” IEEE Trans. Inf. Technol. Biomed., vol.
16, no. 6, pp. 1341–1352, Nov. 2012.
[20] S. Hwang, J. Oh, J. Cox, S. J. Tang, and H. F. Tibbals, “Blood detection in
wireless capsule endoscopy using expectation maximization clustering,”
Med. Imag., vol. 6144, pp. 61441P-1–61441P-11, 2006.
[21] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman,
and A. Y. Wu, “An efficient k-means clustering algorithm: Analysis and
implementation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7,
pp. 881–892, Jul. 2002.
[22] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector machines,” ACM Trans. Intell. Syst. Technol., vol. 2, pp. 1–27, 2011.
[23] J. Schanda, Colorimetry: Understanding the CIE system. New York, NY,
USA: Wiley, 2007.
[24] Y. Zhao, Z. Fan, and M. E. Hoover, “Frequency domain infrared watermarking for printed CMYK image,” in Proc. IEEE 18th Int. Conf. Image
Process., 2011, pp. 2725–2728.
[25] F. Conversano, E. Casciaro, R. Franchini, S. Casciaro, and A. LayEkuakille, “Fully automatic 3D segmentation measurements of human
liver vessels from contrast-enhanced CT,” in Proc. IEEE Int. Symp. Med.
Meas. Appl., 2014, pp. 1–5.
[26] F. Conversano, R. Franchini, C. Demitri, L. Massoptier, F. Montagna, A.
Maffezzoli, A. Malvasi, and S. Casciaro, “Hepatic vessel segmentation
for 3D planning of liver surgery: Experimental evaluation of a new fully
automatic algorithm,” Acad. Radiol., vol. 18, pp. 461–470, 2011.
[27] Q. Li and K. Doi, “Reduction of bias and variance for evaluation of
computer-aided diagnostic schemes,” Med. Phys., vol. 33, pp. 868–875,
2006.
[28] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid matching
using sparse coding for image classification,” in Proc. IEEE Conf. Comput.
Vision Pattern Recog., 2009, pp. 1794–1801.
[29] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, and Y. Gong, “Localityconstrained linear coding for image classification,” in Proc. IEEE Conf.
Comput. Vision Pattern Recog., 2010, pp. 3360–3367.
[30] N. Otsu, “A threshold selection method from gray-level histograms,” Automatica, vol. 11, pp. 23–27, 1975.

Authors’ photographs and biographies not available at the time of publication.

