IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

177

Personalized Multilayer Daily Life Profiling Through
Context Enabled Activity Classification and Motion
Reconstruction: An Integrated System Approach
James Y. Xu, Student Member, IEEE, Yan Wang, Mick Barrett, Bruce Dobkin, Greg J. Pottie, Fellow, IEEE,
and William J. Kaiser, Senior Member, IEEE

Abstract—Profiling the daily activity of a physically disabled
person in the community would enable healthcare professionals
to monitor the type, quantity, and quality of their patients’ compliance with recommendations for exercise, fitness, and practice
of skilled movements, as well as enable feedback about performance in real-world situations. Based on our early research in
in-community activity profiling, we present in this paper an endto-end system capable of reporting a patient’s daily activity at
multiple levels of granularity: 1) at the highest level, information
on the location categories a patient is able to visit; 2) within each
location category, information on the activities a patient is able to
perform; and 3) at the lowest level, motion trajectory, visualization,
and metrics computation of each activity. Our methodology is built
upon a physical activity prescription model coupled with MEMS
inertial sensors and mobile device kits that can be sent to a patient
at home. A novel context-guided activity-monitoring concept with
categorical location context is used to achieve enhanced classification accuracy and throughput. The methodology is then seamlessly
integrated with motion reconstruction and metrics computation to
provide comprehensive layered reporting of a patient’s daily life.
We also present an implementation of the methodology featuring a
novel location context detection algorithm using WiFi augmented
GPS and overlays, with motion reconstruction and visualization algorithms for practical in-community deployment. Finally, we use
a series of experimental field evaluations to confirm the accuracy
of the system.
Index Terms—Activity monitoring, context driven, mobile
health, motion reconstruction, wireless health.

I. INTRODUCTION
NE of the biggest problems faced by many nations is the
staggering cost of health care and the growing number of
people with physical disabilities that limit their independence
in performing daily activities. For example, in the U.S., stroke

O

Manuscript received August 14, 2014; revised October 27, 2014 and December 13, 2014; accepted December 16, 2014. Date of publication December 23,
2014; date of current version December 31, 2015. This work was supported
by the National Science Foundation (NSF) under Grant 0120778 (Center for
Embedded Networked Systems) and Qualcomm Innovation Fellowship S20121638. Any opinions, findings and conclusions expressed are those of the authors
and do not necessarily reflect the views of Qualcomm or the NSF.
J. Y. Xu, Y. Wang, G. J. Pottie, and W. J. Kaiser are with the Department of Electrical Engineering, University of California Los Angeles, Los Angeles, CA 90095 USA (e-mail: jyxu@ucla.edu; wangyanphyllis@gmail.com;
pottie@ee.ucla.edu; kaiser@ee.ucla.edu).
M. Barrett is with Qualcomm, San Diego, CA 02121 USA (e-mail: mbarrett
@qti.qualcomm.com).
B. Dobkin is with the Neurology Department, University of California Los
Angeles, Los Angeles, CA 90095 USA (e-mail: bdobkin@mednet.ucla.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2385694

alone disables 650 000 survivors each year, most of whom benefit from physical rehabilitation after discharge from a hospital
[1]. Acute and chronic rehabilitation services for stroke, however, are limited by insurance coverage, access to care, the inability to monitor home-based practice to provide feedback and
safe progression of skills training and measurement tools that
can reveal progress and additional needs for care [2], [3].
The wireless health community endeavors to help solve this
healthcare crisis by developing end-to-end systems that utilize
body worn inertial measurement units and mobile devices to provide complete daily activity profile of a patient to the physician
[2]. The profile may include clinically meaningful information
such as the type, quality, and context of physical activities performed by a patient throughout the day. For example, in treating
chronic diseases such as stroke, multiple sclerosis, heart failure,
diabetes, etc., physicians would like to improve the quality and
safety of a walking pattern that is slow or asymmetric, reduce
the risk of falls, improve fitness through progressive walking
or stationary cycling, lessen the burden of care on the family
by reducing disability, increase daily participation in home and
community activities, and reduce the likelihood of hospitalization. Optimally, patients would not have to travel or seek costly
interventions for therapy. Home- and community-based management via monitoring using mobile health and telerehabilitation services could bring continuing inexpensive treatments that
maximize quality of life and reduce disability.
It is clear that to achieve this vision, a number of capabilities
must be present: 1) activity classification, to recognize activities
of interest; 2) motion reconstruction and metrics extraction, to
provide qualitative and clinically meaningful information on the
activities; 3) environmental context monitoring, to provide vital additional data in understanding where an activity took place
and the patient’s health-related quality of life; 4) a way to present
information in a layered approach, from holistic quality of life
information to context and activity information, expanding further into visualization of individual limb movements during
activities; and 5) a software system that synergizes the various
components and provides a way for researchers, physicians and
caregivers to monitor and evaluate a patient’s daily behavior.
In this study, we continue our effort and address this vision by developing a novel end-to-end methodology and system
that 1) allows the monitoring of prescribed physical activities
by caregivers through inertial sensors and mobile device given
to a patient, with minimal training data; 2) presents a novel
context-guided activity monitoring concept using high level location context to achieve enhanced classification accuracy and

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

178

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

throughput; and 3) seamlessly integrates motion reconstruction
and metrics computation with context information and contextguided activity classification to provide comprehensive, layered
reporting of a patient’s daily life. An innovative implementation
of the methodology is also presented that enables: 1) automatic
identification of location categories through energy efficient,
WIFI augmented GPS and cutting-edge open map APIs; and
2) motion reconstruction, visualization, and metrics computation that are easy to deploy in-community and robust against
abnormal motion patterns.
This paper is organized as follows. Section II provides a survey of related work. Section III describes the system methodology. Section IV presents an implementation example. The
methodology and implementation is evaluated in Section V followed by the conclusions in Section VI.
II. RELATED WORK
The potential health-related benefits of activity monitoring
through sensors are in development [4]–[9]. An activity classification system was developed for promoting exercises in an
effort to reduce injuries [6]. Multiple on body accelerometers
were used with a large number of binary classifiers each trained
to recognize specific activities. A series of optimizations linked
the individual classifiers to produce a final output. A mixtureof-expert model was proposed for activity classification using
cotraining with both labeled and unlabeled data [7]. The method
uses a number of simple classifiers each trained with a labeled dataset and then selects unlabeled data for further training
and improved system performance. A camera-based hierarchical activity classification approach was presented in [8] using
switching hidden Markov model (HMM). The classifier performance degradation problem associated with complex models
was improved using a two tier system. The first tier describes
activities of daily living (ADL) using sequences of atomic activities and the second recognizes ADL through an HMM populated using atomic activity sequences. Several authors applied
accelerometer sensor data from both ankles and machine learning algorithms to monitor the rehabilitation activities of acute
stroke patients from 150 sites in 12 countries [9]. Here, the Naı̈ve
Bayes classifier was used to detect activities such as walking and
a dynamic time warping algorithm was then used to compare
segments of activity against previous templates. This enabled
physicians and therapists to directly measure the type and quantity of retraining to walk with the equivalent of laboratory quality
measurements. In these previous works, most classification approaches suffer decreasing accuracy as the number of potential
motions increases and very few are able to produce meaningful metrics because the classifiers were designed without much
consideration for the biomechanics of motion.
The use of wireless inertial sensors containing an accelerometer, gyroscope, and magnetometer to track body motion has
largely been performed for foot motion during gait [10]–[13].
Strap-down inertial navigation techniques were used with
straightforward zero velocity update (ZUPT) algorithms to detect when the foot was stationary and corrected the integration errors. In [11], a foot orientation and position-tracking

system was developed. The study augmented GPS data with
gait data such as walking speed and stride length, obtained
using a standard inertial dead-reckoning method. Various gait
features such as stride length, walking speed, gait phase timing,
and incline were estimated in [12], where foot movement was
reconstructed in the sagittal plane using a 2-D accelerometer
with gyroscope. While accurate, these methods face challenges
for in-community deployment since the ZUPT does not take
into account subjects exhibiting irregular gait and little focus is
placed on ease of deployment and minimizing user training and
calibration.
Few studies have emerged that combine context and activity
classification. A multisensor wearable system was developed for
context detection [14], where a total of thirty sensors were embedded into a garment with multiple processing nodes responsible for distributed processing. Physical activities were defined
as contexts and the study focused on sensor fusion techniques.
In [15], a context-aware system was developed to promote exercise and fitness. The system integrates GPS to guide the user
for a running session and indicates when the user should stop
and perform strength exercises before resuming the run. The
user’s average speed is computed using GPS and provided to
the user as feedback. A middleware for managing context data
was presented [16], where context is defined as all measurable
aspects of a person, including physical activities. This middleware first handles the transmission, reception and storage of
context data from sensors. It then provides a query platform to
assist in-community care by healthcare providers. Most studies above originated from pervasive computing where physical
activity is considered to be part of a context and the synergy
between contextual information, activity classification, motion
reconstruction, and system was not evaluated.
Activity recognition, monitoring or context detection individually will not provide the big picture to understand when,
how well, and how often an activity of interest occurs, nor the
functional details and metrics required to scrutinize the skillfulness of an activity. Recently, we deployed a system that partially
provides the capabilities needed [17]–[19]. There, environmental context was detected to augment activity classification that
enhanced classification accuracy, scalability, speed, and energy
usage. The system was also integrated with body worn inertial
sensors, a mobile device, and end-user activity monitoring. This
paper leverages the prescription-based context-driven activity
classification concept, but significantly extends this strategy by
introducing novel methodologies, architectures, and implementation that enable multitiered daily activity profiling, including
an automatic way to detect and build contexts and scenarios,
episodic motion reconstruction and metrics extraction, data aggregation, and reporting.
III. SYSTEM METHODOLOGY
Our prior work focused on activity monitoring and the benefits of context-driven personalized activity classification coupled
with sensors for rehabilitation. The work in this paper focuses
on improving context detection by removing the end-user training requirements, automatically building scenarios to remove

XU et al.: PERSONALIZED MULTILAYER DAILY LIFE PROFILING THROUGH CONTEXT ENABLED ACTIVITY CLASSIFICATION AND MOTION

Fig. 1.

Physical exercise prescription and monitoring.

context model building during prescription time, providing additional biomechanical details and metrics of activities through
episodic motion reconstruction, and developing a comprehensive tiered daily reporting system.
A. Context-Driven Activity Classification
In our previous work in novel context-guided activity classification using machine learning and signal processing techniques
[16]–[18], a context was defined as a subset of all attributes that
characterizes an environment, external to the user. This definition separated physical activities from environmental context
so that the latter can be used to enhance the former through a
prescription model (see Fig. 1).
This prescription-based approach also enables healthcare
providers to prescribe to patients individualized exercise or skill
practice plans with sensor kits (mobile device + inertial sensors). A third party service provider such as a rehabilitation
therapist would then monitor users for exercise quality and compliance.
B. Personalized Multilevel Daily Profile Monitoring
While our prior work in context-driven activity monitoring
can provide accurate, fast, and energy-efficient activity classification, it cannot provide specific metrics computed from the
motion and the context surrounding each activity. For example,
while activity classification may be able to indicate episodes
of walking, physicians and therapists may also want to know
the walking speed under different environmental conditions,
gait symmetry, the foot motion of each stride and where each
episode took place [20]. This requirement translates into a system that can present information at multiple levels of granularity: 1) at the highest level, information on the location categories
a person was able to visit, providing assessment, for example,
of his/her ability to shop or socialize; 2) within each location
category, information about the activities a patient was able to
perform, providing clinicians and patients with, for example, the
level of exercise tolerance, as well as compliance with the exercise prescription; and 3) at the lowest level, motion trajectory
of each activity, allowing visualization and metrics computation
for analysis and feedback. A number of innovations are required
over the current state of the art and this paper presents a novel
system shown in Fig. 2.
1) Prescription and Scenarios: During the prescription
phase (see Fig. 2), a healthcare professional would prescribe
a set of exercises to the patient, submit a monitoring request
to a service provider, and state details needed for each activity

179

(location, duration, repetitions, trajectory of activity of an arm
or leg, etc.). This information is formalized into a scenario document that contains a list of activities to be monitored, the level
of details required, and the models necessary for classification,
tracking, and metrics computation.
Once documented by the health care professional, the scenario is transferred to a server for preprocessing. First, prescriptions from multiple sources for the same patient are merged to
form a single document. Second, appropriate motion monitoring and metrics models are linked with the activities in the scenario. For example, a motion reconstruction model for walking
is added to the scenario document if reconstruction is requested
for walking. Third, each scenario contains user specific parameters, calibration requirements, and the sensors required. These
parameters are determined by the activities and models in the
scenario. Initial values are set and a kit containing the sensors
and a mobile device is sent to the patient.
2) Automated Location Category Context Detection: A key
feature of the proposed system is the ability to link activities
with the context of where the activities occurred. In practice, we
found the most reliable context feature for associating activities
to be the person’s location. We also observed that it is not the
knowledge of the physical location that is important, rather the
category. For example, knowing that a user is in McDonald’s at
123 Cook Street is not as meaningful as categorically knowing
that the person is in a restaurant. Interestingly, since there is only
a small set of location categories an average person may visit
each day, the classification of such categories can be performed
automatically based on location sensors such as GPS and point
of interest (POI) databases. This removes the need for a classifier
to recognize individual places a person visits, making the system
significantly more scalable by eliminating prohibitive cost of
classifier training.
3) System Training: Once the kit containing sensors and the
mobile device arrives, the mobile device can automatically connect to the body worn sensors and guide the user in performing
system training. There are two training phases, with only the first
requiring user interaction. In the first part, the users are asked to
perform each activity of interest for five to ten repetitions or, for
example, walk 10 m at several speeds. Disabled persons have
their individual “signature” movements in terms of, for example,
patterns of accelerations and decelerations throughout the stepping cycle. During this phase, a standard activity classifier can
be trained using any of the approaches reviewed in Section II.
This classifier may contain a large model for recognizing all the
activities prescribed and serve as the basis to bootstrap the rest
of the system.
Once the activity classifier is trained, the next training phase
requires the system to automatically associate activities with
location categories. During this phase, the automatic location
category context system starts to provide context, while the activity classifier provides activity classification. Over a few days,
the system would learn which activities are likely to happen
under each context and build multiple context-specific activity classification models from the association to capture events
where they are most likely to occur. These models can then be
trained using the training data collected from phase one. Our

180

Fig. 2.

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Overview of methodology and system architecture.

previous work described this context-guided activity classification approach [17]–[19].
4) Day-to-Day Operation: After training, the newly built
generic activity classifier, context-activity association, and the
context-specific classifiers are sent to the server to be included
into the scenario, which is now complete with the original information plus personalized parameters and context-specific classifiers. There is no more learning required for the system, and
it can now perform daily monitoring of the user (see Fig. 2).
During operation, data flow from the inertial sensors worn on
the user’s body to the mobile device carried by the user. The
inertial sensors always provide acceleration data and by default disable the more power hungry sensors such as gyroscope
and magnetometer. The automated context-detection module
utilizes location sensing augmented by auxiliary sensors to efficiently and accurately determine the user’s location category.
From there, the context-specific activity classification model associated with the current context is used to detect activities of
interest. After an activity of interest is detected, the scenario
is queried to determine if motion reconstruction is needed. For
activities requesting motion reconstruction, the motion model
dictates extra inertial sensor components to be activated and uses
the raw data stream to reconstruct a trajectory that can be visualized. If metrics are also required, then the trajectory data can
be used by a metrics computation unit to obtain measurement
directly utilizing biomechanics.

The final aggregate report can contain location category information (context), activities performed, motion trajectory, and
metrics (such as walking speed and swing/stance symmetry for
gait).
C. System Architecture
The system methodology (see Fig. 2) translates into an implementable system architecture following a client/server design
with two clients and one server. The clients are the domain expert client for prescribing the scenarios and the end-user client
for guiding the end-user through training, performing activity
classification, and automatic context detection. The server implements scenario management (receiving, merging, and sending) and components that require high computational power:
classifier training, context-specific classifier generation, motion
reconstruction, and metrics computation.
IV. SYSTEM IMPLEMENTATION
A reference system was implemented to validate all the major
innovations. This is a large end-to-end system representing the
culmination of multiyear research and development at the Wireless Health Institute at UCLA in sensor instrumentation, data
collection, location category detection, classification, motion
reconstruction, and motion metrics extraction. The following
sections focus on the major research contributions.

XU et al.: PERSONALIZED MULTILAYER DAILY LIFE PROFILING THROUGH CONTEXT ENABLED ACTIVITY CLASSIFICATION AND MOTION

181

A. Clients: Prescription, Sensor Instrumentation, and Data
Collection
The system has two clients. The first is used by healthcare
professionals (doctors, nurses, therapist, etc., caring for the subject) to construct and assign scenarios to their subjects. This is
done via a secure web interface using industrial standard HTTPS
protocol, where a healthcare professional can select the patient,
select the activities to be monitored, and for each activity, select the level of monitoring details. The second is the end-user
client responsible for communication/control of the body worn
sensors (through the use of Bluetooth and the AirInterface sensor instrumentation architecture [17]) and for guiding a user
through training.
Our particular usage scenario prescribes to the users four
low-cost nine-degree-of-freedom (9-DOF) sensors (InvenSense
MPU-9150) to be worn on the front part of both shoes and on the
elbow and wrist of the dominant arm (see Figs. 7 and 8). This
setup provides the best opportunity to capture both upper and
lower body activities at very low costs. For specific use cases
such as monitoring stroke patients, sensors can be repositioned
or increased to cover afflicted limbs. The sensors are configured
to sample at 200 Hz, the accelerometer is enabled by default with
16G sensitivity and the gyroscope can be enabled as required
and has 1000 °/s sensitivity. Data transfer is done over Bluetooth.
The user is also given a mobile device with the end-user client
installed.
B. Activity Classification
Activity classification is a critical component used in two
areas of the system: the generic classifier for learning contextactivity association and the context-specific classifiers for daily
operation after the learning is complete. In both cases, the universal hybrid decision tree (UHDT) is used [18], [21]. UHDT
classifier provides multimodal hierarchical classification based
on Naive Bayes and Support Vector Machine. Starting with
raw data from multiple sensors, UHDT combines streams of
data into a single structure. Features such as short time energy,
mean, and variance are computed from the combined data structure. There are a number of diverse features, providing freedom
in selecting the ones that best suit each application. From the
selected features, tree-like hierarchical structures can be built.
Fig. 3 shows an example structure for classifying eight different
activities using sensors worn on the waist, wrists, and ankles.
This model is more generic and demonstrates the inclusion of
more sensors than what is used in the study.
By grouping activities that share common features together
(such as stationary versus nonstationary), the hierarchical structure can model the classification problem. At each level of the
tree, appropriate features are selected for the classifier (either
Naı̈ve Bayes or SVM) to separate unknown data into one of
the branches. The final output is produced when a leaf node
is reached. This structure provides a layered approach towards
large classification problems and is less susceptible to performance degradation with large models. Another advantage of
the UHDT is that given a set of activities, the structure can
be automatically generated through brute force or other means

Fig. 3.

Example of hierarchical UHDT model.

Fig. 4.

Overview of context detection scheme.

[17], [21]. This is later exploited to automatically build contextspecific activity classifiers.
C. High-Level Location Context Detection
A location category solver is required to automatically detect
the location context the user is currently in. In this system, we
present an automated context detection algorithm using GPS
by exploiting the already extensive geo-coding and mapping
databases. GPS for location detection has a number of significant
disadvantages: heavy battery drain and poor signal indoors. The
algorithm presented in this paper solves these two challenges by
augmenting GPS with WiFi and user movement information to
reduce activation and eliminate the need for localization once a
user moves indoors or becomes stationary (see Fig. 4).
1) WiFi Augmented GPS: The algorithm proposed first detects if the user is moving based on wireless information and
activates the GPS accordingly. Nearby wifi access points (AP)
are scanned every 5 s (tunable) and each AP’s signal strength
is recorded. Each scan produces a scan set and the operation is
power efficient since many devices already maintain this information for internal use. Every five scan sets are grouped into a
decision window (sliding window with 50% overlap) and preprocessing removes ad-hoc APs (by checking the U/L bit of the
MAC address), APs appearing in less than 80% of scan sets in the
window and any AP with low receiving power (RSSI < 65 dB).
The first step removes local peer-to-peer networks that are inherently unstable and the next two produce a stable set of scan
results that are resistant to small changes due to environmental
conditions such as a person walking by. Within each window, a

182

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Fig. 6.
Fig. 5.

Generation of search overlay and placement on map.

FSM for detecting context and activating GPS/WIFI.

movement score is computed by
|A ∩ B|
|A ∪ B|
Jδ (A, B) = 1 − J(A, B)
S = Jδ (A, B) · |B|
J (A, B) =

(1)

where A and B are two neighboring decision windows, Jδ is
the Jaccard similarity function, and S is the score.
2) Movement Augmented GPS: A user’s movement behavior
is modeled by a simple finite-state machine (FSM), as shown
in Fig. 5. The FSM starts in the Stationary state, where WiFi
scan sets are taken to compute S. If S is below a threshold
(T h = 200) and is nonempty, then the FSM remains in the
Stationary state; otherwise, the FSM transits into the Moving
state.
During each entry into Moving state, GPS is activated with a
rate of
60
.
(2)
R=
(30 ∗ Jδ (A, B))
Once GPS is active, it stays active for a minimum of 1 min
at the determined polling rate R to detect if the user is actually
moving. During this time, WiFi scan is disabled. At the end of the
active GPS period, the possible outcomes are: if all coordinates
within the 1-min GPS polling fall within the distance root meant
square (DRMS) radius of the first coordinate [22] or if there is
no good GPS fix, then the person is declared not moving and
the FSM transits to the Stationary state and GPS is disabled.
Otherwise, the person is declared moving and the movement
speed is computed from GPS data and the FSM remains in the
Moving state.
3) Movement Direction Estimation and Context Decision:
During each transition into the Stationary state, the location
context of the current position is determined. This is done by
estimating a trajectory from the last (n = 5) known good GPS
coordinates (p1 to pn ), where a good coordinate is one with
DRMS error radius r ≤ 16 m, a common accuracy measure
meaning that using the coordinate as the center, a circle with
radius r has 68% probability to contain the exact location [22].
→
p−
The estimation algorithm first draws n − 1 vectors v n = −
1 pn ,
−
−
−
−
−
→
−
−
→
v n −1 = p1 , pn −1 to v 2 = p1 p2 . Each vector is assigned a weight
1
and each is then extended in the direction
w = 1, 12 . . . n −1
of movement by r. The region bounded by v n . . . v 2 is chosen
as the search overlay and each point on the overlay is assigned
a score s = wa + wb , where wa and wb are the weights of the
two closest vectors. Fig. 6 demonstrates the overlay formation.

To use the overlay, Google Places API is queried for POI information omnidirectionally using p0 as the origin with radius
|v5 |. The overlay is then placed on the search results, and each
POI on the returned result receives a score s and the result with
the highest score is selected. Compared to the omnidirectional
search results, searching in the direction of the movement reduces false identifications but is highly affected by the accuracy
of the GPS. Instead of choosing an arbitrary search pattern, we
make use of all available information by including the uncertainties of coordinates p1 to pn : the more aligned all of the
vectors are, the more confident we are of the true direction of
movement and thus the smaller the search overlay. Furthermore,
the use of open APIs such as Google Places allows us to exploit
vast crowd sourced and accurate knowledge at no cost.
D. Context Specific Activity Classifier
By continuously monitoring a person under different contexts, an association table can be built over time to track which
activities are performed during each context. Then, contextspecific classifier models are constructed to recognize the frequent activities within each context. The UHDT classifier model
construction is automatic through brute force testing of different structure and feature combinations with the original training
data. Once the training is complete, these context-specific classifiers are invoked during daily use, where the mobile device
is constantly monitoring context changes and also receiving
accelerometer data from the body worn sensors. Whenever a
new context is detected, the corresponding context-specific activity classifier model is selected from the scenario document.
The UHDT component loads the tree structure, and real-time
classification is performed [17], [18].
E. Body Motion Reconstruction
Most generic activities of interest such as walking, reaching,
and stair climbing can be effectively reconstructed by tracking
the trajectory of arms and feet. In the system presented here, the
body motion is reconstructed by the kit of four sensors described
in Section IV-A. Different models for upper and lower body are
developed using the relevant biomechanical principles.
1) Lower Body Motion Tracking: To reconstruct lower body
activities, accelerometer and gyroscope measurements of the
9-DOF sensor mounted on the tip of both shoes are used.
The sensor measurements are represented by quaternion q in
the sensor’s frame of reference when it is powered ON. Based
on the orientation, gravity subtraction is carried out to extract
the pure motion acceleration (see Fig. 7).

XU et al.: PERSONALIZED MULTILAYER DAILY LIFE PROFILING THROUGH CONTEXT ENABLED ACTIVITY CLASSIFICATION AND MOTION

Fig. 7. Overview of lower body motion tracking model. 1) Unaligned visualization due to different sensor frames of reference. 2) Visualization in the
visualization frame.

Ideally, by integrating the motion acceleration using strapdown inertial navigation techniques [23]–[25], the foot velocity
can be obtained. However, since the sensor measurement is
usually very noisy, a ZUPT method is required to improve the
velocity estimation by resetting the velocity to a known value
(usually zero) at known reference points of motion (such as
during foot stance) [23]. Most research have used simple methods such as acceleration magnitude detector [24], where the
mean acceleration energy within a sliding window is used to
determine when a foot lands on the ground. While workable on
normal subjects, this method fails for subjects who are hemiparetic or exhibit other gait deviations. This section introduces
a novel method for robustly detecting ZUPT windows even on
abnormal gait and for automatically reconciling the different
reference frames of the two sensors (vital for visualization).
To perform ZUPT, a standard thresholding method is applied
to the healthier side to determine the zero velocity windows
(window of no motion) and the motion windows (signals between two neighboring zero velocity windows). Second, the
motion windows of the healthier side are mapped to the injured
side as zero velocity windows. This mapping is possible due to
gait biomechanics indicating that when one foot is in swing, the
other is in stance [25]. Third, adjustments of the windows are
performed through window merging to reduce false positives
and splitting to reduce false negatives: the average length of the
motion windows is calculated and if the length of a particular
window is smaller than half of the average, it will be removed
by merging with two neighboring windows; the average length
of the zero velocity windows is also calculated and any window
that is twice the average is recomputed with tighter threshold
until it can be broken up.
Successful implementation of the above procedure requires
prior knowledge of the impaired side. To automate this discovery, we compute
m = max(|psd(ay )|)

(3)

where ay is the y-axis acceleration after gravity subtraction.
Energy-based measurements better capture the additional force
the unimpaired side has to carry to compensate for the weak

183

limb. Frequency-domain features characterize energy in each
periodic stride well. Equation (3) is computed for both left and
right legs (mleft , mright ) and the ratio r = mmrliegfhtt can be used
to infer whether the data are from hemiparetic gait (normal gait
would result in a ratio close to 1) and if so which side is the
weak leg (with lower m). Worth noting is that time domain
features could also be used, albeit careful windowing is needed
to capture all phases of a stride (difficult in abnormal gait).
Once the velocity has been updated by ZUPT, it is integrated
to estimate the foot displacement. To visualize this displacement
as position and movement, it is essential to project the motion
of both feet onto the same frame of reference. In theory this can
be achieved by fixing both sensor’s coordinate frame through
perfectly aligning them when worn. However, it is infeasible to
instruct the users to do so and an automatic reference frame is
needed. This paper introduces an automated way of obtaining a
global coordinate frame of reference (visualization frame) using
only sensor data.
During periods when the feet are stationary, the algorithm
generates a per sensor initialization frame. First, the mean of
the acceleration signal a0 and the mean of the quaternion q 0
are computed for each sensor. a0 is a measurement of gravity in
the initialization frame and the quaternion division qq produces
0
q 0 , which is the sensor orientation represented in the initialization frame for any q. As new data arrive, standard strap-down
inertial navigation techniques produce foot position p0i in the
initialization frame for data index i given a and q [23]. Periods of walking are prefixed and suffixed by a zero velocity
window. The walking direction v 0d can be computed by subtracting the starting foot position p0start from ending position
p0end . Using these parameters, a global visualization frame can
be constructed by each sensor using its own data:
z 0 = a0 / a0 


v0
x0 = − d0
×z
v d 
y 0 = (z × x)/ z × x .

(4)
(5)
(6)

The visualization frame essentially uses the direction of gravity as z-axis, the cross product of z and the walking direction vd0
as the x-axis and y-axis completes the frame. Fig. 7 depicts the
entire process.
From reconstructing the motion, several gait-related clinical
parameters can be computed: walking speed WS, stride length
SL(k), swing time SwingT (k), and stance time StanceT (k):

 0
p − p0 
in
i0
(7)
WS =
(in − i0 ) · R


SL (k) = p0i k − p0i k −1  , k = 1, 2, . . . , N
(8)
SwingT (k) = (iHS (k) − iTO (k)) · R,
k = 1, 2, . . . , N

(9)

StanceT (k) = (iTO (k) − iHS (k − 1)) · R,
k = 1, 2, . . . , N

(10)

184

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Fig. 8. Overview of upper body motion tracking model. 1) Unaligned visualization due to different sensor frames of reference. 2) Visualization in the
visualization frame after calibration.
Fig. 9. Example report showing a person’s movement, visited contexts, activities detected within each context and motion playback of a running event.

where ik is the data index of the kth stance, defined as the center
of a zero velocity window for each foot. iHS and iTO are the data
index of heel strike and toe off, defined as the beginning and end
of a zero velocity window. R is the sampling rate (200 Hz in
our sensor configuration). From a clinical point of view, walking
speed reflects an overall measure of walking skill, while leg
symmetry such as time spent in stance versus swing for each
leg reflects the level of motor control. These metrics are thought
after in most clinical trials and clinical care, where activity is an
important outcome, especially for disabled persons with lower
extremity weakness. These metrics were designed in a highly
iterative fashion between engineers, physicians, and volunteer
patients with various neurological diseases and severity of gait
impairments.
2) Upper Body Motion Tracking: Upper body motion tracking can be performed using data from the elbow and wrist sensors with a different tracking model. The complimentary filter is
again used to produce sensor orientation represented by quaternion q. Using biomechanical properties of the arm, we model
it as two rigid links each rotating around its preceding joint
and can then apply the well-known double pendulum model to
produce the arm’s motion trajectory. Whereas in the lower body
case elaborate ZUPT algorithms are required to correct double
integration errors from the noisy accelerometer, reliable motion
trajectory of the arm can be obtained directly through q due to
significantly less noisy gyroscopes.
The algorithm is shown in Fig. 8, where the quaternion representing the arm orientation is expressed in both sensors’ own
frames of reference when they are powered on. If visualized,
the swing of the limb would be scrambled [see 1) in Fig. 8]
instead of following two concentric arcs [see 2) in Fig. 8] and
the length of the arm would be incorrect. To correctly visualize,
it is necessary to have a global frame of reference to project
the individual sensor’s data and also find the arm length. Compared to the lower body case, finding a global reference here
is more difficult due to the 2 DOFs of the arm. As a result, our
method requires a single calibration motion by the end-user. The
motion requires the end-user to keep his/her arm straight down
for a few seconds, then swing up to the side along the coronal
plane and return to the starting posture. During this calibration,
the trajectories of the elbow and wrist can be approximated by

Fig. 10.

Location search overlays and detected context categories.

Fig. 11.

Active time and battery cost of WIFI augmented GPS.

Fig. 12.

Motion reconstruction results.

two concentric arcs in the coronal plane whose radii are equal
to the length of the upper arm and the whole arm respectively,
since the sensors are worn near the joints. The calibration is
firstly used to estimate the upper and lower arm length. During
the swing, the two sensors are going through a circular motion
whose instantaneous linear velocity v and rotational velocity ω

XU et al.: PERSONALIZED MULTILAYER DAILY LIFE PROFILING THROUGH CONTEXT ENABLED ACTIVITY CLASSIFICATION AND MOTION

185

TABLE I
CONTEXT DETECTION RESULTS
Category
General Store
Restaurant
Gym
Department Store
Residence

Physical Locations Picked by Users

Counts

Correctly Detected

Missed Detection%

Black Market, Smart & Final, Country Market
The Counter Burger, KFC, McDonald, Volcano Tea
LA Fitness, 24 Hour Fitness, Equinox, YMCA
Blooming Dale, Nordstrom, Target
Three different homes

5
8
7
5
3

80%
75%
100%
100%
33%

0%
13%
0%
0%
67%

TABLE III
COMPUTED GAIT PARAMETERS (H = HEALTH SIDE, I = AFFLICTED SIDE)
Side

SL (m)

SwingT (s)

StanceT (s)

H
H
I
H
I
H
I
H
I
H
I
H

1.36 ± 0.06
1.38 ± 0.06
0.82 ± 0.11
0.85 ± 0.06
0.79 ± 0.09
0.79 ± 0.05
0.71 ± 0.37
0.72 ± 0.10
0.68 ± 0.12
0.75 ± 0.09
0.64 ± 0.08
0.69 ± 0.06

0.37 ± 0.02
0.35 ± 0.01
1.34 ± 0.14
0.24 ± 0.03
1.52 ± 0.15
0.27 ± 0.03
0.98 ± 0.19
0.26 ± 0.06
2.13 ± 0.56
0.26 ± 0.04
1.28 ± 0.17
0.24 ± 0.03

0.76 ± 0.09
0.74 ± 0.03
1.22 ± 0.33
2.24 ± 0.20
1.15 ± 0.18
2.39 ± 0.20
1.30 ± 0.29
2.02 ± 0.22
1.23 ± 0.44
3.03 ± 0.28
1.21 ± 0.35
2.23 ± 0.13

C
H1
H2
H3
H4

Fig. 13. Three-dimensional visualization of strides from different gait. Center
red line indicates the mean, green line indicates mean ± 1 standard deviation.
(a) Control. (b) Hemiparetic 1. (c) Hemiparetic 2. (d) Hemiparetic 3. (e) Hemiparetic 4. (f) Hemiparetic 5.

TABLE II
WALKING SPEED OF DIFFERENT GAITS (C = CONTROL, HI = HEMIPARETIC)

S (m/s)

C

H1

H2

H3

H4

H5

1.06

0.30

0.28

0.28

0.21

0.25

can be characterized as
⎡
0
v = ⎣ ωz
−ωy

−ωz
0
ωx

⎤
ωy
−ωx ⎦ × r
0

r

TABLE IV
ARM LENGTH ESTIMATION (FOR SUBJECTS S1–S6)

Upper arm (m)
Whole arm (m)
Upper err. (%)
Whole err. (%)

(11)

where r is the vector representing either the upper or whole arm
in the sensor’s local reference frame. Due to low drift of the
gyroscope, its measurements can directly substitute the matrix
elements in (11). The linear velocity v can be estimated using
the accelerometer measurements and the orientation quaternion
q. The ZUPT algorithm can be used to eliminate velocity drift
by using the stationary phases before and after the arm swing.
Using (11), r for both the upper arm and the whole arm can be
obtained through MMSE:
min ω × r − v .

H5

(12)

The calibration motion is also used to find a global reference
frame where: the arm straight down position is defined as the
z-axis; the x-axis is defined to be perpendicular to the coronal
plane, easily estimated by finding the vector orthogonal to the
swinging arc; and the remaining y-axis completes the frame
using allocentric reference.

S1

S2

S3

S4

S5

S6

0.244
0.450
5.48
7.67

0.272
0.266
7.36
2.11

0.232
0.481
9.94
0.65

0.308
0.592
3.87
6.84

0.289
0.525
1.07
0.49

0.265
0.532
0.86
8.07

Once the visualization frame is found, a rotation matrix can
be calculated to project the arm represented by vector r to the
reference frame. Note that the upper body vector needs to be
subtracted from the whole arm vector to get the r representing
the lower arm. After the calibration, the arm rotations described
by new quaternion q i (ith data point in sensor frame) can be
projected onto the global reference frame and used to animate
the arm:
ri = q × r0 × q∗

(13)

where r i is the arm orientation of the ith data point and r 0
is the initial arm vector (pointing straight down) in the global
reference frame.
F. Data Aggregation and Presentation
The system collects a user’s movement events, location contexts, activity classification results, motion reconstructions, and
metrics. All of the data are aggregated to the server daily, where
the client transmits a compressed file containing today’s GPS
traces, context events, activity classification results and raw
sensor data for activities requiring motion reconstruction and

186

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

Fig. 14. Motion trajectories of the arm performing calibration motions. (a)
Sensor-based motion trajectory. (b) Kinect captured motion trajectory.

TABLE V
SCENARIOS
Context
General Store
Restaurant
Gym
Department Store
Residence

time so it is plugged in while transiting between destinations.
Most categories, except home due to the lack of residential POI
from the APIs used, were detected with a high success rate.
This weakness could potentially be removed by allowing users
to input their home coordinates.
Fig. 10 shows two particular instances of the detection overlay
and results. We can see the search vectors and the resulting
overlay drawn on the map. The correct location is within the
search overlays.
Fig. 11 shows the average amount of active minutes and the
average amount of energy (percentage of battery) used per hour
by the augmented GPS location context solver compared to
standard GPS usage. A large improvement in battery life can be
observed, allowing around 6 h of continued use for the device.

Actions
Walking, Reaching, Standing
Sitting, Eating, Walking
Walking, Running, Cycling, Standing
Stair climbing, Reaching, Walking, Standing
Sitting, Eating, Walking, Stair climbing

metrics extraction. The server then performs the motion reconstruction and metrics extraction. An interactive report of all the
data is finally sent to the physician for analysis.
To present the data, first the GPS traces are drawn onto a map
using Google Places API and any location contexts detected
during the day are marked. An information window can be
shown by clicking on the markers. The window provides a list
of all the activities that were detected in that context and the
level of details available for each activity (matching that selected by the healthcare professional). Each of the details can
be clicked to show the content such as motion reconstructions
video rendered by the server or graphed metrics data (see Fig. 9).
The report is self-contained into a single HTML archive with
user interactions and interfaces developed using standard web
development techniques (HTML, CSS, and JavaScript).
V. EVALUATIONS
The system and its individual components were evaluated. Context detection, the generic and context-specific activity classifiers were tested using in-community data. Motion
reconstruction and metrics extraction are difficult to evaluate
against ground-truth in-community and were evaluated against
lab equipment, with activities matching that observed in real
life.
A. Context Detection
Four subjects were each given a kit containing the four 9DOF sensors and a Nexus 7 tablet. The sensors were worn on
the upper and lower arm of the dominant arm near the elbow and
wrist joints and on the top front side of both shoes. Each subject
was asked to record around 7 h of data and to visit any place
matching the categories listed in Table I (actual physical location
is not restricted). The operating life for the tablet using WiFi
augmented GPS is around 6 h and is the limiting factor for usage

B. Generic and Context-Specific Activity Classification
The UHDT classifiers were used as both the generic and
context-specific activity classifiers. The results from this study
are in agreement with the previous results [17]–[19], which confirms further the benefits of context specific activity classifiers
in terms of classification accuracy and speed improvements reported in our early studies.
C. Lower Body Motion Tracking
To validate the motion tracking algorithm, three healthy subjects were recruited and each performed two sets of 40-m level
walking, ten-step stair ascending, and ten-step stair descending (Fig. 12). The absolute error of the distance estimation of
the total distance travelled by both the left and right sides is
(3.08 ± 1.77)%. Foot position and orientation waveforms of
individual steps are verified by comparing the sensor reconstruction with Vicon captured data of level walking from individuals.
The Vicon system is very accurate with standard error of 0.02 cm
for step length and 0.06 m/s for gait velocity. The results show
that our method is able to accurately reconstruct a variety of
lower body motions.
To test with hemiparetic gait, five common hemiparetic gaits
were collected from patients with stroke at the UCLA Neurological Rehabilitation & Research Unit during a 10-m walk (a
standard clinical test). The absolute error of distance estimation from the healthy side is (1.01 ± 0.72)%, while that from
the hemiparetic side is (3.55 ± 3.60)%. This demonstrates that
our system is able to correctly compute lower body gait related
metrics from the reconstructed motion trajectory.
To verify the effectiveness of the evaluation metrics, a set
of normal 10-m walks was also collected as the control group.
Fig. 13 illustrates the 3-D visualization of the different gaits. The
red line indicates the mean where strides belonging to the same
group are stretched in time to the same length and the green lines
indicates mean ±1 standard deviation. The stretching allows for
more uniform presentation and aids in the future analysis of
different gait patterns as the gait phases can now be compared. Table II summarizes the average walking speed calculated
from the left and right foot sensors, and Table III lists the rest
of the gait quality related parameters. From these parameters,

XU et al.: PERSONALIZED MULTILAYER DAILY LIFE PROFILING THROUGH CONTEXT ENABLED ACTIVITY CLASSIFICATION AND MOTION

Fig. 15.

187

Interactive report showing details about each context (A), about each activity detected within context (B) and motion reconstructions (C & D).

the difference between normal gait and hemiparetic gait is easily
observable and the degree of hemiparesis can be inferred.
D. Upper Body Motion Tracking
To verify upper body motion reconstruction, three female
and three male subjects with different heights were asked to
perform a range of arm motions starting with the calibration
motion. A Kinect system was set up to capture the skeleton
movements and record the shoulder, elbow, and wrist positions
in the individual frames. Based on the rigid link assumption, the
upper arm and whole arm length were estimated as the distance
from the shoulder to elbow and from the shoulder to wrist.
Table IV presents the estimation accuracy of the calibration
algorithm compared to the Kinect captured ground truth (the
Kinect system can report positions to within 2–5 cm of true
value). Overall, the average error is 4.53%. In addition, the arm
motion reconstructed from the inertial sensors was compared
with trajectory captured by the Kinect (see Fig. 14).
E. Data Aggregation and System Verification
To verify the system as a whole, the generated report from
the data aggregation unit was viewed and compared against
user reports. During context data collection phase (see Table I),
motion data were also captured from the subjects and groundtruth were reported in the form of written reports. The users
were instructed to at least perform the actions listed in Table V
within each location context.
The generated report for each user was found to be complete
with no omissions. Fig. 15 depicts some of the results. In this
example report, the user’s entry and exit events are marked with
red and yellow markers, respectively. An information screen
(Dialog A) is displayed for a particular context when a marker is
clicked and displays all the detected activities within the context.
Each of these activities is clickable for additional information

(Dialog B). For activities that have motion reconstruction, a
reconstruction video can also be viewed from the link in Dialog
B. The example figure shows two motion playbacks (Dialog
C and D). Using this report, healthcare professionals can gain
insights into a subject’s daily behavior that no previous system
has been able to deliver.
VI. CONCLUSION
Profiling the daily activity of a patient in-community is becoming essential to assess and enhance aspects of healthcare for
persons with chronic diseases and physical disabilities. Building on our previous work focused on context-driven activity
monitoring, this paper introduced a novel methodology and system design to provide comprehensive multitiered daily activity
profiling by integrating automatic context detection and awareness context-driven activity classification, motion reconstruction, data aggregation, and reporting.
On the methodology level, we first presented a novel activity
prescription model that allows physicians to prescribe skill practice and exercise scenarios to patients and then monitor these
through a sensor and mobile phone kit. Based on the scenarios,
we further introduced the concept of context-guided activity
classification that uses high-level location context to achieve
enhanced classification accuracy and throughput. Within each
classified activity, we described motion specific tracking models and algorithms that can deliver fine grained data: motion
tracking, visualization, and metrics of individual movements.
Finally, we described a system architecture that synergizes the
various components, models, and algorithms to provide a single comprehensive report to caregivers. We implemented the
system in pilot studies and featured automatic identification
of context through energy efficient, WIFI augmented GPS and
motion reconstruction, visualization, and metrics computation
algorithms that can be deployed in the community to potentially
enhance clinical care and outcomes. In the end, we used a series

188

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 20, NO. 1, JANUARY 2016

of experimental field evaluations to validate the system and its
methodology.
While the evaluations provided in this paper demonstrated
the potential value of the methodology and end-to-end system,
a larger study needs to be performed in the future to ascertain
the robustness of the system and to compare the results of subsystems such as motion reconstruction against gold standards in
laboratory environment. More collaboration with clinical partners will be needed to develop meaningful upper body metrics.
Clinical trials will also be needed to assess the efficacy of providing the robust data to patients and clinicians.
REFERENCES
[1] American Heart Association, “Heart disease and stroke statistics—2010
update,” Circulation, vol. 121, pp. e46–e215, 2010.
[2] B. Dobkin and A. Dorsch, “The promise of mHealth: Daily activity monitoring and outcome assessments by wearable sensors,” Neurorehabil.
Neural Repair, vol. 25, pp. 788–98, 2011.
[3] U. Varshney, “Pervasive healthcare and wireless health monitoring,” J.
Mobile Netw. Appl., vol. 12, pp. 113–127, 2007.
[4] B. Najafi, K. Aminian, A. Paraschiv-Ionescu, F. Loew, C. J. Bula, and
P. Robert, “Ambulatory system for human motion analysis using a kinematic sensor: Monitoring of daily physical activity in the elderly,” IEEE
Trans. Biomed. Eng., vol. 50, no. 6, pp. 711–723, Jun. 2003.
[5] M. Lingfei, L. Shaopeng, R. X. Gao, D. John, J. W. Staudenmayer, and
P.S. Freedson, “Wireless design of a multisensor system for physical activity monitoring,” IEEE Trans. Biomed. Eng., vol. 59, no. 11, pp. 3230–3237,
Nov. 2012.
[6] O. Banos, M. Damas, H. Pomares, and I. Rojas, “Recognition of human
physical activity based on a novel hierarchical weighted classification
scheme,” in Proc. Int. Joint Conf. Neural Netw., San Jose, CA, USA,
2011, pp. 2205–2209.
[7] Y. Lee and S. Cho, “Activity recognition with android phone using
mixture-of-experts co-trained with labeled and unlabeled data,” Neurocomputing, vol. 126, pp. 106–115, 2014.
[8] T. V. Duong, H. H. Bui, D. Q. Phung, and S. Venkatesh, “Activity recognition and abnormality detection with the switching hidden semi-Markov
model,” in Proc. IEEE Conf. Comput. Vision Pattern Recog., San Diego,
CA, USA, 2005, pp. 838–845.
[9] A. K. Dorsch, S. Thomas, X. Xu, W. J. Kaiser, and B. H. Dobkin, “SIRRACT: An international randomized clinical trial of activity feedback
during inpatient stroke rehabilitation enabled by wireless sensing,” Neurorehabil. Neural Repair, Sep. 26, 2014.
[10] A. Jimenez, F. Seco, C. Prieto, and J. Guevara, “A comparison of
pedestrian dead-reckoning algorithms using a low-cost MEMS IMU,”
in Proc. IEEE Int. Symp. Intell. Signal Process., Budapest, Hungry, 2009,
pp. 37–42.

[11] F. Cavallo, A. M. Sabatini, and V. Genovese, “A step toward GPS/INS
personal navigation systems: Real-time assessment of gait by foot inertial sensing,” in Proc. Intell. Robots Syst., Edmonton, Canada, 2005,
pp. 1187–1191.
[12] L. Tao, I. Yoshio, and S. Kyoko, “Development of a wearable sensor
system for quantitative gait analysis,” Meas. J., vol. 42, pp. 978–988,
2009.
[13] D. Roetenberg, P. J. Slycke, and P. H. Veltink, “Ambulatory position and
orientation tracking fusing magnetic and inertial sensing,” IEEE Trans.
Biomed. Eng., vol. 54, no. 5, pp. 883–890, May 2007.
[14] K. Van Laerhoven, A. Schmidt, and H. W. Gellersen, “Multi-sensor context
aware clothing,” in Proc. Int. Symp. Wearable Comput., Sardinia, Italy,
2002, pp. 49–56.
[15] F. Buttussi and L. Chittaro, “MOPET: A context-aware and user-adaptive
wearable system for fitness training,” J. Artif. Intell. Med., vol. 42,
pp. 153–163, 2007.
[16] H. Pung, T. Gu, W. Xue, P. Palmes, J. Zhu, W. Ng, and C. Tang, “Contextaware middleware for pervasive elderly homecare,” IEEE J. Select. Areas
Commun., vol. 27, no. 4, pp. 510–524, May 2009.
[17] J. Y. Xu, H. Chang, C. Chieng, W. J. Kaiser, and G. J. Pottie, “Contextdriven, prescription-based personal activity classification: Methodology,
architecture, and end-to-end implementation,” IEEE J. Biomed. Health
Informat., vol. 18, no. 3, pp. 1015–1025, May 2014.
[18] H. Chang, C. Chien, J. Y. Xu, and G. J. Pottie, “Context-guided universal
hybrid decision tree for activity classification,” in Proc. IEEE Conf. Body
Sensor Netw., Cambridge, MA, USA, 2013, pp. 1–6.
[19] J. Y. Xu, Y. Sun, Z. Wang, W. J. Kaiser, and G. J. Pottie, “Context guided
and personalized activity classification system,” presented at the Proc.
Wireless Health, San Diego, CA, USA, 2011.
[20] M. F. Gordon, “Physical activity and exercise recommendations for stroke
survivors,” Stroke, vol. 109, pp. 1230–1240, 2004.
[21] C. Chien and G. J. Pottie, “A universal hybrid decision tree classifier
design for human activity classification,” in Proc. Conf. Eng. Med. Biol.
Soc., Osaka, Japan, 2012, pp. 1065–1068.
[22] G. Y. Chin, “2 DRMS Error Measures and HDOP,” in Two-Dimensional
Measures of Accuracy in Navigational Systems, Cambridge, MA, USA:
U.S. Dept. Transp., 1987.
[23] D. Titterton and J. Weston, “Basic principles of strapdown navigation
systems,” in Strapdown Inertial Navigation, 2nd ed. Cornwell, U.K.: IET
& MPG Books, 2005.
[24] I. Tien, S. D. Glaser, R. Bajcsy, D. S. Goodin, and M. J. Aminoff, “Results
of using a wireless inertial measuring system to quantify gait motions in
control subjects,” IEEE Trans. Inform. Technol. Biomed., vol. 14, no. 4,
pp. 904–915, Jul. 2010.
[25] H. M. Schepers, H. F. J. M. Koopman, and P. H. Veltink, “Ambulatory
assessment of ankle and foot dynamics,” IEEE Trans. Biomed. Eng., vol.
54, no. 5, pp. 895–902, May 2007.

Authors’ photographs and biographies not available at the time of publication.

