IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

995

EEG Compression of Scalp Recordings Based
on Dipole Fitting
Hoda Daou, Student Member, IEEE, and Fabrice Labeau, Senior Member, IEEE

Abstract—A novel technique for electroencephalogram (EEG)
compression is proposed in this paper. This technique models the
intrinsic dependence inherent between the different EEG channels. It is based on methods borrowed from dipole fitting that is
usually used in order to find a solution to the classic problems
in EEG analysis: inverse and forward problems. To compress the
EEG signals, the forward model based on approximated source
dipoles is first used to provide an approximation of the recorded
signals. Then, (based on a smoothness factor) appropriate coding
techniques are suggested to compress the residuals of the fitting
process. Results show that this technique works well for different
recordings and for different patients, and is even able to provide
near-lossless compression for certain types of recordings.
Index Terms—Compression, discrete cosine transform (DCT),
dipole fitting, electroencephalogram (EEG), forward problem, inverse problem, lead field, set partitioning in hierarchical trees
(SPIHT), wavelet transform (WT).

I. INTRODUCTION
LECTROENCEPHALOGRAPHY is the monitoring or
recording of electrical activity in the brain. Electroencephalogram (EEG) recording can result in huge amounts of
data to be stored and/or transmitted, which calls for efficient
compression techniques [1], [2].
EEGs are used to visualize and analyze the brain activity.
While reading EEG signals, redundancy is highly visible in a
single channel between different time segments, and between
different channels. This correlation should be exploited when
building a compression algorithm. In the following paragraphs,
different methods that have been recently suggested to compress
EEG signals are presented.
Predictors, such as linear predictors [3], and neural networks
predictors both using lossless [4], [5], and near-lossless [1] algorithms, have been suggested to compress EEG signals. In
addition, a combination of both neural network and linear predictors is also used to achieve lossless compression of EEG
signals [6], [7]. In these models, a predictor is used in the first
stage to decorrelate the source data by removing the correlation
between neighboring samples.

E

Manuscript received November 5, 2013; revised July 26, 2014; accepted
July 31, 2014. Date of publication August 8, 2014; date of current version
May 7, 2015. This work was supported by the Natural Sciences and Engineering Research Council and industrial and government partners, through the
Healthcare Support through Information Technology Enhancements Strategic
Research Network.
The authors are with the Electrical and Computer Engineering, McGill University, Montreal, QC H3Z1J9 Canada (e-mail: hoda.daou@mail.mcgill.ca;
fabrice.labeau@mcgill.ca).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2346493

Iterative function sets and genetic algorithms are also used to
compress EEG signals [8]. This compression technique involves
the use of sets of linear transformations to provide an approximation of the signal. Using genetic algorithms, self-similarities
in the EEG signals are explored to identify the proper sets of
transformations. Gain/shape vector quantization is also used to
approximate the EEG signals [9]. This technique uses classified
signature and envelope vector sets and assumes both transmitter
and receiver share the same sets.
Many compression algorithms use wavelet transform (WT)
to decompose a signal and take advantage of the properties of
these coefficients in energy compaction. WT provides multiresolution, locality, and compression when combined with zero-tree
coding techniques. It was shown that choosing an appropriate
mother wavelet gives better performance results than when using an arbitrary wavelet [10], [11].
Another type of WT, where the signal is passed through more
filters, wavelet packet transform is used by Cardenas-Barrera
et al. to segment and decompose the EEG signals [12]. The compression algorithm is mainly composed of thresholding of the
low-relevance coefficients, then applying quantization and runlength coding (RLC). However, calculating the proper threshold
is the main issue in this model.
WT can be combined with efficient coding techniques, such
as embedded zero-tree wavelet [13], that take advantages of this
transform’s characteristics. Set partitioning in hierarchical trees
(SPIHT) has also been used, both in 1-D [14], [15] and 2-D
[16], [17], to compress scalp EEG recordings. In fact 2-D-based
methods explore both time and spatial dimensions when coding
the signals, and thus, are able to capture more redundancy and
similarities.
Three-dimensional transforms are also used to further decorrelate the signals and provide efficient compression [2]. To
perform compression, the authors consider wavelet-based volumetric coding, energy-based lossless coding, and tensor
decomposition-based coding. Afterwards, the residual signals
are coded either in a lossy [2] or lossless [18] fashion, then
transmitted.
Overall, these schemes are based on applying different transforms and coding schemes in order to extract the inherent redundancy present in both the spatial, i.e., between different channels, and the temporal, i.e., between different time samples,
domains.
The work presented in this paper targets the compression
of scalp recordings of EEG signals for the purpose of reduction of storage space and bandwidth used to store and/or send
these signals. We do not explore a specific clinical application
of the targeted signals; however, compression could be of great

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

996

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

importance in ambulatory EEG systems. In this paper, we suggest exploring the inverse problem of EEG recordings and applying it in the context of compression. In other terms, we propose
modeling the recorded signals using the source dipole that generates these waveforms on the scalp of the head (see Section II
for details). Therefore, by using a physiologically meaningful
model, we hope to get a better space for EEG signal representation in order to achieve better approximation for a low bit rate.
Our experiments show the superior performance of this method
compared to methods based on classic transforms.
In our modeling technique, we use approximations of the actual physical reality in order to build the compression system.
Such approximations are commonly used in the compression
of various types of signals. For example, as part of the compression system, block transforms [19], motion compensation
[20], and linear modeling of the vocal tract [21] are commonly
used in coding images, videos, and speech signals, respectively.
Different assumptions are taken in order to provide rough approximations of these different types of signals and still yield
very efficient compression algorithms.
In this paper, a method to localize the sources behind the
recorded electrical activity on the scalp is explored. First, the
forward/inverse problems are briefly introduced. Then, a technique used to localize the source dipoles and build the forward
model that maps these dipoles to the observed scalp recordings is described in details. At the end, compression methods of
the EEG residuals and the dipoles’ moments are suggested and
results are shown for three different databases of recordings,
having different characteristics.
II. EEG INVERSE AND FORWARD PROBLEMS
The noninvasive localization of the neuronal generators that
are behind the observed EEG signals, which is known as inverse
solution, is one of the main concerns in electrophysiology [22].
Relying on the pattern of EEG recorded at the scalp’s surface to
determine the generators is a big challenge and of great interest
[23, p. 73]. Finding a solution of the inverse problem is able
to give us a model that maps the generators to the recorded
projections.
Thus, the inverse problem aims at finding the true functional
tomography by localizing the sources with minimum error.
However, the main challenge that such a problem faces is that
the measurements do not contain enough information about the
generators, which makes the problem ill-posed, and thus, a perfect tomography cannot exist. The reason behind such challenge
is that different internal source configurations can generate the
same external electromagnetic fields and these fields are measured only at a relatively small number of locations [22].
The cerebral cortex, which is the outer surface of the brain, is
comprised of 10 billion neurons. Most observed scalp activity
is generated within this part of the brain which is 1.5–4.5-mm
thick. A synchronous synaptic simulation of a very large number
of neurons results in a dipolar current source oriented orthogonal to the cortical surface [24]. The measured EEG is actually
the propagation of this current onto the different electrodes’
locations.

In order to compress EEG recordings, one aims at estimating
the scalp recordings while minimizing a certain chosen distortion criterion. Having solved the inverse problem, one can use
such model to generate, from the calculated dipoles, an approximation of the EEG recordings. This is known as the forward
problem.
Therefore, we first analyze the EEG signal by using the
inverse problem; then, an approximation is generated using
the forward model. This technique is somewhat close to the
analysis-by-synthesis techniques.
In order to define the EEG forward problem, one needs to well
define the head geometry or model, the exact electrode positions
within this geometry and the calculated current dipoles. Details
of the method are presented in the following sections.
III. FINDING APPROXIMATIONS OF THE SOURCE DIPOLES
To model the EEG recordings using the forward problem, the
relationship between the distribution of the different primary
current dipoles and the observed data at the sensors or electrodes
needs to be defined [25]. There are many different head models
that aim at approximating the volume conduction inside the head
in order to find a solution to the inverse problem. In this section,
descriptions of the electrode positions, head models, lead field,
and forward model algorithm are presented.
EEG inverse problem aims at finding and localizing the source
behind the observed electrical activity, which is important in the
diagnosis and treatment of illnesses. For such cases, accuracy in
building and solving the model is very important. However, in
the context of compression, the solution to the inverse problem is
used simply to compute approximations of the measured signals.
Thus, high precision in dipole localization is not mandatory. For
this reason, approximations about the electrodes’ positions, head
models, and other parameters that influence dipole localization
can be tolerated.
It should be noted that the inverse and forward models are
based on the dipole fitting algorithm of the field trip toolbox,
which is a MATLAB toolbox for MEG and EEG analysis that
includes algorithms for source reconstruction using dipoles, distributed sources and beamformers [26]. The algorithm is modified and adapted to fit into the context of compression; details
can be found in Section III-C.
A. Electrode Positions
EEG electrodes are usually located on the scalp according
to the International 10–20 System of Electrode Placement [27,
p. 139]. Spacing between electrodes is standardized in function of the horizontal and vertical widths of the head of the
patient. Thus, approximations of the positions can be easily calculated by simply using the standardized nomenclature of the
electrodes. However, certain EEG labs provide the specific electrode positions with the EEG recordings, which makes the step
of specifying the electrode positions on the scalp straightforward
and more precise.
Certain EEG labs choose to publish the exact electrode locations used to acquire the EEG recordings. In fact, several
examples of EEG channel locations can be found in [28]. These

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

positions can either be in the polar, linear, or spherical coordinates. Mapping is done according to the name of the channels
that is based on the International 10–20 System. These positions
can be directly used when referential montages are used in the
recording.
As mentioned previously, high precision in dipole localization is not mandatory. For this reason, approximations about the
electrodes’ positions can be tolerated. Testing showed that using approximations for the electrodes’ positions, even for bipolar montages, can still provide a rough dipole localization that
gives good approximations of the signals. This will be further
discussed in Section VI.
B. Head Models
There are many different head models that aim at approximating the volume conduction inside the head in order to find
a solution to the inverse problem. The simplest model is the
single homogeneous sphere, where the head is considered to be
a homogeneous sphere of a certain radius R and conductivity
σ. The potential at radius rd = R for all θ (azimuth) and ψ
(latitude) in the coordinate system (rd ,θ,ψ ), that results from a
dipole with moments (Gx , Gy , Gz ), located along the z-axis at
rd = R can be easily calculated using (1) of finding the potential
at the surface of a homogeneous sphere in [29].
Homogeneous spheres neglect the effects of conductive inhomogeneities and irregular geometry that characterize the cranium [29]. These inhomogeneities are able to take into account
the attenuation and smearing of the scalp potentials when performing dipole localization. Thus, they can provide more accuracy in finding the dipoles’ positions and amplitude.
Inhomogeneous spheres are suggested to account for these
irregularities. In this model, the head model is divided into
different compartments that are limited by concentric spheres
as follows.
1) The brain and the Cerebrospinal fluid (CSF) are modeled
as a spherical volume of radius rd 1 and conductivity σ1 .
2) The skull is modeled with conductivity σ2 and thickness
rd 1 − rd 2 .
3) The scalp is modeled as a layer of conductivity σ3 and
outer radius rd 3 .
Different numbers of compartments can also be used. The
surface potential, at the different electrode locations is computed
at the boundary rd 3 . Certain studies argue that, by comparing
both models, the homogeneous assumption effect does not cause
a degradation of the estimation problem [29]. However, the
nested concentric sphere model is more commonly used since
it better approximates the volume conductor model.
A third model assumes a more realistic head shape than a
simple sphere by using information from anatomical images of
the head. This model is found using a multiple compartment
boundary element method (BEM) from magnetic resonance images (MRIs). This BEM model uses realistically shaped compartments of isotropic and homogeneous conductivities to better
approximate the volume conductor properties compared to the
simple spherical shells [30]. The main problem is that the specific patient’s MRI is needed in order to compute the model.

997

However, research has shown that a standardized boundary element method (sBEM) volume conductor model that is derived
from an average MRI dataset (from the Montreal Neurological
Institute) can be used instead of patient specific models [30].
The standardized BEM model is a compromise between the
simple spherical model that neglects the shape of the head,
and the actual BEM model that requires a description of all
brain compartments of the specific individual and requires a
lot of computation and memory. In Section IV we compare
the effect of the two head models: concentric spheres with four
compartments and standard BEM head model in order to choose
the appropriate model for our algorithm.
To determine the appropriate head model, the DIPFIT validation study in [31] is performed.
C. Combined Inverse and Forward Models Algorithm
This section describes the method used to estimate the EEG
recordings using the inverse and forward models.
In the remainder of this paper, when the recorded data are
segmented into blocks along the time dimension, sl [i, n] refers
to the recorded sample at channel l, block index i, and index n
within block i. However, when no segmentation is applied, the
recorded data are referred to as sl [n], where n is the time sample
computed since the start of recording (i.e., sl [i, n] = sl [iN + n]
with N equal to the block size). In addition, it should be noted
that in this paper symbols in bold refer to a vector notation,
while underlined and bold symbols refer to matrices.
It should be noted that average referencing is first applied
on the EEG samples as a preprocessing step. This is done to
avoid having an excessive weight on a single channel when
choosing the optimal inverse solution (more details can be found
in Section IV-A).
As mentioned previously, the inverse model is used first to
find the number, location, and moments of the dipoles that create
the electrical activity measured on the scalp [25], [32]. Then,
the forward model is used to compute, using these dipoles, the
measured voltages on certain locations on the scalp.
In the next paragraphs, the model that maps dipoles to electrical activity is first explained, and then, the steps used to find
the optimal model parameters and compute the approximations
of the recordings are presented. It should be noted that we do
not perform blind source separation, such as principal and independent component analysis, prior to determining the source
dipoles. Even though these techniques achieve good decorrelation of the data, which improves the accuracy of the source
dipoles, they add a lot of overhead to the compression. In our
study, we are interested in obtaining an approximation of the
recorded EEG for very low overhead, and thus, precision of the
source dipoles is not a priority.
1) Definition of the Model: The data vector of scalp recordings at time sample n, s[n], is equal to
⎤
⎡
s1 [n]
⎢ s2 [n] ⎥
⎥
⎢
s[n] = ⎢ . ⎥
⎣ .. ⎦
sM [n]

998

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

where the subscript indicates the electrode number.
The inverse model [32] defines the production of this data
vector using the following modeling equation:
s[n] = Kg[n] + noise.

(1)

This equation links the dipole moments, locations, and their
number to the recorded data: g[n] is the dipoles moments vector, which contains, for each dipole, its moments (in three directions); K, known as the lead field matrix maps the dipole
moments to the voltage they produce at a given electrode position on the scalp; and the number of dipoles is determined by
the size of these matrices (for Nd dipoles, g[n] is a length-3Nd
vector). At any given time instant n, the model parameters are,
hence, as follows:
1) the number of dipoles Nd ;
2) the dipole moments, concatenated in the dipoles moment
vector g[n];
3) the dipole locations in three dimensions; these are embodied in the lead field matrix K, which also depends on
the electrode locations (each row of K corresponds to one
electrode location) and on the head model used. Once the
number of dipoles, their spatial locations and the head
model are known, K can be computed, as detailed in [32,
ch. 11].
2) Modeling the EEG Recordings: Given a set of measured
scalp recordings, the main purpose of the inverse model is to
find the combination of model parameters that best reproduce
this data through modeling equation (1) [32].
To this end, we first construct a discrete set of possible dipole
locations. This is referred to as the sampling grid. This discretization in space of the interior of the head model is used in
order to reduce the complexity of the fitting process. The grid is
constructed based on the chosen volume conductor model and
the electrode positions. Spatial sampling with uniform spacing
in the x-, y-, and z-directions is applied inside the head volume
conductor.
The inverse model process consists in choosing the combination of dipoles on the sampling grid (number and location) and
their respective 3-D moments that best fit the measured data.
For a given equation maximum number of dipoles Nd , one
can define the set O of all combinations of at most Nd dipole
locations. The total number of possible combinations depends
on the size of the sampling grid. Each dipole position that is a
member of a possible combination, has a number of possible
locations that is equal to the size of the grid, denoted by NS . In
each combination, we have Nd possible dipoles, and thus, Nd
different
locations. The total number of combinations is equal to
N S
k =(N S −N d +1) (k), where NS is the total size of the sampling
grid. For a given combination of dipoles c ∈ O, we can measure
the corresponding lead field matrix Kc .
Neglecting the noise in (1), for a given combination of dipole
locations c ∈ O and scalp measurement s[n], one can estimate
the corresponding moments vector as
ḡc [n] = Kc+ s[n]
Kc+

(2)

where
is the pseudoinverse of Kc . The pseudoinverse can
be calculated using the singular value decomposition of the lead

field matrix Kc .
Kc = UΣVT

(3)

where the columns of U are the eigenvectors of Kc KcT and the
eigenvalues of Kc KcT are the squares of the singular values in
Σ.
The pseudoinverse is then defined as
Kc+ = VΣ−1 UT .

(4)

Using the estimate of the moments and the lead field, we can
calculate an estimate of the potentials for each combination of
dipoles c
s̄c [n] = Kc gc [n].

(5)

As mentioned previously, our aim is to find the best approximation of the measured voltages, s[n] from a set of possible
potentials, s̄c [n], at time sample n.
The squared difference between the measured and modeled
data for a given dipole configuration c is used as criterion when
finding the best fit combination c in the inverse model. It is given
by
dc [n] = (s[n] − s̄c [n])T (s[n] − s̄c [n])
= (s[n] − Kc ḡc [n])T (s[n] − Kc ḡc [n])
= (s[n] − Kc Kc+ s[n])T (s[n] − Kc Kc+ s[n])
= sT [n](I − Kc Kc+ )T (I − Kc Kc+ )s[n].

(6)

Based on the aforementioned, the best combination c∗ ∈ O
at a given time n is such that
c∗ = arg min dc [n].
c∈O

(7)

Thus, having simply the electrode positions and an appropriate head model, one can estimate the dipoles’ locations, and
then, lead field and the dipoles’ moments ḡ in order to finally
compute an approximation of the measured EEG data.
The solution to the inverse problem gives us the optimal
dipoles positions and moments that best map to the observed
signals. This solution is not unique, we are scanning the dipole
grid for the dipole or combination of dipoles that give the lowest
squared difference between the mapped and the recorded EEG
signals. In compression, based on the solution of the inverse
problem, the forward model is used to compute the approximations of the signals. Knowing simply the electrode positions,
the agreed-upon head model and the dipoles positions and moments, we are able to compute the lead field matrix, and thus,
the estimated potentials, s̄[n], for each time sample n.
IV. COMPRESSION USING THE FORWARD MODEL ALGORITHM
In compression, one should be able to control the rate of compression and thus the level of distortion added to the data. Using
the algorithm shown in the previous section, one can specify
the number of dipoles Nd to use in the fitting process. We set a
constant number of dipoles in order to have a low constant overhead to our compression system. As stated previously, we do not
focus on achieving precision in determining the source dipoles.

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

Fig. 1.

999

Block diagram of the overall compression system.

We are mostly interested in obtaining a good approximation of
our signals.
In the context of compression, the EEG measurements are
modeled as the solution potentials of the forward problem. The
parameters that define this model are the head model and the
position and moments of the dipoles. Testing has shown that
using a standard head model and a relatively low number of
dipoles, one can get a good approximation of the EEG signals.
In this section, we focus on coding techniques used to represent
the dipoles’ characteristics and the residual error between the
actual and estimated potentials.
As seen in the previous section, using simply the electrode positions, the agreed-upon head model and the dipoles’ positions
and moments, we are able to compute the lead field matrix, K.
Then, using the values of ḡ[n], we are able to compute s̄[n].
Thus, in compression, the dipoles’ positions and moments need
to be shared between the encoder and decoder for the computation of the approximated signals s̄[n]. Thus, in our system, the
overhead is composed of the dipoles’ positions and moments.
EEG signals are segmented into different matrices. The EEG
matrix s[i] is composed of samples sl [i, n], of matrix index i,
with l going from 1 to the number of channels M and of length N
samples, i.e., n varies between 1 and N . In each matrix, dipoles
are considered to be fixed, which means that the location po of
each dipole o is considered to be constant for all values of n
within the same matrix at index i. Thus, for each EEG block
s[i], we define a set of dipole positions p[i], composed of the
positions of each dipole o, that do not change within the block
i.
It should be noted that when applying dipole fitting on EEG
blocks of length N , moments of the dipoles are generated for all
time samples n varying from 1 to N . Thus, the dipoles moments
are now represented by the matrix G[i] that corresponds to the
dipoles of EEG block s[i]. The size of the matrix G[i] is equal
to 3Nd -by-N .
Fig. 1 shows the block diagram of the overall compression
system with the EEG matrix s[i] as input. Dipole fitting is first
used to compute the position of the dipole, p[i], and the moments
G[i], along the three directions x, y, and z. Then, forward
modeling computes the estimated signals from the dipole and
moments. This step uses the same head model and electrode
locations used when finding the dipoles to compute the dipole
grid. Afterwards, residual coding is applied on e[i].

A. Average Referencing
For each EEG matrix s[i] of index i, dipole fitting attempts to
model the recordings at each time sample n along all channels
l. These vectors correspond to the vertical components of the
EEG matrix s[i] of length M each.
Before performing dipole fitting, EEG data are average referenced. This means removing the means of the vectors s[i, n]
where the channel index l varies and n and i are kept constant:
r[i, n] = s[i, n] − μ̂s [i, n]
	M

(8)

s [i,n ]

, which is equal to the mean of the
where μs [i, n] = l = 1M l
vector s[i, n] and μ̂s [i, n] is the quantized version of μs [i, n].
This is done in order to have average referencing along all
time segments. When performing average referencing, we avoid
having an excessive weight on an arbitrary single channel (i.e.,
the reference channel). However, this adds an overhead of N ×
Bμ , per block, with Bμ equal to the number of bits used to
quantize the mean values.
It should be noted that quantized values of the mean are used
in order to guarantee having the same values at both the coder
and decoder’s sides.
B. Coding the Dipole Moments
We apply the inverse model for the purpose of finding the
dipoles that best fit the measured signals. The solution of this inverse model gives the positions and the moments of each dipole
source. Moments are vectors, in the x-, y-, and z-directions, of
length equal to the number of time samples of the signals we are
trying to model. Thus, each dipole is described with a 3-by-1
position vector and a 3-by-N moments matrix, where N is the
chosen segment length of the EEG channels. The compression
ratio depends on the number of dipoles used to fit the model, the
coding technique of the positions and moments of these dipoles,
and the coding technique used for the residual.
When analyzing the characteristics of the moments, we notice
that a lot of energy compaction can be achieved when performing DWT on these moments. This is highlighted in Fig. 2 when
N = 256 and Daubechies (db1) wavelet is used. Thus, appropriate zero-tree coder such as SPIHT in 1-D [33] [34] applied
on the DWT coefficients yields low values of distortion when

1000

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Fig. 3.
1-D.

Fig. 2.

Example of the DWT coefficients of a dipole moment with N = 256.

using a constant bit rate of 3 bps. Detailed results are shown in
Section VI of this paper.
C. Coding the Residuals
In our compression system, we use set of parameters, such
as constant number of dipoles, predefined head model, and approximate electrode positions, in order to obtain a constant low
overhead in our coding. This might lead to a large discrepancy between the exact and the approximated source dipoles.
To solve this issue and to obtain a good representation of the
EEG in our compression, we choose to compute and code the
modeling residuals. Residuals of the dipole modeling part of the
compression algorithm are equal to
el [i, n] = rl [i, n] − s̄l [i, n]

(9)

where rl [i, n] is the mean averaged EEG sample of matrix index
i, at channel l and time sample n; and s̄l [i, n] is the modeled
sample also of matrix index i, at channel l and time sample n
obtained after applying the forward model.
In this section, we present two different methods to code the
residual. First, we propose a coding method targeted for discrete
cosine transform (DCT) coefficients and based on significance
and refinement passes, which is inspired by the SPIHT coder
applied on DWT coefficients [33], [34]. This method performs
well when the dipoles are able to well model the measured
signals. However, for certain types of signals, it is hard to obtain
a good approximation. Thus, for such cases, a different residual
coding method is also suggested. This method aims at further
decorrelating the computed residuals. Details of both methods
are explained in this section.
1) Coding of DCT Coefficients: When a good approximation
is achieved, residuals have most of the energy concentrated in the low frequency coefficients. In the suggested
method, an M-point DCT is applied on each residual channel l of el [i] and the resulting coefficients are referred to
as Dl [i]. It should be noted that el [i] is composed of the
residual samples el [i, n] of channel l and block index i,
with n going from 1 to block size N .

Suggested arrangement of the DCT coefficients of all M residuals in

We first combine the DCT coefficients of the residuals
from their 2-D arrangement, where the two dimensions
are time sample and channel, into a 1-D arrangement,
where Dl [i, n] is the DCT coefficient of the residual of
channel l, matrix index i, and time sample n: the coefficients of matrix D[i] are combined in a way to have
the first coefficient of channel l = 1, D1 [i, 1] followed by
Dl [i, 1] with l going from 2 to M . Then, we add the DCT
coefficients at n = 2, D1 [i, 2], followed by Dl [i, 2] with l
going from 2 to M , and so on. Fig. 3 shows the suggested
arrangement.
The resulting 1-D arrangement of the coefficients of the
EEG matrix at index i is denoted as a 1-D vector D1D [i]
composed of coefficients D1D [i, m] with m between 1 and
M × N . D1D [i, m] is then coded sequentially as follows.
First, we compute the power of 2 coefficient, np , of the
maximum value of D1D [i]
np = log2 (max(|D1D [i]|))

(10)

we take the initial threshold to be equal to
T = 2n p

(11)

we then start looking at each coefficient of D1D [i] starting
from index 1 to M × N . For each coefficient at index m,
we test for significance by comparing to T . If the absolute
value of D1D [i, m] is greater or equal to T , we output a
1 bit, followed by the sign bit, 1 for negative and 0 for
positive values. We then add D1D [i, m] to the significance
list. If the absolute value is smaller than T , then we only
output a 0 bit.
Refinement is also applied directly when checking for
significance. At each index m, before checking for significance with respect to T , we check if D1D [i, m] is already
in the significance list. It if it’s not, then we apply the
significance test and output the corresponding bits. If it
has already been added to the significance list, we check
for refinement. We output a 0 bit if a refinement of T /2
needs to be added and a 1 if a refinement of −T /2 should
be added.
After having coded all coefficients in D1D [i], we divide T
by 2 and restart checking for significance and refinement
for each index up to M × N . The coder needs to send the
length of D1D [i] (M and N ) and the initial value of np to
the decoder.

êl [i, n] = w1,B êL [l,1] [i, n − B + 1] + w1,B −1 êL [l,1] [i, n − B + 2] + · · · + w1,1 êL [l,1] [i, n] + w2,B êL [l,2] [i, n − B + 1] +
w2,B −1 êL [l,2] [i, n − B + 2] + · · · + wN c ,B êL [l,N c ] [i, n − B + 1] + · · · + wN c ,1 êL [l,N c ] [i, n]

(12)

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

Table I shows an example of applying this method on
coding the following values of D1D [i]: D1D [i] = [36 −18
−8 4 10 −5 1].
The bitstream generated by the suggested coding results
in large sections of zeros indicating nonsignificance. For
this reason, RLC is applied and the count of zeros and
ones are coded using adaptive arithmetic coding.
2) ARX Modeling of the Residual: For certain segments of
recording, the computed dipoles are not able to extract
the correlation between the different channels and the
calculated signals are highly distorted compared to the
measured signals. This occurs mostly in recordings where
sampling frequency is low and there is a lot of activity in
the high frequency band. In such cases, it becomes hard to
model the recorded signals well when using the computed
approximations of the source dipoles.
In such cases, we have devised another coding method
based on autoregressive modeling with exogenous inputs
(ARX) and single output of the residuals from the inverse/forward modeling to predict the different channels
of these residuals. This method explores the redundancy
still present after forward modeling using approximated
source dipoles and tries to model the channels using other
channels in the residuals.
ARX prediction is a way to decorrelate the channels using
a small number of channels. It is a prediction model with
multiple inputs and single output. To build this model, the
first question is to choose which channel to predict from a
given set of channels. One possibility would be to use as
predictors channels that are physically close to the channel to be predicted, using Hjorth graph [35]; our initial
tests revealed that, because of the nature of the signals at
hand, this method does not give good performance. We
instead use clustering in order to appropriately choose the
different inputs and outputs of the ARX blocks. Clustering
allows us to group channels that are mostly correlated together. Thus, we choose the inputs and outputs of the ARX
coder from the same cluster in order to guarantee certain
correlation that will be exploited in ARX modeling.
The suggested method first performs clustering of channels based on the L-2 norm distances between the channels. In this clustering method, centroids are first chosen
randomly from the channels and initial clusters are formed
by associating each channel to the least distant centroid.
The optimal centroid for each cluster is then chosen as
the member of the cluster having the minimum sum of
distances with all other members of the same cluster.
After having grouped all M channels into different clusters, we find a second centroid per cluster by taking the
channel in each cluster that is the least distant from all
other members of the same cluster. Thus, each cluster has
two different centroids. We keep repeating the process
until we have Nc centroids for each cluster. The value of
Nc depends on the chosen number of clusters and total
number of channels.
The centroids are taken as the inputs to the ARX system
to model each member of the corresponding cluster. In

1001

this method, L[l, j] refers to the centroid indices that correspond to channel at index l. Thus, the channel at index
l is modeled using the channels at indices L[l, j] with j
from 1 to Nc , the chosen number of centroids per cluster. Centroids are first coded using the DCT-based method
suggested in Section IV-C1. The decoded centroids are
used as inputs in the ARX blocks. This guarantees that
both the encoder and decoder use the same inputs in the
ARX blocks.
The number of clusters depends on the number of channels since it is used to group the channels into different
sets where members of each set display a certain redundancy between each other. For this reason, this number
should not be too low so that the redundancy is substantial and can be exploited in our coding. This number also should not be too high; otherwise, we would end
up with a large total number of centroids, and thus, most
channels will be coded using the DCT-based scheme. To
find the optimal number of clusters and centroids in our
coding scheme, we tested with different combinations of
these two parameters: number of clusters and number of
centroids. The optimal values are chosen based on their
effect on the compression. This is further discussed in
Section VI.
The equation of the predicted output of the ARX block
for EEG residual el [i, n] of channel l, block i and sample
n using Nc DCT-coded residuals êL [l,1] [i, n], êL [l,2] [i, n],
..., and êL [l,N c ] [i, n], with filter length B, is shown in (12).
The filter coefficients are the following:



w[i] = w1,B · · · w1,1 · · · wN c ,B · · · wN c ,1 .

(13)

The filter uses B samples of each of the Nc input channels
for each of the channel to be decorrelated. The overall
number of coefficients is equal to the number of channels
used in the decorrelation multiplied by the value of B.
These coefficients are quantized and sent to the decoder. A
value of B = 1 is chosen in this method since it gives good
prediction while ensuring a small overhead. In addition,
using a small number of filter coefficients limits the effect
of quantization errors.
The filter coefficients are calculated using the pseudoinverse method. The input matrix, X[i], of the filter is formulated using the Nc centroids. When B is set to a value
of 1, this matrix is equal to
⎡

eL [l,1] [i, 1] eL [l,2] [i, 1]

⎢
⎢ eL [l,1] [i, 2]
⎢
X[i]= ⎢
..
⎢
.
⎢
⎣
eL [l,1] [i, N ]

eL [l,2] [i, 2]
..
.
eL [l,2] [i, N ]

···

eL [l,N c ] [i, 1]

⎤

⎥
· · · eL [l,N c ] [i, 2] ⎥
⎥
⎥.
..
..
⎥
.
.
⎥
⎦
· · · eL [l,N c ] [i, N ]
(14)

1002

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

The desired ARX filter output is equal to
⎡
⎤
el [i, 1]
⎢
⎥
⎢ el [i, 2] ⎥
⎢
⎥
⎢
⎥
el [i] = ⎢ el [i, 3] ⎥ .
⎢
⎥
⎢
⎥
..
⎣
⎦
.
el [i, N ]

to M

(15)

(16)

Pseudoinverse, in addition to guaranteeing to find the minimum (Euclidean) norm solution to a system of linear
equations, is also relatively fast and simple. The pseudoinverse gives the solution with the smallest sum of
squared filter weights, which is desirable since these
weights should be quantized and sent.
The predicted output is equal to
ēl [i] = X[i]wT [i].

In this equation, N is the block size and M is the number
of channels used in the coding. This smoothness measure is
calculated from the DCT coefficients of el [i], Dl [i]. In order
to find a measure of smoothness for the residuals, we analyzed
matrices over a few minutes of recording and compared the
DCT coefficients of the smooth and nonsmooth matrices. N/4
was chosen based on its effect on computing the DCT-based
smoothness measure and its impact on the compression. In fact,
plotting the distributions as in Fig. 8 (see Section VI) helped us
better determine an appropriate coefficient for N , and thus, an
appropriate equation for ρ.
Based on values of ρ[i], we choose a certain threshold Tρ that
determines whether the residual matrix is considered smooth
or not. When ρ[i] is greater or equal to Tρ , the matrix can be
considered smooth and the DCT-based coder is used, otherwise,
the ARX-based coder is used.

(17)

And the ARX prediction residual is
yl [i] = el [i] − ēl [i].

(19)

l=1

The filter coefficients are found using the desired output
and the pseudoinverse of the input matrix X+
wT [i] = X+ [i]el [i].


 	
M
N /4
1 
n =1 |Dl [i, n]|
.
ρ[i] =
	N
M
n =N /4+1 |Dl [i, n]|

(18)

Fig. 4 shows the block diagram for coding the EEG residual at channel index l with Nc = 2. êl [i] refers to the
decoded residual at channel l. Lloyd–Max quantizers are
used to quantize the residuals and the filter coefficients.
The number of bits used to quantize the filter coefficients is
kept constant, whereas for the ARX prediction residuals,
this number varies and is used to control the compression
rate (CR).
D. Smoothness Measure and Conditional Coding
As explained previously, depending on how well the approximated dipoles are able to model the measured data, different
coding techniques should be used to code the residuals. When
the data are well modeled, dipoles are able to extract the redundancy between the different channels, and thus, the method
based on ARX prediction incurs too much overhead for the gain
it brings for coding the residuals. For such cases, using the DCTbased method we are able to code the residuals at very low bit
rates while adding little distortion.
However, as previously mentioned, for matrices with a lot of
high-frequency content, the computed dipoles are not able to
model the signals well. In such cases, the residuals are not well
decorrelated and the resulting matrix is not smooth. By smooth,
we mean that the adjacent samples, both vertically and horizontally, vary a lot. These types of matrices have high energy in the
high frequency band. For such matrices, ARX modeling is able
to further decorrelate the channels and provide better coding.
For this reason, choosing the appropriate coding technique is
based on a certain smoothness measure calculated on the residual matrix. We choose the following smoothness measure for
the residual matrix el [i] at index i for all channels with l from 1

V. DATASETS
This section presents a detailed description of the datasets
used in testing the compression performance of the suggested
method.
A. Dataset 1—MIT dB
The first set of data, CHB-MIT Scalp EEG Database, were
collected at the Children’s Hospital, Boston, MA, USA. This
dataset consists of EEG scalp recordings from pediatric subjects
with intractable seizures [36]. Testing was done on 11 patients,1
with ages varying between 6 and 22 years. Each patient can
have multiple seizures during recording, ranging from a single
seizure to seven seizures. This dataset is sampled at 256 Hz.
Recording is done over 25 channels with bipolar montage and
16 bits are used in the recording’s precision.
B. Dataset 2—MNI dB
To test the compression performance of the system, testing is
also done on recordings acquired at the Montreal Neurological
Institute (MNI dB) from nine patients using 29 channels, 200-Hz
sampling frequency and 16 bits were also used in the recording’s
precision. It should be noted that these recordings were acquired
for the purpose of this study and are not publicly available.
C. Dataset 3—Delorme dB
The data of the third database are published on-line and
recorded by Alain Delorme [37] with 31 channels, sampled at
1000 Hz. Patients were shown certain images during the recording and they were required to indicate whether or not the image
is familiar. This data are recorded for the purpose of building
ERP signals from the recorded EEG. However, we use the raw
version of the acquired signals. The first three records of each
1 Patients

01 to 05, 07, 09, 11, 18, 20, 22 and 23, were used in the testing.

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

1003

Fig. 4.

Block diagram of coding EEG segment e l [i] at block i of channel l using the ARX modeling method.

Fig. 5.

Block diagram of the overall compression system.

of the eight patients available in this database are used in the
testing.
VI. RESULTS
In this section, compression performance of the suggested
system tested on three different datasets is presented.
To determine the appropriate head model assumption in the
inverse and forward models, the DIPFIT validation study in [31]
is tested on patient 1 of Delorme dB to compare the effect of the
two head models: concentric spheres with four compartments
and the standard BEM head model. Results show that fitting
using the two models results in a small difference in dipole
positions. However, there are outliers that account for less than
10% of all dipoles, and are mostly due to bad convergence
when using either of the two models [31]. However, the sBEM
model is chosen in our study because it gives slightly better
approximation when dipole fitting is not able to converge to an
appropriate solution.
Another important parameter to consider in our model is the
sampling grid. As explained in Section IV dipole fitting tries to
find the best dipole, or combination of dipoles, in the sampling
grid, that best maps to the recorded signals. The finer the grid,
the more possible dipole locations in the system. To examine
the effect of granularity of the grid on the system, we modify
the spacing in the x-, y-, and z-directions used to build the
grid. This changes the number of dipoles used in the fitting
scenario. The grid scale was varied to include 116, 181 (default
dipole locations used in EEGLAB), 523 and 887 different dipole
locations used in the fitting process. There was no substantial
difference in distortion for all databases. It should be noted that
the finer the grid, i.e., number of dipoles equal to 887, the larger
the delay caused by searching a very large number of possible
locations.
Fitting is done using a low number of dipoles in order to obtain
an output that is compressed compared to the input. Since we

focus only on a low number of dipoles, the complexity of the
system is linear in terms of the size of the grid. Thus, there is
negligible difference in complexity and delay between a grid size
of 116 and 181. For these reasons, we set the number of dipoles
to the default value of 181. However, if a large number of dipoles
is used, a smaller grid size of 116 locations is recommended to
reduce the complexity and delay of the system that exponentially
increase with the number of dipoles.
Fig. 5 shows the block diagram of the overall compression
system with the EEG matrix r[i] as input. As previously explained in Section IV-A, the matrix r[i] is obtained after applying average referencing on the channels of s[i]. As mentioned in
Section IV dipole fitting is first used to compute the position of
the dipole, p[i], and the moments G[i]. Then, forward modeling
computes the estimated signals from the dipole and moments.
The residual e[i] is found and coded based on the value of ρ[i].
The output of the residual coder, along with the position and
coded moments are sent to the decoder.
At the decoder’s side, the estimated potentials, s̄[i] are first
computed using the dipole’s moments and positions and of
course the agreed-upon head model and electrode positions. Afterwards, the decoded residuals are added to these estimated potentials. In order to reconstruct the original signals, μ̂s [i] should
also be added to compensate for the average referencing block.
The decoded samples of channel l, matrix index i and time
sample n are ŝl [i, n].
The performance parameter used to analyze the results is the
percent-root mean square difference (PRD)
	
PRDl [i](%) =

N
2
n =1 (sl [i, n] − ŝl [i, n])
	N
2
n =1 sl [i, n]

× 100

(20)

where sl [i, n] is the EEG sample n of segment at index i and
channel at index l, and ŝl [i] is the reconstructed EEG segment after compression. Thus, PRD is calculated for each EEG channel

1004

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Fig. 6. Effect of increasing the number of dipoles on the PRD values of
the output of the forward model (mean PRD of dipole fitting without residual
coding).

Fig. 8. Distribution of smoothness factor values of the three datasets based on
low and high values of PRD for the DCT-based suggested coder.

Fig. 7. Performance Comparison of the ARX-based residual coding method
with varying number of centroids and clusters applied on the residuals of Patient
1 of MNI dB.

l and segment i, then the mean over all segments and channels
is calculated to reflect the PRD at a certain CR.
As previously mentioned, the lower the number of dipoles, the
lower the number of moments that need to be sent to the decoder.
Testing has shown that the higher the number of dipoles used
in the fitting, the more accurate the model . However, when we
increase the number of dipoles by 1, we are actually multiplying
the overhead by 2 since the position vector and the moments
matrix double in size. Thus, the overhead increases a lot, while
the improvement in accuracy is small. Thus, a low number of
dipoles is recommended in order to ensure high CRs. In our
compression algorithm, we limit the number of dipoles to 1.
Fig. 6 shows the effect of varying the number of dipoles on
the forward model accuracy tested on patient 1 of Delorme dB.
To compute the PRD values in this plot, we replace ŝl [i, n] by
the modeled signal s̄l [i, n] in (20). We can directly see from
these results that when we increase the number of dipoles, distortion decreases. However, as mentioned previously, the slight
improvement caused by using one additional dipole doubles the
overhead, and thus, increases significantly the bit rate. For this
reason, only one dipole is used in our suggested system.
The residual coding method based on ARX and clustering is
highly influenced by the number of clusters and centroids used.
Fig. 7 shows the effect of changing these two parameters on
the recording of patient 1 of MNI dB in terms of compression
distortion (PRD) versus bit rate (BR). The bit rate represents

the average number of bits used to code each sample of the
residuals. It should be noted that 4 bits are used to quantize
the filter coefficients and B is chosen to be equal to 1. Testing
showed that increasing the number of previous samples used in
prediction does not improve the performance of the system, and
introduces a slightly larger overhead.
We can directly see from Fig. 7 that when increasing the
number of centroids, there is degradation in performance. In fact,
when we increase the number of centroids, more channels are
coded using the DCT-based coder. In addition, a higher number
of centroids means that inputs that might not be correlated to a
certain channel are used in the ARX prediction of that channel.
Further increasing both parameters implies that most of the
channels are coded using the DCT-based coding method and
the performance deteriorates at high bit rates. It should be noted
that the ARX-based method used to compute the results in the
rest of this paper uses five clusters and two centroids.
The threshold of the smoothness factor Tρ is calculated and
chosen based on the recordings of patient 1 of all three databases.
Depending on the value of ρ[i], the appropriate coding method
of the residuals is chosen. When the value of ρ[i] is greater than
or equal to the threshold Tρ , residuals are coded using the DCTbased coder, otherwise, they are coded using the ARX-based
coder.
The distributions of the smoothness factor are shown in
Fig. 8(a) and (b). These plots show along the vertical axis the
percentage of occurrence of the different values of the smoothness factor; these values are shown along the horizontal axis.
Fig. 8(a) shows the distributions when testing is done on only
patient 1 of all three databases, whereas Fig. 8(b) shows the
distributions when testing is done on all patients of all three
databases. In these figures, the first black plot corresponds to
values of ρ[i] where the corresponding PRD is high, whereas

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

Fig. 9. Performance comparison between the different coding methods of the
suggested compression system tested on Delorme dB.

the second distribution plot corresponds to low values of PRD.
The PRD values are taken for the DCT-based suggested coder
for a bit rate of around 1 and PRD values are compared to a
value of 15% to determine if the value is low or high.
These figures shows that, in fact, for low values of ρ, the
DCT-based coder does not perform well since there is not much
energy in low frequency components. It should be noted that
in Fig. 8(a) the distribution that corresponds to low values of
PRD exhibits two separate bell shapes, each corresponding to
a certain dataset. Residuals of Delorme dB exhibit very high
energy in the low frequencies, therefore, the second bell shape
corresponds to this dataset, while the first bell shape corresponds
to the recordings of MIT dB. When taking a larger sample size,
i.e., as in Fig. 8(b), these two bell shapes merge into one. Both
figures have a consistent threshold value below which we obtain
high values of distortion.
We set the threshold Tρ to be equal to 10 and run the conditional compression algorithm on all patients of the three
databases. Results of all compression methods and the conditional coding are shown in Figs. 9–11. In these figures, the variations of results between the different patients of each dataset is
highlighted using the box-plots of 75 and 25 percentiles.
We choose to also compare the proposed compression method
to a previously suggested EEG compression algorithm: 2-D
SPIHT-based method [16]. This method uses DWT and SPIHT
in 2-D to code the EEG channels. Smoothness transforms are
added prior to applying wavelet transform in order to optimize
the performance of the SPIHT coding on the EEG signals.
This method is tested on the datasets of MNI DB and MIT
DB to analyze the compression performance in terms of PRD
as a function of Compression Ratio (CR). Results are shown
in Figs. 12 and 13. We included in these figures the compression results obtained for the dictionary-based method, shown
in Figs. 10 and 11, to better visualize and compare the performance of the three different compression methods in terms of

1005

Fig. 10. Performance comparison between the different coding methods of
the suggested compression system and the previously suggested referential 1-D
algorithm [15] tested on MIT dB.

Fig. 11. Performance comparison between the different coding methods of
the suggested compression system and the previously suggested referential 1-D
algorithm [15] tested on MNI dB.

Fig. 12. Performance comparison between the dipole-based (“−o−”),
dictionary-based (“− × −”) and SPIHT-based (“-”) methods tested on patients
of MNI DB. The plot shows the 25th and 75th percentiles of all mean values
between the different patients.

1006

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Fig. 13. Performance comparison between the dipole-based (“−o−”),
dictionary-based (“− × −”), and SPIHT-based (“-”) methods tested on patients
of MIT DB. The plot shows the 25th and 75th percentiles of all mean values
between the different patients.

CR. These figures also show the 25th and 75th percentiles of
PRD results for all patients of each dataset.
Results of both datasets show that there is a lot of improvement when compressing using the dipole-based method compared to the other two methods. In addition, the dictionarybased method gives better performance compared to the 2-D
SPIHT-based method. For MNI DB, when compressing using
the dipole-based method, mean PRD values are almost constant
between the 12 patients. However, for the other two methods,
results vary more between the different patients.
VII. DISCUSSION
Fig. 9 shows the compression performance of the suggested
system when tested on the recordings of Delorme dB. We can
directly see from the results that distortion, for bit rates greater
than around 1.5 bps, does not vary a lot between the different
patients and we can achieve very low PRD for very low bit rates.
Both residual coding methods give good results, however, lower
distortion can be achieved with the DCT-based coder with N =
1024. It should be noted that when comparing with Tρ = 10, all
values of ρ[i] are greater than Tρ , and thus, conditional coding
always chooses the DCT-based coder.
Figs. 10 and 11 show the compression performance of the
suggested system compared to the compression method in [15],
dictionary-based method. This method is based on the use of
DWT and dynamic reference lists to compute and send the
decorrelated subband coefficients.
Results of MIT dB are shown in Fig. 10. Segments of length
N = 1024 are used in the testing. This value gave better results
compared to smaller values such as 256 and 512. As previously
mentioned, bipolar montage of adjacent electrodes is used in
the recording. In order to compute the electrode positions used
to find the lead field, the midpoint between the two electrode
positions of each bipolar channel is used as approximation.
Using these position points, we were able to achieve a good
approximation of the signals.

These results show that the suggested method gives better
performance than the dictionary-based method of [15]. In addition, results show that since the smoothness factor is relatively
high, the modeling residuals are mostly coded using the DCTbased method. However, when applying conditional coding of
the residuals with Tρ = 10, we obtain an improvement in performance.
Results of MNI dB are shown in Fig. 11. In these plots, N is
chosen to be equal to 256 since it gave better results than higher
values. The suggested method with ARX-based residual coding
gives the best compression performance. Conditional coding
achieves almost the same results as the ARX-based coder. This
shows that most segments have low values of ρ[i], and thus, the
inverse model did not converge to a good approximation.
Results shown in Figs. 9 to 11 show that, for a bit rate less
than around 1.5 bps, PRD values vary a lot between the different
patients. This variation is higher for MIT and MNI datasets
where PRD values are very high at low bit rates compared to
Delorme dB. These results suggest that, when compressing at
very low bit rates, the proposed system is more appropriate
for compressing data having characteristics similar to Delorme
datasets (for example, sampled at high frequencies and recorded
for the purpose of building ERP signals).
Results show that the method is able to adapt to changes in
the characteristics of the data. When recordings are done at high
sampling frequencies and for certain evoked potentials, as for
Delorme dB, the computed dipoles approximate very well the
data and the DCT-based coder provides good compression of
the residuals. Near lossless compression can be achieved for
these types of recordings at bit rates greater or equal to 2.5 bps.
This corresponds to CRs lower or equal to 6.4.
As mentioned previously, for certain segments of recordings,
the forward model that is built on approximated dipoles is not
able to generate good approximations of the recorded signals.
For such cases, the ARX-based coder of the residuals is able
to compensate for this high distortion in the approximated signals. This is highlighted in Figs. 10 and 11. In Fig. 10, there
is a slight improvement when using conditional coding compared to the DCT-based coder. This shows that some of the
segments were coded with ARX-based coder, which caused an
improvement in performance. However, in Fig. 11, almost all
the segments of recordings are coded using the ARX-based
coder. For MNI dB, most of the segments do not converge to a
good approximation, and thus, there is still a lot of redundancy
present in the residuals that is taken care of using the ARX-based
coder.
Results shown in Figs. 12 and 13 show that the suggested
method performs better than the method based on SPIHT [16]
and the method based on a dynamic dictionary [15]. It is able
to provide lower distortion values at low bit rates compared
to the other two compression methods. It should be noted that
testing this method on three different databases, MNI DB, MIT
DB, and Delorme DB, shows that the worst forward modeling
performance is actually obtained for MNI DB. However, even
if we did not obtain a good approximation of the recordings’
segments for this database, compression performance is still
better than the previously suggested methods.

DAOU AND LABEAU: EEG COMPRESSION OF SCALP RECORDINGS BASED ON DIPOLE FITTING

As mentioned previously, the suggested method is based on
modeling the relationships between the different channels using
dipoles and their moments. Thus, it provides better extraction
of this redundancy between the different channels. In addition
the suggested coding techniques further decorrelate the EEG
matrices in time. The previous section presents and analyzes
the different parameters and assumptions used in our algorithm.
These include the granularity of the sampling gird, the predefined number of dipoles, the choice of the smoothness factor,
and the choice of the number of clusters and centroids used to
code certain residuals. Results shown in Figs. 9 to 13 highlight
the fact that using these different assumptions and predefined
parameters, we are able to provide good approximations of the
different signals at very low bit rates.
VIII. CONCLUSION
The suggested method is able to provide good compression
performance when tested on three datasets of different characteristics. Low distortion values are obtained for bit rates lower
than 3 bps. In addition, near-lossless compression is achieved
for a certain type of recordings: event-related potentials.
Further analysis is needed to study the distortion added to the
signals. The suggested compression scheme should still preserve
important diagnosis information. It would be important to test
abnormality detection systems, like epileptic seizure detection,
on both the original data and the compressed output to further
analyze the performance of the compression algorithm [15],
[16], [38].
REFERENCES
[1] N. Sriraam, “Neural network based near-lossless compression of EEG
signals with non uniform quantization,” in Proc. IEEE 29th Annu. Int.
Conf. Eng. Med. Biol. Soc., 2007, pp. 3236–3240.
[2] J. Dauwels, K. Srinivasan, M. R. Reddy, and A. Cichocki, “Multichannel EEG compression based on matrix and tensor decompositions,”
in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., Jul. 2011,
pp. 629–632.
[3] N. Spradhan and D. N. Dutt, “Data compression by linear prediction
for storage and transmission of EEG signals,” Int. J. Biomed. Comput.,
vol. 35, no. 3, pp. 207–217, 1994.
[4] N. Sriraam. (2012). Correlation dimension based lossless compression of {EEG} signals. Biomed. Signal Process. Control. 7(4), pp.
379–388. [Online]. Available: http://www.sciencedirect.com/science/
article/pii/S1746809411000607
[5] N. Sriraam. (2012, Jan.). Correlation dimension based lossless compression of {EEG} signals,’ “A high-performance lossless compression
scheme for eeg signals using wavelet transform and neural network predictors. Int. J. Telemedicine Appl.. 2012, pp. 5:5–5:5. [Online]. Available:
http://dx.doi.org/10.1155/2012/302581
[6] N. Sriraam and C. Eswaran. (2006). Context based error modeling for
lossless compression of EEG signals using neural networks. J. Med.
Syst. 30, pp. 439–448. 10.1007/s10916-006-9025-0. [Online]. Available:
http://dx.doi.org/10.1007/s10916-006-9025-0
[7] N. Sriraam and C. Eswaran, “An adaptive error modeling scheme for
the lossless compression of EEG signals,” IEEE Trans. Inform. Technol.
Biomed., vol. 12, no. 5, pp. 587–594, 2008.
[8] S. Mitra and S. Sarbadhikari, “Iterative function system and genetic
algorithm based eeg compression,” Med. Eng. Phys., vol. 19, no. 7,
pp. 605–617, 1997.
[9] H. Gurkan, U. Guz, and B. S. Yarman, “EEG signal compression based
on classified signature and envelope vector sets,” Online, vol. 37, no. 2,
pp. 351–363, 2009.

1007

[10] M. Nielsen, E. Kamavuako, M. Andersen, M. Lucas, and D. Farina. (2006).
Optimal wavelets for biomedical signal compression. Med. Biological
Eng. Comput.. 44, pp. 561–568. 10.1007/s11517-006-0062-0. [Online].
Available: http://dx.doi.org/10.1007/s11517-006-0062-0
[11] K. Srinivasan and M. Reddy, “Selection of optimal wavelet for lossless
EEG compression for real-time applications,” in Proc. 2nd Nat. Conf.
Bio-Mech. IIT Roorkee, India, Mar. 2009, pp. 241–245.
[12] J. Cardenas-Barrera, J. Lorenzo-Ginori, and E. Rodriguez-Valdivia, “A
wavelet-packets based algorithm for EEG signal compression,” Informat.
Health Soc. Care, vol. 29, no. 1, pp. 15–27, 2004.
[13] V. R. Dehkordi, H. Daou, and F. Labeau. (2011 Nov.). A channel differential EZW coding scheme for EEG data compression. IEEE Trans.
Inform. Technol. Biomed.. 15(6), pp. 831–838. [Online]. Available:
http://dx.doi.org/10.1109/TITB.2011.2171703
[14] G. Higgins, B. McGinley, N. Walsh, M. Glavin, and E. Jones, “Lossy
compression of EEG signals using SPIHT,” Electron. Lett., vol. 47,
no. 18, Sep. 2011.
[15] H. Daou and F. Labeau, “Dynamic dictionary for combined EEG compression and seizure detection,” IEEE J. Biomed. Health Informat.,
vol. 18, no. 1, pp. 247–256, Jan. 2014.
[16] H. Daou and F. Labeau, “Pre-Processing of multi-channel EEG for improved compression performance using SPIHT,” in Proc. IEEE 34th Annu.
Int. Conf. Eng. Med. Biol. Soc., San Diego, CA, USA, 28 Aug.–1 Sep.,
2012, pp. 2232–2235.
[17] K. Srinivasan and M. Reddy. (2010). Efficient preprocessing technique
for real-time lossless EEG compression. Electron. Lett.. 46(1), pp. 26–27.
[Online]. Available: http://link.aip.org/link/?ELL/46/26/1
[18] K. Srinivasan, J. Dauwels, and M. R. Reddy, “Multichannel EEG compression: Wavelet-based image and volumetric coding approach,” IEEE J.
Biomed. Health Informat., vol. 17, no. 1, pp. 113–120, Jan. 2013.
[19] J. Vaisey and A. Gersho, “Image compression with variable block size
segmentation,” IEEE Trans. Signal Process., vol. 40, no. 8, pp. 2040–
2060, Aug. 1992.
[20] M. T. Orchard and G. J Sullivan, “Overlapped block motion compensation:
An estimation-theoretic approach,” IEEE Trans. Image Process., vol. 3,
no. 5, pp. 693–699, Sep. 1994.
[21] B. S. Atal and S. L. Hanauer, “Speech analysis and synthesis by linear
prediction of the speech wave,” J. Acoust. Soc. Amer., vol. 50, no. 2B,
pp. 637–655, 1971.
[22] R. Pascual-Marqui, “Review of methods for solving the EEG inverse
problem,” Int. J. Bioelectromagn., vol. 1, no. 1, pp. 75–86, 1999.
[23] J. Ebersole and T. Pedley, Eds., Current Practice of Clinical Electroencephalogrpahy, 3rd ed. Philadelphia, PA, USA: Lippincott Williams &
Wilkins, 2005.
[24] L. Zhukov, D. Weinstein, and C. Johnson, “Independent component analysis for EEG source localization,” IEEE Eng. Med. Biol. Mag., vol. 19,
no. 3, pp. 87–96, May/Jun. 2000.
[25] J. Mosher, R. Leahy, and P. Lewis, “EEG and MEG: Forward solutions for
inverse methods,” IEEE Trans. Biomed. Eng., vol. 46, no. 3, pp. 245–259,
Mar. 1999.
[26] E. M. R. Oostenveld, P. Fries, and J.-M. Schoffelen, “FieldTrip: Open
source software for advanced analysis of MEG, EEG, and invasive electrophysiological data,” Comput. Intell. Neurosci., vol. 2011.
[27] E. Niedermeyer and F. Da Silva, Eds., Electroencephalography, vol. 7,
5th ed. Philadelphia, PA, USA: Lippincott Williams & Wilkins, 2005.
[28] Swartz Canter for Computational Neuroscience. (2014). EEG Channel Location Files. [Online]. Available: http://sccn.ucsd.edu/eeglab/
channellocation.html
[29] R. N Kavanagk, T. M Darcey, D. Lehmann, and D. H Fender, “Evaluation
of methods for three-dimensional localization of electrical sources in the
human brain,” IEEE Trans. Biomed. Eng., vol. BME-25, no. 5, pp. 421–
429, Sep. 1978.
[30] M. Fuchs, J. Kastner, M. S. Hawes, and J. Ebersole, “A standardized
boundary element method volume conductor model,” Clinical Neurophysiology, vol. 113, no. 5, pp. 702–712, May 2002.
[31] A. Delorme, “Dipfit plug-in: Equivalent dipole source localization of independent components—DIPFIT validation study using the spherical head
model,” http://sccn.ucsd.edu/wiki/A08:_DIPFIT, February 2013.
[32] M. Jaako and R. Plonsey, Eds., Bioelectromagnetism, Principles and Applications of Bioelectric and Biomagnetic Fields. London, U.K.: Oxford
Univ. Press, 1995.
[33] Z. Lu, Y. Kim, Z. Lu, D. Y. Kim, and W. Pearlman, “Wavelet compression of ECG signals by the set partitioning in hierarchical trees (SPIHT)
algorithm,” IEEE Trans. Biomed. Eng., vol. 47, no. 7, pp. 849–856, Jul.
2000.

1008

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

[34] M. Pooyan, A. Taheri, M. Moazami-Goudarzi, and I. Saboori, “Wavelet
compression of ECG signals using SPIHT algorithm,” Int. J. Signal Process., vol. 1, no. 3, 2004.
[35] S. M. L. Qiang and R. Sclabassi, “Decorrelation of multichannel EEG
based on Hjorth filter and graph theory,” in Proc. 6th Int. Conf. Signal
Process., vol. 2, 2002, pp. 1516–1519.
[36] A. L. Goldberger, L. A. N. Amaral, L. Glass, J. M. Hausdorff, P. C. Ivanov,
R. G. Mark, J. E. Mietus, G. B. Moody, C.-K. Peng, and H. E. Stanley,
“Physiobank, physiotoolkit, and physionet: Components of a new research
resource for complex physiologic signals,” Circulation, vol. 101, no. 23,
p. 215–220, 2000.
[37] A. Delorme, G. Rousselet, M. Mace, and M. Fabre-Thorpe, “Interaction
of bottom-up and top-down processing in the fast visual analysis of natural
scenes,” Cognitive Brain Res., vol. 19, pp. 103–113, 2004.
[38] H. Daou and F. Labeau, “Performance analysis of a 2-D EEG Compression
Algorithm using an Automatic Seizure Detection System,” presented at
the Asilomar Conf. on Signals, Systems, and Computers, Pacific Grove,
CA, USA, 4–7 Nov. 2012.

Hoda Daou (S’02) received the Bachelor of Computer and Communication Engineering degree from
Notre Dame University, Zouk Mosbeh, Lebanon, in
2006, and the Master of Engineering in Electrical
and Computer Engineering degree from the American University of Beirut, Beirut, Lebanon, in 2008.
She is currently working toward the Ph.D. degree in
the Department of Electrical and Computer Engineering, McGill University, Montreal, QC, Canada.
From June 2008 to June 2009, she worked as a
Customized Performance Solutions Technical Architect at Nokia Siemens Networks, Beirut. She is also a Research Assistant in
the Department of Electrical and Computer Engineering, McGill University.
Her current research interests include signal processing, data compression, data
mining and machine learning.

Fabrice Labeau (SM’07) received the Electrical
Engineer, the Diplôme d’études spécialisées en Sciences Appliquées, orientation Télécommunications,
and the Ph.D. degrees from Université catholique
de Louvain (UCL), Louvain-La-Neuve, Belgium, in
1995, 1996, and 2000, respectively.
He is an Associate Professor with the Electrical
and Computer Engineering Department, McGill University, Montreal, QC, Canada, where he holds the
NSERC/Hydro-Qubec Industrial Research Chair in
Interactive Information Infrastructure for the Power
Grid. From 1996 to 2000, he was with the Communications and Remote Sensing
Laboratory, Universit Catholique de Louvain, Belgium. In 1999, he was a Visiting Scientist at the Signal and Image Department (TSI), ENST Paris, France.
His research interests include applications of signal processing to healthcare,
power grids, communications, and signal compression. He has authored more
than 150 papers in refereed journals and conference proceedings in these areas.
He has held several administrative and management positions at McGill University, including Associate Department Chair, Associate Dean, Interim Chair,
and Acting Dean. He currently is the Associate Dean for Faculty Affairs at the
McGill Faculty of Engineering.
He is currently the elected President of the IEEE Vehicular Technology
Society. He was part of the organizing committee of ICASSP 2004, Montreal,
Canada, and is/was a Technical Program Committee Cochair for the IEEE Vehicular Technology Conference in the Fall of 2006 and 2012, and the IEEE
International Conference on Image Processing 2015.

