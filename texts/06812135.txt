IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

677

On the Monte-Carlo Expectation Maximization
for Finding Motifs in DNA Sequences
Aniruddha Maiti and Anirban Mukherjee, Member, IEEE

Abstract—Finding conserved locations or motifs in genomic sequences is of paramount importance. Expectation maximization
(EM)-based algorithms are widely employed to solve motif finding problems. The present study proposes a novel initialization
technique and model-shifting scheme for Monte-Carlo-based EM
methods for motif finding. Two popular EM-based motif finding
algorithms are compared to the proposed method, which offers
improved motif prediction accuracy on a synthetic dataset and a
true biological dataset.
Index Terms—DNA, expectation maximization (EM), MonteCarlo, motif finding.

I. INTRODUCTION
HE motif finding problem has received intense attention
in the field of computational biology over the last two
decades given the availability of large number of genomic sequences. For DNA, short conserved patterns or motifs can represent transcription factor binding sites. For RNA, it may represent splice junctions [1]. For proteins, the motifs may represent
binding domains. Thus, discovering short conserved substrings
in biological sequences can lead to a better understanding of
transcriptional regulation, mRNA splicing and formation, and
classification of protein complexes [2], [3].
In the motif finding problem, the objective is to locate short
conserved substrings or motifs in a set of long strings. A more
general and difficult problem is to find short substrings which
are almost conserved, e.g., the (l, d) motif finding problem.
Given a set S of N number of sequences of length Li (i ∈
{1, 2, . . . , N }), the task is to find a substring m of length l that
appears frequently in S accompanied by mutations in at most
d random positions. An example is (15, 4) motif finding problem. The problem is a difficult one as two mutated versions of
substring m can differ in at most 2d (8) positions. This problem
is commonly known as the challenge problem [4]. Later Buhler
and Tompa provided mathematical analysis explaining the inherent intractability of the problem [5]. Given the intractability
and the importance of the motif finding problem in the context of transcription factor binding site identification, a number of computational tools (such as MEME [6], Monte-Carlo
EM Motif Discovery Algorithm (MCEMDA) [7], Projection

T

Manuscript received March 5, 2014; revised May 1, 2014; accepted May 2,
2014. Date of publication May 8, 2014; date of current version March 2, 2015.
This work was supported by the Council of Scientific and Industrial Research,
New Delhi, India, via Grant 22(0551)/11/EMR-II.
The authors are with the Department of Electrical Engineering, Indian Institute of Technology Kharagpur, Kharagpur 721302, India (e-mail:
aniruddha.maiti87@gmail.com; anirban@ee.iitkgp.ernet.in).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2322694

[5], Weeder [8], MUSCLE [9], ClustalW [10], BioProspector
[11], etc.) have been developed to solve the problem. Among
them, the MEME and the MCEMDA are expectation maximization (EM)-based methods. The projection method uses a random
projection technique to identify a favorable starting seed for EMbased algorithms. The MUSCLE and the ClustalW are multiple
sequence alignment algorithms. MUSCLE incorporates a fast
distance estimation using k-mer counting, a progressive alignment using a profile function, called the log-expectation score,
and a refinement technique using a tree-dependent restricted partitioning [9]. ClustalW is also a progressive multiple sequence
alignment algorithm that uses sequence weighting, positionspecific gap penalties [10]. The BioProspector uses Markov
background models [11] to search regulatory sequence motifs.
It uses the Gibbs sampling strategy.
Here, in this study, a modification is proposed in EM-based
motif finding methods. It is shown that these EM-based algorithms for motif finding can be made more effective for de-novo
motif discovery by employing the proposed modifications.
The rest of the paper is organized as follows. In Section II,
the motif discovery problem and two related EM-based methods are described. The motivation and the methodology of the
proposed study are discussed in Section III. In Section IV, the
implementation technique of the proposed method is described.
These two sections provide the principal contribution of this
study. Experimental results are presented in Section V.. Finally,
Section VI concludes the paper.
II. PRELIMINARIES
The EM method was introduced in conserved site identification problem by Lawrence and Reilly[12]. This method
identifies motifs that occur only once in each sequence. Later
Bailey and Elkan provided a more generalized model for
EM-based motif identification problem [6]. This iterative
model is effective given a reasonably good starting point. In
[5], Buhler and Tompa proposed a locality-sensitive hashing
method called random projection to pinpoint a good starting
point for EM-based algorithms. In [13], it is shown that the
performance of uniform projection is better than that of the
random projection. If the initial guess about the starting point of
EM is not reasonably good enough, the deterministic algorithm
converges quickly to a local optimum point. To ameliorate
this limitation, the Monte-Carlo expectation maximization
(MC EM) method is proposed by Wei and Tanner, where
expectation step is calculated through Monte-Carlo simulation
[14]. This incorporates randomness in EM algorithm. In
MCEMDA [7], this concept is used to discover motifs in
DNA sequences. In this section, the problem of multiple local

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

678

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

alignment for motif discovery is discussed along with the
two related algorithms, the EM [6] and the MCEMDA [7].

A. Multiple Local Alignment for Motif Discovery
Let S = {S1 , S2 , . . . , SN } be the dataset containing N sequences of length Li (i ∈ {1, 2, . . . , N }). Let Sij ∈ Σ denote
the residue symbol at position j of the ith sequence Si ,
wherein Σ is the alphabet of biomolecules. For DNA sequences
Σ = {A, T, G, C} and |Σ| = 4. If one occurrence per sequence
model (oops model) is assumed, then N motifs are present in
S. This model can be generalized to any number of motif occurrences in S. In the stochastic model of sequence generation,
the assumption is that the w-mer motifs and the background
(nonmotif portion of sequence) are generated from different
A-T-G-C distributions, i.e., the ratio of different residues in wmer motifs present in S is essentially different from that of the
background. The background is drawn from an independent and
identical multinomial distribution θ0 . The residues constituting
the w-motifs in S are drawn from an independent but not identical multinomial distribution θj , where j (1 ≤ j ≤ w) is the
position of the residue in the motif. The A-T-G-C distributions,
θj vary with j. Any motif, present in S, can be considered as a
sequence whose residues are drawn from a product of w multinomial distributions : Θ = [θ1 , θ2 , . . . , θw ]. The joint distribution
Θ together with background distribution θ0 completely characterize the position weight matrix (PWM) whose each element
wj k = log (θj k /θ0k ) (where k ∈ Σ) is a measure of dissimilarities between the motif model at jth position and the background
model. All EM-based motif finding algorithm iteratively update
PWM to maximize this dissimilarity. Let Ai ∈ {0, 1}L i −w +1
be the indicator vector containing the information of motif start
locations in sequence Si . The value of jth element of Ai , Aij ,
is one if a w-mer motif starts from location j in Si . The value
is zero otherwise. A = [AT1 , AT2 , . . . , ATN ] represents a possible local alignment
and the total number
of such possible
LS
 of
L i −w +1
i −w +1
.
Here,
|A
|
=
Ail , is
alignments is N
i
i=1
l=1
|A i |
the total number of motifs present in sequence Si . For the oops
model, |Ai | = 1 and a single variable ai is sufficient to store the
information. The variable ai = j if Aij = 1. For a given A, the
model parameters θ0 and Θ can be computed. From θ0 and Θ,
one can infer about the extent to which the motif is conserved.
For example, a score Qn can be assigned to an alignment A as
a measure of conservation of the motifs such that

Qn = |A|.


k ∈Σ

θ0k ln (θ0k ) + |A| ·

w 


θj k ln θj k .

(1)

j =1 k ∈Σ

The first term corresponds to the background uncertainty and the
second term relates to that of the motif model. The objective of
a motif finding algorithm is to find a suitable alignment, i.e., A,
or equivalently a model Θ and θ0 , so that the corresponding Qn
of the model is maximized. Given the large alignment space and
NP-completeness of the problem [15], EM-based algorithms are
employed to solve this problem iteratively.

B. EM in Motif Finding
The EM algorithm is employed to learn the parametric model
of a partially observable stochastic process. In the multiple local
alignment problem, the dataset S containing the sequences is
the observable data. The indicator variable A is not observable
and corresponds to hidden data. The model parameters are Θ
and θ0 . The objective here is to find a model that maximizes the
marginal probability P (S|Θ, θ0 ). A simple EM-based model
is presented in this section to formalize the concept. A random
alignment is taken at the initialization step with a random choice
of A. From A, the model parameters, Θ and θ0 , are updated.
The model update scheme can be simplified by counting AT-G-C residues to compute the elements of Θ and θ0 . From
these model parameters, the expected value of each element
of A is calculated. This two-step iteration goes on until the
convergence. At the rth iteration, in E-step, the expected value
of Aij is calculated as follows:
Âij = E[Aij |Si , Θr , θ0r ] = 1 × P r(Aij = 1 | Si , Θr , θ0r )


P r(Aij = 1 | Θr , θ0r )
r
r
= P r(Si | Aij = 1, Θ , θ0 )
. (2)
P r(Si |Θr , θ0r )
In (2), Aij ∈ {0, 1} is a binary variable. In the final step, Bayes’
Theorem is used. As motifs occur independently in different
sequences, Aij depends only on the sequence Si . The numerator inside the bracket is the prior probability that motif starts
at position j, which is taken to be th same as (Li − w + 1)−1
∀j. The model parameters in rth iteration are denoted as Θr
and θ0r . Without calculating the bracketed term in (2) explic
itly, 
at each step, Âij is divided by ∀j Âij using the fact
that ∀j P r(Aij = 1 | Si , Θr , θ0r ) = 1. The likelihood is determined as follows:
w 	
	 	 δ Sc 	
δ Sb
θ0c i u
θm bi j + m −1 .
P r(Si | Aij = 1, Θr , θ0r ) =
u ∈μ c∈Σ

m =1 b∈Σ

(3)
In (3), μ represents the set of indices from j to j + w − 1 in Si . δ
is the Kronecker delta function, δba = 1 if a = b, and δba = 0, otherwise. In (3), given a sufficiently large value of the length Li of
the ith sequence, the first term corresponding to the background
model remains almost constant. So, it can be assumed that
δ Sb


i j + m −1
P r(Si | Aij = 1, Θr , θ0r ) ∝ w
. The prom =1
b∈Σ θm b
portionality constant is taken care of during the normalization
process of Âij .
In the M-step, Θr+ 1 is found for which the probability
P r(S, Â | Θ) (or equivalently its logarithm) is maximized. Here,
the term θ0 is omitted as it is nearly constant. The model parameter is updated for 1 ≤ t ≤ w and b ∈ Σ as follows:
r +1
θtb
=

N L −w +1
1  i
Aij δSb i , j + t −1 .
N i=1 j =1

(4)

In (4), a weighted sum of frequencies of different b ∈ Σ is
used. After a few iterations (with a good initialization) the algorithm will converge toward the true motif start locations. An advanced EM-based algorithm such as MEME uses word statistics
in order to identify a motif having best statistical relevance with

MAITI AND MUKHERJEE: ON THE MONTE-CARLO EXPECTATION MAXIMIZATION FOR FINDING MOTIFS IN DNA SEQUENCES

679

Fig. 2. Improvement of the Q-function during first 400 iterations. (Number
of MC simulation in each stage: m = 3).
Fig. 1. Visualization of the MCEMDA as a Markov chain (number of MC
simulation is 3).

respect to the background [6]. The parameter θ0 is an example
of low-order background model that considers the frequency of
individual letters. Additionally, MEME takes into account the
frequency of words (combination of A-T-G-C). In a sense, it
uses higher-order Markov background model and, thus, is more
effective than a naive EM-based motif finding algorithm.
C. Monte-Carlo EM Motif Discovery Algorithm
In MCEMDA [7], randomness is introduced in the M-step by
carrying out Monte-Carlo simulation to update Θ. The deterministic averaging makes EM a local greedy search algorithm.
The randomness, introduced in MCEMDA, may help the algorithm to escape from local optima and, thus, may increase the
chance of producing a better solution as described in [7]. The
introduction of this randomness makes MCEMDA more effective compared to the conventional EM algorithm. In an M-step
of the MCEMDA, an integer value is assigned to ai according
to the distribution

 θ r δ Sb


u (ai = l | Si , Θ ) =
r

w
j =1

jb

b∈Σ


L i −w +1 w
t=1

j =1

θ 0r b


b∈Σ

i , l + j −1


 θ r δ Sb
jb
θ 0r b


i , t + j −1

(5)
where 1 ≤ ai ≤ Li − w + 1 (refer to Section II-A). Then, Θ is
updated as follows:

N 
 b
δ
S i , a i + j −1 + βj b
i=1



 
 . (6)
θjrb+1 = 
N
b
β
δ
+
j
b
i=1
b∈Σ
b∈Σ
S i , a + j −1
i

The vector β serves as a pseudocount to overcome the problem of zero count. It can also be thought of as a Dirichlet prior.
The value of ai is drawn multiple times (say m times) from
the distribution given in (5). In each simulation, Θ is calculated
according to (6), and the Q-function in (1) is evaluated. If Θ
corresponding to the best Q-function is stored and is used in the
next iteration, then the strategy is called as m-best strategy. On
the other hand, if an average of all model parameters is used
in the next iteration, then the strategy is called as m-average
strategy. The m-average strategy is computationally demanding
and slower than the m-best strategy [7].
1) MCEMDA as a Markov Chain: MCEMDA can be visualized as a Markov chain as shown in Fig. 1. At each step, the

process has the chance of taking new direction depending on
the distribution in (5), and thus it has the potential of avoiding
local optima. At each step, the chain can follow m- different
directions as opposed to a single direction of deterministic EM
algorithm. R such Markov chains are generated (each with different seed) at the beginning to get n ≤ R alignments at the end
of the chains. Multiple Markov chains may produce identical
alignment. Furthermore, two alignments may not be identical
but they can be almost similar to each other (owing to the phase
shift in motif model [7]). For this reason, a clustering algorithm
is used in MCEMDA using normalized longest common block
as a measure of the distance metric between two alignments.
Each cluster center is considered as a potential motif.
III. MOTIVATION AND METHODOLOGY
Consider a family of Markov chains (Hi i ∈ {1, 2, . . . , R}),
shown in Fig. 1, corresponding to the MCEMDA. The transitions and states are denoted by arrows and circles, respectively.
The number (m) of the MC simulation in each iteration is taken
as 3. During first few iterations, Hi visits states that are unreliable in nature. These initial stages, called the burn-in phase,
are crucial for Hi in terms of convergence. If it fails to track a
suitable path, it may never converge to the actual solution [7].
This situation is shown in Fig. 2. The dynamics of the goodness measure Qn is shown for some of the Markov chains with
different initialization. The Markov chain converges to the true
motif model if Qn ≥ QT . Here, the true motif model refers
to the hidden parameter Θ to be estimated, or a neighborhood
of Θ that produces the same motif as does Θ. Out of R such
chains, only a few converges to the true motif model (or to its
phase-shifted version).
As a result, a clustering algorithm is employed at the end of
all Hi (i ∈ {1, 2, . . . R}). It would be better off stopping some
unreliable chains at the earlier stages. If the unpromising chains
are stopped after first few iterations, then the computational cost
reduces. Additionally, the requirement of the clustering stage
may be relaxed. This study focuses on an algorithm which will
stop some unpromising Markov chains at the beginning.
A. Simplified Q Function
The negative entropy like goodness measure of a motif model
is shown in (1). Given the motif length, w  Li , the distribution of background residues almost constant for different motif

680

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

positions. As a result, the first term in (1) may be omitted and
the modified measure is taken to be
w 

Q = |A|.
θj k ln θj k .
(7)
j =1 k ∈Σ

This modification reduces the computation in each iteration
while maintaining the true purpose of the measure.
B. Selection of Promising Markov Chains
The determination of a suitable threshold QT is essential
to identify and stop the unpromising Markov chains. However, finding such hard threshold of Q proves to be cumbersome task. To overcome this limitation, the proposed method
employs a greedy scheme by initiating itrm ax number of different Markov chains, Hi (i ∈ {1, 2, . . . , itrm ax }), with random seeds. After the first q iterations (called look-ahead) in
each Hi , the corresponding goodness measure Q is assumed
to be Qi (i ∈ {1, 2, . . . , itrm ax }). Following this look-ahead
stage, the algorithm proceeds with the best chain Hm , where
m = argmax∀i∈{1,2,..., itr m a x } Qi . The rest of the chains are discarded.
C. Goodness Measure: ψ
In order to validate the effectiveness of the proposed method,
it is assumed that the true motif start locations are available.
Given these locations, the effectiveness can be measured from
γN ]T be the known true motif
Aik . Let γ = [γ1 γ2 · · ·
start position vector. In the final iteration, the distribution of
the index variable ai is assumed to be u [refer to (5)]. Let
argmax
u (ai = l | Si , Θ) be the most probable
γi	 =
∀l∈{1, 2,..., L i −w +1}

motif start location of ith sequence (i ∈ {1, 2, . . . , N }). The
effectiveness of the proposed algorithm can beevaluated using
N
	 T
γ and γ 	 = [γ1	 γ2	 · · · γN
] . A score, Ψ = N1
i=1 ψi , is used
in this study, where
 (w −|γ 	 −γ |)
i
i
if |γi	 − γi | ≤ w
w
(8)
ψi =
0
otherwise.
This metric is similar to the nucleotide-level accuracy (nla)
[7], which is defined as follows:

Fig. 3. Convergence of Monte-Carlo EM-based algorithm to the shifted version of true motif model. (a) Planted (15, 3) motif finding problem with m = 1.
(b) Planted (15, 3) motif finding problem with m = 3. (c) Planted (12, 2) motif
finding problem with m = 1. (d) Planted (12, 2) motif finding problem with
m = 3.

|N |

1  γi	 ∩ γi
nla(γ , γ) =
|N | i=1 w
	

(9)

where γi	 ∩ γi represents the size of the overlapping block between predicted and observed motif.
D. Overcoming the Limitation of the Phase Shift
The EM-based algorithms have a propensity to converge to a
shifted version of the true motif model. As any shifted version of
the motif model also possesses high score compared to any random alignment, more often than not the EM-based algorithms
converge to a shifted version of the true motif model. In a sense,
the shifted versions of the true motif model form local minima
in motif alignment space. EM-based algorithms have inherent

inability to come out of these local minima. To illustrate this
phenomenon, two relatively easy planted (l, d) motif finding
problems are considered. The dynamics of the scoring function,
Ψ, is shown in Fig. 3 taking different (l, d) and number of MC
simulation (m). It is observed that the algorithm frequently converges to the shifted version of the true motif model. Here, the
true motif model is considered as the motif model that produces
the correct motif without any mutation. The A-T-G-C distribution of the true motif model need not necessarily be the same
as that in the planted motifs. As long as the probability of the
correct nucleotide is more than the rest of the nucleotide in a
particular motif position, the model will produce the correct

MAITI AND MUKHERJEE: ON THE MONTE-CARLO EXPECTATION MAXIMIZATION FOR FINDING MOTIFS IN DNA SEQUENCES

Fig. 5.

681

Improvement in (15, 3) motif finding problem.

TABLE I
CONVERGENCE TO THE TRUE MOTIF MODEL DUE TO SHIFTING
ψc
True motif
Monte-Carlo EM solution
Proposed method

Fig. 4.

0.93
1

Flowchart to overcome the phase shift problem.

motif. In Fig. 3, the false motif model refers to a model that
does not produces the correct motif or its phase-shifted version.
As this phase-shift phenomenon leads to erroneous results, it
would be better if some measure is taken to force the algorithm
to converge to the true motif model. This is accomplished by
shifting the motif model Θ to the left or to the right with a
given probability pshift . After a few iterations, if there is no
improvement in the goodness measure Q, then the algorithm
will return to the original Θ. The scheme is illustrated in Fig. 4,
where the proposed phase-shifting mechanism is marked by the
discontinuous lines. A better but complex option might be to
make the shifting probability pshift dependent on Q because the
shifting of the current motif model is only helpful when the
algorithm reaches in the neighborhood of the true motif model.
But, in this study, pshift is kept constant to avoid complexities.
If the problem is difficult (i.e., motifs are less conserved), then
it is likely that all the EM-based methods will end up producing
a wrong result. But somehow, if the algorithm manages to reach
to a nearby model, the proposed shifting mechanism will look
around the shifted versions of the current motif model in the
hope of finding a better alignment.
E. Example of Shifted Θ
Let the motif length w be 5. The model parameter at tth
iteration is as follows:
(t)

(t)

(t)

(t)

(t)

Θ(t) = [θ1 θ2 θ3 θ4 θ5 ].
The left-shifted and right-shifted model parameters at tth
(t)
(t)
(t)
(t)
(t)
(t)
iteration are ΘLeft = [θ2 θ3 θ4 θ5 v l ] and ΘRight =
(t)

CGCTAAATGAGCTAA
- GCTAAATGAGCTAAT
CGCTAAATGAGCTAA

(t)

(t)

(t)

[v r θ1 θ2 θ3 θ4 ], respectively. The empty columns (that
arise due to this shifting) are padded by a nonnegative random
x T
] , x ∈ {l, r}, such that v x 1 =
vector
v x = [vAx vTx vCx vD

x
k ∈Σ |vk | = 1. If the goodness measure Q is not improved due
to this shifting at (t + t	 )th iteration, the algorithm retains to

the prestored original motif model, obtained at tth iteration. In
Fig. 5, a (15, 3) motif finding problem with background length
of 300 is considered to demonstrate the effectiveness of the proposed shifting technique. It can be observed that the proposed
methodology helps in avoiding the local minima that arise due
to the high alignment score of the phase-shifted version of the
true motif model. The proposed method converges to the true
motif, while the MC EM converges to a phase-shifted version of
the true motif. This is reflected on the final value of the scoring
function Ψ as shown in Fig. 5. Here, the proposed method offers
the exact match to the true motif with Ψ = 1.0, whereas the MC
EM offers Ψ = 0.8 after 500 iterations. In Table I, the true motif
is shown in the first row. The final motif produced by the MC
EM and the proposed method are shown for comparison. The
proposed method converges to the true motif, whereas the MC
EM converges to a phase-shifted version (shift = 1) of the true
motif of length 15. The values of ψc in (8) for the consensus
sequences are 1 and 15−1
15 = 0.93 for the proposed method and
the MC EM, respectively.
IV. IMPLEMENTATION
Given a set of N sequences S, motif width w, and the number of MC simulations m, the proposed algorithm starts by
randomly initializing an alignment A(0) . The look-ahead is assumed to be tm ax . The pseudocode of the proposed initialization
scheme (described in Section B.) is given in Algorithm 1 . It
finds a promising motif model ΘBEST .
Once the best initialization is identified, then the Markov
chain is started using the initialized motif model ΘBEST . At
each step, when the motif model Θ(t) is updated, the algorithm
calls a function that shifts Θ(t) to the left or right, with a
given probability pshift as described in Section E.. This shifting
mechanism is performed by the function ModelShift(), and its
pseudocode is given in Algorithm 2 .
The proposed method employs both the initialization and
shifting scheme. The corresponding pseudocode is given in

682

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Algorithm 1 Proposed Initialization
1: Initialize : itr m ax , tm ax
2: for itr = 1 to itr m ax do
3:
Initialize : S, A(0) , t ← 0, Qm ax ← −∞, Q ← −∞
4:
Evaluate Θ(0) and Q(0) from A(0)
5:
Θitr ← Θ(0) ; Qm ax ← Q(0)
6:
while t ≤ tm ax do
7:
t←t+1
8:
for r = 1 to m do
9:
for i = 1 to N do
10:
draw a sample according to (5)
11:
end for
12:
update Θ(t,r) and Q(t,r )
13:
end for
14:
evaluate Θ(t) and Q(t) using m-best strategy
15:
if Q(t) ≥ Qm ax then
16:
Qm ax ← Q(t) and Θitr ← Θ(t)
17:
end if
18:
end while
19:
if Qm ax ≥ Q then
20:
ΘBEST ← Θitr
21:
Q ← Qm ax
22:
end if
23: end for
24: Output the best initialized motif model Θ
BEST
Algorithm 2 Proposed Shifting: ModelShift(Θ(t) , Q(t) )
Input : motif model, Θ(t)
(t)
2: ΘBEST ← Θ(t)
(t)
3: With a given probability pshift :Θ(t) ← ΘLeft or Θ(t) ←
(t)
ΘRight
1:

(t+t 	 )

if Qn
≤ Qn at the end of (t + t	 )th iterations then
(t)
(t)
5:
Θ ← ΘBEST
6: end if
7: Return: Motif Model Θ(t)
4:

Fig. 6.

(t)

Simplified description of the proposed algorithm.

Algorithm 3 . The entire work flow, along with the end clustering, is shown in Fig. 6.
The algorithm, described here, is available at http://
www.facweb.iitkgp.ernet.in/manirban/MSPML-soft.html.
A. Computational Leverage
In MCEMDA, let the total number of iterations performed
for a Markov chain Hi , i ∈ {1, 2, . . . , R}, be L. If a single iteration of MC EM algorithm is considered as a unit computation,
then the total computational cost required, before employing any
clustering algorithm, is RL. On the other hand, in the proposed

Algorithm 3 Proposed Algorithm of Entire Work Flow
1: Input : initialized Motif Model Θ
BEST Output : Final Motif
Model Θ
2: Initialize : tm ax
m ax
3: Θ(0) ← Θ
← −∞
BEST ; Q
4: while t ≤ tm ax do
5:
t+1←t
6:
for r = 1 to m do
7:
for i = 1 to N do
8:
draw a sample according to (5)
9:
end for
10:
update Θ(t,r) and Q(t,r )
11:
Θ(t,r) ← ModelShift(Θ(t,r) , Q(t,r ) )
12:
end for
(t)
13:
evaluate Θ(t) and Qn using m-best strategy
14:
if Q(t) ≥ Qm ax then
(t)
15:
Qm ax ← Qn and Θ ← Θ(t)
16:
end if
17: end while
18: Output Final Motif Model Θ

method, if a q-step (q<L) look-ahead and a total R	 randomly
initialized Markov chains are employed, then the total computational cost is R	 q + (R	 /10)(L − q). The parameters used in
MCEMDA are R = 1000 and L = 100. If R	 and q are chosen
as 1000 and 15, respectively, then proposed method offers an
improvement in computational cost approximately by a factor
of 4.
V. RESULTS AND ANALYSIS
A. Dataset
To validate the efficiency of the proposed algorithm, two
types of datasets are used. The first one is a synthetic dataset
having different instances of randomly generated (l, d) motifs.
A second dataset is prepared taking motif instances from the
JASPAR website [16].
1) Synthetic Dataset: The procedure to test the effectiveness
of any motif finding algorithm is to use a synthetic dataset
containing (l, d) motif instances and then apply the algorithm to the dataset to verify its efficiency. Although there
are some fundamental differences between a synthetic
dataset and a true biological dataset, this type of testing
platform approximately provides a good measure of effectiveness of motif finding algorithms. Motif instance of
the dataset, used in this study, is generated by taking a
random l-length genomic sequence and mutating at most
d positions randomly. This motif instance is implanted in
a random location of a random background sequence with
a fixed GC fraction. Different combinations of l, d and
background length are used to vary the difficulty level of
the motif finding problem.
2) JASPAR Dataset: This dataset is prepared by taking experimentally verified biological motif instances (transcription
factors of Eukaryotes) from JASPAR website [16]. These

MAITI AND MUKHERJEE: ON THE MONTE-CARLO EXPECTATION MAXIMIZATION FOR FINDING MOTIFS IN DNA SEQUENCES

TABLE II
PERFORMANCE ON SYNTHETIC DATASET
(l, d)

Background

motif

length

TABLE III
PERFORMANCE ON JASPAR DATASET: LARGE GROUP

Av. Score Ψ in 100 runs
Conventional EM

MC EM

Av. Score Ψ in 100 runs

Proposed method
Large Group

(10, 2)
(10, 2)
(10, 2)
(11, 2)
(11, 2)
(11, 2)
(12, 3)
(13, 3)
(13, 3)
(14, 4)
(14, 3)
(15, 4)
(15, 4)
(16, 5)
(16, 5)
(17, 5)
(18, 5)
(19, 6)
(20, 6)

200
300
500
300
400
500
300
300
500
300
500
300
500
300
500
500
500
500
500

0.26
0.11
0.02
0.35
0.30
0.15
0.09
0.12
0.07
0.18
0.21
0.31
0.25
0.10
0.07
0.20
0.15
0.15
0.19

0.35
0.18
0.09
0.58
0.49
0.39
0.19
0.31
0.28
0.36
0.38
0.41
0.33
0.18
0.16
0.30
0.17
0.27
0.36

683

0.37
0.28
0.14
0.65
0.53
0.38
0.23
0.39
0.28
0.32
0.41
0.49
0.41
0.23
0.20
0.38
0.22
0.31
0.43

instances are then implanted in random background sequences at random locations. These backgrounds act as
promoter regions. The GC fraction of promoter regions
and the background length are taken as 0.45 and 500,
respectively.
B. Comparison With Other EM-Based Algorithms
There are multiple methods available for finding motifs, e.g.,
MEME [6], MCEMDA [7], Random Projection [5], Weeder [8],
MUSCLE [9], ClustalW [10], [11], etc. Out of these, MCEMDA,
an excellent algorithm in itself, outperforms the other algorithms
in most of the cases [7]. MCEMDA and a simple EM-based
method are selected for comparison. For difficult motif finding
problems, the probability of EM-based algorithms to converge
to an erroneous motif model is high. For this reason, conventionally, the EM-based algorithms are applied to the dataset multiple
times by taking different seeds and at the end, a clustering algorithm is applied to identify the largest cluster in motif model
space. The intuition is that all spurious motif model will remain
scattered in motif model space but the optimal and near optimal
solutions will form a larger cluster. This clustering technique improves the probability of detection of the true motif model. As
clustering is a form of hard quantization technique, the proposed
method is compared to other two EM-based methods without
this clustering stage. If all the algorithms are EM-based, the
calculation of the average score after a fixed number of iterations may suit the purpose more appropriately. It may be noted
that the use of identical clustering technique at the final stage
can equally improve the performance of every EM-based algorithms, discussed in Section E.
1) Results on Synthetic Dataset: The synthetic dataset with
various combinations of planted motifs is used to validate the
effectiveness of the proposed method. The average scores of
each algorithm for 100 independent trials are given in Table II.

MA0094
MA0036 MA0075
MA0037 MA0080 MA0086
MA0089
MA0098 MA0103 MA0104
MA0081 MA0093 MA0096
= MA0067
MA0002 MA0054 MA0077
MA0078 MA0084 MA0118
MA0001 MA0015 MA0028
MA0038
MA0052 MA0061 MA0092
MA0005 MA0009
MA0019 MA0041 MA0048
MA0083 MA0091 MA0097
MA0114
MA0029 MA0069 MA0072
MA0082
MA0116
MA0060
MA0065 MA0066

Motif
length

Conventional
EM

MonteCarlo EM

Proposed
method

4
5
6

0.02
0.09
0.14

0.06
0.12
0.24

0.05
0.12
0.21

7
8
9

0.36
0.10
0.12

0.46
0.07
0.26

0.47
0.10
0.29

10

0.11

0.29

0.36

11
12

0.16
0.21

0.31
0.36

0.35
0.39

13
14

0.29
0.30

0.42
0.52

0.49
0.54

15
16
20

0.35
0.42
0.36

0.51
0.48
0.54

0.59
0.53
0.60

Note: Motifs of identical length are grouped together.

It may be noted that the scores can be improved by employing
a suitable clustering technique. The conventional EM, used for
comparison, is a simplistic one. The score might be better in
case of a sophisticated EM-based algorithm, such as MEME,
where additional statistical information (such as word statistics)
is incorporated [6].
2) Results on JASPAR Dataset: The results on JASPAR
dataset are shown in the Tables III and IV. The dataset is divided into two groups such that Table III shows the results from
sequences with N ≥ 25. Table IV shows the results for smaller
(N < 25) groups. In both the cases, the Monte-Carlo method
outperforms the conventional EM. This is due to the advantage
of Monte-Carlo method in avoiding local minima. The proposed
initialization and model-shifting mechanism, when applied together with MC EM, shows another level of improvement in
most of the cases.
It may be noted that the proposed method has better performance as the motif length increases. This phenomenon may be
attributed to the fact that for smaller motifs, the deviation of the
current motif model due to model shifting is relatively more than
that in longer motif model. It may be inferred that the advantage
of the model-shifting mechanism is minimal in case of small
motif lengths.
C. Study on the Proposed Initialization Scheme
In order to further evaluate the effectiveness of the proposed
algorithm, a study is performed on the proposed initialization
scheme. With the increase in the number of initial candidate
chains to find a promising Markov chain (itrm ax in Algorithm
1), the performance of the proposed method improves. This
improvement is due to the additional computational cost spent

684

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

TABLE IV
PERFORMANCE ON JASPAR DATASET: SMALL GROUP

TABLE V
COMPARISON OF THE STAND-ALONE SHIFTING SCHEME

Av. Score Ψ in 100 runs

Small Group
MA0053 MA0064
MA0004 MA0006 MA0020
MA0021 MA0056 MA0095
MA0026 MA0063 MA0087
MA0008 MA0011 MA0024
MA0031 MA0117 MA0121
MA0044 MA0076 MA0122
MA0023 MA0034 MA0049
MA0057
MA0058 MA0062 MA0071
MA0079 MA0101 MA0107
MA0012 MA0013 MA0025
MA0027 MA0040 MA0059
MA0105 MA0111
MA0018 MA0022 MA0043
MA0047
MA0070 MA0102 MA0120
MA0010 MA0017
MA0046 MA0119
MA0074
MA0045 MA0085
MA0115
MA0051 MA0112 MA0113
MA0014 MA0073
MA0088 MA0106
MA0007

Motif
Length

Conventional
EM

MonteCarlo EM

Proposed
Method

5
6

0.08
0.12

0.06
0.18

0.01
0.13

7
8

0.11
0.16

0.18
0.21

0.17
0.24

9
10

0.10
0.14

0.25
0.29

0.31
0.31

11

0.14

0.26

0.26

12

0.12

0.28

0.35

14

0.36

0.44

0.51

15
16
17
18
20

0.50
0.17
0.51
0.19
0.12

0.57
0.30
0.60
0.22
0.24

0.60
0.31
0.67
0.26
0.32

22

0.29

0.46

0.51

(l, d)

Background

Average score Ψ in 100 runs

Motif

length

MC EM

Proposed model shifting

(10,2)
(11,2)
(12,3)
(13,3)
(14,4)
(15,4)
(16,5)
(17,5)
(18,5)

200
400
300
300
300
500
500
500
500

0.35
0.49
0.19
0.31
0.36
0.33
0.16
0.30
0.37

0.39 (0.37)
0.47 (0.53)
0.24 (0.23)
0.35 (0.39)
0.29 (0.32)
0.40 (0.41)
0.19 (0.20)
0.34 (0.38)
0.40 (0.42)

Note: Results are based on synthetic dataset.

Note: Motifs of identical length are grouped together.

Fig. 7. Evolution of goodness measure ψ with respect to the number of
iteration for finding a promising Markov chain (itr m a x ).

in the proposed initialization scheme. In Tables III and IV, the
results are shown taking the value of itrm ax = 25. This can be
further improved by increasing its value. A study is performed
for (15, 4) motif model to demonstrate the usefulness of the
proposed initialization scheme. The evolution of the goodness
measure ψ is shown (continuous line) in Fig. 7. The value of ψ
is measured by computing the average score for 100 different
instances of (15, 4) motif finding problem with background
length L = 500. The value of ψ for MC EM is also shown with
the discontinuous line. It can be observed from the figure that the
performance of the proposed method improves at the expense
of computational cost.
D. Study on the Stand-Alone Model-Shifting Mechanism
In order to identify the improvement due to the stand-alone
shifting mechanism, a study is performed on a few (l, d) motif
finding problems excluding the proposed initialization scheme.

The average scores in 100 runs are computed for MC EM and
the proposed method. The results are presented in Table V.
In the fourth column, the score due to the stand-alone shifting
mechanism (i.e., without the proposed initialization step) is provided. The overall score, considering both the initialization and
shifting, is also provided here inside the parenthesis.
E. Effect of End Clustering
As discussed in Section-A., careful selection of the parameters associated with the proposed algorithm can be cost effective
than the conventional EM or MC EM method. Due to the initial screening process and model-shifting mechanism, a single
Markov chain associated with the proposed method is computationally expensive than that of the conventional EM or MC
EM. It should be noted that the Markov chain selected by the
proposed method is more likely to converge to the true motif
model as observed in the Tables II, III, and IV. Therefore, the
required number of chains to identify the motif using clustering
technique is less than that of the conventional EM or MC EM.
To establish this claim, a study is performed on different instances of (l, d) motif. As the output after the clustering scheme
is the motif sequence (not the motif start positions), the nla
is considered as a metric for goodness measure. The values of
the itrm ax and tm ax in Algorithm are taken to be 20 and 15,
respectively. For the conventional EM and MC EM, 100 random
seeds are considered. The final motif consensus is achieved via
the k-means clustering (with varying k) at the final stage. It
is observed that for difficult problems, a few seeds produces
an output motif model in the neighborhood of the true motif.
Therefore, the cluster corresponding to the desired solution is
small. In such cases, the value of k should be kept as high as
10-15. For relatively easy problems, k = 5 is sufficient for obtaining satisfactory result. In Table VI, the value of k is taken as
5 for both the conventional EM and the MC EM method. In the
proposed method, the number of seeds is taken as 20. The number of cluster is taken as 3. If the motif finding problem is easy
(say (10, 1) problem), then EM-based techniques may produce
correct results in almost all the cases resulting in one or more
empty cluster(s). In such cases, a simple majority consensus
may be used instead of clustering.

MAITI AND MUKHERJEE: ON THE MONTE-CARLO EXPECTATION MAXIMIZATION FOR FINDING MOTIFS IN DNA SEQUENCES

TABLE VI
PERFORMANCE IMPROVEMENT ON SYNTHETIC DATASET DUE TO CLUSTERING
(l, d)

Background

Motif

length

n la
score†

Av. time
(minute)

n la
score†

Av. time
(minute)

n la
score†

Av. time
(minute)

10,2
11,2
12,3
13,3
14,4
15,4
16,5
17,5
18,5
19,6

300
300
300
300
300
300
500
500
500
500

0.41
0.48
0.28
0.5
0.13
0.64
0.28
0.44
0.28
0.36

8.45
8.83
9.03
9.15
9.22
9.38
16.00
16.35
16.55
16.98

0.52
0.67
0.39
0.64
0.29
0.60
0.29
0.53
0.27
0.42

8.97
9.23
9.62
9.7
9.83
10.07
16.92
17.1
17.22
17.35

0.49
0.71
0.39
0.67
0.36
0.52
0.35
0.50
0.36
0.51

7.42
7.43
7.46
7.49
7.57
7.94
13.01
13.27
13.54
13.83

Conventional EM

MC EM

Proposed method

[† ] n la score is computed by performing clustering ten times. Note : The computation is
performed on a PC with Intel 3.0 GHz i5 processor and 4-GB memory.

685

synthetic and biological dataset containing experimentally verified motifs. The proposed algorithm may be used to identify the
transcription factor binding sites in promoter regions of DNA
sequences. This algorithm is valid for DNA sequences; however,
motif finding problem is also important in the context of protein
sequences. For example, the helix-turn-helix (HTH) motif is a
common pattern used by transcription regulators of prokaryotes
and eukaryotes. Any EM-based motif identification algorithm
can be used to identify HTH motifs in protein sequences. The
proposed initialization technique may be employed in this case.
But the proposed model-shifting scheme needs to be modified
to suit an alphabet size as large as 20. This is required because
the shifting of a column of the model parameter Θ might move
the point of interest far away from the true motif model resulting
in a very slow rate of convergence. As a future extension of this
study, a similar algorithm may be developed to deal with motifs
in protein sequences.

TABLE VII
COMPUTATION TIME COMPARISON OF A SINGLE CHAIN

REFERENCES
Motif

Background

Chain

Time for a single chain (in second)

length

length

length

EM

MC EM

Proposed method

10
10
13
13
15
15
17
17
19
19

300
300
300
300
300
300
500
500
500
500

100
500
100
500
100
500
100
500
100
500

5.07
25.01
5.49
27.5
5.63
28.2
9.81
49.08
10.19
50.92

5.38
26.87
5.82
29.19
6.04
30.3
10.26
51.34
10.41
52.12

22.25 (5.41)
43.85 (27.98)
22.48 (5.96)
46.64 (31.90)
23.81 (6.18)
48.48 (31.91)
39.8 (10.41)
82.23 (53.10)
41.49 (10.59)
83.42 (54.89)

F. Computation Time of Individual Markov Chain
Due to the proposed seed initialization and model-shifting
techniques, a single Markov chain of the proposed method is
computationally expensive than the one produced by the conventional EM or MC EM as discussed in Section E. In Table VII,
the computation time of a single Markov chain associated with
the three EM-based algorithms are shown for different motif
lengths. The values of itrm ax and tm ax in Algorithm are taken
as 20 and 15, respectively. On the right-most column, the values
within parentheses indicate the computation time without the
proposed initialization scheme.
VI. CONCLUSION AND DISCUSSION
An improved version of the Monte-Carlo EM method for
motif finding in genomic sequences is proposed. The method
identifies and terminates unpromising Markov chains to improve the overall performance. A model-shifting method is proposed to avoid local optima that arise due to high alignment
score of the shifted version of a true motif model. These two
modifications, when incorporated, improve the performance of
EM-based motif finding techniques. This proposed modification
can be incorporated to any EM-based motif finding algorithm.
The effectiveness of the proposed algorithm is validated on both

[1] A. R. Kornblihtt, M. D. L. Mata, J. P. Fededa, M. J. Munoz, and G.
Nogues, “Multiple links between transcription and splicing,” RNA, vol.
10, pp. 1489–1498, 2004.
[2] C. E. Lawrence, S. F. Altschul, M. S. Boguski, J. S. Liu, A. F. Neuwald,
and J. C. Wootton, “Detecting subtle sequence signals: A Gibbs sampling
strategy for multiple alignment,” Science, vol. 262, pp. 208–214, 1993.
[3] A. F. Neuwald, J. S. Liu, and C. E. Lawrence, “Gibbs motif sampling:
Detection of bacterial outer membrane protein repeats,” Protein Sci.,
vol. 4, pp. 1618–1632, 1995.
[4] P. A. Pevzner and S. H. Sze, “Combinatorial approaches to finding subtle
signals in DNA sequences,” in Proc. 8th Int. Conf. Intell. Syst. Molecular
Biol., 2000, pp.269–278.
[5] J. Buhler and M. Tompa, “Finding motifs using random projections,”
J. Comput. Biol., vol. 9, no. 2, pp. 225–242, 2002.
[6] T. L. Bailey and C. Elkan, “Unsupervised learning of multiple motifs
in biopolymers using expectation maximization,” Mach. Learn., vol. 21,
no. 1/2, pp. 51–80, 1995.
[7] C. Bi, “A Monte Carlo EM algorithm for de novo motif discovery in
biomolecular sequences,” IEEE/ACM Trans. Comput. Biol. Bioinformat.,
vol. 6, no. 3, pp. 370–386, Jul. 2009.
[8] G. Pavesi, G. Mauri, and G. Pesole, “An algorithm for finding signals
of unknown length in DNA sequences,” Bioinformat., vol. 7, no. 1,
pp. S207–S214, 2001.
[9] R. Edgar, “MUSCLE: Multiple sequence alignment with high accuracy
and high throughput,” Nucleic Acids Res., vol. 32, no. 5, pp. 1792–1797,
2004.
[10] J. D. Thompson, D. G. Higgins, and T. J. Gibson, “CLUSTAL W: Improving the sensitivity of progressive multiple sequence alignment through
sequence weighting, position-specific gap penalties and weight matrix
choice,” Nucleic Acids Res., vol. 22, pp. 4673–4680, 1994.
[11] X. Liu, D. Brutlag, and J. Liu, “BioProspector: Discovering conserved
DNA motifs in upstream regulatory regions of co-expressed genes,” in
Proc. 6th Pacific Symp. Biocomput., 2001, vol. 6, pp.127–138.
[12] C. E. Lawrence and A. A. Reilly, “An expectation maximization (EM)
algorithm for the identification and characterization of common sites
in unaligned biopolymer sequences,” Proteins: Struct., Funct., Genet.,
vol. 7, no. 1, pp. 41–51, 1990.
[13] B. Raphael, L.-T. Liu, and G. Varghese, “A uniform projection method
for motif discovery in DNA sequences,” IEEE/ACM Trans. Comput. Biol.
Bioinformat., vol. 1, no. 2, pp. 91–94, Apr.–Jun. 2004.
[14] G. C. G. Wei and M. A. Tanner, “A Monte Carlo implementation of the
EM algorithm and the poor man’s data augmentation algorithms,” J. Amer.
Statist. Assoc., vol. 85, no. 411, pp. 699–704, 1990.
[15] L. Wang and T. Jiang, “On the complexity of multiple sequence alignment,” J. Comput. Biol., vol. 1, pp. 337–348, 1994.
[16] A. Sandelin, W. Alkema, P. Engstrom, W. Wasserman, and B. Lenhard,
“JASPAR: An open-access database for eukaryotic transcription factor
binding profiles,” Nucleic Acids Res., vol. 32, pp. D91–D94, 2004.

686

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Aniruddha Maiti received the B.E. degree in electronics and telecommunication engineering from the
Bengal Engineering and Science University, Shibpur,
India, in 2010. He is currently working toward the
M.S. degree in the Department of Electrical Engineering, Indian Institute of Technology Kharagpur,
Kharagpur, India.
His principal research interests include machine
learning and computational biology.

Anirban Mukherjee (M’06) received the B.E.E. degree in electrical engineering from Jadavpur University, Kolkata, India, in 1998 and the M.Tech. and
Ph.D. degrees in electrical engineering from the Indian Institute of Technology Kharagpur, India.
He is currently with the Faculty of the Department of Electrical Engineering, Indian Institute of
Technology Kharagpur. His principal research interests include signal processing and machine learning.

