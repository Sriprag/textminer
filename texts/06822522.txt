IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

529

Compressed Sensing for Bioelectric Signals:
A Review
Darren Craven, Student Member, IEEE, Brian McGinley, Liam Kilmartin, Member, IEEE,
Martin Glavin, Member, IEEE, and Edward Jones, Senior Member, IEEE

Abstract—This paper provides a comprehensive review of compressed sensing or compressive sampling (CS) in bioelectric signal
compression applications. The aim is to provide a detailed analysis
of the current trends in CS, focusing on the advantages and disadvantages in compressing different biosignals and its suitability for
deployment in embedded hardware. Performance metrics such as
percent root-mean-squared difference (PRD), signal-to-noise ratio
(SNR), and power consumption are used to objectively quantify the
capabilities of CS. Furthermore, CS is compared to state-of-the-art
compression algorithms in compressing electrocardiogram (ECG)
and electroencephalography (EEG) as examples of typical biosignals. The main technical challenges associated with CS are discussed along with the predicted future trends.
Index Terms—Bioelectric signal compression, body area networks (BAN), compressed sensing (CS), electrocardiogram (ECG),
electroencephalography (EEG).

I. INTRODUCTION
OMPRESSED sensing (CS) or compressive sampling is
an emerging technique for acquiring and reconstructing
a digital signal with potential benefits in many applications.
The method of CS takes advantage of a signal’s sparseness in
a particular domain to significantly reduce the number of samples needed to reconstruct the signal. Generally, it requires far
fewer samples than Nyquist sampling, which has been the fundamental principle governing signal acquisition for many years.
The Nyquist sampling theorem states that when sampling a signal, the sampling rate must be at least twice the bandwidth of
that signal, i.e., the Nyquist rate. The conventional approach to
compressing a waveform takes the following steps: First, the
signal is sampled at the Nyquist rate, then the sampled signal
is compressed using methods such as the Wavelet or Fourier
transforms, retaining and quantizing the relevant coefficients,
and discarding the coefficients that are deemed unnecessary.
The signal may then be stored or transmitted, following which
it is decompressed and an approximation of the original signal
is recovered. Clearly, problems arise if the signal being measured has a very wide bandwidth, as this would require a very

C

Manuscript received November 21, 2013; revised April 8, 2014; accepted
May 19, 2014. Date of publication May 29, 2014; date of current version March
2, 2015. This work was supported by the Irish Research Council (IRC) and the
Higher Education Authority (HEA) under the Program for Research in Third
Level Institutions.
The authors are with the College of Engineering and Informatics, National University of Ireland, Galway, Ireland (e-mail: d.craven1@nuigalway.ie;
brian.mcginley@nuigalway.ie; liam.kilmartin@nuigalway.ie; martin.glavin@
nuigalway.ie; edward.jones@nuigalway.ie).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2327194

high sampling rate resulting in the generation of large amounts
of data. It can also be considered an inefficient process if a
substantial number of signal elements are then deemed to be
unneccessary and are discarded. This often occurs if the signal
is sparse after decomposition during the compression process,
even if it is not sparse in the time domain.
In contrast, CS acquires a small number of samples representing a sparse signal and uses mathematical techniques such
as L1 optimization to recover the original signal during decompression. Interest in CS has grown since a number of seminal
publications which demonstrate that a small number of linear
projections of a sparse signal can produce a good quality reconstruction of the original signal [1]–[4]. Despite its relatively
short lifespan, CS is now a widely researched compression technique. However, to date no review of CS applied to biomedical
signal compression exists.
In recent years, there has been increased interest in ambulatory monitoring of bioelectric signals. Such monitoring presents
major advantages such as increased patient mobility, constant
patient observation, and a reduction in healthcare costs. However, there are also many challenges relating to ambulatory
monitoring of biosignals including wearable device size, power
consumption, and cost. Perhaps the biggest challenge is to reduce the power consumption. In an ambulatory environment,
there is a strong need to effectively manage the large quantities
of real-time biosignal data being generated. Therefore, lossy
compression techniques are increasingly being used to manage
these data. These techniques allow for higher compression rates
which minimize the power consumption, albeit at a cost of a
slight degradation in the reconstructed signal. The main goal
of compression is to efficiently reduce the data quantities while
retaining maximum signal quality. CS is an attractive compression technique which is well suited to mobile, wearable or low
power systems as it reduces both the amount of data to be stored
and the resources required in the analog-to-digital converter
(ADC) and microcontroller. In an architecture, where compression is considered, there are three main power consumers: signal
acquisition, digital signal processing for the compression operation, and the wireless transmission. CS allows for a reduction in
power across all three components. As mentioned, CS reduces
the resources required in signal acquisition [5]. The sub-Nyquist
nature of the sampling process produces fewer digital measurements than for Nyquist sampling, and removes the requirement
for digital signal compression by simultaneously sampling and
compressing. Wireless transmission, which is the main energy
consumer in such an architecture [6], is also minimized as CS, by
its nature reduces the data to be transmitted. There is a tradeoff

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

530

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

however; the power consumption and computational requirements of CS reconstruction are significantly higher relative to
other compression techniques. However, since reconstruction
can be performed offline, where power and time constraints are
not so stringent, this does not affect the ambulatory monitoring
aspect of the overall system architecture.
This paper aims to provide the reader with a detailed literature
review of CS applied to biosignal compression. Previous work
has explored the use of CS in biomedical imaging applications
such as magnetic resonance imaging (MRI) [7], [8], ultrasound
[9], and computed tomography (CT) [10]; however, this review
focuses on 1-D bioelectric signals, particularly those of interest
in an ambulatory environment.
The paper takes the following structure. Section II gives a
background on CS, defining the algorithm and summarizing performance metrics which are used throughout the paper. Section
III presents a summary of the current state of CS in compressing
biosignals, in particular compression of EEG and ECG. Section
IV focuses on the use of CS in body area networks (BAN) and
discusses the current state of the design of hardware to implement CS. Section V presents results from a simulation that
evaluates several different basis function sets for compressing
electroencephalography (EEG) and electrocardiogram (ECG)
signals using CS. Finally, Section VI presents the conclusion of
the paper.
II. COMPRESSED SENSING
A. Background
Compressed sensing is a compression technique which allows sampling of analog signals at sub-Nyquist rates while
avoiding the traditional issue of aliasing, and allowing for effective compression of signals during the sampling process.
To achieve this compression gain, CS benefits from two fundamental concepts: sparsity of the signal and incoherence. It
is critical to CS that the signal being acquired is sparse in at
least one domain, i.e., the majority of entries in the signal are
zero in that domain (such as the frequency domain). Many realworld signals meet this criterion [1]–[4]. If a signal possesses
this sparse property, an accurate reconstruction can be obtained
by taking a relatively small number of measurements when
sampling. The incoherence principle states that the measurement matrix used to acquire the signal must be incoherent with
the dictionary that represents the signal sparsely. In general,
the more sparse the signal being measured, the more CS exploits the signal characteristics and the better the reconstruction
quality.
The CS paradigm allows for the reconstruction of N samples
of a signal X by the acquisition/transmission of only M samples
of compressed data representing X, where M << N. Therefore,
the overhead from signal acquistion and/or transmission is reduced. However, there is a tradeoff, which is an increase in the
computational complexity of the reconstruction. To recover the
N samples of data from the original signal with only M samples of data available, a convex optimization problem must be
solved by finding the minimum L1-norm solution [1]. If nec-

cessary, resource constrained architectures can deal with this
computationally expensive stage by offloading the signal reconstruction via transmission to another system which would have
a much larger power budget or processing capability. For example, the reconstruction could be carried out on a cloud-based
server. This allows for the acquisition stage of the system to be
low in complexity and power, which is desirable for ambulatory
monitoring.
B. Method
1) Acquisition: The first step in CS involves correlating the
data X consisting of N samples with a fixed matrix Φ of size M ×
N demonstrated by (1). Commonly the matrix Φ has components
which are known to be independently identically distributed
(i.i.d). These i.i.d entries are often chosen from a Gaussian
distribution or Bernoulli distribution and thus Φ can be seen as
a random matrix.
[Y ]M ,1 = [Φ]M ,N [X]N ,1

(1)

This random matrix Φ is known as a sensing or measurement
matrix. Note that the sensing matrix remains constant and a
version must be stored at the decoding stage to enable reconstruction. The data Y of length M is a representation of the signal
X. Generally, this is the only step in the acquisition stage of CS.
2) Reconstruction: To reconstruct the acquired signal, a vector α as in (2) is required, where α is a sparse signal represented
in the sparse dictionary or sparsifying matrix Ψ. If X is sufficiently sparse in the domain it is acquired, then Ψ is simply an
identity matrix. Note the sparsifying matrix is not necessarily a
square matrix and the number of columns P can be increased
above N to create overcomplete dictionaries.
[X]N ,1 = [Ψ]N ,P [α]P ,1

(2)

Thus, the aim of the reconstruction is to reproduce a version
of the vector α (of length P) from the vector Y (of length M).
The equation for Y is defined in:
[Y ]M ,1 = [Φ]M ,N [Ψ]N ,P [α]P ,1

(3)

The equation to solve for α is of the form of the classic linear
algebra problem Ax = B, where x is unknown. Since the set of
equations consists of N unknowns in M equations, it can be a
computationally expensive system to solve or approximate for
sparse real-world signals. Recently, it has been shown that if the
signal is sparse in the dictionary Ψ, the probability of finding
a proper or exact solution to the undetermined set of linear
equations is high [1], [3], [4].
By solving the set of equations using an L1-norm minimization, it has been proven that the vector with the minimum L1-norm will correspond to the correct reconstructed
X, provided that enough measurements M have been taken.
The minimization is described in (4). The original X can then
be recovered by using (2). The CS approach is summarized
in Table I.
Min α1 subject to Y = ΦΨα

(4)

CRAVEN et al.: COMPRESSED SENSING FOR BIOELECTRIC SIGNALS: A REVIEW

TABLE I
PSEUDOCODE SUMMARY OF COMPRESSED SENSING

531

have been implemented for signal reconstruction that iteratively
solve (4) by making locally optimal decisions, with the aim of
locating the nonsparse coefficients to enable quicker reconstruction [14], [15]. Algorithms such as orthogonal matching pursuit
(OMP) [16], [17] and compressive sampling matching pursuit
(CoSaMP) [18], [19] generate a solution that may not be globally optimal but is computed more quickly than the standard
L1-norm BP method.

Input: X
Acquisition:
Step 1. Compute random measurements Y where Y = ΦX
(and where Φ is a M × N matrix of random i.i.d. entries)
Reconstruction:
Step 2. Compute α using L1 minimization: Minα 1 subject to Y = ΦΨα
Step 3. Reconstruct original signal. X  = α Ψ
Output: X

C. Performance Metrics
However, there is an important condition which must be satisfied when utilizing this convex L1-norm method to ensure
accuracy and robustness in the recovery of the signal. The condition states that the sensing matrix should satisfy the restricted
isometric property (RIP) defined in:
(1 − δk ) α2 ≤ ΦΨα2 ≤ (1 + δk ) α2

(5)

where δk , the isometry constant of Φ, must be smaller than 1
and the smaller the value of δk the higher the probability of an
exact reconstruction.
In practice, this RIP is difficult to verify and instead the coherence between the sensing matrix and the sparse dictionary
can be measured. The coherence μ between the sensing matrix
and the dictionary measures the largest correlation between any
two elements of Ψ and Φ, defined in (6) [11]. Ideally, the coherence will be small, i.e., the two are incoherent, as the value
for μ is effectively proportional to the number of measurements
required when sampling.
√
(6)
μ (Φ, Ψ) = N · max 1≤k , j ≤N |Φk , Ψj |
Gaussian or Bernoulli distributions are the most commonly
used methods for creating sensing matrices. Because these matrices have very small isometry constants, they are incoherent
with very high probability, with a fixed dictionary Ψ. It has been
shown that with using these i.i.d entries only M > Ck Slog(N/S)
measurements are required for accurate reconstruction (where
Ck is an empirical constant and S refers to the number of nonzero
signal entries) [2]. Another common and useful sensing matrix
is a random discrete Fourier matrix. Fourier matrices also have
a low isometry constant and require M > Ck Slog(N)6 measurements for accurate and stable reconstruction [12].
L1-norm is chosen ahead of L0-norm and L2-norm minimization for a number of reasons [2]. L0-norm minimization
is guaranteed to exactly recover the original signal if it is
sparse. However, this procedure is a nonconvex optimization
problem and has no numerical solution as it is NP-hard [12].
The L2-norm minimization can be easily solved, but the reconstructed signal in this case is not sparse and carries a large
unusable error. L2 minimization returns nonsparse solutions as
its energy is distributed across all the signal elements rather than
being condensed into a small number of nonzero signal entries
as is the case with L1 minimization.
While the convex optimization basis pursuit (BP) [13] L1
method effectively solves (4) and has been shown to give an
accurate reconstruction, there are other Greedy methods which

A range of performance metrics are used in the literature reviewed in this paper: sparsity, compression ratio, and percentage
root-mean-squared difference.
1) Sparsity: A signal is sparse in a domain if it contains
mostly zero entries in that domain. The effect of sparsity is
that these zero entries can be effectively discarded without the
loss of relevant signal information. If a sparse signal contains
S nonzero entries then the signal is said to be S-sparse. With a
signal of length N, this means that (N – S) signal coefficients
can be removed while maintaining the significant information
about the signal. In this case, percentage sparsity is defined as:
% Sparsity =

N −S
· 100
N

(7)

2) Compression Ratio: CR is a measure of the reduction in
the data required to represent the signal X. In the case of CS,
it is a ratio between the length of the original and compressed
signal vectors, i.e., the number of measurements M required to
accurately reconstruct the signal X. If N is the length of the
vector X and M is the dimension of the sensing matrix, then CR
is defined as:
CR =

N
M

(8)

3) Percentage Root-mean-squared Difference: PRD is a measure of the distortion or difference between the reconstructed
signal X and the original signal X:
PRD =

X − X  
· 100
X

(9)

Equation (9) defines the common method used for PRD in
the CS literature. However, despite this, a PRD measurement
independent of the dc level in the original signal would be more
appropriate. This can be achieved by subtracting the mean of
the signal from the denominator before calculating the PRD.
The main issue with (9) is that the PRD values can appear
artifically lower than the true distortion between the original
and reconstructed signal if a dc bias exists in the signal.
III. COMPRESSED SENSING OF BIOSIGNALS
This section reviews the current literature regarding the use
of CS with biosignals. Particular focus is placed on EEG and
ECG signals, though other biosignals are also discussed briefly.

532

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

TABLE II
SUMMARY OF EEG CS APPROACHES AND COMPARISON WITH SPIHT
Publication
Abdulghani et al. [21,26,33]
Gangopadhyay et al. [23]
Casson and Rodriguez-Villegas [20]
Higgins et al. [29]

Notes

Algorithm

PRD (%)

CR

SNR (dB)

Cubic B-Spline chosen as it has lowest PRD value.
Enforces 95% sparsity.
PRD an estimate based on a graph used in paper.
SPHIT.

BP
BP
BP
-

18.6
25
30

3
4
2.5
30

15.28
> 60
-

A. Compressed Sensing of EEG Signals
EEG records the electrical activity of the brain. It is a clinical tool used for diagnosis of neurological disease or disorders.
Clinical applications include the diagnosis of coma, epilepsy,
and sleep disorders. Recent commercial and research interests
have focused on the development of mobile EEG technology
specifically to address the clinical needs of capturing events such
as seizures in the ambulatory environment. EEG signals are typically recorded over multiple channels, sometimes in multiple
sessions, and as a result there is a requirement for a large amount
of storage of the EEG data. This generation of large amounts of
data has been seen as one of the major challenges to EEG data
processing. With a portable embedded system where the data
are transmitted wirelessly, the system would benefit from more
power efficient sampling and/or compression prior to wireless
transmission. The feasibility of employing CS as an EEG compression technique to reduce the storage and processing load
has already been investigated [20], [21]. Abdulghani et al. [21]
investigated the feasibility of employing CS in an EEG monitoring system for reducing power consumption. The results show
that for their particular architecture, CS should only be used
with advantage if there are at least 22 channels recording the
EEG. At fewer than 22 channels, transmitting the raw EEG data
was more power efficient on their system. This particular test
was carried out at a CR of 9.
Due to the nonsparse nature of EEG in the time or frequency
domain, a variety of different dictionary functions or basis are
generally used. There have been several publications describing dictionaries such as a Slepian basis and a Gabor frame
[20], [22]–[28]. Senay et al. [22] illustrated a CS framework for
EEG compression with a Slepian basis. The results show that
EEG can be efficiently compressed with CS at a very low error
rate by projecting EEG signals onto this Slepian basis which
emphasizes the signals sparseness. Aviyente [27] presented a
CS framework for EEG compression and analyzed the mean
square error (MSE) of the EEG signals based on a Gabor frame
method. Aviyente posited that the sparsity of EEG signals could
be further increased by employing a “chirped Gabor dictionary.”
However, Gangopadhyay et al. claimed that a wavelet domainbased method is more suitable for EEG CS compression than
the Gabor frame method [23].
A small number of papers on EEG compression have explored the range of acceptable PRD levels of a reconstructed
EEG signal [30], [31]. It has been determined that the maximum acceptable PRD in order to retain the most important EEG
signal information is 7% [30]. This threshold was chosen because it maintains 99.5% of the original signal energy. Higgins

et al. demonstrated that up to 30% PRD is tolerable with EEG
compression for applications such as automated seizure detection (since a high PRD does not obscure the key information
required to detect a seizure [31]). It also reports that for a 7%
PRD, a CR of 5 can be achieved by using the set partitioning
in hierarchical trees (SPIHT) algorithm for EEG compression
and for a PRD of 30% a CR of 30 can be achieved. SPIHT is
a state-of-the-art wavelet-based compression method originally
proposed by Said and Pearlman [32]. It is useful to compare such
an algorithm and the performance of CS for EEG compression
in the current literature, and this is summarized in Table II.
Abdulghani et al. quantified the effect of CS on scalp EEG
signals [26], [33] by comparing the original and reconstructed
signals at CRs ranging from 2.5 up to 37.5, including analysis
of the peak signal-to-noise (PSNR) between the original and
reconstructed signals over the range of CRs. At a CR of approximately 3.5, there is a relatively constant PSNR of 20 dB.
The PSNR decreases rapidly as the CR is increased. This paper
also presents an analysis of different reconstruction algorithms
for CS EEG reconstruction. BP [13] L1 minimization and two
greedy algorithms (matching pursuit (MP) [16] and OMP [17])
are investigated. Their performance on six different EEG dictionaries is recorded, namely Gabor, Mexican Hat, Cubic and
Linear Spline, and Cubic and Linear B-Spline. Each of the dictionaries was tested on each of the reconstruction algorithms resulting in 18 different test conditions. The tests were performed
at a CR of 3 and performance metrics such as signal-to-noise ratio (SNR), PSNR, PRD, and reconstruction time are presented.
The B-Spline dictionaries proved to be the most promising by
yielding the best reconstruction results and lowest error rates in
EEG, with little difference between linear and cubic B-spline.
As expected, the BP algorithm provides superior performance
in terms of reconstruction quality compared to the greedy approaches, but at an increased computational cost. At a CR of 3,
the BP algorithm recorded PRD values ranging from 18–25%
for the six dictionaries tested. OMP’s PRD values ranged from
26% to 34%. Finally, the MP method performed the worst with
significantly higher average PRD values. Considering 30% PRD
as an acceptable performance threshold for applications such as
automated seizure detection, the BP algorithm and in some cases
the OMP algorithm meet this criteria. Casson and RodriguezVillegas [20] also used a Cubic B-Spline and the BP algorithm
to reconstruct EEG signals, achieving on average a PRD of 25%
at a CR of 2.5. When comparing these CS implementations to
those obtained by implementing the SPIHT algorithm in terms
of PRD, it is clear they are not as effective at the higher CRs, as
illustrated in Table II. It is important to remember that despite

CRAVEN et al.: COMPRESSED SENSING FOR BIOELECTRIC SIGNALS: A REVIEW

Fig. 1. Typical healthy ECG signal showing the prominent high activity
region—the QRS complex. Reproduced from [34].

this, a CS implementation requires significantly less measurements than SPIHT and is a lower complexity algorithm on the
acquisition side.
B. Compressed Sensing of ECG Signals
ECG signals are used to detect the electrical activity of the
heart. Each heartbeat is represented by a spike in the signal
known as the QRS complex as illustrated in Fig. 1. The prominence of this complex in normal healthy ECGs makes it a relatively easy feature to extract, and it is useful in calculating
metrics such as heart rate or heart rate variability. Another advantage of the unique shape of the ECG signal is that the most
relevant information in the signal (the QRS complex) is contained in a narrow window of the time domain since the QRS
peaks have much larger amplitudes than the P and T waves as
illustrated in Fig. 1.
A number of recent publications have analyzed CS and its
performance on ECG signals (see Table III). Polania et al. [35]
present a method for ECG compression using CS that uses a
preproccessing stage to detect the QRS complex. The method
then fixes the period of each heartbeat and incorporates a sliding
window approach to focus on a particular number of hearbeats
each time. It is not clear how the proposed method would cope
with a highly varying heart rate or occurence of an arrhythmia.
Also a disadvantage of the method is the need to store/transmit
the beat information which adds computational complexity to
the encoder. The reconstruction of the signal is performed using
a modified version of the simultaneous OMP recovery algorithm
and is tested on four ECG records at different CRs. The results
present a comparison between the SPIHT compression method
and CS at various CRs. The approach outperforms SPIHT (i.e.,
has a lower PRD) for CRs above approximately 8. However, it is
not quite the best example of a SPIHT algorithm applied to ECG
compression, when compared with the PRD values obtained in
[36]. Pooyan et al. [36] demonstrated that in most records, a

533

CR of up to 20 can be achieved with ECG SPIHT compression
while still maintaining a PRD of less than 10% (which would
outperform the CS approach of [35]).
Chae et al. [37] investigated the performance of CS against
a thresholding discrete wavelet transform (TH-DWT) method
on noisy signals (typical of an ambulatory ECG environment)
and artefacts consistent with the occurrence of body movement.
The superior performance of the TH-DWT over a CS method in
ECG compression was demonstrated in their research. Initially,
the two approaches were compared on nonnoisy signals. The
TH-DWT method achieved a PRD of 9% at a CR of 5 whereas
the CS approach obtained a CR of 1.67 for the same PRD. On
the noisy signals, the TH-DWT method maintained a constant
SNR up to a CR of 2.5 before decreasing slightly, while the CS
underwent a sharper fall, with performance decreasing from a
CR of 1.25. Chae et al. [37] acknowledge that CS is attractive
due to its encoder simplicity but recommend exercising caution
when considering CS for ECG compression, where noisy signals
would be expected.
Mishra et al. [38]–[40] evaluated a selection of the best
wavelet basis function for compressing ECG using CS by
evaulating several wavelet families: Coiflets, Haar, Symlets,
Daubechies, Biorthogonal, and Reverse Biorthogonal were
tested and evaluated based on their performance and their ability to create sparsity in an ECG signal. Metrics such as MSE,
PRD, and PSNR were calculated at numerous CRs ranging from
2 to 10, and at each CR the best wavelet family was identified. For the majority of CRs, it is suggested that the wavelets
rbio3.7 and rbio3.9 perform best with ECG compression based
on the PRD values.
Another area which has been explored in the recent literature
is CS reconstruction algorithms and their performance with ECG
compression [41]–[43]. Dixon et al. used ECG compression to
compare BP convex optimization with four greedy reconstruction algorithms and tested them on ECG signals for accuracy,
reliability, and computational time at different sparsity levels
[42]. The algorithms tested were BP convex optimization [13],
OMP [17], CoSaMP [18], [19], regularized orthogonal least
squares [44], and normalized iterative hard thresholding [45].
The results showed that OMP is the recommended algorithm
for ECG CS reconstruction based on its reconstruction accuracy
and computational time. This appears to be the only paper in the
CS literature that advocates the use of OMP over BP for optimal
reconstruction accuracy.
As stated earlier, CS algorithms typically create their measurement matrix using random methods such as Bernoulli or
Gaussian distributions. Ansari-Ram and Hosseini-Khayat proposed a new method for creating the sensing matrix which
improves the PRD vs. CR analysis for ECG signals [46].
The proposed nonuniform binary sensing measurement matrix is designed specifically for ECG signals and takes into
account the region of interest in a healthy ECG signal (i.e.,
the QRS complex). A drawback of this method is the requirement to alter and transmit the sensing matrix in order
to ensure it aligns with the QRS complex for each frame. It
could also be expected that abnormalities in the ECG would
significantly effect this approach. Dixon et al. also considered

534

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

TABLE III
SUMMARY OF ECG CS APPROACHES AND SPIHT COMPARISON
Publication
Polania et al. [35]
Mishra et al. [38–40]
Ansari-Ram and Hosseini-Khayat [46]
Mamaghanian et al. [43, 48]
Casson and Rodriguez-Villegas [20]
Chae et al. [37]
Pooyan et al. [36]

Notes

PRD an estimate based on formula used in paper.

SPIHT approach.

additional sensing matrices (which are variations of Bernoulli
random sampling): Toepiltz, Circular, and Triangular [41]
for low complexity acquisition.
Zigel et al. [47] classified different PRD ranges in terms of the
quality of the reconstructed signal as perceived by a clinician.
This research demonstrated that PRDs of less than 9% for ECG
signals are either “good” or “very good” quality. Based on the
thresholds proposed by [47], the recent literature has shown that
CS can generally reach CR’s of 4 before exceeding this 9%
PRD measure. Ansari-Ram and Hosseini-Khayat [46] obtained
a PRD of 8.527% at a CR of 4 and [48] demonstrated an average
PRD of less than 9% at a CR of approximately 3.5. In [35], the
authors achieved a CR of around 8 before going above 9%
PRD, however the method for calculating the CR differs from
the traditional methods. Casson and Rodriguez-Villegas [20]
obtained a PRD of 2.6% at a CR of 2.5. These results show that
the reconstructed signals are of sufficient quality for “successful
use of the signals” based on their median PRD values. Chae et al.
[37] obtained a PRD of 9% at a CR of 2.5. Table III provides a
summary of these publications and their best case performance
with CS, in terms of PRD.
C. Compressed Sensing of Other Biosignals
Casson and Rodriguez-Villegas also investigated the performance of CS on other biosignals [20], studying EOG (eye) and
EMG (muscle). An analysis of PRD vs. CR is reported for each
of the biosignals studied at CRs ranging from 2.5 to 10. This
study concludes that CS is not recommended for the compression of EMG signals as the PRD was above 100% for each of
the recorded CRs. The performance with EOG is superior to
ECG and EEG, with PRD values lower at all CRs tested.
Even though Casson and Rodriguez-Villegas’s findings seem
to indicate that CS is not suitable for compression of EMG
signals, Dixon et al. presented different design considerations
for a BAN system that monitors ECG and EMG, indicating the
suitability of compressing EMG signals with CS when their
techniques are applied [41]. This study enforces sparsity on
the ECG and EMG signals in order to achieve higher rates of
compression. An increase in sparsity levels improves the CR obtained, however, the tradeoff is that the signal fidelity is reduced.
To create the sparsity levels, dynamic time-domain thresholding
methods are used whereby values under a certain threshold amplitude are set to zero and values above the threshold remain as

Algorithm

PRD (%)

CR

Simultaneous OMP
BP
BP
BP
BP
-

2.57
1.66
8.58
<9
2.6
9
3.1

7.23
4
5
3.44
2.5
2.5
21.4

they are. The idea is that the high activity regions are preserved
which is useful in applications which require heart rate detection for ECG or body movements, in the case of EMG. Dixon
et al. also considered different sensing matrices and found that
with 1-bit Bernoulli measurements in the sensing matrix, a CR
of 16 is achievable for the sparse EMG and ECG signals. To
demonstrate this, an analysis of the CR and SNR was illustrated at sparsity levels of 90%, 95%, 98%, and 99%. The ECG
and EMG signals employed were from the Physiobank database
[49].
IV. HARDWARE IMPLEMENTATIONS OF COMPRESSED SENSING
Research into the implementation of CS in an ambulatory environment faces many challenges. This review will now examine
key challenges in relation to the practical implementation of CS.
On this point, this review indicates that CS does not compare favorably with other state-of-the-art lossy compression techniques
when considering only CR vs. reconstruction quality. Therefore,
the choice of CS depends on its ability to provide a low power
implementation of lossy compression. First, this section investigates the suitability of utilizing CS in terms of overall power
efficiency by reviewing papers which have employed CS in a
BAN. Second, this section focuses on the design of hardware
capable of efficiently performing the sub-Nyquist acquisition
which is currently the focus of significant research interest.
A. Body Area Networks
1) Introduction: BANs are still a relatively niche technology in terms of actual deployment but there has been significant
focus on them from a research and development perspective in
recent times [50]–[52]. A primary driver for this focus is the
considerable healthcare costs associated with patient monitoring in a clinic or hospital environment. The advancements made
in technologies such as wireless communication and mobile
smartphones (now equipped with multiple sensors and everincreasing processing capabilities) has accelerated research and
development in mobile healthcare. The notion that a patient can
be constantly monitored (both in and out of a clinical environment) is an attractive prospect for healthcare providers and has
significant potential to reduce healthcare costs. It is also advantageous for patients as it improves their mobility, where they
are no longer required to remain in a clinical environment for
extended periods of time.

CRAVEN et al.: COMPRESSED SENSING FOR BIOELECTRIC SIGNALS: A REVIEW

Constant patient monitoring can be achieved by having a
sensor placed on a person’s body, implanted in a person, worn
on a person’s clothes or from sensors embedded in their mobile
handset. BANs promise to provide the capability for real-time
remote monitoring, diagnosis, and in some cases treatment of a
patient. These architectures generally consist of sensors which
record and store biomedical data. The data are then typically
transmitted wirelessly to a base station such as a smartphone or
server for viewing by a clinician or the patient themselves.
2) CS in Body Area Networks: It has been shown that in BAN
biomedical applications the main consumer of power is wireless
data transmission [6]. However, another significant consumer of
power is the biosignal sampling process. Consider a BAN architecture where a 2 channel, 16-bit ECG signal is being sampled
at 250 Hz, thereby generating 48 kB of data per minute. It is
important to note that certain applications such as a Holter monitor (for monitoring the electrical activity of the heart) will have
some form of continuous monitoring as a requirement. This
places a constant burden on the sensor and also increases the
amount of data that has to be wirelessly transmitted. CS has
the potential to reduce the amount of data that are sampled and
transmitted in a BAN.
A number of recent publications have explored the potential of CS in BANs [20], [41], [53]–[56]. Balouchestani et al.
[55] demonstrated that it is possible to employ CS in a BAN,
where sampling rates can be reduced to 25% of the Nyquist
rate without sacrificing the quality of the signal, and in some
cases, the overall power consumption can be reduced by 35%.
It is not clear what performance metric is used to conclude that
the signal is not degraded but the reported reduced power consumption is achieved at a CR of 5. These tests were carried
out on ECG signals.
In another hardware simulation of CS, Dixon et al. [41] added
white Gaussian noise to yield a SNR of 80 dB (a level deemed
typical of an analog front end ECG sensor). The results present
the SNR of the reconstructed signal at CRs up to 16 and the ability of the reconstructed signal to maintain this SNR at each CR
is reported. As expected with higher levels of sparsity, higher
CRs are achieved while maintaining the SNR. Chae et al. investigated the performance of CS on noisy signals (typical of
an ambulatory ECG environment) and artefacts consistent with
the occurrence of body movement [37]. On the noisy signals,
the SNR underwent a sharper fall, with performance decreasing
from a CR of 1.25.
The final implementation of interest demonstrates the potential of applying CS in BANs. Mamaghanian et al. [48] explored
the role of CS in wireless body sensor nodes for acquisition
and compression of ECG signals. The technique was compared against a discrete wavelet transform (DWT)-based lossy
compression technique in terms of energy consumption on a
Shimmer device [57], which was being used as a sensor node.
The energy consumption analysis focused on three different
stages for each approach: sampling, compression, and the wireless transmission. The lifetime of this node is compared in three
scenarios: no compression, DWT-based compression, and CS.
The results demonstrated a 37.1% extension in the battery life for
the CS approach over the DWT-based approach. This is mainly

535

due to the low complexity nature of CS’s simultaneous sampling
and compression. The energy consumption of wireless transmission for the DWT-based approach is lower than that of CS as
higher CRs are achieveable within the acceptable PRD values.
The ECG signals were initially stored on the memory of the
Shimmer device and hence it could be argued that a more thorough analysis should take into account the power consumption
of compression occuring at the sensor prior to the ADC. This
analog front end power profiling analysis has since been investigated [5] and will be described in the next section.
B. Compressed Sensing Analog Front End Design
Recent years have seen significant research focus on the design, development, and evaluation of hardware implementations
to deliver CS. One of the early publications on the implementation of hardware in CS was Duarte et al.’s in 2008 [58], proposing single pixel imaging using CS. Even at this early stage,
Duarte et al. indicated that CS has the potential to “substantially
increase the performance and capabilities of data acquisiton and
processing” while still being aware of the “clear tradeoffs and
challenges” in its implementation. Since then, a number of significant advances have been made. Considering an application,
where the number of samples in an acquired frame N is equal to
the sampling frequency, conventional sampling in BANs using
an ADC will sample an input signal every 1/N seconds. The theory of CS suggests the ability to reduce the effective sampling
rate of the acquisition process to a rate of 1/M. In order to achieve
this sub-Nyquist sampling in an efficient manner, the design of
the signal acquisition system requires careful attention. The CS
hardware architectures in the literature can be divided into two
main approaches: digital CS and analog CS. Digital CS comprises Nyquist sampling prior to the linear multiplication with
a sensing matrix to obtain the M measurements. Analog CS is
where M measurements are obtained in the analog domain prior
to being digitized. Analog CS is the main area of interest for
this review since it has the most potential for delivering the low
power acquisition possibilities that CS offers.
1) Random Demodulator Architecture: Analog CS hardware acquisition systems in the literature are generally variations
of a random demodulator (RD) architecture [59]–[61]. The RD
architecture is composed of three main stages: mixer, integrator,
and the ADC. The mixer or demodulator multiplies the input
signal by a pseudorandom sequence. This step corresponds to
the multiplication of an input signal X by a sensing matrix Φ.
This multiplication must be continuous at a rate of at least the
Nyquist rate. The integrator or low-pass filter accumulates the
output voltage of the demodulator. Functionality must exist to
reset the value of the filter after each sample is taken. These
samples correspond to the compressed measurements. The final
component is the ADC which now samples at a rate of 1/M
instead of 1/N (which is the case with digital CS).
A RD architecture operating in isolation has certain drawbacks. The coherence in Φ would not be at a minimum due
to the fact that Φ would be populated with many 0’s and the
rows of the matrix all use the same random sequence. As the
compression increases, the coherence will also increase. A

536

Fig. 2.

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Typical block diagram of random demodulator scheme for CS.

common practical implementation of CS comprises parallel
blocks of the RD shown in Fig. 2, known as the RD preintegrator (RMPI). This architecture allows for the signal to be
multiplied by different random sequences thereby increasing
the incoherence as each parallel block produces its own compressed measurements. RMPI allows for a further reduction
in the ADC requirements with the tradeoff being that more
accumulators and mixers operating at the Nyquist rate are
required.
2) Alternative Analog CS Designs: Charbiwala et al. [62]
designed a hardware architectue for CS, comprising a scalable
analog low power front end. Charbiwala et al. acknowledges
that since the development of hardware for CS began, designing hardware to perform the projection to a lower dimension
(i.e., the multiplication of the original signal of N samples by
the measurement matrix to form M measurements) has proven
difficult. Furthermore, the implementations of such a projection
are limited by their inability to scale to low-power applications.
Charbiwala et al. address this problem by multiplying the original signal by each random vector independently using a time
multiplexing method to control access to a single shared analog processing chain. The approach named “CapMux” is tested
in the time, frequency, and wavelet domains to demonstrate
its “universality”. The design implements an integrator which
stores state values while switching between time slots. This enables a reduction in the number of chains required as only a
single mixer is needed for the multiplication and the parallel
integrators are accessed in a time-shared round robin sequence.
The results presented are from a 16-channel CapMux prototype
and demonstrate a signal with 64 samples recovered with a SNR
of 35.7 dB. The errors of the projection of the signal and the
basis are small for all 16 channels and are in the millivolt region.
In general, the results show the SNR can be over 30 dB in most
instances for sparsity levels which are < 4 (3 is the maximum
amount of nonzero entries).
Yenduri et al. present a time-based signal processing hardware architecture for CS [63] showing a new ADC for low power
CS. A theroretical analysis of their hardware design and reconstruction algorithm is presented. The design takes advantage
of time-based ADC signal processing techniques to optimize
performance in CS. For signals exhibiting 97% sparsity in the
frequency domain, this architecture demonstrates a reduction in
sampling rate of 90% when compared to the Nyquist rate.

Gangopadhyay et al. designed a low power CS analog front
end sensor for the use on ECG signals [64]. The proposed architecture consists of parallel channels using a variation of the
RD. It includes a 6-bit multiplying digital-to-analog converter
(MDAC), an integrator and a 10-bit successive approximation
register (SAR) ADC. The design, which was tested on time
domain signals, applies a threshold to increase signal sparsity,
thereby allowing for higher CRs. The design was tested on
wavelet and discrete Fourier transform (DFT) matrix reconstruction techniques. The analog front end was fabricated and
the total power dissipation is 28 nW and 1.8 μW for 1 and 64
channel designs, respectively.
3) Comparison of Analog and Digital CS Implementations:
Chen et al. presented a hardware implementation of CS with a
view to implementing the algorithm on wireless sensor nodes
[65], [66]. A detailed analysis of both analog and digital implementations of the encoder stage of CS is presented in [65]. The
analog encoder consists of a parallel RD architecture. The digital encoder model consists of an amplifier and ADC followed
by an accumulator and XOR (the XOR is used with the carry-in
of the accumulator for the multiplication of the signal by the ±1
entries in the measurement matrix) to generate the compressed
measurements. The digital encoder first acquires the signal using
a Nyquist rate ADC before obtaining the fewer measurements
through multiplication by the measurement matrix. Their analysis shows a comparison of the power consumption between
the digital and analog implementations. Each implementation
was tested with the aim of obtaining a target CR of 10. The
power consumptions of each implementation shows the digital
implementation to be the preferred approach for wireless sensor
applications. To validate the predictions of the circuit model
analysis, the encoder circuit for the digital CS design was fabricated on a 90-nm CMOS process. The experiments carried
out on this design compressed EEG signals while consuming
1.9 μW at 0.6 V. The main problem with the analog design that
Chen et al. presented is that the parallel channels are designed
so that the number of channels equals the number of desired
measurements. Mamaghanian et al. [5] noted that the implemenentation uses very high-frequency mixers which results in the
analog implementation consuming more power than is neccessary. A low-power analog design for CS was also presented
in [5]. The paper compared the traditional RMPI with digital
CS (Nyquist sampling) and their own spread spectrum random
modulation preintegrator (SRMPI). The results demonstrate that
analog CS consumes less power than the Nyquist sampling. The
RMPI can reduce the power consumption by 63% whereas the
SRMPI can reduce it by 75% when compared to Nyquist sampling in an 8 channel architecture. However, as the number of
channels increases, the power consumption increases toward
the Nyquist implementation. The SRMPI is an innovative design which introduces a mixer before the traditional parallel
RD blocks. This allows only a single mixer to operate at the
Nyquist rate but then allows the other mixers in the design to
reduce their sampling rate, therefore providing a more efficient
implementation to the RMPI.

CRAVEN et al.: COMPRESSED SENSING FOR BIOELECTRIC SIGNALS: A REVIEW

537

TABLE IV
ECG AND EEG COMPRESSION RESULTS

V. COMPARISON OF BASIS FUNCTIONS
IN COMPRESSED SENSING
Section III has reviewed the literature relating to compression
of biosignals using various approaches to CS (choice of basis
functions, reconstruction algorithms, etc.), while Section IV has
reviewed hardware implementations of CS. In order to evaluate
some aspects of the different methods presented in this review,
simulation results that compare their performance on commonly
used EEG and ECG databases is presented in this section. This
analysis focuses specifically on different sparse dictionaries and
their performance in CS. The EEG and ECG databases chosen
are the Freiburg EEG database [66] and the MIT-BIH Arrhythmia database, respectively [49], [67]. Both databases were divided into frames of 1024 nonoverlapping samples. Each frame
was compressed and reconstructed separately and the results
have been averaged. The results presented are based on CRs of
2 and 4 and the approaches are compared on the basis of PRD
and reconstruction time. The random sensing matrices Φ are
made up of Bernoulli distribution entries which are set to either
±1. For consistency, the same sensing matrix was used for each
method at the CRs of 2 and 4. The state-of-the-art BP convex
optimization reconstruction technique was used to reconstruct
the compressed signals. Reconstruction was carried out using
the BP algorithm from the SPGL1 solver toolbox [68]. Although
alternative greedy algorithms exist allowing for shorter reconstruction times, BP achieves the best solution. For this paper,
reconstruction is considered to be performed in a nonresource
constrained computational environment, and the reconstruction
times are presented normalized to the fastest algorithm to allow
for relative comparisons.
In general, the authors have tried to mirror the techniques
used in the literature as accurately as possible. However, in certain cases sufficient information on the implementation was not
provided to ensure the exact same methodology is used. A total of seven sampling basis Ψ are presented: Wavelet, Gabor,
Mexican Hat, and different spline functions. A wavelet basis
has been commonly tested in the CS literature. For the purposes
of these tests, initial tests were executed to find the optimal
wavelet and wavelet parameters for these databases. Based on
our preliminary testing, the Daubechies 4 wavelet was chosen.
A Daubechies wavelet has been commonly chosen for CS implementations [35], [46], [48]. The wavelet basis was created
using seven levels of decomposition. A Gabor dictionary has
been used for EEG CS in [27], [33] and it is implemented here
based on equations in those papers. Mexican Hat dictionaries
were created using the interval parameters proposed in [33].
Reconstruction with the Mexican Hat dictionary was performed
using the CVX toolbox due to improved performance in our preliminary testing [69]. The spline dictionaries were created from
a number of dictionary functions for each spline selected based
on the parameters used in [33]. All of these functions are then
time-shifted across the 1024 samples to create the additional
dictionary functions. The results from comparing the different
basis sets are summarized in Table IV.
The metrics used to quantify the performance of each method
are PRD and the average reconstruction time. The average re-

Basis

Dictionary Elements

CR

ECG
PRD∗ (%)

ECG
Normalized Time

Wavelet

1024
3072

Cubic Spline

8192

Cubic B-Spline

5888

Linear Spline

7168

Linear B-Spline

3584

Mexican Hat

2048

6.97
27.61
5.18
20.46
15.335
25.79
14.4
27.05
8.03
19.66
19.24
44.86
4.39
8.63

1

Gabor

2
4
2
4
2
4
2
4
2
4
2
4
2
4

Dictionary Elements

CR

EEG
PRD ∗ (%)

EEG
Normalized Time

Wavelet

1024
3072

Cubic Spline

8192

Cubic B-Spline

5888

Linear Spline

7168

Linear B-Spline

3584

Mexican Hat

2048

21.16
59.78
12.21
33.74
21.71
36.88
21.77
42.32
17.05
36.23
32.6
44.86
5.27
12.48

1

Gabor

2
4
2
4
2
4
2
4
2
4
2
4
2
4

Basis

11.26
23.24
18.86
21.54
13.1
13.63

4.95
10.74
8.89
9.81
5.83
6.09

construction time refers to the time to compress a frame at each
CR, and is calculated as an average over the database for each
CR. The values are normalized according to the time taken to
reconstruct using the wavelet basis since it was the quickest to
complete. Despite the majority of papers in the literature using
PRD as defined in (9), this review uses an adjusted PRD metric
which takes into account the mean of the signal. The authors
recommend this method be used to give a true indication of the
distortion of signals.
PRD∗ =

X − X  
· 100
X − X̄

(10)

Overall the results presented show that, for each dictionary
employed, CS performs better when compressing ECG compared to EEG. For both biosignals, the tests demonstrated
that wavelets can compress well compared to other methods
for lower CRs but struggle to create the sparsity in signals for
higher CRs. The advantage associated with wavelets is their
shorter reconstruction times but they are not recommended for
high reconstruction quality. The spline reconstruction accuracy
tends to be dependent on the number of dictionary functions,
with the cubic and linear splines providing the best performance
in this instance. The potential of both the Gabor and Mexican
Hat function is clear from the experiments. In particular, the
Mexican Hat provides the best PRD at a CR of both 2 and 4,

538

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

with a reasonable reconstruction time compared to the other
basis functions. The reconstruction accuracies of these dictionaries could also be further improved by adding more dictionary
functions, in applications where reconstruction time does not
need to be limited.
VI. CONCLUSION
This paper has presented a comprehensive review of the CS
literature, focused on biomedical applications and in particular, on the compression of biosignals commonly of interest in
ambulatory monitoring applications. An introduction to the CS
theory and methods was given in the opening section and its
performance when applied to biomedical signal processing applications was reviewed (using a set of performance metrics).
CS research has demonstrated its potential of being employed in
BAN architectures. In fact, predictions show it is a suitable alternative to widely used state-of-the-art compression algorithms
such as SPIHT in terms of energy efficiency. An analysis of the
various hardware approaches that have been developed were presented and compared, with each option presenting a low power
approach to sub-Nyquist signal acquisition. These summaries
provide power-consumption and compression performance information on current CS hardware results, and compare them
with current state-of-the-art compression methods. Given that
CS is a relatively new technique, there are some grounds for
optimism about the effectiveness of CS in these applications.
The main challenge facing CS is in improving the quality of the
reconstruction. At present, in most cases, the PRD values obtained using CS do not compare favorably to the values obtained
with SPIHT compression. Many CS techniques have investigated the most appropriate sensing matrix or sparse dictionary.
Approaches have also commonly performed thresholding to improve sparsity, in order to optimize the reconstruction quality.
However, it is clear that much research remains to be done before
CS can approach the CRs commonly achievable with Nyquist
rate approaches.
REFERENCES
[1] E. J. Candes, J. Romberg, and T. Tao, “Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information,”
IEEE Trans. Inf. Theory, vol. 52, no. 2, pp. 489–509, Feb. 2006.
[2] E. J. Candes and M. B. Wakin, “An introduction to compressive sampling,”
IEEE Signal Process. Mag., vol. 25, no. 2, pp. 21–30, Mar. 2008.
[3] D. L. Donoho, “Compressed sensing,” IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289–1306, Apr. 2006.
[4] E. J. Candes and T. Tao, “Near-optimal signal recovery from random
projections: Universal encoding strategies?,” IEEE Trans. Inf. Theory,
vol. 52, no. 12, pp. 5406–5425, Dec. 2006.
[5] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst
, “Design and exploration of low-power analog to information
conversion based on compressed sensing,” IEEE J. Emerg. Sel. Topics
Circuits Syst., vol. 2, no. 3, pp. 493–501, Sep. 2012.
[6] S. Feng-Tso, C. Kuo, and M. Griss, “PEAR: Power efficiency through
activity recognition (for ECG-based sensing),” in Proc. 5th Int. Conf.
Pervas. Comput. Technol. Healthcare, 2011, pp. 115–122.
[7] S. G. Lingala and M. Jacob, “Blind compressive sensing dynamic MRI,”
IEEE Trans. Med. Imag., vol. 32, no. 6, pp. 1132–1145, Jun. 2013.
[8] M. Lustig, D. L. Donoho, J. M. Santos, and J. M. Pauly, “Compressed
sensing MRI,” IEEE Signal Process. Mag., vol. 25, no. 2, pp. 72–82,
Mar. 2008.

[9] C. Quinsac, A. Basarab, J. Girault, and D. Kouame, “Compressed sensing
of ultrasound images: Sampling of spatial and frequency domains,” in
Proc. IEEE Workshop Signal Process. Syst., 2010, pp. 231–236.
[10] C. Guang-Hong, P. Theriault-Lauzier, T. Jie, B. Nett, L. Shuai, J. Zambelli,
Q. Zhihua, N. Bevins, A. Raval, S. Reeder, and H. Rowley, “Time-resolved
interventional cardiac C-arm cone-beam CT: An application of the PICCS
algorithm,” IEEE Trans. Med. Imag., vol. 31, no. 4, pp. 907–923,
Apr. 2012.
[11] J. A. Tropp, “Just relax: Convex programming methods for identifying sparse signals in noise,” IEEE Trans. Inf. Theory, vol. 52, no. 3,
pp. 1030–1051, Mar. 2006.
[12] V. M. Patel and R. Chellappa, Sparse Representations and Compressive Sensing for Imaging and Vision.,” New York, NY, USA:
Springer, 2013.
[13] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” J. Sci. Comput., vol. 20, pp. 33–61, 1999.
[14] D. Linfeng, W. Rui, W. Wanggen, Y. X. Qing, and Y. Shuai, “Analysis on
greedy reconstruction algorithms based on compressed sensing,” in Proc.
Int. Conf. Audio, Lang. Image Process., 2012, pp. 783–789.
[15] S. A. Razavi, E. Ollila, and V. Koivunen, “Robust greedy algorithms for
compressed sensing,” in Proc. 20th Eur. Signal Process. Conf., 2012,
pp. 969–973.
[16] S. G. Mallat and Z. Zhang, “Matching pursuits with time-frequency dictionaries,” IEEE Trans. Signal Process., vol. 41, no. 12, pp. 3397–3415,
Dec. 1993.
[17] J. A. Tropp and A. C. Gilbert, “Signal recovery from random measurements via orthogonal matching pursuit,” IEEE Trans. Inf. Theory, vol. 53,
no. 12, pp. 4655–4666, Dec. 2007.
[18] D. Needell and J. A. Tropp, “CoSaMP: Iterative signal recovery from noisy
samples,” Appl. Comput. Harmon. Anal., vol. 26, pp. 301–321, 2008.
[19] D. Wei and O. Milenkovic, “Subspace pursuit for compressive sensing signal reconstruction,” IEEE Trans. Inf. Theory, vol. 55, no. 5,
pp. 2230–2249, May 2009.
[20] A. J. Casson and E. Rodriguez-Villegas, “Signal agnostic compressive sensing for body area networks: Comparison of signal reconstructions,” in Proc. IEEE Annu. Int. Conf. Eng. Med. Biol. Soc., 2012,
pp. 4497–4500.
[21] A. M. Abdulghani, A. J. Casson, and E. Rodriguez-Villegas, “Quantifying
the feasibility of compressive sensing in portable electroencephalography
systems,” in Proc. 5th Int. Conf. Found. Augment. Cognit. Neuroergon.
Operat. Neurosci.: Held Part HCI Int., San Diego, CA, USA, 2009,
pp. 319–328.
[22] S. Senay, L. F. Chaparro, M. Sun, and R. J. Sclabassi, “Compressive
sensing and random filtering of EEG signals using slepian basis,” presented
at the 16th Eur. Signal Process. Conf., 2008, Lausanne, Switzerland.
[23] D. Gangopadhyay, E. G. Allstot, A. M. R. Dixon, and D. J. Allstot,
“System considerations for the compressive sampling of EEG and ECoG
bio-signals,” in Proc. Biomed. Circuits Syst. Conf., 2011, pp. 129–132.
[24] J. Haboba, M. Mangia, R. Rovatti, and G. Setti, “An architecture for 1-bit
localized compressive sensing with applications to EEG,” in Proc. IEEE
Biomed. Circuits Syst. Conf., 2011, pp. 137–140.
[25] H. Qi and H. Fei, “A compressive eletroencephalography (EEG) sensor
design,” in Proc. IEEE Sens., 2010, pp. 318–322.
[26] A. M. Abdulghani, A. J. Casson, and E. Rodriguez-Villegas, “Quantifying
the performance of compressive sensing on scalp EEG signals,” in Proc.
3rd Int. Symp. Appl. Sci. Biomed. Commun. Technol., 2010, pp. 1–5.
[27] S. Aviyente, “Compressed sensing framework for EEG compression,” in
Proc. IEEE/SP 14th Worksop. Statist. Signal Process., 2007, pp. 181–184.
[28] Z. Zhang, T. Jung, S. Makeig, and B. Rao, “Compressed sensing of EEG
for wireless telemonitoring with low energy consumption and inexpensive hardware,” IEEE Trans. Biomed. Eng., vol. 60, no. 1, pp. 221–224,
Jan. 2013.
[29] G. Higgins, B. McGinley, N. Walsh, M. Glavin, and E. Jones, “Lossy
compression of EEG signals using SPIHT,” Electron. Lett., vol. 47,
no. 18, pp. 1017–1018, Sep. 2011.
[30] J. Cárdenas-Barrera, Lorenzo-Ginori, and E. Rodrı́guez-Valdivia, “A
wavelet-packets based algorithm for EEG signal compression.,” Med. Informat. Internet Med., vol. 29, pp. 15–27, 2004.
[31] G. Higgins, S. Faul, R. P. McEvoy, B. McGinley, M. Glavin, W. P. Marnane, and E. Jones, “EEG compression using JPEG2000: How much loss
is too much?” in Proc. IEEE Annu. Int. Conf. Eng. Med. Biol. Soc., 2010,
pp. 614–617.
[32] A. Said and W. A. Pearlman, “A new, fast, and efficient image codec based
on set partitioning in hierarchical trees,” IEEE Trans. Circuits Syst. Video
Technol.,vol. 6, no. 3, pp. 243–250, Jun. 1996.

CRAVEN et al.: COMPRESSED SENSING FOR BIOELECTRIC SIGNALS: A REVIEW

[33] A. M. Abdulghani, A. J. Casson, and E. Rodrı́guez-Villegas, “Compressive
sensing scalp EEG signals: Implementations and practical performance,”
J Med. Bio. Eng. Comput., vol. 50, pp. 1137–1145, 2012.
[34] Medlibes Medical Library. (2010). [Online]. Available: http://
medlibes.com/entry/u-wave
[35] L. F. Polania, R. E. Carrillo, M. Blanco-Velasco, and K. E. Barner, “Compressed sensing based method for ECG compression,” in Proc. IEEE Int.
Conf. Acoust., Speech Signal Process., 2011, pp. 761–764.
[36] M. Pooyan, A. Taheri, M. Moazami-Goudarzi, and I. Saboori, “Wavelet
compression of ECG signals using SPIHT algorithm,” Int. J. Inf. Commun.
Eng., vol. 1, pp. 219–225, 2005.
[37] D. H. Chae, Y. F. Alem, S. Durrani, and R. A. Kennedy, “Performance
study of compressive sampling for ECG signal compression in noisy and
varying sparsity acquisition,” in Proc. IEEE Int. Conf. Acoust., Speech
Signal Process., 2013, pp. 1306–1309.
[38] A. Mishra, F. Thakkar, C. Modi, and R. Kher, “ECG signal compression
using compressive sensing and wavelet transform,” in Proc. IEEE Annu.
Int. Conf. Eng. Med. Biol. Soc., 2012, pp. 3404–3407.
[39] A. Mishra, F. N. Thakkar, C. Modi, and R. Kher, “Selecting the most
favorable wavelet for compressing ECG signals using compressive sensing approach,” in Proc. Int. Conf. Commun. Syst. Netw. Technol., 2012,
pp. 128–132.
[40] A. Mishra, F. Thakkar, C. Modi, and R. Kher, “Comparative analysis of
wavelet basis functions for ECG signal compression through compressive
sensing,” Int. J. Comput. Sci. Telecommun., vol. 3, pp. 23–31, 2012.
[41] A. M. R. Dixon, E. G. Allstot, D. Gangopadhyay, and D. J. Allstot,
“Compressed sensing system considerations for ECG and EMG wireless
biosensors,” IEEE Trans. Biomed. Circuits Syst., vol. 6, no. 2, pp. 156–166,
Apr. 2012.
[42] A. M. R. Dixon, E. G. Allstot, A. Y. Chen, D. Gangopadhyay, and
D. J. Allstot, “Compressed sensing reconstruction: Comparative study
with applications to ECG bio-signals,” in Proc. IEEE Int. Symp. Circuits
Syst., 2011, pp. 805–808.
[43] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, “Structured sparsity models for compressively sensed electrocardiogram signals:
A comparative study,” in Proc. IEEE Biomed. Circuits Syst. Conf., 2011,
pp. 125–128.
[44] T. Blumensath and M. E. Davies, “On the difference between orthogonal matching pursuit and orthogonal least squares,” Univ. Edinburgh,
Edinburgh, U.K., Tech. Rep., Mar. 2007.
[45] T. Blumensath and M. E. Davies, “Normalized iterative hard thresholding: Guaranteed stability and performance,” IEEE J. Sel. Topics Signal
Process., vol. 4, no. 2, pp. 298–309, Apr. 2010.
[46] F. Ansari-Ram and S. Hosseini-Khayat, “ECG signal compression using
compressed sensing with nonuniform binary matrices,” in Proc. 16th CSI
Int. Symp., Artif. Intell. Signal Process., 2012, pp. 305–309.
[47] Y. Zigel, A. Cohen, and A. Katz, “The weighted diagnostic distortion
(WDD) measure for ECG signal compression,” IEEE Trans. Biomed. Eng.,
vol. 47, no. 11, pp. 1422–1430, Nov. 2000.
[48] H. Mamaghanian, N. Khaled, D. Atienza, and P. Vandergheynst, “Compressed sensing for real-time energy-efficient ECG compression on wireless body sensor nodes,” IEEE Trans. Biomed. Eng., vol. 58, no. 9,
pp. 2456–2466, Sep. 2011.
[49] MIT-BIH Arrhythmia Database. (2000). [Online]. Available:
http://www.physionet.org/physiobank/database/mitdb/
[50] M. Chen, S. Gonzalez, A. Vasilakos, H. Cao, and V. C. Leung, “Body area
networks: A survey,” Mobile Netw. Appl., vol. 16, pp. 171–193, 2011.
[51] G. V. Crosby, T. Ghosh, R. Murimi, and C. A. Chin, “Wireless body area
networks for healthcare: A Survey,” Int. J. Ad hoc, Sens. Ubiquit. Comput.,
vol. 3, pp. 1–8, 2012.
[52] S. Ullah, H. Higgins, B. Braem, B. Latre, C. Blondia, I. Moerman,
S. Saleem, Z. Rahman, and K. S. Kwak, “A comprehensive survey of
wireless body area networks,” J. Med. Syst., vol. 36, pp. 1065–1094,
2012.
[53] A. Abrardo, C. M. Carretti, and A. Mecocci, “A compressive sampling data
gathering approach for wireless sensor networks using a sparse acquisition
matrix with abnormal values,” in Proc. 5th Int. Symp. Commun. Control
Signal Process., 2012, pp. 1–4.
[54] M. Balouchestani, K. Raahemifar, and S. Krishnan, “Low power wireless
body area networks with compressed sensing theory,” in Proc. IEEE 55th
Int. Midwest Symp. Circuits Syst., 2012, pp. 916–919.
[55] M. Balouchestani, K. Raahemifar, and S. Krishnan, “Wireless body area
networks with compressed sensing theory,” in Proc. ICME Int. Conf.
Complex Med. Eng., 2012, pp. 364–369.

539

[56] S. Li, L. Xu, and X. Wang, “Compressed sensing signal and data acquisition in wireless sensor networks and internet of things,” IEEE Trans. Ind.
Inf., vol. 9, no. 4, pp. 1–1, Nov. 2013.
[57] “Shimmer Research,” (2008). [Online]. Available: http://www.shimmerresearch.com/
[58] M. F. Duarte, M. A. Davenport, D. Takhar, J. N. Laska, S. Ting, K.
F. Kelly, and R. G. Baraniuk, “Single-pixel imaging via compressive
sampling,” IEEE Signal Process. Mag., vol. 25, no. 2, pp. 83–91, Mar.
2008.
[59] J. A. Tropp, J. N. Laska, M. F. Duarte, J. K. Romberg, and R. G. Baraniuk,
“Beyond Nyquist: Efficient sampling of sparse bandlimited signals,” IEEE
Trans. Inf. Theory, vol. 56, no. 1, pp. 520–544, Jan. 2010.
[60] J. N. Laska, S. Kirolos, M. F. Duarte, T. S. Ragheb, R. G. Baraniuk, and
Y. Massoud, “Theory and implementation of an analog-to-information
converter using random demodulation,” in Proc. IEEE Int. Symp. Circuits
Syst., 2007, pp. 1959–1962.
[61] S. Kirolos, J. Laska, M. Wakin, M. Duarte, D. Baron, T. Ragheb,
Y. Massoud, and R. Baraniuk, “Analog-to-information conversion via
random demodulation,” in Proc. IEEE Dallas/CAS Workshop, Des., Appl.,
Integrat. Softw., 2006, pp. 71–74.
[62] Z. Charbiwala, P. Martin, and M. B. Srivastava, “CapMux: A scalable
analog front end for low power compressed sensing,” in Proc. Int., Green
Comput. Conf., 2012, pp. 1–10.
[63] P. K. Yenduri, A. Z. Rocca, A. S. Rao, S. Naraghi, M. P. Flynn, and
A. C. Gilbert, “A low-power compressive sampling time-based analog-todigital converter,” IEEE J. Emerg. Sel. Topics Circuits Syst., vol. 2, no. 3,
pp. 502–515, Sep. 2012.
[64] D. Gangopadhyay, E. G. Allstot, A. M. R. Dixon, K. Natarajan, S. Gupta,
and D. J. Allstot, “Compressed sensing analog front-end for bio-sensor
applications,” IEEE J. Solid-State Circuits, vol. 49, no. 2, pp. 426–438,
Feb. 2014.
[65] F. Chen, A. P. Chandrakasan, and V. M. Stojanovic, “Design and analysis
of a hardware-efficient compressed sensing architecture for data compression in wireless sensors,” IEEE J. Solid-State Circuits, vol. 47, no. 3,
pp. 744–756, Mar. 2012.
[66] F. Chen, A. P. Chandrakasan, and V. Stojanovic, “A signal-agnostic compressed sensing acquisition system for wireless and implantable sensors,”
in Proc. IEEE Custom Integr. Circuits Conf., 2010, pp. 1–4.
[67] Univ. of Freiburg, Freiburg, Germany. The Freiburg EEG database. (2011).
[Online]. Available: http://epilepsy.uni-freiburg.de/freiburg-seizureprediction-project/eeg-database
[68] E. V. Berg and M. P. Friedlander, SPGL1: A solver for large-scale
sparse reconstruction. (2007). [Online]. Available: http://www.cs.ubc.ca/
∼mpf/spgl1/download.html
[69] CVX: Matlab Software for Disciplined Convex Programming. (2013).
[Online]. Available: http://cvxr.com/cvx

Darren Craven (S’13) received the B.E.(Hons.) degree in electronic and computer engineering from the
National University of Ireland, Galway, Ireland, in
2011. He is currently working toward the Ph.D. degree in the Department of Electrical and Electronic
Engineering, NUI Galway.
His research interests include biomedical signal processing, compressed sensing, and body area
networks.

Brian McGinley received the B.E.(Hons.) degree in electronic and computer engineering and
the Ph.D. degree from the National University of
Ireland, Galway, Ireland.
His Ph.D. research investigated spiking neural networks and genetic algorithm parameter adaptation.
Since 2009, he has been with NUI, as a Postdoctoral
Researcher in the area of digital signal processing. He
is currently working as a Member of the CAR group
in the area of video compression and object detection
for the automotive environment.

540

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Liam Kilmartin (M’97) received the B.E. and
M.Eng.Sc. degrees in electronic engineering from
National University of Ireland, Galway, Ireland, in
1990 and 1994, respectively.
Since 1994, he has been a Lecturer with the Department of Electrical and Electronic Engineering,
National University of Ireland. His research interests
include the use of signal processing, machine learning and modeling techniques in communication, and
biomedical application domains.

Martin Glavin (M’95) received the B.E.(Hons.) degree in electronic engineering and the Ph.D. degree in advanced equalization techniques for highspeed digital communications from National University of Ireland, Galway, Ireland, in 1997, and 2004,
respectively.
Since 1999, he has been a Lecturer with the Department of Electrical and Electronic Engineering,
NUI Galway. His research interests include image
processing and embedded systems in the areas of automotive systems and biomedical signal processing.

Edward Jones (M’91–SM’12) received the B.E. and
Ph.D. degrees in electronic engineering from National University of Ireland, Galway, Ireland.
He was previously with Toucan Technology Ltd.,
Galway, Ireland, and PMC-Sierra, Inc., developing
digital signal processing (DSP) algorithms for digitalsubscriber-line modems. From 2001 to 2002, he was a
Senior DSP Architect with Innovada Ltd., developing
software for embedded voice-band modem technology. From 2002 to 2003, he was with Duolog Technologies Ltd., where he developed DSP algorithms
and implementations for wireless networking systems. He is currently a faculty
member in Electrical and Electronic Engineering at NUI Galway, and is the
Vice-Dean of the College of Engineering and Informatics. His research interests
include DSP algorithm development for applications in biomedical engineering,
image processing, and speech processing.

