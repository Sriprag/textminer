3432

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 12, DECEMBER 2013

Long-Range Gaze Tracking System for
Large Movements
Dong-Chan Cho, Student Member, IEEE, and Whoi-Yul Kim∗ , Member, IEEE

Abstract—In the vision-based remote gaze tracking systems, the
most challenging topics are to allow natural movement of a user
and to increase the working volume and distance of the system.
Several eye gaze estimation methods considering the natural movement of a user have been proposed. However, their working volume and distance are narrow and close. In this paper, we propose
a novel 2-D mapping-based gaze estimation method that allows
large-movement of user. Conventional 2-D mapping-based methods utilize mapping function between calibration points on the
screen and pupil center corneal reflection (PCCR) vectors obtained
in user calibration step. However, PCCR vectors and their associated mapping function are only valid at or near to the position
where the user calibration is performed. The proposed movement
mapping function, complementing the user’s movement, estimates
scale factors between two PCCR vector sets: one obtained at the
user calibration position and another obtained at the new user position. The proposed system targets a longer range gaze tracking
which operates from 1.4 to 3 m. A narrow-view camera mounted
on a pan and tilt unit is used by the proposed system to capture
high-resolution eye image, providing a wide and long working volume of about 100 cm × 40 cm × 100 cm. The experimental results
show that the proposed method successfully compensated the poor
performance due to user’s large movement. Average angular error
was 0.8◦ and only 0.07◦ of angular error was increased while the
user moved around 81 cm.
Index Terms—Gaze tracking, infrared, long range, natural
movement.

I. INTRODUCTION
AZE information of the user, which is widely used in
various fields such as visual system research, human–
computer interaction, and product design, can be acquired by
the vision-based gaze tracking system. The eye is captured by
a camera which is either placed at remote location [1]–[11] or
attached on a head-mounted device [12]–[15]. Most of gaze
tracking methods typically use an infrared (IR) illuminator to
make a reference point in the image. The corneal reflection of

G

Manuscript received March 20, 2013; revised May 19, 2013; accepted June
1, 2013. Date of publication June 6, 2013; date of current version November
18, 2013. This work was supported by the Korea Communications Commission, Korea, under the R&D program supervised by the Korea Communications
Agency (KCA-2012-(09912-03002), Development of Interactive View Control
Technologies for IPTV). Asterisk indicates corresponding author.
D.-C. Cho is with the Department of Electronics and Computer Engineering, Hanyang University, Seoul 133-791, Korea (e-mail: dccho@
vision.hanyang.ac.kr).
∗ W.-Y. Kim is with the Department of Electronics and Computer Engineering,
Hanyang University, Seoul 133-791, Korea (e-mail: wykim@hanyang.ac.kr).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2266413

IR ray is captured as a bright spot, i.e., glint, in the image. When
the IR light is used, the captured image is not affected by the
variation of the visual light, and thus, the difference between
pupil and iris areas is emphasized against the visual light. Gaze
information is obtained by using the relationship between a pupil
and the glint. In general, there are two different approaches
in estimating the gaze information, namely 3-D model-based
gaze estimation method and 2-D mapping-based gaze estimation
method.
The 3-D model-based gaze estimation method estimates the
3-D direction of gaze which is a vector from the center of the
cornea to the center of the pupil [1], [3], [7], [8], [11], [16]–[19].
The intersection between the 3-D vector of the gaze and the
screen, whose 3-D position is measured in advance, becomes
the gaze point. The 3-D model-based gaze estimation method
is relatively easy to incorporate the head movement of the user.
However, the 3-D model-based gaze estimation method requires
a global geometric model of position and orientation of light
sources, camera, and monitor [20]. Furthermore, user-dependent
parameters, such as cornea radius, angle between visual and
optical axes (i.e., kappa), and the distance between the pupil
center and cornea center, also need to be obtained in advance.
Maximum working distance of the gaze tracking methods using
3-D model-based gaze estimation is usually limited around 60–
70 cm.
The 2-D mapping-based gaze estimation method estimates
the point of gaze (POG) on the screen using a precalculated
gaze mapping function [1], [2], [4]–[6], [10]. Gaze mapping
function translates 2-D eye feature information obtained from
the eye image to pixel location on the screen. The pupil center
corneal reflection (PCCR) vector is commonly used for 2-D eye
feature because of easy extraction of pupil and glint in the eye
image. The gaze tracking system using 2-D mapping-based gaze
estimation method is easy to set up as the system calibration is
not required. Moreover, gaze of the user is easily calculated
because gaze information is obtained from PCCR vector only.
However, this method is not robust to allow natural movement of
the user. This is because the gaze mapping function is valid only
at or close to the position where the user calibration is performed.
Although few methods have been successful in solving this
problem, but their working distance is very limited and so does
the range of movement [1], [5], [6], [10]. The working distance
and its corresponding working range of previous methods are
listed in Table III.
Yoo and Chung used cross-ratio to estimate user’s gaze allowing head movement [6]. In their method, four IR illuminators
were affixed to each corner of the monitor and another IR illuminator was positioned at the center of the zoom camera which

0018-9294 © 2013 IEEE

CHO AND KIM: LONG-RANGE GAZE TRACKING SYSTEM FOR LARGE MOVEMENTS

captured the magnified eye image. Face and eyes were tracked
by a wide-view camera mounted on the zoom camera. The eyes
were tracked continuously by the zoom camera using pan and
tilt unit even when the user was moving. Gaze point was estimated using a cross-ratio between the pupil center and the
virtual projection points which were calculated by taking twice
the vector from the center glint to each corner glint.
Zhu and Ji proposed the 2-D mapping-based gaze estimation method with a dynamic head compensation model to allow
natural head movement [1]. The dynamic head compensation
model modifies the gaze mapping function by utilizing the 3-D
eye position to compensate for head movement.
Hennessey and Lawrence proposed the scaling correction
method of the PCCR vector to compensate for head displacement [10]. In this method, the scale of the PCCR vector is
modified by the ratio of the two distances: the distance from the
camera to the user’s current location and the distance from the
camera to the location where the user calibration is performed.
However, the gaze estimation error using this method rapidly
increased due to the movement of the user and only Z-direction
movement was compensated.
Most of gaze tracking methods allowing natural movement
of the user operated only in close distance of less than 100 cm.
Recently, a few methods targeting long-range gaze tracking systems have been proposed, but most of them either do not allow
the movement of the user or have very limited working volume [2], [4], [5], [9].
Lee et al. proposed a long-range gaze tracking system using
2-D mapping-based gaze estimation method [4]. To capture the
magnified eye image of the user at 200 cm, a 300-mm optical
zoom lens was used. However, as the zoom lens and the camera
were locked on the tripod, the user had to hold his head extremely
still and natural movement of the user was not allowed at all.
Hennessey et al. proposed the system that consists of similar
gaze tracking system with a zoom lens [11] and a wide-view
camera of MS Kinect device [5]. The location of the user was
obtained from the wide-view camera and long-range gaze tracking module was rotated by the pan and tilt unit. When the user
located at 300 cm moved in a horizontal or vertical direction,
the pan or tilt unit was operated to capture the eye of the user.
To compensate for the motion of pan or tilt unit, pixel offset was
calculated by using the pan or tilt angle and geometric information. However, this method was tested for horizontal and vertical
movement and the POG was not effectively compensated for the
motion of the user.
Recently, a long-range gaze tracking system has been proposed, which consists of wide- and narrow-view cameras (rotated by the pan and tilt unit), two for stereo and two for capturing
each eye of the user (located at 200 cm), respectively [2]. The
PCCR vector scaling method was proposed using a quadratic
polynomial that was computed by the empirical data to compensate the movement of the user in the Z-direction. PCCR
vectors obtained through the user calibration were adjusted by
a scale factor which was also calculated by the difference of
distances between the user calibration location and the current
user location. The 2-D mapping function using the modified
PCCR vectors was computed and the final POG was estimated

3433

by the updated 2-D mapping function. However, this method
only compensated for the movement of the user in depth and the
quadratic polynomial estimated by the empirical data was inaccurate at the different location where the empirical data were
not available.
In this paper, we propose the long-range gaze tracking system
that provides the large working volume, 100 cm in horizontal
axis, 40 cm in vertical axis, and 100 cm in depth, and a long
working range from 1.4 to 3 m using pan and tilt unit and
a narrow-view camera. Two wide-view cameras to obtain 3D user position are affixed under the TV screen. This system
allows the natural movement of the user without big sacrifice
in accuracy by applying 2-D mapping-based gaze estimation
with the proposed movement mapping function. The proposed
movement mapping function estimates the scale factor of PCCR
vector to compensate the movement of the user using some
geometric information and 3-D user position.
This paper is organized as follows: In Section II, the brief
description of the structure of the proposed system, calibration,
2-D feature detection, and gaze estimation are presented. In Section III, the proposed movement mapping function is described.
The experimental results are presented in Section IV, and the
conclusions are given in Section V.
II. SYSTEM OVERVIEW
A. Hardware
As shown in Fig. 1(a), the proposed system obtains gaze of a
user who can be located from 1.4 to 3 m and can move within
100 cm × 40 cm × 100 cm cuboid space. The proposed system
consists of two wide-view cameras for stereo, a narrow-view
camera mounted on pan and tilt unit, and two IR illuminators
as shown in Fig. 1(b). Two wide-view cameras (baseline is
50 cm) are attached on the bottom of 60-in TV whose resolution
is 1920 × 1080 pixels. The narrow-view camera captures the
high-resolution eye image. The viewing direction of the narrowview camera is controlled by the pan and tilt unit. The focus unit
changes the focus of the narrow-view camera to capture a clear
eye image, while the eye is located at various distances [2], [7].
Two IR illuminators are placed at either side of the narrowview camera. In order to enlighten the user at a distance, each
IR illuminator consists of 16 IR LEDs. Special spread lens are
attached in front of IR LEDs to shine IR light at wider area.
B. Calibration
To obtain the 3-D position of face and eye, stereo calibration
is performed to the two wide-view cameras. The moving angles
of the pan and tilt unit are calculated from the 3-D position
of the eye using the calibration method proposed in [2]. In
the virtual camera model proposed in [2], the moving angles
of pan and tilt unit are converted to the 2-D positions on the
virtual image plane. The pairs of 3-D position of the target
object in the real-world coordinate and 2-D position of the target
object on the virtual image plane are obtained in calibration step.
From these pairs, the projection matrix M is calculated by the
direct linear transformation and nonlinear optimization method,

3434

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 12, DECEMBER 2013

Fig. 1. Configuration of the proposed system. (a) Proposed system targets the
user located from 1.4 to 3 m and the working volume of the proposed system is
100 cm × 40 cm × 100 cm. (b) Setup of cameras and IR illuminators.

Levenberg–Marquardt algorithm [21]. The focus calibration is
performed by making a lookup table between the distance of the
target object and the pulse of the focus motor.
C. Pupil Center and Glint Detection
The eye image captured by the narrow-view camera is shown
in Fig. 2(a). The resolution of the eye image is 1920 × 1080
pixels. Pupil and glint are detected by the method proposed
in [2]. The original pupil region is shown in Fig. 2(b) and the
detection result is depicted in Fig. 2(c). The center of the pupil
which is depicted as a green cross in Fig. 2(c) is estimated by
the ellipse fitting technique. The average position of two glints
which is depicted as a red cross in Fig. 2(c) is used in the
proposed method described in Section III-A.
D. Gaze Estimation
In 2-D mapping-based gaze estimation, PCCR vector is
mapped to the pixel position on the screen. When the user
gazes calibration points on the screen in the user calibration
step, PCCR vectors, v = (vx , vy ), in the eye image and the positions of each calibration point, u = (ux , uy ), in pixel on the
screen are collected. The mapping function between two vector
sets, v and u, is estimated as follows:
ux = a0 + a1 vx + a2 vy + a3 vx vy + a4 vx2 + a5 vy2
uy = b0 + b1 vx + b2 vy + b3 vx vy + b4 vx2 + b5 vy2

(1)

Fig. 2. Eye image captured by the narrow-view camera: (a) the original image,
(b) pupil and glints, and (c) the result of pupil and glint detection. Red ellipse is
the result of the ellipse fitting. Green and red crosses are the center of the pupil
and the average position of two glints, respectively, detected by the method
proposed in [2].

where a and b are mapping parameters of the mapping function
[2], [10], [22].
III. PROPOSED METHOD
When the user changes the position after the user calibration
is performed, PCCR vectors v need to be obtained again by performing the user calibration again and the mapping parameters
in (1), a and b, need to be recalculated because v is only valid at
or near to the position where the user calibration is performed.
If it is possible to find the movement mapping function h between two PCCR vector sets, one obtained at the user calibration
position and another obtained at the new position, robust gaze
estimation is possible without additional user calibration process even though the user moves away from the user calibration
position.
A. Movement Mapping Function
Fig. 3 shows the formation of the cornea, screen plane such
as TV or monitor, and image plane of the narrow-view camera
when the eye is at two different positions Er and En and the user
gazes the same point S0 on the screen. Subscript r stands for
the reference position where the pan and tilt angles are zero and

CHO AND KIM: LONG-RANGE GAZE TRACKING SYSTEM FOR LARGE MOVEMENTS

3435

projection of S0 onto the optical axis of the narrow-view camera.
The relation between the pupil and the calibration point can be
expressed as follows:




−−−→
−−−→
(5)
Dxr Er Sr  = Vxr Er Qr 




−−−→
−−−→
Dxn En Sn  = Vxn En Qn  .
(6)
By substituting (5) and (6) into (4), the following equation is
obtained:





−−→
−−−→ −−−→
OQr  Dxn En Qn  Er Sr 




vxn = 
(7)
−−−→ Dxr −−−→ −−−→ vxr .
OQn 
Er Qr  En Sn 
Another relation between the pupil and the calibration point
can be expressed as follows:






−−−→ −−−→
−−−→ −−−→
(8)
Er Pr  Er Qr  = Er S0  Er Sr 






−−−→ −−−→
−−−→ −−−→
(9)
En Pn  En Qn  = En S0  En Sn  .

Fig. 3. Pupil, glint, screen plane, and image plane formations when the user is
at two locations while he gazes the same point on the screen. The image plane
of the narrow-view camera is rotated by the pan and tilt unit.

screen plane and image plane are parallel. Subscript n stands for
the new position of the user. IR illuminator is assumed to be at O
which is the optical center of the narrow-view camera. Because
IR illuminator cannot be physically placed at O, the average
position of two glints which are corneal reflections of two IR
illuminators is treated as the corneal reflection of the virtual IR
illuminator at O. If Fig. 3 is assumed to the top view of the system, the user moves along horizontal direction and θ in Fig. 3 is
the angle of the pan unit, θp . Let P and p be the location of the
center of the pupil on the cornea and in the image, respectively.
Let G and g be the location of the virtual glint on the cornea
−−→
→
and in the image, respectively. Assume that g−−
r pr and gn pn are
represented as vr = (vxr , vy r ) and vn = (vxn , vy n ), respec−−−→
−−−→
tively, and Qr Pr and Qn Pn are represented as (Vxr , Vy r ) and
(Vxn , Vy n ), respectively, where Q is the orthogonal projection
of P onto the optical axis of the narrow-view camera.
When the user gazes the same point S0 on the screen, the
relation between PCCR vector in the image and pupil center
can be expressed as follows:


−−→
(2)
Vxr OQr  = −vxr /f



−−−→
Vxn OQn  = −vxn /f
(3)
where f is the focal length of the narrow-view camera. The
following equation is derived by combining (2) and (3):



−−→ −−−→
(4)
vxn = OQr  OQn  ∗ (Vxn /Vxr ) ∗ vxr .
−−−→
−−−→
Assume that Sr S0 and Sn S0 are represented as (Dxr , Dy r )
and (Dxn , Dy n ), respectively, where Sr and Sn are orthogonal

Therefore, the following equation can be derived by substituting
−−−→
−−−→
(8) and (9) into (7), and Er Pr  = En Pn :




−−→
−−−→
OQr  Dxn Er S0 



(10)
vxn = 
−−−→ Dxr −−−→ vxr = kx (vxr )
OQn 
En S0 
where kx is the movement mapping function of x component.
If Fig. 3 is assumed to the side view of the system, the user
moves along vertical direction and θ in Fig. 3 is the angle of the
tilt unit, θt . The following equation which is similar to (10) can
be derived:




−−→
−−−→
OQr  Dy n Er S0 



(11)
vy n = 
−−−→ Dy r −−−→ vy r = ky (vy r ).
OQn 
En S0 
where ky is the movement mapping function of y component.
The new PCCR vector vn at the new user position can be
calculated by changing the scale of PCCR vector vr at the
reference position using (10) and (11). In the rest of the section,
the methods to calculate unknown values in (10) and (11) are
described.
−−−→
−−→
OQr  and OQn  in (10) and (11) are the distance from the
optical center of the narrow-view camera to eyes. Assume that
3-D positions of eyes, Er and En , obtained by two wide-view
cameras are represented as (xr , yr , zr ) and (xn , yn , zn ). These
3-D positions of eyes are the relative position of eyes from the
origin of the world coordinate which is at one of two wide-view
−−−→
−−→
cameras. In order to calculate OQr  and OQn , the origin
of the world coordinate is needed to move to the optical center
of the narrow-view camera. Projection matrix M obtained from
the pan–tilt calibration, which is described in Section II-B, is
decomposed as follows:
M = K[R|t]

(12)

where K, R, and t are intrinsic matrix, rotation matrix, and
translation vector, respectively [2]. In (12), R and t are rotation
and translation between the origin of the world coordinate and

3436

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 12, DECEMBER 2013

Fig. 5. Final movement mapping function is calculated by the two steps: 1)
the inverse of the movement mapping function from the reference position to the
user calibration position; 2) the movement mapping function from the reference
position to the new user position.

Fig. 4.

Placement of the user calibration points on the screen.

the intersection of pan and tilt axis which is very close to optical
center of the narrow-view camera. Using R and t, distances
between the optical center O and 3-D positions of eyes, Er and
En , are obtained as follows:
 

 

 −−→
−−→ 
T
(13)
OEr  = R [xr , yr , zr ] + t ≈ OQr 

 

 
−−→ 
 −−−→
T
(14)
OEn  = R [xn , yn , zn ] + t ≈ OQn  .
−−→
−−−→
−−→
OQr  and OQn  are approximately equal to OEr  and
−−−→
−−−→
−−→
OEn , respectively, because Er Qr  and En Qn  are ex−−→
−−→
tremely small comparing to OEr  and OEn , respectively.
Dxr and Dy r are the horizontal and vertical distances from
the user calibration position to the optical axis, respectively, as
−−→
shown in Fig. 4. OSr  is the distance from the screen to the
optical center of the narrow-view camera. These distances are
measured manually and are not changed unless the system is
set up again. The relative 3-D position of the calibration point
S0 on the screen from the optical center O can be represented
−−→
as (Dxr , Dy r , −OSr ). This calibration point S0 is rotated by
the pan and tilt angles, θp and θt , and the rotated position of S0
is obtained as follows:



−−→ T
(Dxn , Dy n , Dz n )T = Rt Rp Dxr , Dy r , − OSr 
(15)
where Rt and Rp are set as follows:
⎤
⎤
⎡
⎡
1
0
0
cos θp
0 sin θp
1
0 ⎦.
Rt = ⎣0 cos θt − sin θt ⎦, Rp = ⎣ 0
0 sin θt
cos θt
− sin θp 0 cos θp
−−−→
−−−→
Er S0  and En S0  in (10) and (11) are obtained as follows:




 


−−→ −−→ 2
−−−→
2 + D2 + 
OS
+
OE
(16)

Er S0  = Dxr



r
r
yr





2

−−→
−−−→
2 + D2 + 
En S0  = Dxn
OEn  − Dz n .
yn

(17)

Finally, all unknown values in (10) and (11) are obtained
and the new PCCR vector vn can be calculated by movement

Fig. 6.

Eight positions where the evaluation was performed.

mapping functions, kx and ky , from the reference position to the
new user position. When the user performs the user calibration,
the eye position is usually different from the reference position.
Assume that the movement mapping function from the reference
position to the user calibration position is represented as k c and
the movement mapping function from the reference position
to the new user position is represented as k n . PCCR vectors
obtained from the user calibration are transformed to PCCR
vectors at the reference position by the inverse function of k c .
Then, PCCR vectors at the reference position are transformed
to PCCR vectors at the new user position by the function k n as
shown in Fig. 5. The final movement mapping function h from
the user calibration position to the current user position can be
obtained as follows:
−1

vxn = hx (vxc ) = kxn (kxc (vxc ))




−−→
−−−→
OQc  Dxn Ec S0 



=
−−−→ Dxc −−−→ vxc
OQn 
En S0 

(18)

−1

vy n = hy (vy c ) = kyn (kyc (vy c ))




−−→
−−−→
OQc  Dy n Ec S0 



=
−−−→ Dy c −−−→ vy c .
OQn 
En S0 

(19)

In every frame, PCCR vectors obtained from the user calibration are updated by (18) and (19) and mapping parameters
in (1) are recalculated. Finally, the final POG of the user at the
current position is calculated by (1).

CHO AND KIM: LONG-RANGE GAZE TRACKING SYSTEM FOR LARGE MOVEMENTS

3437

TABLE I
AVERAGE GAZE ESTIMATION ACCURACY FOR SEVEN SUBJECTS
AT EIGHT POSITIONS

Fig. 8.

Average gaze estimation error at eight positions for seven subjects.

Fig. 9. Plot of the gaze estimation points and ground truth points. Red “o”
represents the ground truth point and each number represents the estimated gaze
point.

Fig. 7. Eye image at the position 4: (a) the original image, (b) pupil and glints,
and (c) appearance of several distortions such as eccentric pupil ellipse and
twisted glints.
TABLE II
PCCR VECTOR COMPARISON AT POSITION 7

B. User Calibration
In (18), Dxc can be close to zero when the calibration point
is located at the center of the screen in the x-axis. When Dxc
is close to zero, the denominator of (18) is also close to zero
−−→
and the result of (18) is very sensitive to small error of OQc 
−−−→
and Ec S0 . To prevent this problem, the placement of the
user calibration points on the screen is decided as shown in
Fig. 4. When uy in (1) is calculated, PCCR vectors of seven
user calibration points updated by (19) are used. When ux in
(1) is calculated, the six user calibration points except the center
point are updated by (18) and the x element of the PCCR vector
of the center point is calculated by the average of x element of
other updated PCCR vectors.

3438

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 12, DECEMBER 2013

Fig. 10. Plot of the gaze estimation points when the user moves 100 cm to the horizontal direction. (a) Gaze estimation result (user calibration at position 3,
evaluation at position 3). (b) Gaze estimation result after the user moves 100 cm to the horizontal direction (user calibration at position 3, evaluation at position
2). (c) Gaze estimation result (user calibration at position 2, evaluation at position 2). (d) Gaze estimation result after the user moves 100 cm to the horizontal
direction (user calibration at position 2, evaluation at position 3).

IV. EXPERIMENTAL RESULTS
In the user calibration, the subject gazed seven calibration
points on the screen as shown in Fig. 4. User calibration was
performed at the center of the screen, 210 cm away from the
screen while sitting. In the evaluation, the subject gazed nine
points on the screen for 1 s for each point. Evaluation was performed at eight different positions including the user calibration
position as shown in Fig. 6. The proposed method was tested by
six subjects. The subjects included five males and one female,
with ages ranging from 23 to 33 years. The average angular
error of the proposed method over six subjects at eight positions
is shown in Table I. The proposed method was compared with
the basic 2-D mapping-based gaze estimation method whose
mapping parameters were not updated while the user moved
around. In the calibration position, the average angular error of
the proposed method was 0.64◦ . At positions 7 and 8 which were
the farthest positions from the calibration position, the average

angular errors were around 0.71◦ . Only 0.07◦ of gaze estimation
error was increased, while the user moved around 81 cm from
the user calibration position. Overall average angular error of the
proposed method is 0.8◦ . The biggest gaze estimation error was
shown at position 4. When the user sat at position 4 and gazed
the upper part of the screen, the gaze direction of the user had
big difference with the optical axis of the narrow-view camera.
Therefore, the projection of the pupil appeared as ellipse in the
image, as shown in Fig. 7(a), and the center of pupil found in the
image, as shown in Fig. 7(c), was not the real pupil center. Fig. 8
shows the average gaze estimation error for each subject at eight
positions. All users had the biggest gaze estimation error at position 4. In the first experiment, the proposed method was tested
when the user was sitting on the chair or standing. However, it
does not mean that the working range of the proposed method
is limited to 40 cm in vertical axis. Theoretically, the working
range of the proposed method in vertical axis can be extended
up to the horizontal working range of the proposed method.

CHO AND KIM: LONG-RANGE GAZE TRACKING SYSTEM FOR LARGE MOVEMENTS

TABLE III
COMPARISON OF THE PREVIOUS AND PROPOSED METHOD

3439

user. In the experimental results, the proposed method outperformed other gaze estimation methods in terms of the working
volume, working range, and gaze estimation accuracy. The average angular error of the proposed method was found 0.8◦ and
the largest angular error within the working volume 100 cm ×
40 cm × 100 cm was estimated as 1.4◦ .
ACKNOWLEDGMENT

To validate the proposed movement mapping function, three
different PCCR vector sets were compared in Table II. In Table II, original and reference PCCR vectors were obtained by
the user calibration process at positions 1 and 7, respectively.
The updated PCCR vectors were calculated by applying the proposed movement mapping function into original PCCR vectors.
It was observed that the updated PCCR vectors were analogous
to the reference PCCR vectors, which validates the proposed
movement mapping function.
Fig. 9 displays the estimated POG of one user at eight positions and the ground truth points, which are depicted as red
circle. It shows that the proposed method robustly estimates
POG under large movement of the user.
In the second experiments, the user calibration was performed
at either position 2 or position 3 and the evaluation was performed at either position 3 or position 2, respectively. Gaze
estimation result when the user calibration was performed at
position 3 and the evaluation was performed at position 2 is
shown in Fig. 10(a) and (b). Only 0.25◦ of the angular error
is increased, while the user moved 100 cm to the horizontal
direction. Fig. 10(c) and (d) shows the gaze estimation result
when the user calibration was performed at position 2 and the
evaluation was performed at position 3.
Table III shows the comparison among other systems and
the proposed method that outperforms other methods in terms
of accuracy, working volume, and working range. The proposed
method provides a larger working volume and competitive working range and accuracy.
The proposed gaze estimation method is implemented using
C++ on a PC with Intel I7-3770 3.4-GHz CPU and 8-GB RAM.
The image resolution of the wide-view camera is 1280 × 720
pixels and the image resolution of the narrow-view camera is
1920 × 1080 pixels. The computation time of the proposed
movement mapping function is approximately 0.04 ms and the
gaze tracking system can run at 25 frames/s.
V. CONCLUSION
In this paper, a longer range gaze tracking system is proposed,
which allows natural movement of the user without deterioration
in gaze estimation accuracy. The movement mapping function
uses some geometric information and 3-D user position. The
function then adjusts the scale of PCCR vectors and updates
the parameters of 2-D mapping function. The movement mapping function effectively compensates the large movement of the

The authors would like to thank H.-K. Lee, I. Lee, and J.
Cha of Electronics and Telecommunications Research Institute,
Daejeon, Korea, for their support in hardware implementation.
They would also like to thank M. S. Muhammad for his help in
improving the readability of this manuscript.
REFERENCES
[1] Z. Zhu and Q. Ji, “Novel eye gaze tracking techniques under natural head
movement,” IEEE Trans. Biomed. Eng., vol. 54, no. 12, pp. 2246–2260,
Dec. 2007.
[2] D. C. Cho, W. S. Yap, H. K. Lee, I. J. Lee, and W. Y. Kim, “Long range eye
gaze tracking system for a large screen,” IEEE Trans. Consum. Electron.,
vol. 58, no. 4, pp. 1119–1128, Nov. 2012.
[3] T. Ohno and N. Mukawa, “A free-head, simple calibration, gaze tracking
system that enables gaze-based interaction,” in Proc. Symp. Eye Tracking
Res. Appl., 2004, pp. 115–122.
[4] H. C. Lee, D. T. Luong, C. W. Cho, E. C. Lee, and K. R. Park, “Gaze
tracking system at a distance for controlling IPTV,” IEEE Trans. Consum.
Electron., vol. 56, no. 4, pp. 2577–2583, Nov. 2010.
[5] C. Hennessey and J. Fiset, “Long range eye tracking: Bringing eye tracking
into the living room,” in Proc. Symp. Eye Tracking Res. Appl., 2012,
pp. 249–252.
[6] D. H. Yoo and M. J. Chung, “A novel non-intrusive eye gaze estimation
using cross-ratio under large head motion,” Comput. Vis. Image Understanding, vol. 98, pp. 25–51, 2005.
[7] D. Beymer and M. Flickner, “Eye gaze tracking using an active stereo
head,” in Proc. Int. Conf. Comput. Vis. Pattern Recog., 2003, vol. 2, p. II451-8.
[8] T. Ohno, N. Mukawa, and A. Yoshikawa, “FreeGaze: A gaze tracking
system for everyday gaze interaction,” in Proc. Symp. Eye Tracking Res.
Appl., 2002, pp. 125–132.
[9] M. J. Reale, S. Canavan, L. Yin, K. Hu, and T. Hung, “A multi-gesture
interaction system using a 3-D Iris disk model for gaze estimation and an
active appearance model for 3-D hand pointing,” IEEE Trans. Multimedia,
vol. 13, no. 3, pp. 474–486, Jun. 2011.
[10] C. A. Hennessey and P. D. Lawrence, “Improving the accuracy and reliability of remote system-calibration-free eye-gaze tracking,” IEEE Trans.
Biomed. Eng., vol. 56, no. 7, pp. 1891–1900, Jul. 2009.
[11] C. Hennessey, B. Noureddin, and P. Lawrence, “A single camera eye-gaze
tracking system with free head motion,” in Proc. Symp. Eye Tracking Res.
Appl., 2006, pp. 87–94.
[12] D. Mardanbegi and D. W. Hansen, “Mobile gaze-based screen interaction
in 3D environments,” in Proc. Novel Gaze-Controlled Appl., 2011, p. 2.
[13] S. M. Kolakowski and J. B. Pelz, “Compensating for eye tracker camera
movement,” in Proc. Symp. Eye Tracking Res. Appl., 2006, pp. 79–85.
[14] D. Li, D. Winfield, and D. J. Parkhurst, “Starburst: A hybrid algorithm
for video-based eye tracking combining feature-based and model-based
approaches,” in Proc. CVPR Workshops, 2005, pp. 79–79.
[15] T. Toyama, T. Kieninger, F. Shafait, and A. Dengel, “Gaze guided object recognition using a head-mounted eye tracker,” in Proc. Symp. Eye
Tracking Res. Appl., 2012, pp. 91–98.
[16] J. G. Wang, E. Sung, and R. Venkateswarlu, “Estimating the eye gaze from
one eye,” Comput. Vis. Image Understanding, vol. 98, pp. 83–103, 2005.
[17] A. Villanueva, R. Cabeza, and S. Porta, “Eye tracking: Pupil orientation
geometrical modeling,” Image Vis. Comput., vol. 24, pp. 663–679, 2006.
[18] A. Villanueva, R. Cabeza, and S. Porta, “Gaze tracking system model
based on physical parameters,” Int. J. Pattern Recog. Artif. Intell., vol. 21,
pp. 855–877, 2007.
[19] C. H. Morimoto, A. Amir, and M. Flickner, “Detecting eye position and
gaze from a single camera and 2 light sources,” in Proc. Int. Conf. Pattern
Recog., 2002, pp. 314–317.

3440

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 12, DECEMBER 2013

[20] D. W. Hansen and Q. Ji, “In the eye of the beholder: A survey of models
for eyes and gaze,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 32, no. 3,
pp. 478–500, Mar. 2010.
[21] J. More, “The Levenberg-Marquardt algorithm: Implementation and theory,” Numerical Anal., vol. 630, pp. 105–116, 1978.
[22] C. H. Morimoto and M. R. M. Mimica, “Eye gaze tracking techniques
for interactive applications,” Comput. Vis. Image Understanding, vol. 98,
pp. 4–24, 2005.

Dong-Chan Cho (S’12) received the B.S. degree in
electrical and computer engineering from Hanyang
University, Seoul, Korea, in 2007, where he is currently working toward the Ph.D. degree in the Department of Electronics and Computer Engineering.
His research interests include eye gaze tracking
and intelligent vehicle system.

Whoi-Yul Kim (M’85) received the Ph.D. degree in
electronics engineering from Purdue University, West
Lafayette, IN, USA, in 1989.
From 1989 to 1994, he was with the Erick Johnson School of Engineering and Computer Science,
The University of Texas at Dallas. Since 1994, he
has been on the faculty of Electronic Engineering at
Hanyang University, Seoul, Korea. His research interests include eye gaze tracking and correction, gesture
recognition using various depth sensors, intelligent
vehicle system, and biomimicry.

