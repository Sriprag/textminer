266

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

FEEL: A System for Frequent Event and
Electrodermal Activity Labeling
Yadid Ayzenberg and Rosalind W. Picard, Fellow, IEEE

Abstract—The wide availability of low-cost wearable biophysiological sensors enables us to measure how the environment and
our experiences impact our physiology. This creates a challenge: in
order to interpret the longitudinal data, we require the matching
contextual information as well. Collecting continuous biophysiological data makes it unfeasible to rely solely on our memory for
contextual information. In this paper, we first present an architecture and implementation of a system for the acquisition, processing, and visualization of biophysiological signals and contextual
information. Next, we present the results of a user study: users
wore electrodermal activity wrist sensors that measured their autonomic arousal. These users uploaded the sensor data at the end
of each day. At first, they annotated their events at the end of each
day; then, after a two-day break, they annotated the data from two
days earlier. One group of users had access to both the signal and
the contextual information collected by the mobile phone and the
other group could only access the biophysiological signal. At the
end of the study, the users filled in a system usability scale and user
experience surveys. Our results show that the system enables the
users to annotate biophysiological signals at a greater effectiveness
than the current state of the art while also providing very good
usability.
Index Terms—Context, mobile computing, pervasive computing,
wearable sensors.

I. INTRODUCTION
A. Problem of Missing Context
TATE-OF-THE-ART technology has made it possible to
monitor various physiological signals for prolonged periods. A person can now choose to don a wearable sensor, collect
data 24/7, and save it digitally for later analysis. Such a technology opens the door for a multitude of exciting and innovative
applications. We could learn the effects of the environment and
our day-to-day actions, and choices on our physiology. How
does the number of sleep hours affect our activity levels during
the following day? How do the times we schedule our meals
impact our performance? How does physical activity affect our
quality of sleep? Do such choices have an impact on chronic
conditions?
Physiological signals are only part of the information required
to answer such questions. It is necessary to label these signals

S

Manuscript received December 27, 2012; revised May 28, 2013; accepted
August 5, 2013. Date of publication August 15, 2013; date of current version
December 31, 2013. This work was supported in part by the Media Lab member
consortium.
Y. Ayzenberg and R. W. Picard are with the MIT Media Lab, Cambridge, MA
02139 USA (e-mail: yadid@media.mit.edu; picard@media.mit.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2278213

with contextual information. Context is defined as: “the circumstances that form the setting for an event, statement, or idea, and
in terms of which it can be fully understood and assessed” [1].
Ideally, one would want to have access to full contextual information alongside with the sensor data. This may include
location, activity, nutrition, social interactions, etc.
Envision the following scenario: you wear a biophysiological
sensor for a few weeks as part of a study. At the end of the
study, you meet with the researcher who has analyzed the data.
During the meeting, the researcher points your attention to a
very large spike in the data from approximately two weeks ago.
He would like to know what had occurred at that time. You have
trouble remembering where you were, what you were doing,
or who you were with at the time. It is very difficult for you
to remember without relying on external aids. You open your
notebook to see if you jotted down relevant notes during that
timeframe, access your online calendar, your phone and inbox
to see if they contain any clues to as what you were doing at the
time. Imagine repeating this process for multiple spikes.
The scenario above makes it evident that a context acquisition
system could prove to be a valuable tool. The collected context
around the time of interest could assist in recalling even if the
system did not collect the context at the exact time that was
required for analysis.
The contextual information can be acquired by one of the
following two methods.
1) Self-report—the individual being monitored maintains a
diary and logs the required information.
2) Observation —an entity (human or machine) observes
the monitored individual and maintains a journal of the
individuals’ actions.
The first method has one major drawback: It requires the monitored individual to actively report the necessary information.
This by itself is intrusive causing a disruption in the person’s
routines. Another limitation is that the individual is unlikely to
be aware of all the minute events and their significance as they
occur. A seemingly insignificant event may have a notable impact on our physiology. An event’s true significance may only be
revealed during the analysis of the physiological data alongside
the contextual information.
The second method may seem attractive at first: enable a
machine to record the entire context: video, audio, and all other
information. But this technique is not without limitations: It
will result in massive amounts of raw data that will require the
data classification for finding pertinent data points, for instance,
classifying whether someone appearing in a video recording is
considered a social interaction or just a person passing by. It is
necessary to filter all of the contextual information in order to
determine the “relevant context.”

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

A third approach, and the focus of our work here, is to build
a tool that balances the strengths of human and machine, and
makes it easy to collect the most relevant context while not
burdening the user. Human input can trigger the automated collection of contextual data during significant events. Triggers
can also be automated based on changes in variables such as
location, voice, or human action such as receiving a phone call.
Below, we describe the motivation for our system and how it is
built, tested, and shown to be highly usable. We also show how
our new way of adding context is more effective than what was
available before.
B. Motivation—the “Crit Day” Study
Each year, the MIT Media Lab organizes an event in which
second year master’s students present their thesis proposal to
all of faculty, students, and researchers. Our research group has
long been interested in how to help people better measure and
understand real-world emotions. We decided that “Crit Day”
was a valuable opportunity to measure the physiological effects
of public speaking, together with the stress that precedes the
event, and the (hopeful) reduction in stress that follows it.
We recruited 11 graduate students, 8 males and 3 females,
who were designated to be present on “Crit Day”. During the
study, we collected skin conductance (as a measure of electrodermal activity or EDA), skin temperature, and three-axis
accelerometer, using the Affectiva Q wristband sensor. Each
participant received a pair of sensors, one for each wrist, so we
could collect bilateral data. (Bilateral analysis is the subject of
another study.) The participants were asked to wear the sensors
for 72 h starting on the morning of the day before their presentation. In addition, we interviewed each participant a few days
after the end of the measurements. During those interviews, we
asked the participants to describe their experiences during the
72 h. We asked them to note any unexpected events or events
that caused them a great deal of anxiety or emotional strain.
We recorded the exact times of presentation and Q&A sessions after the presentation for each participant. We found that
we had all the necessary information to perform analysis on
the EDA signal recorded on those times. However, on the other
recordings, we were missing key contextual information to determine what had happened. On some of the EDA peaks, we
could determine that there was a high level of activity that could
account for the increase in EDA but for others we lacked the
information needed to make an inference. We needed contextual
information. In order to obtain that data, we asked the participants to maintain journals of what had happened to them during
the three days of the study. We held exit interviews with the
participants at the conclusion of the study. We found that most
of the participants did not have a good memory as to what had
happened during the times when there was a rise in the EDA signal. Most participants consulted their journals, and if the events
were not transcribed in the journal, they used other means such
as phone logs and calendars to determine what they did at the
time. We realized there needed to be a better system for doing
this kind of research.

267

C. FEEL—Frequent EDA and Event Logging
Our specific need was to create a system that could capture
both EDA and events, for gaining insight into emotional experience associated with those events. We thus created a system we
called FEEL—frequent event and EDA labeling, to combine not
only a physiological sensor and a smart mobile phone, but also
to try new approaches that could capture the users’ context in an
unobtrusive manner. One of the drawbacks of this technology
is that the mobile phone does not have all the sensors and logic
to determine the full context of the user. It is aware of whether
or not a user is having a phone call, if the user is at a certain
location, if the user is browsing the web or using an app or reading an email, but it is not aware of other occurrences that are
more minute, such as is the user talking to a colleague giving
encouraging feedback or to one giving threatening feedback.
The resolution of context obtainable by a mobile phone is such
that it provides a high-level picture. We may still need the user
to label and annotate the signal to achieve a higher resolution. At
times, the high-level picture will provide sufficient information
as to whether or not a specific event contributed to the change
of the biological signal.
We decided to test a new approach: We would build a system
to provide users with the biophysiological signal combined with
the contextual information, and measure if they were able to
better recall which event occurred during that time. Even if we
could not record the specific context because the phone lacks
the sensors and logic to decode that context, we still have other
events before and after that could serve as anchors or memory
prosthetics for the user and enable him to gain insight into the
data and annotate and label those events.
This idea was tested by setting up a study comparing two
groups of users, one that could only access the biophysiological
signal and the other that could access both the signal and contextual information acquired by their mobile phone. Both groups
were required to label the signal in terms of their valence and
arousal as well as rate the confidence of their ratings. If the system combining biophysiology with context works better than
the one with biophysiology only, and it is considered easy to
use and not burdensome, then we would find validation for our
idea. Later in this paper we describe the specific study and its
component hypotheses.
The approach was to create a mobile application that runs
unobtrusively on the mobile phone as a service. The service
is constantly recording contextual information. It is aware of
changes in location, phone calls that the user is holding, calendar entries and meetings that appear on the calendar, and emails
that the user is reading. Any one of those events will be recorded
and can later serve as a reference point for annotating the physiological sensor recording. The second element of the system
is a web platform that combines the contextual information and
physiological information and enables the user to view the contextual information overlaid on top of the physiological data in
a user-friendly interface. The user can determine where he was
or what he was doing and view the changes that occur in his
physiology during those times.

268

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

We undertook the most important challenge: testing the system out in the wild, in real life where people are distracted, busy
and encountering true uncertainty and stress, in contrast with
testing it in a laboratory where participants believe that things
are under control, and stress is moderated. The goal was to see
if it was possible to record the users’ physiology and contextual
events in their everyday experiences and provide the users with
a tool that would assist them in annotating the data and also
assist in providing short-term insights into the data.
The long-term goal is to use the system with any type of
biophysiological sensors—not just EDA sensors; however, the
system was evaluated with EDA sensors because they are currently available commercially, are comfortable to wear, and have
sufficient battery life to last for a full day of recording.
This paper makes four main contributions. The first is a set of
guidelines for systems that acquire and process biophysiological
and contextual information. Second, we created an architecture
and implementation of a new mobile system for acquisition and
processing of biophysiological signals and contextual information. Third, we present a novel method to visualize biophysiological data along with contextual information. Finally, we
demonstrated the successful usability of the system with a user
study.
II. RELATED WORK
The proliferation of wearable sensing systems has made it
possible to continuously record various physiological signals
for prolonged periods. Affectiva [1], Bodymedia [2], and others
have created devices that collect a continuous stream of physiological data. Other devices such as Fitbit [3], FuelBand [4],
and Up [5] gather information regarding user activity for later
analysis and display on a PC or a mobile phone.
The common denominator between these devices is that they
collect physiological information but lack contextual information. It is possible to analyze the data in order to detect trends
such as: daily and monthly activity trends, physiological arousal
trends, etc. However, it is very difficult to perform a causal analysis on this data in order to find the underlying reasons for
differences and trends. For instance: inferring the reasons for an
observed increase in the physiological arousal level would require additional information. In order to answer questions of this
sort, it is necessary to determine contextual information. In this
paper, we create and test a system that obtains contextual information from a mobile phone in combination with physiological
data that is displayed in an intuitive interface.
A. Context Recognition Using Mobile Devices
The notion of automatically acquiring context by using mobile phones is well known in the field of life-logging or reality
data mining. This can be attributed to the fact that such phones
are pervasive and personal [6]. Brown and Randell [7] discussed
the idea of building a context sensitive phone and the various pitfalls that one may encounter in the process. Iso et al. [8] showed
that it was possible to extract personal context information by
using a mobile phone equipped with specialized sensors. The
authors extracted physical and environmental context as well
as biological user context that may be used to infer emotional

state. Lee and Kwon [9] proposed an architecture of general
wearable sensors for determining emotional context as well as
an architecture for emotion-based content services.
Kern et al. [10] showed that it was possible to determine
activity context from accelerator readings, social context from
audio, and interruptability from a combination of these along
with WLAN-based location estimation. The authors showed that
by using these techniques in an experimental evaluation, they
were able to achieve detection rates of 97.2% and 90.5% for
social and personal interruptability, respectively. The goal of
study was to demonstrate the viability of a system that would
automatically create real-world meta information that would
ease the retrieval of live life recordings. In a different work,
Lawrence [11] showed how the cell phone and its sensors can
be used to nonintrusively obtain many types of contextual data
such as location, body postures, or activities.
B. Context-Aware Experience Sampling
In traditional experience sampling, the user is polled at random times, and asked to report emotional state as well as current
activity. This is an intrusive process that affects the user’s experience and therefore may alter the reports.
In one study, Froehlich et al. [12] developed MyExperience,
a system for capturing both objective and subjective in situ data
on mobile computing activities. Various events such as device
usage, location, or context can trigger the capture of the current user experience. The user experience can be either readings
from sensors that are connected to the phone, or subjective user
feedback (such as a survey). Although this method is less invasive than triggering the question at random times, the users
still need to devote some cognitive effort and mentally map their
effect to an entry. A similar context-triggered approach was also
used by Picard and Liu [13], [14] who monitored stress levels
of PDA users and also provided a line of empathy while asking them to annotate their levels, a combination that resulted in
reduced stress compared to a similar but nonempathetic annotation condition. The system supported collection of heart signal
data, accelerometer, and pedometer information, as well as automatic labeling of location information from context beacons
and would poll the user to annotate his stress levels dependent
on the context.
In a different study, Kang et al. [15] measured the levels of
stress of cell phone users when using several services in order to create a personalized and more optimal service. Intille
et al. [16] developed a system for context-aware experience
sampling. Their system reduced the need for polling the user
at random times, and enabled researchers to query the user
only within a specific context (e.g. when a user is at a specific
location). Raij et al. [17] designed and evaluated a smartphonebased system that continuously collects and processes multimodal measurements from wearable wireless sensors to infer in
real time whether the user wearing the sensors is stressed and
generates prompts for timely collection of self-reports to record
the experience of stress when it is fresh in the user’s mind.
Work has also been done in the field of automatic assessment of the users’ affective state without the need for selfreport. Picard et al. [18] developed a physiological signal-based

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

emotion recognition system that used a facial muscle tension
sensor, photoplethysmyograph, electrodermal activity (EDA),
and a respiration sensor. Kim and Bang [19] proposed a similar system that uses EDA, skin temperature, and ECG sensors.
Ertin et al. [20] developed a wearable wireless sensor suite
that collects and processes cardiovascular, respiratory, and thermoregularity measurements on a smartphone in order to inform
about the user’s level of stress.
III. SYSTEM DESIGN AND ARCHITECTURE
In a system that annotates biophysiological data, there are
several necessary components. The first is the sensor, which
will acquire the physiological data and store it in digital format.
The second is a component that will acquire the contextual
data and provide the annotations. The third component is the
backend server, which processes, integrates, and stores both
the biophysiological and the contextual data. And finally, the
interface is a component that provides the user with a convenient
means for accessing, annotating, and interpreting the data. In the
following sections, we will describe each of these elements in
detail.
A. Physiological Data Acquisition
The FEEL system was designed to work with any physiological time series data. For the purpose of evaluating the system,
a sensor that would satisfy the following requirements were
selected.
1) Comfortable to wear for prolonged periods of time.
2) Extended battery life that would enable a day of operation
without charging.
3) Simple operation that would enable unobtrusive data acquisition.
The Q sensor manufactured by Affectiva complies with these
criteria. The Q sensor measures EDA or changes in the skin’s
ability to conduct electricity. The changes are a direct result
of the sympathetic nervous system (SNS) [21] controlling the
sweat glands in the subdermal layer. As the glands receive SNS
activation, the skin conductance increases. The changes in the
skin hydration may be the result of emotional changes, cognitive,
physical load, sweating, or thermo-regulation. The Q sensor can
measure these minuscule changes and store them in a digital
format within the sensor for later retrieval. The sensor also
records skin temperature and three-axis accelerometer. A USB
connector enables the user to connect the sensor to a computer
and download the data.
B. Acquiring Contextual Data—Using the Mobile Phone
as a Platform
Mobile technology is ubiquitous [6]: Over 87% of people
in the world [22] currently own a mobile phone. In addition
to enabling individuals to communicate with one another and
access information anywhere, modern mobile phones have a
multitude of sensors. Microphones, cameras, accelerometers,
GPS, and similar all provide the means for applications to read
the state of the user and of his or her immediate environment.

269

These communication and sensing capabilities make the mobile phone an exceptional tool for determining context. The
modern mobile phone is very compact yet has sufficient processing power to execute very demanding applications and therefore
is well suited for this type of project. And finally, the mobile
phone is one of the indispensable objects that people carry with
them [23] and as such is a good proxy for people’s interaction
with the environment.
C. FEEL Mobile Client
The FEEL Mobile Client (FMC) is a Java Android application architected and written by the first author, which runs
as a background service on the user’s mobile phone. Its main
objective is capturing changes in the user’s context and sending them to the backend server for processing and storage. The
FMC starts automatically when the phone is powered on and
is very lightweight in terms of memory, processor, and battery
usage. It was decided to support Android because it is a widely
deployed platform, has excellent developer support, and enables
easy access to all of the phone’s content (calendar, emails) and
sensors (GPS, accelerometers, Bluetooth) via a uniform API.
In the future, it will be possible to support IOS and Windows
Mobile. The design principles leading the design of the FMC
were as follows.
1) Unobtrusiveness—this was probably the most significant
principle. By definition, FEEL should sample the user’s
context without interrupting the user. An application that
requires the user to actively perform actions is intrusive
and may alter the very context that it is trying to measure.
2) Robustness—the primary goal of the FMC is data collection; therefore, it is imperative that the application maintains a persistent data store. If the user is in an area that
does not have mobile coverage, the FMC should retain the
data until connectivity is resumed.
3) Low battery use—short battery life would have a negative
impact on the data collection. If the phone shuts down,
there will be a gap in the data collection process. In addition, the user may not agree to use the application if it
shortens the phone’s battery life.
4) Low processor usage—the FMC should not affect any
other of the applications running on the phone and should
not block or delay any OS services.
5) Extendibility—novel mobile phones with new types of
sensors are constantly introduced to the market. Supporting new types of sensors should have few consequences
on the FMC architecture. The sensor monitors encapsulate the sensor specific data types and the communication
layers need not be aware of those.
6) Portability—the FMC was designed to run on a variety of
Android mobile phones. Because it is lightweight, it may
run on older devices with slower processors and smaller
memory sizes. Additionally, it does not utilize any device manufacture proprietary APIs but rather pure Android
APIs.
The FMC is composed of the following modules.
Activity monitors—these monitors are tailored to listen for
occurrences of specific events or changes in the users context.

270

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

A monitor may be of one of the two types: event-triggered or
periodic. An event-triggered monitor is asynchronous. It registers itself to receive notifications on the occurrences of events as
they occur in real time. For instance, the phone-call monitor will
be activated every time an incoming phone call arrives or when
the user initiates a new phone call (an off-hook event). Periodic
monitors are synchronous and are activated in fixed intervals,
perform a set of actions, and are then suspended until the next
interval. The period is set to create a good balance between
device battery life and event detection latency. An example for
a periodic monitor is the location monitor. It is activated every
30 min and determines whether the current location reading has
changed from the previous reading. After determining that an
event took place, the activity monitors sends a message to the
event listener module.
The following activity monitors are currently supported:
Phone-call monitor—the phone-call monitor is event triggered and supports the following events: incoming call start,
incoming call end, outgoing call start, and outgoing call end.
The monitor also measures the duration of the conversation. At
the end of the call, a message is generated. Calendar monitor—
this is a periodic monitor that is activated every 30 min. It scans
the users Google calendar and determines which calendar entries
start in the upcoming hour and sends these as messages to the
event listener. Email monitor—this is an event-triggered monitor that is triggered every time the user starts reading an email
or finishes reading an email. Currently, the K9mail [24] client is
supported. The K9mail client is an open-source Android email
client that was modified in order to generate an event each time
a user selects an email for reading, and each time a user exits
a mail reading view. The monitor measures when the event occurred and what was the duration of the reading. When the user
finishes reading the email a message is sent to the event listener.
Location monitor—This monitor is a periodic monitor that is activated every 30 min. It determines whether the current location
is different than previously read location using GPS and WiFi.
In the case of a change, a message is sent to the event listener.
Event listener—the event listener provides the activity monitors
with an interface for buffering events prior to their transmission to the FEEL backend server. The event listener stores the
events in a queue located in the phone memory. The buffer is
used to prevent cases where the ingress of event rates exceeds
the egress of the transmission rate to the backend server. Event
transmitter—this module sends events to the backend server located in the cloud. The event transmitter is suspended and waits
for messages in the message queue. It reads each event from the
queue, formats a URL, and tries to initiate an HTTP connection
with the backend server. When the connection succeeds, the
event transmitter sends the event details encoded in an HTTP
GET request. The event transmitter was designed with robustness in mind. There may be times when the connection to the
backend server will fail; the backend server may be down for
maintenance, the user may be located in an area with no network coverage, or at peak times the server may be momentarily
inaccessible due to an overload by other requests. In order to
support unexpected scenarios such as these, a retry mechanism
was implemented.

Fig. 1. FEEL backend server. The server collects the biophysiological sensor
data and mobile phone contextual data and stores it within the database. The
user can access the data using a web browser.

In case the connection fails, the transmitter will retry the
transmission and repeat this process three times. Finally, if the
transmission fails after these retries, the transmitter will suspend
itself and retry the whole transmission process after 20 min in
order to preserve battery life. User interface—the user interface
(UI) module provides a user interface to the FMC. It enables
the user to view the number of events currently stored in the
internal queue waiting for transmission to the backend server as
well as the time of the last connection to the server.
The FMC UI also enables manual initiation of the calendar
monitor to run within a window of two dates. All calendar
entries that appear within the configured window are sent to the
backend server. This is useful in case the user added entries to
the calendar retroactively. The monitor will only detect them
in case they are in the future, and, therefore, it is necessary to
initiate the calendar monitor manually using the interface.
D. Backend Server
The FEEL system was designed with cloud computing
paradigms in mind. Cloud computing enables high availability and scalability, which are desirable qualities in a large data,
computationally intensive application such as this. In addition,
high availability is necessary because the events are sent in
real time to the server. This enables data processing in real
time and in the future will facilitate for the system to perform
interventions.
Another key consideration made in the design of the server
was to maintain separation between the database logic, the application logic, and the user interface and each of these should
be fully self-contained. The model-view-controller (MVC) [25]
design pattern was used. By using MVC, it is possible to extend the application and add additional functionality to specific
elements with little or no modification to others. One could for
instance change the layout of the web page without affecting
the database or the application logic.
The server is composed of the following elements (see Fig. 1).
Web server—the web server has a dual purpose—it is used
both for receiving events from the FMC and for serving the user

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

interface web pages. We will discuss the latter functionality in
the user interface section.
Tornado 2.0 [26] is the web server that was selected for the
system. It is open source, lightweight, scalable, nonblocking,
and can handle thousands of simultaneous connections. Tornado is implemented entirely in Python and does not have any
dependence on external modules. In the FEEL implementation, Tornado is configured to run multiple instances in parallel to support concurrent connections. Each instance runs in
a separate process, and the processes are distributed evenly to
run across multiple processors (or cores) on a single machine.
Core application—the FEEL core application manages the data
and signal processing, data storage and data retrieval, and usersession information.
The core application is written completely in Python, an opensource object-oriented programming language. Python was chosen because of its ease of use, maintainability, platform independence, and the large number of available modules. An additional strength is the availability of NumPy [27] and SciPy [28]
: open-source packages for scientific computing with Python,
rather than using a commercial data analysis product such as
MATLAB that provides comparable functionality. Python version 2.7.1 was used along with NumPy version 1.5.1 and SciPy
version 0.8.0.
Other Python modules that were used include md5—
implements the interface to RSA’s MD5 message digest algorithm; UUID—provides immutable Universally Unique Identifiers (UUIDs); and, hashlib—implements a common interface
to many different secure hash and message digest algorithms.
Database server—The database server stores the following.
1) Physiological sensor data —EDA, skin temperature, and
accelerometer data. The sensor data are divided into segments of 30 min each for fast and flexible access to specific
measurement times.
2) Events and user context —phone call information, calendar entries, email reading, and location information.
3) User self-report data—event descriptions, valence,
arousal, and event recall clarity.
4) User profiles and session information —user email, phone
number, encrypted password, and current session.
MySQL [28], the world’s leading open-source relational
database was chosen as the system database. Its major benefits
are high performance, high reliability, ease of use, and widely
available support from the open-source community.
Server hardware and operating system: The FEEL system
was tested on a 64-bit Quad core AMD Opteron(tm) Processor
6180 SE. The cores were running at 2.5 GHz. The system had
2 GB of RAM. The operating system (OS) was Ubuntu Linux
11.04, kernel version 2.6.38–13.
E. Signal Processing
The FEEL system is designed to determine the high arousal
points in a user’s day. Several algorithms were evaluated to this
end. These algorithms were executed on a labeled dataset with
the end goal of correctly determining the most arousing points
of the day.

271

In all algorithms, the signal is smoothed in order to eliminate high-frequency components, such as movement artifacts.
To achieve this, the Python NumPy convolution function was
used. Algorithm 1 selects the highest peaks. The most arousing
events did not always produce the highest peaks, but rather wider
peaks (a higher level of EDA that lasted longer). Algorithm 2
selects the peaks with “slowest decay” (the highest negative
gradient). The downside of this approach is that very low peaks
are selected as well. Algorithm 3 produced sufficient results—it
sorts the peaks according to the number of skin conductance responses (SCRs) counted during 10 min (5 min prior to the local
maxima, and 5 min after). In most cases, it detected the peaks
associated with the arousing events. Algorithm 4 ordered the
peaks according to accelerometer activity levels (using the first,
second, and infinite moments of accelerometer data as a measure for activity). The downside of this approach is that some of
the most arousing events involved a high level of activity—for
instance, running due to being late to an important meeting.
It was decided to use Algorithm 3 for the duration of the study.
The chosen algorithm finds the peaks associated with the highest
skin conductance level (SCL), which describes the overall conductivity of the skin over a period of time that typically ranges
between tens of seconds to tens of minutes. It is usually thought
that high SCL is not triggered by a specific event, but due to
a general level of high arousal. The next step of the algorithm
counts the number of SCRs over a window of 10 min around the
detected SCL peak (5 min prior to the peak and 5 min after the
peak). SCRs usually last for several seconds, are high-frequency
changes to the EDA signal in contrast with the slow changes in
the SCL, and are elicited by specific stimuli. We assume that a
very arousing event will result in a large number of SCRs.
Whenever the user requests to view a stream of EDA, the
FEEL core application performs peak detection in real time on
the signal. Therefore, designing an algorithm with low run-time
complexity was important for providing a reactive system and a
good user experience.
Initially, the signal-processing code was written in Python.
Preliminary trials suggested that this approach would result in
a very slow system response. Processing the EDA for one day
for a single user took between 30 and 60 s. As a result, the
signal-processing code was re-written in C, which provides significantly faster execution times. The Python Weave module
was used in order to call the C signal-processing code from
within the Python code. EDA processing time was reduced to
5–10 s, which produced a noticeable improvement in system
responsiveness.
F. User Interface
The FEEL user interface is implemented as a web application. This approach was preferred over developing a standalone
graphical user interface (GUI) client application for the following reasons.
1) Platform independence—a standalone GUI application is
heavily dependent on the underlying OS, and it would
have been necessary to develop such an implementation
for multiple OS (Windows, Linux, and MAC OS-X) to

272

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

Fig. 2. Contextual viewer. This interface presents the biophysiological signal and contextual information overlaid on top of it. The signal includes EDA,
skin temperature, accelerometer, and overall arousal level. The contextual information includes email readings, phone calls, calendar entries, and geo-location
information.

support the various users and each with their own choice
of OS. A web application can run on any OS that includes
a modern web browser that supports JavaScript.
2) Ease of deployment—no installation is necessary. Users
access the application by using a web browser and entering
a URL in the browser address bar.
3) Ease of support—in the case of bug fixes or new features,
only one instance of the software needs to be fixed. Redeployment of the new version is as simple as restarting the
backend web server.
4) Low processing power—web applications typically require less processing power on the client. The bulk of
the data processing is done on the server and only the
results are sent to the client.
5) Mobile device support—modern mobile devices include
web browsers that are able to run web applications. An
additional benefit is extended battery life as most of the
processing is done on the backend server.
The UI is rendered using a combination of HTML, JavaScript,
and CSS, which provide the flexibility to format each displayed
element and perform various manipulations (such as hiding,
moving, resizing, etc.) on elements during run-time. AJAX is
used to receive data from the backend server in an asynchronous
manner. This saves the need to reload the HTML page each time
new data are requested and creates a responsive interface.
1) Login Screen: The user may either signup or login to the
system by providing an email address, mobile phone number,
and password. The login screen also contains links to download
the FEEL mobile client and the modified version of the K9
mobile email client.

2) Event Viewer: The event viewer (see Fig. 2, top left) enables the user to view all of the recorded events in a convenient table. The “Type” column shows what type of event was
captured, the “Time” column shows the time that the event occurred, the ‘Duration’ column shows amount of time between
event onset and offset, and the “Memo” column shows additional information used for event recall. The “Memo” column
may contain the phone number and contact name in case of a
phone call, the calendar entry title in case of a calendar entry,
and the email subject in case of a read-email event.
The event viewer also enables sorting of the events according to type, time, duration, or memo. Clicking on the relevant
column header does this. It is possible to switch between ascending and descending sorts by clicking the column header
repeatedly. Clicking on an entry in the event viewer will load
the corresponding physiological recording data in a frame below the event viewer. The event user was implemented using
JqGrid [29].
3) Calendar Widget: This widget (see Fig. 2, top center)
enables the user to view the recordings of a specific date. The
physiological data and event data are displayed in a frame below
the widget. The displayed range is between midnight of the day
before the selected date and midnight of the selected date.
4) EDA Display: This frame (see Fig. 2, bottom left) displays the user’s EDA, skin temperature, and accelerometer data
along with the events that were recorded during the same time
frame. Although in the current implementation the frame is used
for displaying EDA, it may be used for displaying other time
series data as well. Icons overlaid below the EDA signal represent the various types of events. When hovering the mouse over

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

an event, the event details are displayed. This is done in order to
cause minimal obstruction to the EDA signal view. By clicking
on a series type in the legend, it is possible to hide/show that
series in the frame. The EDA display was implemented using
the HighStock [30] framework.
The following types of zoom are supported.
1) Click and drag —clicking within the EDA plot area and
dragging will zoom into the marked area.
2) Navigator series —a small series below the main series,
displaying a view of the entire dataset. It provides tools to
zoom in and out on parts of the data as well as panning
across the dataset.
3) Range selector —the range selector is a tool for selecting
ranges to display within the chart. It provides buttons to
select preconfigured ranges in the chart: 10 min, 1 day and
the full dataset for a selected range.
5) Location Viewer: This frame (see Fig. 2, bottom right)
displays the user’s historical locations overlaid on a map. Each
location is marked by a red pin. When the mouse hovers over
a specific pin, the time of the location recording is displayed.
When a user selects a date, all of the locations that the user
visited during that date are displayed. The locations are also
overlaid on the EDA signal using the same red pin icon. When
clicking a location pin on the EDA signal, the map viewer is
automatically adjusted to display that location in the center of
the map frame. It is possible to select between Map display
mode, Satellite display mode and Street View mode. Panning
and zooming are also supported.
6) User Self-Report: The FEEL system performs tonic level
peak detection on the EDA signal and selects three peaks for the
user to provide additional details. When the user clicks a peak
marked with an exclamation mark, a dialog containing a form
will be displayed. After the user fills in all the fields and clicks
the Save button, the report is sent to the backend server. Once a
report is submitted, the blue exclamation mark is replaced by a
green tick mark, which signifies that a report has been already
submitted for that peak.
IV. SYSTEM EVALUATION
The study protocol was preapproved by the MIT committee
on the use of humans as experimental subjects. The goal of the
system evaluation is twofold.
1) Ensuring that the system operates as expected and that it
fulfills its design goals.
2) Obtaining feedback from the participants about the system
usability and overall value.
We hypothesized that that the FEEL platform increases the
data collection effectiveness and the quality of data that is collected. We examine measures of these in a controlled study,
described below.
A. Participant Recruitment
Participants were recruited by sending emails to multiple
mailing lists at MIT. In order to be eligible, candidates had
to satisfy the following constraints.
1) Participant owns an Android mobile phone.

273

2) Participant uses the Google calendar application to manage activities.
3) Participants read emails mainly on their mobile phones.
Participants would not be eligible if they are taking any medication that can have arousing or calming effects (including
ADHD medication) or have a problem wearing a wrist sensor
(skin condition, etc.).
Each potential candidate was invited to a screening session
that lasted 20 min. During the session, the candidate received an
explanation of the study protocol and requirements. In addition,
the candidate was asked to wear a Bluetooth EDA wrist sensor on
each wrist. The goal of this was to determine if the candidate has
sufficient EDA response to stimuli. At a random point during the
screening session, a loud tone was generated in order to evoke
an EDA startle response. Candidates that exhibited an amplitude
change of less than 0.05 μSiemens could not participate in the
study.
We recruited ten graduate students for the study. The average
age of the participants was 30.8 (SD = 4.2), the youngest was
25 and the oldest was 35. Nine of the participants were male,
and one was female.
B. Study
Participants were asked to wear a pair of commercial EDA
wrist biosensors on their left and right wrists for a period of
10 days. The sensors measure EDA, skin temperature, and threeaxis accelerometer, and store the readings on an internal SD card.
The data can be downloaded from the sensor via a USB cable.
Participants were instructed to wear the sensors for as long as
possible and to take them off and charge them when bathing.
The FMC application and modified K9 were installed on
participant’s phones. Each participant was directed to only read
emails using the K9 client for the duration of the study.
The FMC application collected the following data.
1) Calendar event details—subject, start time, end time, location, and participants.
2) Phone call details—start time, end time, and recipient.
3) Email reading—start time, end time, subject, and recipients.
4) Location—latitude, longitude, accuracy, and time.
The study participants divided into two groups of five. Each
group would have access to a different type of interface during
the stages of the study. Participants were not told that they would
be using different interfaces at different stages.
Interface I—noncontextual viewer: This interface does not
include any contextual data. The user can only view EDA, temperature, and accelerometer data as well as arousal self-report
data. This interface is representative of many of the wearable
sensor systems available today, which mostly provide access to
the raw sensor data (see the red frame within Fig. 2).
Interface II—contextual viewer: This interface presents the
biophysiological signal and contextual information overlaid on
top of it. The signal includes EDA, skin temperature, accelerometer, and overall arousal level. The contextual information includes email readings, phone calls, calendar entries, and geolocation information (see Fig. 2).

274

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

C. Stage 1 (Days 1–5)
At the end of each day, the participants were asked to upload
their right-hand sensor recording files to the FEEL website.
The users were requested to view their EDA recording and to
annotate 3–4 peaks that were marked on the plot by the software.
The algorithm detects local maxima, and sorts them according
to the number of SCRs in a window of 10 min. The peaks with
the highest number of SCRs are selected for annotation.
During this stage, the first group of participants could only
access the noncontextual interface, while the second group could
access the contextual interface. This was done in order to test
whether the participants had access to the contextual data would
affect their event recall.
When a user clicked a marked peak, an annotation dialog
opened. The users were asked to describe what occurred approximately at the time of the peak, and to rate the accuracy of
their label on a seven-point Likert scale. Next, the users were
asked to rate their valence at the time of the peak (negative versus positive) and their arousal (calm versus excited). We used
the nine-point Self-Assessment Manikin pictorial scale developed by Bradley and Lang [31] in order to assess the user’s
emotional reaction to the event. We also asked the users to rate
their confidence in their valence and arousal scores on a sevenpoint Likert scale. Finally, the users were asked how clearly they
recalled what happened at the time on a seven-point Likert scale
and whether they had any additional information to add.
D. Stage 2 (Days 6–10)
In this stage, participants were asked to upload their righthand sensor data at the end of each day, similar to stage 1, but
with the following differences:
1) We wanted to test if the contextual interface enabled a
higher level of recall of the events: the participants were
asked to annotate the peaks for each day after two days
had passed, and not at the end of the day.
2) The first group of participants, which could previously
only access the noncontextual interface, got access to the
contextual interface. The second group, which previously
accessed the contextual interface, could only access the
noncontextual interface. The switch was done in order
to mitigate any recall improvement that the participants
might have due to training themselves to remember information after prolonged use of the noncontextual system.
E. Ethnographic Study
At the end of the study, we performed an ethnographic interview to obtain qualitative information on the usability of the new
tool. In the first part of the interview, we asked the participants
to fill in two sets of surveys. The first survey was asked to assess
system usability. We used the system usability scale (SUS) [45],
which is a simple, 10-item scale giving a global view of subjective assessments of usability. SUS questions are answered on a
seven-point Likert scale and yield a single number representing
a composite measure of the overall usability of the system. The
second survey was used to assess the user’s experience during

the use of the system. This survey contained the following nine
questions that were answered on a seven-point Likert scale (1
= strongly disagree, 7 = strongly agree):
1) This application is fun to use.
2) I would recommend this application to my friend.
3) I prefer maintaining my own journal for recording context.
4) I think FEEL is more reliable than maintaining my own
journal.
5) I would use this application regularly.
6) The data collected with FEEL has provided me with insights regarding my responses to events.
7) I really like having the possibility of viewing the contextual data along with the EDA.
8) The sensors were comfortable to wear.
9) The Mobile Application was nonintrusive and did not require any attention.
In the second part of the interview, we asked the participants
open-ended questions regarding their personal experience during the study. We wanted to determine which features the users
found useful, which features were difficult to user or provide
little value, and what features they would have liked to see
implemented.
V. RESULTS AND DISCUSSION
The FEEL system collected data from 10 participants for
10 days, yielding a total of 100 days of data. There were a total
of 337 annotations and a total of 947 Mbytes of EDA sensor
data uploaded to the system. The mobile client collected 4217
location events, 2595 email reading events, 983 phone calls, and
211 calendar entries.
The system functioned as intended, exhibited stability, and
no serious exceptions were reported. There was no need for any
reboot or any software modifications during the study.
A. System Effectiveness
We wanted to assess whether the system affects user recall of
events and hypothesized that if contextual information is related
to emotional event recall; then providing contextual information
to users will improve their confidence level when annotating
EDA peaks. In order to test this hypothesis, the study population was divided into two groups. One group (A) had access
to the contextual information alongside the sensor data, while
the other group (B) had access to the sensor data alone. During
the first stage of the study, both groups were asked to annotate
their recorded EDA at the end of each day. For each annotation
the participants were asked to rate how clearly they recall the
event, how accurate is the annotation label, and what their confidence level is for both the arousal and valence rating of the
event on the seven-point Likert scales. The average ratings, all
with a median value of 6, can be seen in Table I. These ratings
are also illustrated in Fig. 3. Running the Wilcoxon rank sum
test (equivalent to the Mann–Whitney U test) on both groups
of the participant ratings during stage 1 produced results also
shown in Table I. It is evident that there is no significant difference between the groups in terms of the annotation clarity,
accuracy, and valence and arousal rating confidence. It seems

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

275

TABLE I
AVERAGE RATINGS (INCLUDING STANDARD DEVIATIONS AND MEDIANS) AND WILCOXON RANK SUM TESTS FOR BOTH EXPERIMENT STAGES

Fig. 3.

Average ratings for each stage of the experiment. Stage 1 ratings are on the left side and stage 2 ratings are on the right side.

that providing contextual information did not affect the recall of
the same days’ events; this may be because only a short period
had passed between the event itself and its annotation. During
the second stage of the study, the participants were requested
to annotate their EDA signals only after two days had passed,
instead of at the end of the day for the events being annotated.
In addition, the interfaces were switched: group A was given the
noncontextual interface and group B was given the contextual
interface. Running the Wilcoxon rank sum test (equivalent to the
Mann–Whitney U test) on both groups of the participant ratings
during stage 2 produced the statistically significant results given
in Table I.
On average, the valence rating confidence and arousal rating
confidence are higher by 33% and 30%, respectively, when the
context is provided. In addition, the clarity and accuracy of
annotations is higher by 47.8% and 50.8%, respectively.
These results suggest that by providing users with contextual
information for annotations of “the day before yesterday”, the
FEEL system enables the users to annotate biophysiological
signals with greater perceived clarity. Even though the captured
contextual information is only partial, and may not directly
result in a physiological change, it provides the basis for a user
to feel that they can better recall what they were doing at the
time.
B. User Experience
We conducted a system usability survey, a user experience
survey, and interviewed each participant at the end of the study.
The obtained results suggest that by providing users with contextual information, the FEEL system enables the users to annotate
biophysiological signals at a greater effectiveness. Even though
the captured contextual information is only partial, and may not
directly result in a physiological change, it provides the basis
for a user to feel that they can better recall what they were doing
at the time.
All of the above feedback was positive in the direction that
we had hoped. The users were also asked to fill in a system

usability survey during the study completion interview. The
following plot shows the system usability score for each of the
users in the study (between 0 and100). The system usability
score ranged between 61.67 and 93.33 and the average score
was 76.33 (standard deviation = 11.2).
VI. CONCLUSION AND FUTURE WORK
We designed and implemented an architecture for a system
for the acquisition, processing, and visualization of biophysiological data and contextual information: a mobile client for
context acquisition, a backend application for storage and processing of the physiological signal and contextual information,
and a novel user interface that displays the contextual information overlaid onto the biophysiological signal. In addition, we
ran a user study collecting 100 days of data to test the system’s
usability, effectiveness, and the robustness of all its elements.
As part of the study, users were required to wear EDA wrist
sensors, install a mobile application, and annotate peaks in their
EDA. The system collected data and exhibited durability, functioning as planned throughout the 100-user-days of study. The
system’s usability was evaluated by using the SUS and achieved
a good score. Most of the users reported that they enjoyed using
the system and that there were no major improvements needed.
While a real-world 100 days-of-data study does not make it
possible to assess what all the events actually were, and hence
not confirm the accuracy of the participant’s recalled labels,
we did find that the FEEL system with its context does indeed
enable users to annotate biophysiological signals with greater
perceived clarity than the current state of the art. The contextual
information provided by FEEL is associated with a user feeling
that he/she can more accurately and confidently recall what
he/she were doing at the time of the event.
The work described in this paper is not limited to EDA measurement. There are multiple paths in which this work may
be extended: We could expand the capabilities of the system
in terms of automatic analysis of the user valence, and we
could capture additional context and biophysiological signals by

276

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 1, JANUARY 2014

utilizing additional sensors. In addition, the initial labeling by
the user can be used to train the system specifically for each
user so that salient events will be determined automatically.
These salient events are perceived as having greater personal
significance. Finally, we could expand the practical use cases
of the system by adding different applications that utilize the
platform. The following sections will describe future directions
along these paths.
A. System Capabilities
1) Acquisition of Additional Contextual Information: The
current contextual acquisition functionality serves to enhance
the recall of events that occurred within the surrounding time
frame. At times, the acquired context may be the actual event
we are interested in capturing, for instance, a specific phone call
that caused a significant change in valence or arousal. Utilizing
additional sensors might improve the likelihood that a relevant
event will be recorded. Further analysis of such events may also
provide additional data on the user’s reactions.
a) Proximity detection: It is possible to determine if a user
is in the midst of a social interaction such as a meeting, with
whom, and for how long. Most mobile phones are equipped
with Bluetooth chipsets. In 2010, 906 million mobile phones
were sold and almost 100% of them were Bluetooth capable
[32]. Using the Bluetooth mac address which is transmitted
intermittently when the phone is in discovery mode and reading
the transmission signal strength [33], it is possible to determine
which phone users are located in proximity of one another.
b) Facial expression recognition: It is extremely difficult
to infer valence from only an EDA recording. Using the frontal
phone camera and techniques similar to those described by Nicolaou et al. [34], it is possible to determine the facial expression
of the user during an interaction with the phone and extract
smiles, which also do not perfectly determine valence but can
definitely help [35].
c) Sentiment analysis: Textual sentiment analysis techniques such as those described by Pang and Lee [36] provide a
method for determining the valence and dominance of a passage
of text. Performing sentiment analysis on all textual content (including converting speech to text) such as emails, SMS, instant
messages, and web pages may provide important insights into
the users’ affective state.
d) Photo and video capture: The FMC can be extended
to send photos and videos to the backend server where they will
be integrated with other contextual information. In contrast with
some life-logging approaches in which cameras are constantly
recording, the advantage of this approach is that the user initiates the capture and this increases the probability of capturing
“relevant context”.
e) Speech analysis for emotion detection: Significant
work has been done in the field of emotion detection in spoken dialog. Lee and Narayanan [37] and Liscombe et al. [38]
have tried to classify the emotional state of user turns in a corpus
of dialogs using a combination of standard lexical and prosodic
features augmented by contextual features that exploit the structure of spoken dialog.

Using techniques similar to the ones described above, it is
possible to tag voice calls with affective state metadata.
2) Acquisition of Additional Biophysiological Signals: It is
possible to use other biophysiological sensors that produce time
series data as well, such as heart rate and respiration sensors,
to complement the EDA for affective state estimation. Blood
glucose levels can be read by a device such as IBGStar [39]
which is plugged directly into the mobile phone and can digitize blood glucose test strip results. Wearable ECG and blood
pressure [40], [41] sensors can be used to track user health.
The number of possibilities in this space is growing rapidly and
wearable devices are expected to be at least a 6 B$ market by
2016 [42].
B. New Applications
1) Affective Metadata Tagging: Hangal et al. [43] suggest
that it is a challenge to browse through large unstructured corpuses of text such as email and SMS archives and retrieve useful
information. This is especially important since a large portion of
our digital history is textual information. Weerkamp et al. [44]
claim that retrieval effectiveness is improved when utilizing
contextual information. The approach of Hangal et al. is limited
by the fact that it can only analyze the sentiment of the text
and does not take into account the affective state of the reader
when receiving the message. The FEEL system can be utilized
to apply meta-data tags containing both the contextual information and the affective state of the reader as may be determined
by biophysiological sensors. This approach need not only be
applied to textual information. It may be applied to multimedia
as well. FEEL can tag photos with the user’s arousal level as
they are shot. The user can retrieve photos based on contextual
information and arousal level: “computer, please show me all of
my most exciting photos!”. It is possible to tag content as it is
generated or consumed by the user [45], [46].
2) Real-Time Response and Intervention: The contextual
data are streamed in real time and the biophysiological signal
is recorded on a memory card within the sensor, and uploaded
by the user at a later time. Today’s implementation can be extended to analyze events as they occur and perform real-time
predictions. It is possible to stream the biophysiological signals
to the system in parallel with the contextual information and
have them processed immediately. By providing output in real
time, the system can be utilized to perform interventions in a
wide range of applications such as effective stress management
and chronic disease management.
REFERENCES
[1] Affectiva. (2012, Jun. 21). [Online]. Available: http://www.affectiva.com
[2] Bodymedia. (2012, May 7). [Online]. Available: http://www.bodymedia.
com
[3] Fitbit. (2012, Jun. 21). [Online]. Available: http://www.fitbit.com
[4] Nike+ FuelBand. (2012, Jun. 21). [Online]. Available: http://www.nike.
com/fuelband
[5] Up by Jawbone. (2012, Jun. 21). [Online]. Available: http://jawbone.
com/up/product
[6] M. Raento, A. Oulasvirta, R. Petit, and H. Toivonen, “ContextPhone:
A Prototyping Platform for Context-Aware Mobile Applications,” IEEE
Pervasive Comput., vol. 4, no. 2, pp. 51–59, Apr. 2005.

AYZENBERG AND PICARD: FEEL: A SYSTEM FOR FREQUENT EVENT AND ELECTRODERMAL ACTIVITY LABELING

[7] B. Brown and R. Randell, “Building a context sensitive telephone: Some
hopes and pitfalls for context sensitive computing,” Comput. Supported
Cooperative Work, vol. 13, no. 3–4, pp. 329–345, Aug. 2004.
[8] T. Iso and N. Kawasaki, “Personal context extractor with multiple sensor
on a cell phone,” in Proc. Int. Conf. Mobile, 2005, no. 1, p. D.2C200525.
[9] H. Lee, “Combining context-awareness with wearable computing for
emotion-based contents service,” Int. J. Adv. Sci. Technol., vol. 22, pp. 13–
24, 2010.
[10] N. Kern, B. Schiele, and A. Schmidt, “Recognizing context for annotating
a live life recording,” Pers. Ubiquitous Comput., vol. 11, no. 4, pp. 251–
263, Aug. 2006.
[11] M. L. Blum, “Real-time Context Recognition,” M.S. Thesis, Swiss Federal
Inst. of Technology Zurich (ETH), Zurich, Switzerland, 2005.
[12] J. Froehlich, M. Chen, and S. Consolvo, “My Experience: A system for in
situ tracing and capturing of user feedback on mobile phones,” in Proc.
MobiSys ’07: Proc. 5th Int. Conf. Mobile Syst., Appl. Services, 2007,
pp. 50–57.
[13] R. W. Picard and K. K. Liu, “Relative subjective count and assessment of
interruptive technologies applied to mobile monitoring of stress,” Int. J.
Human-Comput. Stud., vol. 65, no. 4, pp. 361–375, Apr. 2007.
[14] K. K.-L. Liu, “A personal, mobile system for understanding stress and
interruptions.” M.S. Thesis, Massachusetts Inst. Technol., MA, 2004.
[15] D. Kang, “A context aware system for personalized services using wearable biological signal sensors,” in Proc. 2008 Int. Conf. Control, Autom.
Syst., 2008, pp. 888–891.
[16] S. S. Intille, J. Rondoni, C. Kukla, I. Ancona, and L. Bao, “A contextaware experience sampling tool,” in Proc. Extended Abstracts Human
Factors Comput. Syst. CHI’03, ACM Press, New York, New York, USA,
2003, pp. 972–973.
[17] A. Raij, P. Blitz, A. A. Ali, S. Fisk, B. French, M. Somnath, N. Motohiro,
M. Nuyen, K. Plarre, M. Rahman, S. Shah, Y. Shi, N. Stohs, M. Al’Absi,
E. Ertin, T. Kamarck, S. Kumar, M. Scott, D. Siewiorek, and S. Asim,
“mStress: Supporting continuous collection of objective and subjective
measures of psychosocial stress on mobile devices,” Dept. Comput. Sci.,
Univ. Memphis, TN, Tech. Rep. CS-10-004, 2010.
[18] R. W. Picard, E. Vyzas, and J. Healey, “Toward machine emotional intelligence: Analysis of affective physiological state,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 23, no. 10, pp. 1175–1191, Oct. 2001.
[19] K. Kim and S. Bang, “Emotion recognition system using short-term monitoring of physiological signals,” Med. Biol. Eng. Comput., vol. 42, pp. 419–
427, 2004.
[20] E. Ertin, N. Stohs, S. Kumar, A. Raij, M. al’Absi, and S. Shah, “AutoSense,” in Proc. 9th ACM Conf. Embedded Networked Sensor Syst.
SenSys’11, 2011, pp. 274–287.
[21] W. Boucsein, Electrodermal Activity, 2nd ed. New York, NY, USA:
Springer-Verlag, 2011.
[22] ITU statistics. (2012, Jun. 21). [Online]. Available: http://www.itu.
int/ITU-D/ict/statistics
[23] Y. Cui, J. Chipchase, and F. Ichikawa, “A cross culture study on phone
carrying and physical personalization,” in Proc. 2nd Int. Conf. Usability,
Internationalization, 2007, pp. 483–492.
[24] Home· k9mail/k-9 Wiki. (2012, Apr. 14). [Online]. Available:
https://github.com/k9mail/k-9/wiki
[25] S. Burbeck. How to use model-view-controller (MVC). (2012, May
06). [Online]. Available: http://st-www.cs.illinois.edu/users/smarch/stdocs/mvc.html
[26] Tornado Web Server. (2012, Apr. 15). [Online]. Available: http://www.
tornadoweb.org
[27] Scientific Computing Tools For Python— NumPy. (2012, Apr. 15). [Online]. Available: http://numpy.scipy.org
[28] SciPy—open-source software for mathematics, science, and engineering.
(2012, Apr. 15). [Online]. Available: http://www.scipy.org
[29] jQuery Grid Plugin—jqGrid >> jQuery Grid 1.0 beta. (2012, Apr. 16).
[Online]. Available: http://www.trirand.com/blog/?p = 3
[30] Highcharts—Interactive JavaScript charts for your webpage. (2012, Apr.
16). [Online]. Available: http://www.highcharts.com
[31] M. Bradley and P. J. Lang, “Measuring emotion: the self-assessment
manikin and the semantic differential,” Science, vol. 25, no. 1, pp. 49–59,
1994.
[32] The Bluetooth network effect. (2012, Jun. 22). [Online]. Available:
http://www.bluetooth.com/Pages/network-effect.aspx
[33] J. Hallberg, M. Nilsson, and K. Synnes, “Positioning with Bluetooth,” in
Proc. 10th Int. Conf. Telecommun., 2003, vol. 2, pp. 954–958.
[34] M. A. Nicolaou, H. Gunes, and M. Pantic, “Output-associative RVM regression for dimensional and continuous emotion prediction,” Image Vision Comput., vol. 30, pp. 186–196, Jan. 2012.

277

[35] M. E. Hoque, D. J. McDuff, and R. W. Picard, “Exploring temporal patterns in classifying frustrated and delighted smiles,” IEEE Trans. Affective
Comput., vol. 3, no. 3, 2012.
[36] B. Pang and L. Lee, “Opinion mining and sentiment analysis,” Foundations Trends Inform. Retrieval, vol. 2, no. 2, pp. 1–135, 2008.
[37] C. M. Lee and S. S. Narayanan, “Toward detecting emotions in spoken
dialogs,” IEEE Trans. Speech Audio Process., vol. 13, no. 2, pp. 293–303,
Mar. 2005.
[38] J. Liscombe and G. Riccardi, “Using context to improve emotion detection in spoken dialog systems,” in Proc. Interspeech—Eurospeech, 2005,
pp. 1845–1848.
[39] IBGStar. (2012, May 18). [Online]. Available: http://www.ibgstar.us/
what-is-ibgstar.aspx
[40] P. A. Shaltis, A. Reisner, and H. H. Asada, “Wearable, cuff-less PPGbased blood pressure monitor with novel height sensor,” in Conf. Proc:
Annu. Int. Conf. IEEE Eng. Med. Biol. Soc., Jan. 2006, vol. 1, pp. 908–911.
[41] Wearable blood pressure sensor offers 24/7 continuous monitoring—
MIT News Office. (2012, Aug. 16). [Online]. Available: http://web.mit.
edu/newsoffice/2009/blood-pressure-tt0408.html
[42] B. Dolan, Report: Wearable devices a $6B market by 2016 | mobihealthnews. (2012, Aug. 14). [Online]. Available: http://mobihealthnews.
com/18194/report-wearable-devices-a-6b-market-by-2016
[43] S. Hangal, M. S. Lam, and J. Heer, “MUSE: Reviving memories using
email archives,” in Proc. 24th Annual ACM Symp. User Interface Software
Technol., 2011, pp. 75–84.
[44] W. Weerkamp, K. Balog, and M. De Rijke, “Using contextual information
to improve search in email archives,” in Proc. 31th Eur. Conf. IR Research,
ECIR’09, 2009, no. 2, pp. 400–411.
[45] R. W. Picard and J. Healey, “Affective wearables,” Personal Technol.,
vol. 1, no. 4, pp. 231–240, Dec. 1997.
[46] J. Healey and R. W. Picard, “StartleCam: A cybernetic wearable camera,”
in Proc. 2nd IEEE Int. Symp. Wearable Comput., ISWC’98, 1998, p. 42.

Yadid Ayzenberg received the Bachelor’s degree in
mathematics and computer science from Ben-Gurion
University in Beer-Sheva, Israel, in 1999, the MBA
degree jointly from Tel-Aviv University in Tel-Aviv,
Israel and Northwestern University in Evanston, IL,
USA, in 2008, and the M.Sc. degree from Massachusetts Institute of Technology, Cambridge, MA,
USA, in 2012. He is currently working toward his
PhD degree in the Affective Computing group at the
MIT Media Lab in Cambridge, MA.
Prior to joining the Media Lab, he was the Director
of software at PMC-Sierra, a global communications and storage semiconductor company and also held various engineering management roles at Comverse
Technologies, a provider of telecommunication systems.
Mr. Ayzenberg’s research interests include biophysiological sensors, mobile
context awareness, and cloud computing for health and wellness applications.

Rosalind W. Picard (M’81–SM’00–F’05) received
the Bachelor’s degree (with Highest Honors) in electrical engineering from the Georgia Institute of Technology in Atlanta, GA, USA, in 1984, and the S.M.
and Sc.D. degrees in electrical engineering and computer science from the Massachusetts Institute of
Technology, Cambridge, MA, USA, in 1986 and
1991, respectively.
She is a Professor of Media Arts and Sciences
at the MIT Media Lab. She is also the Founder and
Director of the Affective Computing Group and Cofounder of Affectiva, Inc., Waltham, MA, USA. She has authored more than 200
scientific articles. She is best known for pioneering work in image and video
content-based retrieval (the original Photobook system), for developing texture
models, and machine learning for their combination (Society of Models) and for
her book Affective Computing (MIT Press, 1997), which helped launch a field
by that name. Her work experience includes Member of the Technical Staff at
AT&T Bell Labs in Holmdel, NJ, USA (1984–1987), as well as internships and
consulting at many companies including Hewlett Packard, IBM, Motorola, and
Apple. Her current research interests include the development of technology to
help people comfortably and respectfully measure and communicate affective
information and understand the ways in which emotion interacts with health,
learning, and a variety of behaviors.

