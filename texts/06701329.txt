1220

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

Segmentation of Skin Lesions From Digital Images
Using Joint Statistical Texture Distinctiveness
Jeffrey Glaister∗ , Student Member, IEEE, Alexander Wong, Member, IEEE,
and David A. Clausi, Senior Member, IEEE

Abstract—Melanoma is the deadliest form of skin cancer. Incidence rates of melanoma have been increasing, especially among
non-Hispanic white males and females, but survival rates are high
if detected early. Due to the costs for dermatologists to screen
every patient, there is a need for an automated system to assess
a patient’s risk of melanoma using images of their skin lesions
captured using a standard digital camera. One challenge in implementing such a system is locating the skin lesion in the digital
image. A novel texture-based skin lesion segmentation algorithm is
proposed. A set of representative texture distributions are learned
from an illumination-corrected photograph and a texture distinctiveness metric is calculated for each distribution. Next, regions
in the image are classified as normal skin or lesion based on the
occurrence of representative texture distributions. The proposed
segmentation framework is tested by comparing lesion segmentation results and melanoma classification results to results using
other state-of-art algorithms. The proposed framework has higher
segmentation accuracy compared to all other tested algorithms.
Index Terms—Melanoma, segmentation, skin cancer, texture.

I. INTRODUCTION
ELANOMA is the most deadly form of skin cancer,
with an estimated 76 690 people being diagnosed with
melanoma and 9480 people dying of melanoma in the United
States in 2013 [1]. In the United States, the lifetime risk of
getting melanoma is 1 in 49 [1]. Melanoma accounts for approximately 75% of deaths associated with skin cancer [2].
It is a malignant tumour of the melanocytes and usually occurs on the trunk or lower extremities [3]. Recent trends found
that incidence rates for non-Hispanic white males and females
were increasing at an annual rate of approximately 3% [4]. If
melanoma is detected early, while it is classified at Stage I, the
5-year survival rate is 96% [5]; however, the 5-year survival rate
decreases to 5% if the melanoma is in Stage IV [5]. With the rising incidence rates in certain subsets of the general population,

M

Manuscript received October 22, 2013; revised December 13, 2013; accepted
December 21, 2013. Date of publication January 2, 2014; date of current version March 17, 2014. This work was supported by Agfa Healthcare Inc., Ontario
Ministry of Research and Innovation, Ontario Centres of Excellence, the Natural Sciences and Engineering Research Council of Canada, and the Canada
Research Chairs program. Asterisk indicates corresponding author.
∗ J. Glaister is with the Image Analysis and Communications Lab, Department
of Electrical and Computer Engineering, Johns Hopkins University, Baltimore,
MD 21218 USA (e-mail: jglaist1@jhu.edu).
A. Wong and D. A. Clausi are with the Vision and Image Processing Lab,
Department of Systems Design, University of Waterloo, ON N2L 3G1, Canada
(e-mail: a28wong@uwaterloo.ca; dclausi@uwaterloo.ca).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2297622

Fig. 1. Uncorrected and corrected skin lesion images. In (a) and (c), examples
of the uncorrected skin lesion images are shown. In (b) and (d), the images
after being corrected for illumination variation using the MSIM algorithm are
shown [14]. Shadows which appear on the left side of the uncorrected images
are removed in the corrected images, while the color of the lesion has changed
minimally.

it is beneficial to screen for melanoma in order to detect it early.
To reduce costs of screening melanoma in the general population, development of automated melanoma screening algorithms
have been proposed.
Early automated melanoma screening systems assess the risk
of melanoma using images acquired via a digital dermatoscope [6]–[9]. A dermatoscope is a special device for dermatologists to use to look at skin lesions that acts as a filter and
magnifier. Images acquired through a digital dermatoscope are
referred to as dermoscopy images and have relatively low levels
of noise and consistent background illumination. Optional preprocessing algorithms applied to dermatological images include
normalizing or enhancing image colors [10]. However, requiring
dermatologists to have a dermatoscope impedes the adoption of
these systems as only 48% of practicing dermatologists use dermatoscopes [11]. The most common reasons against using the
dermatoscope include a lack of training or interest. Recent work
with automated melanoma screening algorithms tries to adapt
the algorithms to analyze images taken by a standard digital
camera [12], [13]. Examples of digital images of melanoma are
shown in Fig. 1(a) and (c). There is a need for a segmentation algorithm designed specifically for digital images of skin lesions.
Before extracting features from the skin lesion and classifying
the lesion as malignant or benign, the location of the lesion border must be identified using a segmentation algorithm. Finding
an accurate estimate of the lesion border is important because
of the types of features used for classification. One common set

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

GLAISTER et al.: SEGMENTATION OF SKIN LESIONS FROM DIGITAL IMAGES USING JOINT STATISTICAL TEXTURE DISTINCTIVENESS

of features is the ABCD scale: asymmetry, border irregularity,
color variegation, and diameter [15]. In particular, metrics that
measure border irregularity may depend heavily on the accuracy of the estimated lesion border. Therefore, it is important
that the skin lesion segmentation algorithm is accurate, as the
resulting segmentation is used as an input to feature extraction
and melanoma classification algorithms.
Many segmentation algorithms have been proposed to locate
skin lesion in images automatically. The majority of proposed
segmentation algorithms are only applicable to dermoscopy images, which has better contrast between the lesion and surrounding skin area for certain types of lesions [16]. A recent summary
by Celebi et al. [16] reviews the existing segmentation algorithms for dermoscopy images. Algorithms compared in the
summary [16] include using simple thresholding, active contours [17], and region merging [18]. The majority of algorithms
only use features derived from pixel color to drive the segmentation. This includes the blue channel from the RGB color space,
the luminance channel from the CIELUV or CIELAB color
spaces, or an orthogonal transform applied to the color channels. However, to accurately segment lesions with fuzzy edges
is difficult when relying solely on color features.
Segmenting digital photographs of skin lesions is a more difficult problem due to illumination variation. Special segmentation
algorithms are required to take into account illumination variation, which causes shadows and bright areas to appear throughout the photograph. Hance et al. [19] explored different algorithms, including thresholding, active contours and split-andmerge, and modified them to be usable on lesion photographs.
For example, the thresholding algorithm has to be modified to
account for bright areas where there is reflection of the camera’s
flash.
Four separate algorithms by Cavalcanti et al. include a preprocessing step which corrects for illumination variation before
applying a thresholding [12], [20], [21] or level-set segmentation algorithm [22]. Thresholding is performed on single color
channels [21], multiple color channels [20], or a set of channels
derived using principal component analysis (PCA) and other
processing steps [12]. Without the preprocessing step correcting for illumination variation, these algorithms tend to identify
areas with shadows as part of the skin lesion. The proposed
algorithm incorporates this idea and includes a multistage illumination modelling [14] preprocessing step to correct shadows
and bright spots caused by illumination variation. Examples of
corrected images are shown in Fig. 1(b) and (d). In both examples, shadows that appeared on the left side of the uncorrected
images have been removed. The corrected images are used as
the input to the segmentation algorithm.
Most segmentation algorithms for dermatological images or
photographs use color information, either in a single channel
or across three color channels, to find the lesion. Another approach to find skin lesions is to incorporate textural information, because normal skin and lesion areas have different textures. Textures include smoothness, roughness, or the presence
of ridges, bumps or other deformations and are visible by variation in pixel intensities in an area [23]. Features and measurements of a texture in an image are extracted and textures from

1221

different regions are compared. Stoecker et al. [24] analyzed
texture in skin images using basic statistical approaches, such
as the gray-level cooccurrence matrix. They found that texture
analysis could accurately find regions with a smooth texture and
that texture analysis is applicable to segmentation and classification of dermatological images.
Texture-based segmentation algorithms have been applied to
dermoscopy images. Proposed textural lesion segmentation algorithms include using gray-level cooccurrence matrix [25],
first-order region statistics [26], and Markov random field models [27]. The algorithm proposed by Xu et al. [28] learns a
model of the normal skin texture using pixels in the four corners
of the image, which is later used to find the lesion. Hwang and
Celebi [29] use Gabor filters to extract texture features and use
a g-means clustering approach for segmenting the lesion.
In this paper, we propose a segmentation algorithm based
on texture distinctiveness (TD) to locate skin lesions in photographs. This algorithm is referred to as the TD lesion segmentation (TDLS) algorithm. The main contributions are the
introduction of a joint statistical TD metric and a texture-based
region classification algorithm. TD captures the dissimilarity between learned representative texture distributions. In Section II,
the process of learning the sparse texture model and calculating a metric to measure TD is described. As part of this contribution, we introduce the use of joint statistical information
to characterize skin and lesion textures as representative texture
distributions. In Section III, regions in the image are classified as
being part of the lesion or normal skin. This region classification
algorithm incorporates the texture information captured by the
TD metric. Implementation details are provided in Section IV.
Experimental results are shown in Section V and conclusions
are drawn in Section VI.
II. TEXTURE DISTINCTIVENESS
The TDLS algorithm consists of two main steps. First, a set
of sparse texture distributions that represent skin and lesion textures are learned. A TD metric is calculated to measure the
dissimilarity of a texture distribution from all other texture distributions. Second, the TD metric is used to classify regions in
the image as part of the skin class or lesion class. In this section, the first step is described in detail and Fig. 2 illustrates the
overall process to learn the representative texture distributions
and calculate the TD metric.
Existing sparse texture algorithms use sparse texture models for segmentation or classification of images with different
texture patterns. Sparse texture models find a small number of
texture representations, such as texture patches, to characterize
an entire image [30]. Sparse texture models learn important local texture details present in an image. Using a sparse texture
model allows the image to be stored efficiently and allows for
efficient computation of algorithms that involve textures from
the image. There are many ways to learn the model, including clustering or by formulating the problem as an optimization
problem [31]. A common method to learn a sparse texture model
is by employing a dictionary-learning algorithm [30], where a
set of texture patches that can best match details in the original

1222

Fig. 2.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

Algorithm flowchart displaying the steps to learn the representative texture distributions and calculate the TD metric.

characterizing the sparse model as a set of distributions, we can
capture both local and global characteristics in the image. The
texture distributions are able to capture the commonly occurring
texture patterns found in lesion and normal skin regions. The kth
representative texture distribution is defined as Tkr . By using a
small set T r comprised of K representative texture distributions
instead of using all the local texture vectors, the computational
complexity and memory requirements are reduced,
T r = {Tkr |1 ≤ k ≤ K}.
Fig. 3. Extracting a texture vector. For images with multiple channels, a
separate vector is obtained for each channel and concatenated sequentially.

image is learned. We propose incorporating probabilistic information to learn sparse texture distributions, rather than texture
models. To learn whether each texture distribution belongs to
the skin or lesion class, a TD metric is formulated.

Each texture vector belongs to a single representative texture
distribution, which best corresponds with that texture vector. All
parameters needed to characterize the kth texture distribution
are contained in θk . Each distribution has its own distinct set
of parameters. A mixture model is used to represent the set
of texture distributions associated with the input photograph.
Texture distributions are chosen to maximize the log-likelihood
of the mixture model,

A. Representative Texture Distributions
An existing sparse texture model algorithm [32] is modified
to find representative sparse texture distributions from the input
photograph. Our proposed sparse texture model algorithm incorporates statistical information. The advantage of using a joint
probabilistic sparse model is that the sparse texture distributions
can model both local and global texture characteristics.
To learn the sparse texture model, a local texture vector is
obtained for each pixel in the image. The input image has been
corrected for illumination variation, contains N × M pixels and
each pixel has a channels. The texture vector contains pixels in
a neighborhood of size n centered on the pixel of interest. Let s
be a pixel location (x, y) in the photograph. Then, the vector ts
represents the n × n × a texture patch centered at pixel s. The
process of extracting the texture vector for a pixel in a single
channel is illustrated in Fig. 3. To account for edge pixels, the
borders of image are padded.
In the case of a multiple channels, tA ,s is the texture patch
centered at pixel s and corresponding to channel A. The texture
vector is constructed by concatenating each tA ,s corresponding
to the same pixel across all channels. For example, if the color
image contains three channels {R, G, B} for each pixel, three
texture vectors, tR ,s , tG ,s , and tB ,s , are extracted and concatenated such that ts = [tR ,s , tG ,s , tB ,s ]. After extracting the set
of texture vectors for an image, we have a set of N × M texture
vectors is extracted, with each vector of size n × n × a:
T = {ts j |1 ≤ j ≤ N × M }.

(1)

Using the set of all texture vectors extracted from an image, we find a set of representative texture distributions. By

(2)

T̂ r = arg max
Tr

K





log P(ts j |Trk ) .

(3)

k =1 ts j ∈C k

To find the representative texture distributions and the sets
of texture vectors corresponding to each representative distribution, an unsupervised clustering algorithm is used. The set
Ck is comprised of the texture vectors corresponding to texture
distribution Tkr . Implementation details are given in Section IV.
A Gaussian distribution is assumed, so θk contains the two required parameters to define a multivariate Gaussian distribution.
The mean and covariance of the kth texture distribution are represented by trk and Σk , respectively. P (ts j |Tkr ) is the probability
of the jth texture vector given the parameters of the kth texture distribution. The parameters of the texture distributions are
chosen to maximize the log-likelihood in (3).
Examples of photographs where pixels associated with a set of
five representative texture distributions are shown in Fig. 4. Each
solid color in Fig. 4(b) and (e) represents pixels belonging to the
same representative texture distribution. In Fig. 4(b), the lesion
is represented by texture distribution associated with dark blue
and in Fig. 4(e), the lesion is represented by texture distributions
associated with dark blue and light green.
B. TD Metric
A TD metric is formulated using the learned sparse texture
model. Since we are only interested in two classes, normal skin
and lesion, but have learned many texture distributions, multiple texture distributions must represent the same class. To measure similarity of two texture distributions, we first measure the

GLAISTER et al.: SEGMENTATION OF SKIN LESIONS FROM DIGITAL IMAGES USING JOINT STATISTICAL TEXTURE DISTINCTIVENESS

1223

Fig. 4. Map of representative texture distributions. In (a) and (d), the original images are shown. In (b) and (e), five representative texture distributions have been
learned and each pixel in the image is replaced by one of five colors, depending on which texture distribution that pixel is associated with. In (c) and (f), maps of
the texture distinctive metric are constructed. The pixel intensities in (c) and (f) depend on the TD of the texture distribution associated with each pixel.

probability that the mean of one texture distribution is a realization of the mean of the other texture distribution, which is
defined as lj,k in (4). Because we assume that the texture distributions are Gaussian, trj and Σj are the mean and covariance
of distribution Tjr . The metric lj,k is asymmetric, because when
comparing most pairs of distributions, Σi = Σj . The measure of
similarity Lj,k given in (5) is the average of lj,k and lk ,j . After
Lj,k has been calculated for each pair of texture distributions,
they are normalized to be between 0 and 1,
lj, k = 
Lj, k =



1
exp − (trj − trk )T Σ−1
(trj − trk ) (4)
j
2
(2π)n ×n ×a |Σj |
1

1
(lj, k + lj, k ) .
2

(5)

We are interested in finding distinct texture distributions. For
example, lesion texture distributions are both dissimilar from
the normal skin texture distributions and also from other texture
distributions, due to color variegation and textural patterns found
in skin lesions. The probability that a texture distribution is
distinct from another texture distribution is given by dj,k :
dj,k = 1 − Lj,k .

(6)

Using the texture distributions and probabilities of distinctiveness, a weighted graphical model can be constructed to characterize all pair-wise relationships. The graphical model is defined
as G = {V, E}. V represents the set of vertices for the graphical
model, which are the texture distributions associated with each
pixel in the image. E represents the set of edges between every
pair of texture distributions, which are given a weight based on
the probability of distinctiveness, dj,k .
A TD metric Dj is used to capture the dissimilarity of texture
distribution Tjr from other texture distributions. The metric is
defined in (7) and measures the expected distinctiveness of Tjr
given the photograph I, where P (Tkr |I) is the probability of
occurrence of a pixel being associated with a texture distribution

Tkr . P (Tkr |I) is estimated using the histogram of the number of
pixels associated with each texture distribution across the entire
image,
Dj =

K


dj,k P (Tkr |I).

(7)

k =1

In the case of normal skin texture distributions, the dissimilarity of one skin texture distribution from other skin texture
distributions is very small. The TD metric for skin texture distributions is small overall. Lesion texture distributions are dissimilar from other skin and lesion texture distributions, so the
textural distinctiveness metric is large.
Fig. 4(c) and (f) give illustrative examples of the TD metric
corresponding to each pixel in the images. A brighter pixel corresponds to a higher TD metric. In both figures, the lesion is
predominately white, meaning that the lesion texture distributions have higher TD metrics, as expected. In Fig. 4(f), there are
two texture distributions that correspond to the lesion class and
have high TD. However, in Fig. 4(c), some normal skin pixels
to the right of the lesion also have high TD. This can occur
when there are unique texture patterns in normal skin areas.
This commonly occurs, motivating the region classification step
of the TDLS algorithm. The region classification step allows the
algorithm to be more robust and minimize misclassification of
pixels.
III. REGION CLASSIFICATION
The second main step in the TDLS algorithm is to find and
classify regions in the input image as being part of the lesion
based on the sparse texture distributions and their associated TD
metric. First, the image is oversegmented, which results in the
image being divided into a large number of regions. Next, each
region is independently classified as representing normal skin

1224

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

or lesion based on the textural contents of that region. Finally,
postprocessing steps refine the lesion segmentation.

TD across a region,
y(R) =

A. Initial Regions
The corrected lesion image is divided into a large number
of regions. This initial oversegmentation step is incorporated
to increase the TDLS algorithm’s robustness to noise. Furthermore, it allows for the use of an efficient and fast classification
algorithm to find which regions belong to the skin or lesion
class. The initial oversegmentation algorithm is adapted from
the statistical region merging (SRM) [33] algorithm. The main
difference is that the SRM algorithm uses the image in the RGB
color space, while the TDLS algorithm converts the photograph
to the XYZ color space, as mentioned in Section IV. The advantages of using the SRM algorithm as the initial oversegmentation
algorithm are that it directly takes into account pixel location,
is simple and is computationally efficient.
SRM contains two main steps: a sorting step and a merging step. SRM sorts pixels in an image to determine the order
in which pixels are compared, and then merges pairs of pixels
into regions based on their similarity. A four-connected graph
is constructed so that each pixel in the photograph is connected
with its neighbors. The pixels are sorted based on their similarity with their neighboring pixel. Both horizontal and vertical
neighboring pixels are considered when sorting the pixels. The
merging predicate determines whether two regions are merged
together, based on pixel intensities. The predicate depends on
the difference between average pixel intensity for each channel
for the two regions. Furthermore, it depends on the number of
pixels in the regions. It includes a tunable parameter Q to change
the likelihood that two regions are merged. The parameter Q is
set to 128 following experimental testing. Additional details are
available in the paper by Nock and Nielson [33].
The result of the initial oversegmentation step is a map of
several regions which correspond to the normal skin or lesion
classes. To reduce the number of regions, all segments that
touch the edges of the photograph are merged into a single
region. This is based on the assumption that the lesion is not
touching the edges of the photograph, which is reasonable for
situations where the photographs are captured in controlled,
clinical environments. As such, regions touching the edges are
all likely to be part of the normal skin class.

B. Distinctiveness-based Segment Classification
Following the initial oversegmentation step, each region must
be classified as belonging to the normal skin class or lesion
class based on a criterion. The classification step is illustrated
in (8), where y is the resulting segmentation map. Each element
in y is either 1 (lesion) or 0 (normal skin), depending on the
classification results for that element’s corresponding region.
The threshold is denoted by τ and it represents the decision
boundary between the normal skin and lesion class. The feature
used to discriminate between the two classes is the regional
textural distinctiveness metric DR . This metric is based on the



1,
0,

if DR ≥ τ (lesion)
otherwise (normal skin).

(8)

From Section II-A, each pixel in the input photograph is associated with a texture distribution. A TD metric D is calculated
for each texture distribution based on the probability of it being
similar to other texture distributions. This information is combined with the contents of each region to determine a regional
TD metric, DR . DR represents the average TD across region R
(9), where P (Tjr |R) is the probability of a pixel being associated
with the jth texture distribution in region R. Again, P (Tjr |R) is
estimated using the histogram of the number of pixels associated
with each texture distribution across the region R,
DR =

K


Dj P (Tjr |R).

(9)

j =1

Finally, a threshold τ is defined to divide the set of representative texture distributions into two classes, normal skin and
lesion, and is also based on the TD metrics. There are many
ways to find two classes from a one-dimensional set of features.
In the TDLS algorithm, the threshold is found that divides the
set of texture distributions into two classes such that the total
intraclass variance of the TD metric for each class is minimized
as
	


τ = arg min σC2 1 (τ ) P (T r |C1 (τ )) + σC2 2 (τ ) P (T r |C2 (τ )) .
τ

(10)
The threshold τ is used to divide the set of texture distributions into two classes C1 (τ ) and C2 (τ ). The classes depend
directly on τ because if the distinctiveness metric of the associated texture distribution is above τ , that texture distribution is
in class C1 (τ ). Likewise, if it is below τ , it is in class C2 (τ ).
The probability that a texture distribution is in the class C for a
given τ is P (T r |C(τ )) and the variance of the TD based on the
elements in the class is σC (τ ) . This threshold is known as the
Otsu’s threshold [34].
C. Segmentation Refinement
After the regions are classified as being normal skin or lesion, the following postprocessing steps are applied to refine the
lesion border: morphological dilation and region selection.
First, the morphological dilation operator is applied to fill
holes and smooth the border. Morphological dilation is a process
that expands binary masks to fill small holes [35]. The shape
and amount that the binary mask is expanded is controlled by a
structuring element, which is a disc with a radius of 5 pixels in
the TDLS algorithm.
Next, since multiple noncontiguous regions may have been
identified as part of the lesion class, the number of regions is
reduced to one. While it is possible to have multiple lesions in
a single image, it is necessary to reduce the number of lesions
for the feature extraction step. Features proposed by both Celebi
et al. [6] and Cavalcanti and Scharcanski [12] assume that only
a single lesion is being analyzed in the image. To eliminate the
small regions, the number of pixels in each contiguous region

GLAISTER et al.: SEGMENTATION OF SKIN LESIONS FROM DIGITAL IMAGES USING JOINT STATISTICAL TEXTURE DISTINCTIVENESS

is counted. The contiguous region with the largest number of
pixels is assumed to correspond to the lesion class and any other
regions are converted to the normal skin class. This gives the
final lesion segmentation.

proportions based on the results of the k-means clustering,
Θ̂ = arg min
Θ

where

IV. IMPLEMENTATION DETAILS

1225

K


n 
K




log αi P (ts j |μk , Σk )

j=1 k=1

αk = 1 and

k=1

A. Color Space

Θ = {μ1 , μ2 , . . . , μK , Σ1 , Σ2 , . . . , ΣK , α1 , α2 , . . . , αK }.

In the implementation of the TDLS algorithm, the photograph
is in the RGB domain and has three channels (a = 3). However,
the algorithm can be generalized and expanded to take into
account multi- or hyperspectral images of a skin lesion, where
a is much greater than three channels.
For standard digital images, we convert the image to the XYZ
color space to find texture distributions and during the initial
oversegmentation. Work by Terrillon et al. [36] found that the
XYZ color space proved to be an efficient color space in which
to segment the skin region of human faces. This color space is
designed to better model color perception and reduce correlation
between the XYZ channels, compared to the standard RGB color
space.
B. Learning Representative Texture Distributions
In this implementation, a two-step clustering algorithm is
used. First, a k-means clustering algorithm is run, which is followed by learning a finite mixture model. K-means clustering is
used as an initial step to increase the robustness and to speed up
the number of iterations required for the finite mixture model to
converge. K-means clustering finds K clusters of texture data
points that minimizes the sum of squared error between cluster
members and the cluster mean. The optimization function for
k-means clustering is shown in (11), where Ck is the kth set
of texture vectors, and μk is defined as the mean vector for
the kth set. Implementation details for k-means clustering can
be found in [37]. Here, the initial cluster means are randomly
assigned. Other methods to initialize the clusters could be used
to decrease the sensitivity of k-means clustering to initial cluster
placement [38],

Ĉ = arg min
C

K



ts j − μk 2 .

(11)

k =1 ts j ∈C k

One limitation with k-means clustering is that it does not
take into account any probabilistic information. Therefore, the
second step is to apply finite mixture model clustering. To fit
the finite mixture model, the model parameters in the set Θ are
found to maximize the log-likelihood function shown in (12).
In this implementation, a Gaussian distribution is assumed for
all clusters and the model parameters are the distribution mean
μ and distribution covariance Σ. Θ also contains the parameter α, which is the mixing proportion. No closed form solution
exists for (12) in general, so an expectation-maximization iterative algorithm is used [39]. The expectation-maximization algorithm is initialized using cluster means, covariances, and mixing

(12)
Expectation-maximization is an iterative algorithm. The initial parameters for the Gaussian mixture model are obtained
from the results of the k-means clustering. That is, the initial
Gaussian means are equal to the k-means cluster means:
μk = μC k

(13)

and the distribution covariances and mixing proportions are also
dependent on the cluster results. The initial estimate of the initial
mixing proportion is P (ts j ∈ Ck ). It is calculated by assuming
that the clusters found using k-means clustering have a Gaussian
distribution with mean μC k and covariance ΣC k ,
Σk = ΣC k

(14)

αk = P (ts j ∈ Ck ).

(15)

The parameters defining the K representative texture distributions are taken to be the mean and covariances for the
r
K-estimated Gaussian distributions (t̂k = μk ). Furthermore,
each texture vector is assigned to belong to the distribution
which maximizes the weighted probability αk P (ts j |μk , Σk ).
The number of clusters in k-means clustering or distributions in
the Gaussian mixture model is 10, which is determined to best
model the set of skin and lesion textures.
C. Summary of the TDLS Segmentation Algorithm
1) Convert the corrected image to the XYZ color space.
2) For each pixel s in image I, extract the texture vector ts
to obtain the set of texture vectors T (1).
3) Cluster the texture vectors in T , as described in Section IVB, to obtain the representative texture distributions.
4) Calculate probability that two texture distributions are
distinct dj,k using (6) for all possible pairs of texture
distributions.
5) Calculate the textural distinctiveness metric Dj (7) for
each texture distribution.
6) Apply the SRM algorithm to find the initial regions.
7) Calculate the region distinctiveness metric DR for each
initial region using (9).
8) Calculate the threshold τ between the normal skin and
lesion classes (10).
9) Classify each region as normal skin or lesion based on the
results of steps 7 and 8 (8).
10) Apply a morphological dilation operator to the initial lesion classification.
11) For each contiguous region in the initial segmentation,
count the number of pixels in the region.

1226

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

12) As the final lesion segmentation, return the contiguous
region consisting of the most pixels.
V. EXPERIMENTAL RESULTS
Two experiments are performed to compare the TDLS algorithm to other state-of-the-art algorithms. In the first experiment,
the first step of the TDLS step, calculating the TD metric, is compared to a similar algorithm. The compared algorithm calculates
a TD metric, but does not include statistical information. The
second experiment compares the segmentation results obtained
using the TDLS algorithm with four other segmentation algorithms designed for skin lesion images. The TDLS algorithm
is implemented in MATLAB on a computer with an Intel Core
i5-2400s CPU (2.5 GHz, 6-GB RAM). To segment a skin lesion
in a 1640 × 1043 image, the algorithm has an average runtime
of 62.45 s.
A. TD Comparison
The first step of the TDLS algorithm is compared to the results from the algorithm by Scharfenberger et al. [32], which
calculates a similar TD metric and is referred to as the TD
algorithm. The difference between the two algorithms is that
the TDLS algorithm introduces the use of probabilistic information to determine representative texture distributions and to
measure TD. To determine if incorporating this information is
useful, TD maps produced using the first step of the TDLS algorithm are compared to distinctiveness maps produced using
the TD algorithm. The TD algorithm only uses the k-means
clustering algorithm to find the representative texture distributions. Furthermore, the TD algorithm does not take into account
the covariance corresponding to each cluster when calculating
the distinctiveness metric. Finally, because the TD algorithm is
designed to compute saliency maps, the distinctiveness metric
includes an additional term based on the distance between a
pixel and the center of the image. Since we are interested in
understanding the effect of the additional probabilistic information, this term was omitted in the comparisons.
The TD maps are compared visually. Select skin lesion images from the Dermquest database [40] are used for comparison,
after being corrected for illumination variation using the MSIM
algorithm [14]. These examples are selected because they highlight cases with significant differences between the TD and
TDLS algorithms and are shown in Fig. 5. Also, the dynamic
range of pixels is scaled to the maximum pixel intensity and
minimum pixel intensity, resulting in a different dynamic range
for each TD map.
Some interesting observations can be made from the examples. First, lesions in Fig. 5(a) and (c) are comprised of two
distinct textures. For example, in Fig. 5(a), there are pronounced
dark areas and lighter red areas. However, when using the TD
algorithm, only the first texture is highlighted in the TD map.
Using the TDLS algorithm, both textures are highlighted. This
is also seen in Fig. 5(c).
As a tradeoff, distinct nonlesion areas that occur due to natural
pigmentation and texture characteristics of the skin are also
highlighted when using the TDLS algorithm. For example, in

Fig. 5(a), the presence of shading on the left side of the image
is highlighted when using the TDLS algorithm, but not when
using the TD algorithm. This motivates use of the texture-based
segmentation step in the TDLS algorithm, rather than using the
textural distinctiveness maps directly.
B. Segmentation Comparison
The TDLS algorithm is compared to four state-of-the-art lesion segmentation algorithms. The first algorithm (L-SRM) is
designed for dermatological images, but can be applied to lesion photographs as well. It applies the SRM algorithm outlined
in Section III-A and uses the normal skin color to find the regions corresponding to the lesion. The three other algorithms
are proposed by Cavalcanti et al. and are designed specifically
for lesion photographs. One algorithm (Otsu-R) finds the Otsu
threshold using the red color channel. The second (Otsu-RGB)
uses all three RGB color channels and finds Otsu thresholds
for each channel. The final algorithm (Otsu-PCA) processes the
RGB color channels to find three more efficient channels to
threshold. A texture channel is obtained using Gaussian filtering, a color channel is obtained using the inverse of the red color
channel, and the third channel is found using PCA. For simplicity, this algorithm is referred to as Otsu-PCA. All algorithms
have additional postprocessing steps to clean up the contour,
and these steps have been implemented as described in their
publication.
A set of 126 images from the Dermquest database [40] are
used to test the segmentation algorithms. There are 66 photographs with lesions diagnosed as melanoma and 60brk photographs with lesions diagnosed as nonmelanoma. These images are selected because they satisfy the stated assumptions
and can be adequately corrected for illumination variation. All
tested photographs were first corrected using the MSIM algorithm [14]. The segmentation algorithms are compared to manually segmented ground truth. The algorithms are compared
visually and by calculating sensitivity, specificity, and accuracy
of the algorithm to properly classify each pixel as normal skin
or lesion.
1) Visual Comparison: The objective of the visual comparison is to analyze the segmentation results qualitatively. Interesting examples of segmentation results are shown in Fig. 6,
along with the ground truth. Many of these examples illustrate
situations where existing state-of-the-art algorithms cannot accurately locate the lesion and the proposed algorithm can.
Fig. 6(a)–(f) is images of melanoma lesions, and Fig. 6(g)
and (h) is nonmelanoma lesions. Fig. 6(a) and (b) correspond to
the examples shown in Figs. 4 and 1.
Areas where illumination variation has not been fully corrected can be misclassified as part of the lesion, as seen in
Fig. 6(c) and (g). In both those examples, there are uncorrected
shadows which all algorithms except for the TDLS algorithm
consider as part of the lesion. Complicated texture patterns in
the skin area, as seen in Fig. 6(h), and artifacts such as hair, as
seen in Fig. 6(e), are also often misclassified as part of the lesion.
However, the TDLS algorithm is able to reasonably segment the
lesion in those images.

GLAISTER et al.: SEGMENTATION OF SKIN LESIONS FROM DIGITAL IMAGES USING JOINT STATISTICAL TEXTURE DISTINCTIVENESS

1227

Fig. 5. Corrected skin lesion images and their corresponding textural distinctiveness maps. The textural distinctiveness maps are produced using the TD
algorithm [32] and the first step of the TDLS algorithm. The pixel intensity corresponds to the TD of the pixel’s associated texture distribution. The TDLS
algorithm is able to better highlight the lesion area, compared to the TD algorithm. However, in (a) and (c), nonlesion areas are also highlighted by the TDLS
algorithm.

Lesions can be comprised of different colors and textures,
such as in Fig. 6(d) and (f). In fact, color variegation across a
lesion is a feature that is used to classify lesions as melanoma.
It is critical that segmentation algorithms can account for the
color and texture variation when locating the skin lesion. The
compared algorithms only find the most prominent color or
texture and fail to include the subtler regions as part of the
lesion. However, because the TDLS algorithm learns the lesion
textures and normal skin textures, it is able to locate the entire
lesion.
2) Segmentation Accuracy Comparison: The objective of
this experiment is to measure sensitivity, specificity, and accuracy of each segmentation algorithm after the algorithms classify each pixel as belonging to the normal skin class or lesion
class. Each algorithm is applied to the corrected images and

the resulting segmentation is compared to the manually drawn
segmentation acting as ground truth. The metrics used to compare to the ground truth are sensitivity, specificity, and accuracy.
Their formulas are given in (16), (17), and (18), where TP is the
number of true positive pixels, FP is the number of false positive
pixels, TF is the number of true negative pixels, and FN is the
number of false negative pixels,

TP
TP + FN
TN
Speciﬁcity =
TN + FP
TP + TN
.
Accuracy =
TP + FN + TN + FP

Sensitivity =

(16)
(17)
(18)

1228

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

Fig. 6. Segmentation of skin lesions using various algorithms. In the first column, the manually segmented ground truth is shown. In the second to fifth columns,
the results of state-of-the-art skin lesion segmentation algorithms are shown. The last column contains the results of the proposed TDLS algorithm.
TABLE I
SEGMENTATION ACCURACY RESULTS FOR ALL LESION PHOTOGRAPHS

TABLE III
SEGMENTATION ACCURACY RESULTS FOR NONMELANOMA
LESION PHOTOGRAPHS

TABLE II
SEGMENTATION ACCURACY RESULTS FOR MELANOMA LESION PHOTOGRAPHS

Tables I, II, and III show the average sensitivity, specificity,
and accuracy across the entire set of images or for just the
melanoma or nonmelanoma photographs.
Table I shows that the TDLS algorithm has the highest
accuracy across all tested photographs, followed closely by
the Otsu-PCA algorithm. The TDLS algorithm also has the

GLAISTER et al.: SEGMENTATION OF SKIN LESIONS FROM DIGITAL IMAGES USING JOINT STATISTICAL TEXTURE DISTINCTIVENESS

second highest sensitivity and specificity. This trend follows
when looking at subsets of melanoma or nonmelanoma photographs, as seen in Tables II and III. The Otsu-PCA algorithm
has similar specificity and accuracy results, while the Otsu-RGB
algorithm has better sensitivity. However, the TDLS algorithm
is able to perform well in all three metrics.
VI. CONCLUSION
In summary, a novel lesion segmentation algorithm using the
concept of TD is proposed. A probabilistic TD metric is introduced based on a learned model of normal skin and lesion
textures. Representative texture distributions are learned from
the image itself and the TD metric captures the dissimilarity between pairs of texture distributions. Then, the image is divided
into numerous smaller regions and each of those regions are classified as lesion or skin based on the TD map. The entire proposed
framework is tested by using the illumination corrected images
as the input to the texture-based segmentation algorithm. It is
compared to state-of-art lesion segmentation algorithms, including three algorithms designed for lesion images. The proposed
framework produces the highest segmentation accuracy using
manually segmented images as ground truth. A larger data collection and annotation process, including additional testing on a
wide range of images, will be undertaken as future work. While
the experimental results show that the proposed method is able
to segment the lesion in images of different scales and levels
of quality, it is worth conducting a more comprehensive analysis on the impact of image quality and scale on the proposed
method.
REFERENCES
[1] N. Howlader, A. M. Noone, M. Krapcho, J. Garshell, N. Neyman, S.
F. Altekruse, C. L. Kosary, M. Yu, J. Ruhl, Z. Tatalovich, H. Cho, A.
Mariotto, D. R. Lewis, H. S. Chen, E. J. Feuer, and K. A. Cronin, “SEER
cancer statistics review, 1975-2010,” Nat. Cancer Inst., Bethesda, MD,
USA, Tech. Rep., 2013
[2] A. F. Jerants, J. T. Johnson, C. D. Sheridan, and T. J. Caffrey, “Early detection and treatment of skin cancer,” Amer. Family Phys., vol. 62, no. 2,
pp. 1–6, Jul. 2000.
[3] Public Health Agency of Canada. (2013). Melanoma skin cancer. [Online]. Available:http://www.phac-aspc.gc.ca/cd-mc/cancer/
melanoma skin cancer-cancer peau melanome-eng.php
[4] A. Jemal, M. Saraiya, P. Patel, S. S. Cherala, J. Barnholtz-Sloan, J. Kim,
C. L. Wiggins, and P. A. Wingo, “Recent trends in cutaneous melanoma
incidence and death rates in the united states, 1992-2006,” J. Amer. Acad.
Dermatol., vol. 65, no. 5, pp. S17.e1–S17.e11, Nov. 2011.
[5] K. A. Freedberg, A. C. Geller, D. R. Miller, R. A. Lew, and H. K. Koh,
“Screening for malignant melanoma: A cost-effectiveness analysis,” J.
Amer. Acad. Dermatol., vol. 41, no. 5, pt. 1, pp. 738–745, Nov. 1999.
[6] M. E. Celebi, H. A. Kingravi, B. Uddin, H. Iyatomi, Y. A. Aslandogan,
W. V. Stoecker, and R. H. Moss, “A methodological approach to the classication of dermoscopy images,” Comput. Med. Imag. Graph., vol. 31,
no. 6, pp. 362–373, Sep. 2007.
[7] S. W. Menzies, L. Bischof, H. Talbot et al., “The performance of solarscan:
An automated dermoscopy image analysis instrument for the diagnosis of
primary melanoma,” Archives Dermatol., vol. 141, no. 11, pp. 1388–1396,
Nov. 2005.
[8] H. Iyatomi, H. Oka, M. E. Celebi, M. Hashimoto, M. Hagiwara,
M. Tanaka, and K. Ogawa, “An improved internet-based melanoma
screening system with dermatologist-like tumor area extraction algorithm,” Comput. Med. Imag. Graph., vol. 32, no. 7, pp. 566–579, Oct
2008.

1229

[9] H. Ganster, A. Pinz, R. Rohrer, E. Wildling, M. Binder, and H. Kittler,
“Automated melanoma recognition,” IEEE Trans. Med. Imag., vol. 20,
no. 3, pp. 233–239, Mar. 2001.
[10] H. Iyatomi, M. Celebi, G. Schaefer, and M. Tanaka, “Automated color
calibration method for dermoscopy images,” Comput. Med. Imag. Graph.,
vol. 35, no. 2, pp. 89–98, Mar. 2011.
[11] H. C. Engasser and E. M. Warshaw, “Dermatoscopy use by US dermatologists: A cross-sectional survey,” J. Amer. Acad. Dermatol., vol. 63, no. 3,
pp. 412–419, 2010.
[12] P. G. Cavalcanti and J. Scharcanski, “Automated prescreening of pigmented skin lesions using standard cameras,” Comput. Med. Imag. Graph.,
vol. 35, no. 6, pp. 481–491, Sep. 2011.
[13] J. Alcon, C. Ciuhu, W. ten Kate, A. Heinrich, N. Uzunbajakava,
G. Krekels, D. Siem, and G. De Haan, “Automatic imaging system with
decision support for inspection of pigmented skin lesions and melanoma
diagnosis,” IEEE J. Sel. Topics Signal Process., vol. 3, no. 1, pp. 14–25,
Feb. 2009.
[14] J. Glaister, R. Amelard, A. Wong, and D. A. Clausi, “MSIM: Multi-stage
illumination modeling of dermatological photographs for illuminationcorrected skin lesion analysis,” IEEE Trans. Biomed. Eng., vol. 60, no. 7,
pp. 1873–1883, Jul. 2013.
[15] R. J. Friedman, D. S. Rigel, and A. W. Kopf, “Early diagnosis of cutaneous
melanoma: Revisiting the ABCD criteria,” CA: A Cancer J. Clinicians,
vol. 35, no. 3, pp. 130–151, May 1985.
[16] M. Celebi, H. Iyatomi, G. Schaefer, and W. V. Stoecker, “Lesion border
detection in dermoscopy images,” Comput. Med. Imag. Graph., vol. 33,
no. 2, pp. 148–153, 2009.
[17] B. Erkol, R. H. Moss, R. Joe Stanley, W. V. Stoecker, and E. Hvatum,
“Automatic lesion boundary detection in dermoscopy images using gradient vector flow snakes,” Skin Res. Technol., vol. 11, no. 1, pp. 17–26,
2005.
[18] M. E. Celebi, H. A. Kingravi, H. Iyatomi, Y. A. Aslandogan,
W. V. Stoecker, R. H. Moss, J. M. Malters, J. M. Grichnik,
A. A. Marghoob, H. S. Rabinovitz, and S. W. Menzies, “Border detection in dermoscopy images using statistical region merging,” Skin Res.
Technol., vol. 14, no. 3, pp. 347–353, 2008.
[19] G. Hance, S. Umbaugh, R. Moss, and W. Stoecker, “Unsupervised color
image segmentation: with application to skin tumor borders,” IEEE Eng.
Med. Biology Mag., vol. 15, no. 1, pp. 104–111, Jan./Feb. 1996.
[20] P. G. Cavalcanti, J. Scharcanski, and C. B. O. Lopes, “Shading attenuation
in human skin color images,” in Advances in Visual Computing, G. Bebis,
R. Boyle, B. Parvin, D. Koracin, R. Chung, R. Hammoud, M. Hussain,
T. Kar-Han, R. Crawfis, D. Thalmann, D. Kao, and L. Avila, Eds., (ser.
Lecture Notes in Computer Science), vol. 6453 Heidelberg, Germany:
Springer, 2010, pp. 190–198.
[21] P. Cavalcanti, Y. Yari, and J. Scharcanski, “Pigmented skin lesion segmentation on macroscopic images,” in Proc. 25th Int. Conf. Image Vision
Comput. New Zealand., 2010, pp. 1–7.
[22] P. Cavalcanti, J. Scharcanski, L. Di Persia, and D. Milone, “An ICA-based
method for the segmentation of pigmented skin lesions in macroscopic
images,” in Proc. IEEE Annu. Int. Conf. Eng. Med. Biol. Soc., 2011,
pp. 5993–5996.
[23] M. Anantha, R. H. Moss, and W. V. Stoecker, “Detection of pigment
network in dermatoscopy images using texture analysis,” Comput. Med.
Imag. Graph., vol. 28, no. 5, pp. 225–234, 2004.
[24] W. V. Stoecker, C.-S. Chiang, and R. H. Moss, “Texture in skin images:
Comparison of three methods to determine smoothness,” Comput. Med.
Imag. Graph., vol. 16, no. 3, pp. 179–190, 1992.
[25] A. P. Dhawan and A. Sim, “Segmentation of images of skin lesions using
color and texture information of surface pigmentation,” Comput. Med.
Imag. Graph., vol. 16, no. 3, pp. 163–177, 1992.
[26] M. Silveira, J. Nascimento, J. Marques, A. R. S. Marcal, T. Mendonca,
S. Yamauchi, J. Maeda, and J. Rozeira, “Comparison of segmentation
methods for melanoma diagnosis in dermoscopy images,” IEEE J. Sel.
Topics Signal Process., vol. 3, no. 1, pp. 35–45, 2009.
[27] C. Serrano and B. Acha, “Pattern analysis of dermoscopic images based
on Markov random fields,” Pattern Recog., vol. 42, no. 6, pp. 1052–1057,
2009.
[28] L. Xu, M. Jackowskia, A. Goshtasby, D. Roseman, S. Bines, C. Yu,
A. Dhawan, and A. Huntley, “Segmentation of skin cancer images,” Image
Vis. Comput., vol. 17, pp. 65–74, 1999.
[29] S. Hwang and M. E. Celebi, “Texture segmentation of dermoscopy images
using Gabor filters and g-means clustering,” in Proc. Int. Conf. Image
Process., Comput. Vision, Pattern Recog, Jul. 2010, pp. 882–886.

1230

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 4, APRIL 2014

[30] G. Peyre, “Sparse modeling of textures,” J. Math. Imag. Vis., vol. 34, no. 1,
pp. 17–31, 2009.
[31] J.-L. Starck, M. Elad, and D. Donoho, “Image decomposition via the
combination of sparse representations and a variational approach,” IEEE
Trans. Image Process., vol. 14, no. 10, pp. 1570–1582, Oct. 2005.
[32] C. Scharfenberger, A. Wong, K. Fergani, J. S. Zelek, and D. A. Clausi,
“Statistical textural distinctiveness for salient region detection in natural
images,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog., Jun. 2013,
pp. 979–986.
[33] R. Nock and F. Nielsen, “Statistical region merging,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 26, no. 11, pp. 1452–1458, Nov 2004.
[34] N. Otsu, “A threshold selection method from gray-level histograms,” IEEE
Trans. Syst., Man, Cybern., vol. 9, no. 1, pp. 62–66, 1979.
[35] P. Soille, Morphological Image Analysis: Principles and Applications
Image Processing, 2nd ed. ed. Berlin, Germany: Springer, 2003.
[36] J.-C. Terrillon, M. Shirazi, H. Fukamachi, and S. Akamatsu, “Comparative performance of different skin chrominance models and chrominance
spaces for the automatic detection of human faces in color images,” in
Proc. 4th IEEE Int. Conf. Autom. Face Gesture Recog., 2000, pp. 54–61.
[37] A. K. Jain, R. P. W. Duin, and J. Mao, “Statistical pattern recognition: a
review,” IEEE Tran. Pattern Anal. Mach. Intell., vol. 22, no. 1, pp. 4–37,
Jan 2000.
[38] M. Celebi, H. Kingravi, and P. A. Vela, “A comparative study of efficient
initialization methods for the k-means clustering algorithm,” Expert Syst.
Appl., vol. 40, no. 1, pp. 200–210, Sep. 2012.
[39] M. A. T. Figueiredo and A. K. Jain, “Unsupervised learning of finite mixture models,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 24, no. 3,
pp. 381–396, Mar. 2002.
[40] DermQuest, (2012). [Online]. Available:http://www.dermquest.com

Jeffrey Glaister (S’12) received the B.A.Sc. degree in systems design engineering in 2011 and the
M.A.Sc. degree in systems design engineering in
2013, both from the University of Waterloo, Waterloo, ON, Canada. He is currently working toward the
Ph.D. degree in electrical and computer engineering
at Johns Hopkins University, Baltimore, MD, USA.
He is a member of the Image Analysis and Communications lab, Johns Hopkins University. His current research topic is parcellation of thalamic nuclei
from magnetic resonance images and past research
included segmentation of skin lesions from dermatological photographs. His
research interests include biomedical image processing, remote sensing, and
pattern recognition.

Alexander Wong (M’05) received the B.A.Sc. degree in computer engineering, the M.A.Sc. degree in
electrical and computer Engineering, and the Ph.D.
degree in systems design engineering from the University of Waterloo, Waterloo, ON, Canada, in 2005,
2007, and 2010 respectively. He is currently the
Canada Research Chair in Medical Imaging Systems
and an Assistant Professor in the Department of Systems Design Engineering, University of Waterloo. He
has published refereed journal and conference papers,
as well as patents, in various fields such as computer
vision, graphics, image processing, multimedia systems, and wireless communications. His research interests include image processing, computer vision,
pattern recognition, and cognitive radio networks, with a focus on biomedical
and remote sensing image processing and analysis such as image registration,
image denoising and reconstruction, image superresolution, image segmentation, tracking, and image, and video coding and transmission.
Dr. Wong received an Outstanding Performance Award, an Engineering Research Excellence Award, an Early Researcher Award from the Ministry of
Economic Development and Innovation, a Best Paper Award by the Canadian
Image Processing and Pattern Recognition Society, and the Alumni Gold Medal.

David A. Clausi (S’93–M’96–SM’03) received the
Ph.D. degree in systems design engineering at the
University of Waterloo, Waterloo, ON, Canada, in
1996.
He then worked in software medical imaging at
AGFA, Waterloo. He started his academic career in
1997 as an Assistant Professor in Geomatics Engineering at the University of Calgary, Calgary, AB,
Canada. In 1999, he returned to his alma mater and
is currently a Professor specializing in the fields of
Intelligent and Environmental Systems and recently
became the Associate Chair for graduate studies. He is an active interdisciplinary
and multidisciplinary Researcher. He has an extensive publication record, publishing refereed journal and conference papers in the diverse fields of remote
sensing, computer vision, algorithm design, and biomechanics. His research
efforts have led to successful commercial implementations including creating,
building, and selling his own company.
Dr. Clausi was the Co-chair of IAPR Technical Committee 7 Remote Sensing
during 2004–2006. He has received numerous scholarships, paper awards, and
two Teaching Excellence Awards. In 2010, he received the award for Research
Excellence and Service to the Research Community by the Canadian Image
Processing and Pattern Recognition Society.

