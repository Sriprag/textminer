Journal of Informetrics 1 (2007) 145–154

Lifting the crown—citation z-score
Jonas Lundberg a,b,∗
a

Medical Management Centre at the Department of Learning, Informatics, Management and Ethics (LIME),
Karolinska Institutet, SE-17177 Stockholm, Sweden
b Strategy and Development Ofﬁce, Karolinska Institutet, SE-17177 Stockholm, Sweden
Received 31 August 2006; received in revised form 22 September 2006; accepted 22 September 2006

Abstract
Researchers worldwide are increasingly being assessed by the citation rates of their papers. These rates have potential impact on
academic promotions and funding decisions. Currently there are several different ways that citation rates are being calculated, with
the state of the art indicator being the crown indicator. This indicator has flaws and improvements could be considered. An item
oriented ﬁeld normalized citation score average (c̄f ) is an incremental improvement as it differs from the crown indicator in so much as
normalization takes place on the level of individual publication (or item) rather than on aggregated levels, and therefore assigns equal
weight to each publication. The normalization on item level also makes it possible to calculate the second suggested indicator: total
field normalized citation score (cf ). A more radical improvement (or complement) is suggested in the item oriented ﬁeld normalized
logarithm-based citation z-score average (c̄fz[ln] or citation z-score). This indicator assigns equal weight to each included publication
and takes the citation rate variability of different fields into account as well as the skewed distribution of citations over publications.
Even though the citation z-score could be considered a considerable improvement it should not be used as a sole indicator of
research performance. Instead it should be used as one of many indicators as input for informed peer review.
© 2007 Elsevier Ltd. All rights reserved.
Keywords: Citation; Indicator; Normalization; Research Assessment; z-score

1. Introduction
Researchers worldwide are increasingly being assessed by the citation rates of their papers. These rates have potential
impact on academic promotions and funding decisions. Currently there are several different ways in which citation
rates are being calculated, from basic calculations like raw citation counts and the h-index (Hirsch, 2005) to citation
rates controlled for research field, publication year and document type. The reason for controlling for the first two
factors is shown in Fig. 1. As can be seen in the figure articles in different research fields and from different publication
years have varying average citation rates. Articles in Cell Biology journals on average receive about five times as many
citations as do articles in Crystallography journals. The difference between fields is quite consistent over time. The
older a publication is, the more likely it is that it has been cited (regardless of field). As seen in the figure, articles in the
displayed areas on average are cited between six (Biochemistry & Molecular Biology) and eight (Clinical Neurology)
times more often if they were published in 1998 than if they were published 6 years later.
∗ Correspondence address: Medical Management Centre at the Department of Learning, Informatics, Management and Ethics (LIME), Karolinska
Institutet, SE-17177 Stockholm, Sweden
E-mail address: jonas.lundberg@ki.se.

1751-1577/$ – see front matter © 2007 Elsevier Ltd. All rights reserved.
doi:10.1016/j.joi.2006.09.007

146

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

Fig. 1. Average citation rates 1998–2004 for articles in Cell Biology, Biochemistry & Molecular Biology, Clinical Neurology and Crystallography.
Data from the citation indices produced by Thomson Scientific, self citations are included and whole counting is performed.

In Fig. 2 an example is shown of why it is important to control for document type. Publications classified as Articles
receive about two fifths as many citations as publications classified as Reviews (35–40% between 1998 and 2004).
Letters in turn receive one fifth as many citations as Articles (17–21% between 1998 and 2004). In summary, there are
obvious differences in average citation rates for publications of different types, of different age, published in journals
within different fields.
The first suggestions on how to control for these factors and calculate ‘normalized’ citation rates were made in the
1980s (Schubert, Glänzel, & Braun, 1983; Vinkler, 1986). In general, all proposed normalization methods are computed
by dividing the actual number of received citations for a group of publications with the number of citations that could be
expected for similar publications. Currently the state of the art indicator is the so called ‘crown indicator’, developed at
the Centre for Science and Technology Studies (CWTS) at Leiden University (Moed, Debruin, & Vanleeuwen, 1995).
The indicator is calculated by dividing the average number of received citations for a group of publications with the
average number that could be expected for publications of the same type, from the same year, published in journals
within the same field. An example is shown in part I of Table 1.

Fig. 2. Average citation rates for articles, letters and reviews in CI 1998–2004. Data from the citation indices produced by Thomson Scientific, self
citations are included and whole counting is performed.

Article

Type

Year

Journal

Field

c

μf

σf

cf

ln(c + 1)

μfz[ln]

σ fz[ln]

cfz[ln]

A
B
C
D
E

Article
Article
Review
Article
Letter

2003
2003
2000
2000
2001

J. Appl. Crystallogr.
J. Cryst. Growth
Nat. Rev. Immunol.
J. Immunol.
Am. J. Hum. Genet.

Crystallography
Crystallography
Immunology
Immunology
Genetics & Heredity

12
5
66
17
17

2.6
2.6
33.4
17.0
9.7

22.1
22.1
76.1
27.4
32.2

4.6
1.9
2.0
1.0
1.8

2.6
1.8
4.2
2.9
2.9

0.9
0.9
2.7
2.3
1.6

0.8
0.8
1.3
1.2
1.2

2.2
1.2
1.1
0.6
1.1

c: Number of received citations; μf : the average value of citations to publications of the same type, published the same year in the same research area; σ f : standard deviation for the average number
of citations received for publications of the same type, from the same year, published in journals within the same field; cf : field normalized citation score (c/μf ); ln(c + 1): logarithm of the number
of received citations plus one; μfz[ln] : the logarithm-based field citation score: the average value of the logarithmic number of citations (plus one) to publications of the same type, published the
same year in the same research area; σ fz[ln] : the standard deviation of the μfz[ln] distribution; cfz[ln] : field normalized logarithm-based citation z-score;
P

P
(I) ‘Crown indicator’:
c / i=1 [μf ]i = (12 + 5 + 66 + 17 + 17/2.6 + 2.6 + 33.4 + 17.0 + 9.7) ≈ (117/65.3) ≈ 1.8.
i=1 i
(II) c̄f —item oriented field normalized citation score average: (1/P)

P

i=1

(ci /[μf ]i ) = (4.6 + 1.9 + 2.0 + 1.0 + 1.8/5) ≈ 2.2.

P

(III) c̄fz[ln] —item oriented field normalized logarithm-based citation z-score average: (1/P) i=1 (ln(ci + 1) − [μf[ln] ]i /[σf[ln] ]i ) = (2.2 + 1.2 + 1.1 + 0.6 + 1.1/5) ≈ 1.2.
ci : number of citations to publication i; P: the unit’s number of publications; [μf ]i : the average value of citations to publications of the same type, published the same year in the same research
area as article i; [μf[ln] ]i : the logarithm-based field citation score; the average value of the logarithmic number of citations (plus one) to publications of the same type, published the same year in
the same research area as article i; [σ f[ln] ]i : the standard deviation of the [μf[ln] ]i distribution.

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

Table 1
Calculation of the ‘crown indicator’, item oriented field normalized citation score average, and item oriented field normalized logarithm-based citation z-score average

147

148

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

First the average citation rate (c̄ or CPP in CWTS terminology) for the studied five publications is
calculated ((12 + 5 + 66 + 17 + 17)/5 = 23.4). Second the expected average citation rate for publications of the
same type, from the same year, within the same field is calculated (μ̄f or FCSm in CWTS terminology)
((2.6 + 2.6 + 33.4 + 17.0 + 9.7)/5 = 13.1). Finally the crown indicator value is received by dividing the actual citation
rate with the expected citation rate (23.4/13.1 = 1.8).
In this study one letter acronyms are used in indicator calculations instead of the multi-letter acronyms suggested
earlier (e.g. c̄ instead of CPP for average number of citations per publication, or μ̄f instead of FCSm for average citation
rate for a specific field, document type and publishing year). The use of one letter acronyms makes it possible to write
down calculations more clearly.
2. Data sources
The figures in this study are based on data from the citation indices produced by Thomson Scientific.1 These include
the Science Citation Index (SCI), Social Science Citation Index (SSCI) and the Arts & Humantities Citation Index
(AHCI). They are jointly referred to as the Thomson Scientific Citation Indices (CI). CI data has been imported into
a MySQL database. The figures were created using Microsoft® Office Excel 2003. Self citations were included in all
figures and whole counting was performed (not fractional counting). It should be noted that the suggested normalized
citation rates of individual publications easily could be calculated and published by Thomson Scientific on their Web
of Science® . Until this occurs the calculation of the suggested indicators requires direct access to world-wide citation
data.2
3. Lifting the crown
It can be argued that the crown indicator has flaws. The first is that citation rates are not normalized on the level
of individual publications, but on a higher aggregation level where the average citation rate of a researcher, group or
department is compared to the average citation rate of the fields in which the researcher or group has published. This
way of calculating gives more weight to older publications (particularly reviews), published in fields with dense citation
traffic. In order to give each publication equal weight the normalization should take place on the level of the individual
publication. The calculation of such an item oriented ﬁeld normalized citation score average – c̄f – is shown in part II of
Table 1. Here, instead of first calculating the actual average citation rate, and then divide that with the average expected
citation rate, each publication (or item) is normalized individually (hence ‘item oriented’). For example, article A has
received 12 citations (c). The average number of citations that similar publications (Articles from 2003 published in
Crystallography journals) have received is 2.6 (μf ). The field normalized citation score (cf ) is thus c/μf = 12/2.6 = 4.6.
This procedure is repeated for each of the publications that the unit one intends to assess has published. In the example
one continues with B (cf : 1.9), C (2.0), D (1.0) and E (1.8). In the final step, the average for the field normalized citation
scores is calculated (c̄f = (4.6 + 1.9 + 2.0 + 1.0 + 1.8)/5 = 2.2).
The field normalized citation scores can also be summed in order to calculate a total field normalized citation score
(cf ) for a research group, university or country.
4. Citation z-score
The distribution of citations over publications differs between ‘normalization groups’ (publications of a specific
type, within a specific research field, published a specific year). Therefore, does not only the average citation rate differ,
but also the standard deviation. It could thus be argued that using a z-score in the normalization procedure would be
more appropriate. A z-score expresses how far a value is from the population mean, and expresses this difference in
terms of the number of standard deviations by which it differs (Kirkwood & Sterne, 2003). A second issue that needs to
1

Certain data included herein is derived from the 1995 to 2005 Science Citation Index Expanded, Social Sciences Citation Index and, Arts &
Humanities Citation Index Tagged Data prepared by the Thomson Scientific Inc. (TS), Philadelphia, Pennsylvania, USA: ©Copyright Thomson
Scientific Inc® 2006. All rights reserved.
2 For more information on how Karolinska Institutet has developed its bibliometric system please contact the author.

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

149

be dealt with is that the distribution of citations over publications is highly skewed. The skewed distribution has been
seen for fields and journals, as well as for individual scientists (Seglen, 1992). It could thus be an alternative to use the
geometric mean or median value as comparison when calculating normalized citation rates. Another alternative is to
make normalizations using logarithmically transformed citation rates.
Combining the observations above, it could be argued that it would be appropriate to use a logarithmic citation
z-score as a complementary indicator to the item oriented field normalized citation score average. A citation z-score
would compare the logarithm of the number of citations that a publication has received with the mean and standard
deviation for the logarithms of the citation rates for all the corresponding reference publications.
When calculating the proposed indicator one should add one to the number of citations for each publication. This
is necessary since it is not possible to calculate the logarithm of zero. For example, a review article published in
2000 in Nature Reviews Immunology (Article C in Table 1) has received 66 citations. The natural logarithm of this
value plus one (ln(67) = 4.2) would then be compared with the average number (2.7) and standard deviation (1.3)
of the natural logarithms of citation rates (plus one) of all reviews from 2003 in immunology. The citation z-score
for this article is then (4.2 − 2.7)/1.3 ≈ 1.1. Observe that the comparison is made with average of the logarithms of
the number of citations received by comparable items and not with the logarithm of the average number of citations
received by comparable items. The bibliometric indicator for a research group, department or university is then the item
oriented ﬁeld normalized logarithm-based citation z-score average (c̄fz[ln] —or ‘citation z-score’). An example of the
calculation is shown in part III of Table 1. The citation z-score would in this case be (2.2 + 1.2 + 1.1 + 0.6 + 1.1)/5 = 1.2.
The publications in the example are thus, after logarithmic transformation, on average cited 1.2 standard deviations
above the world average for publications of the same type, from the same year, published in journals belonging to the
same subject category.
5. Examples
5.1. Distribution of publications over cfz[ln]
Between 1998 and 2003, 60,082 publications with at least one co-author affiliated with an organisation in Sweden,
were published in journal belonging to 57 life science related areas.3 49,490 were articles, letters or reviews belonging
to normalization groups that meet the inclusion criteria that the average citation rate should be at least one citation
and there should be at least 100 publication of the same type, from same year, published in journals within the same
subject category. The Swedish life science publications on average have been cited 26% above the world average.
After logarithmic transformation, the average publication has received citations 0.23 standard deviations above the
world average (mean: 0.23; median: 0.26; max 5.9; min −2.51). The distribution of publications over cfz[ln] is shown
in Fig. 3. As can be seen in the figure the distribution is approximately normal with a slight positive skew (skewness:
0.03).
Similarly to the shown distribution of publications over cfz[ln] for a country (as shown in Fig. 3) the distribution can
be shown on meso (university or department) and micro level (individual researcher or research group). An example of
how the citation rates for publications by research groups builds up the distribution for a whole department is shown in
Fig. 4. The figure is based on 453 publications by 20 research groups constituting a department at Karolinska Institutet
(KI), published between 1998 and 2003. The research groups have published between 3 and 70 publications during the
time period. The distribution of publications over categories of cfz[ln] for each of the 20 research groups is represented
by a line. The lines are placed on top of each other, giving the distribution for the department as a whole.

3 Acoustics, Anesthesiology, Behavioral Sciences, Biochemistry & Molecular Biology, Biophysics, Biotechnology & Applied Microbiology,
Cardiac & Cardiovascular System, Cell Biology, Clinical Neurology, Critical Care Medicine, Dentistry, Oral Surgery & Medicine, Dermatology
& Venereal Diseases, Education, Special, Endocrinology & Metabolism, Engineering, Biomedical, Gastroenterology & Hepatology, Genetics &
Heredity, Geriatrics & Gerontology, Gerontology, Health Care Sciences & Services, Hematology, Immunology, Infectious Diseases, Language &
Linguistics Theory, Medical Informatics, Medicine, General & Internal, Medicine, Legal, Medicine, Research & Experimental, Microbiology, Multidisciplinary Sciences, Neurosciences, Nursing, Obstetrics & Gynecology, Oncology, Ophthalmology, Orthopedics, Otorhinolaryngology, Pathology,
Pediatrics, Peripheral Vascular Diseases, Pharmacology & Pharmacy, Philosophy, Psychiatry, Psychology, Clinical, Psychology, Development, Public, Environmental & Occupational Health, Radiology, Nuclear Medicine & Medical Imaging, Rehabilitation, Reproductive Biology, Respiratory
System, Rheumatology, Social Issues, Social Sciences, Interdisciplinary, Sport Sciences, Surgery, Transplantation, Urology & Nephrology.

150

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

Fig. 3. Distribution of publications over classes of cfz[ln] . The figure is based on 49,490 Swedish publications in 57 life science related areas,
published between 1998 and 2003. Bars including publications with cfz[ln] < −0.25 are coloured grey, cfz[ln] between −0.25 and +0.25 white, and
cfz[ln] > +0.25 black. Data from the citation indices produced by Thomson Scientific, self citations are included and whole counting is performed.

The distribution of publications over cfz[ln] can be used for comparisons between different aggregation levels,
for example a department, a university and a country. In Fig. 5 the relative distribution of publications over cfz[ln]
for a department at KI is compared with the distribution for KI as a whole, and the distribution of all Swedish
publications within the 57 life science related research areas (1998–2003). In the example 61% of the Swedish life
science publications have a cfz[ln] above zero, while KI as a whole has 66% and the selected department 69%. If instead
looking at the share of publications with a value above one, KI as a whole has a higher share (24%) than the department
(22%).
5.2. Distribution of units over citation z-score
On higher aggregation levels distribution of citation z-scores also approaches normal distribution. An example is
shown in Fig. 6. The figure show data for 334 research units at KI that have published at least 30 publications between
1995 and 2004 (mean c̄fz[ln] 0.37; median 0.36; min −0.39; max 1.32). Ninety percent of the units have citation z-scores
above 0, and 3% have a value more than one standard deviation above world average.

Fig. 4. Distribution of publications over classes of cfz[ln] . The figure is based on 453 publications by the 20 research groups at a department at
Karolinska Institutet, published between 1998 and 2003. Each line represent the publications by a research group. The lines are placed on top of
each other. Data from the citation indices produced by Thomson Scientific, self citations are included and whole counting is performed.

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

151

Fig. 5. Relative distribution of publications over classes of cfz[ln] for a department at KI, KI as a whole, and all Swedish publications within 57
research areas (1998–2003).

Fig. 6. Distribution of research units over classes of citation z-scores. The figure is based on data for 334 research unites at KI (at least 30 publications
1995–2004). Bars including groups with citation z-score <−0.2 are coloured grey, citation z-score −0.2 and +0.2 white, and citation z-score >+0.2
black. Data from the citation indices produced by Thomson Scientific, self citations are included and whole counting is performed.

5.3. Development on country level
How cf and c̄fz[ln] can be used on macro level is shown in figure Figs. 7 and 8. In Fig. 7 indicator values are shown
for life science publications published between 1998 and 2003 by researchers affiliated with organisations in USA,
France, Germany, Russia and China. As can be seen in the figure, the average citation rate is highest for USA, but both
Germany and France are catching up. A faster increase can be seen for China and Russia, but from a much lower level.
When instead looking at the total field normalized citation score (cf ) size becomes a factor (Fig. 8). The indicator
value for the United States is six to nine times larger than for Germany and France. The development in China is
apparent in the figure, going from about 1% of USA level in 1998 to 4% in 2003.
6. Discussion: an improvement but still many caveats
In this study three new indicators are suggested. The indicators build on earlier research which has shown how citation
rates could be normalized for research field, publication year and document type (Moed et al., 1995; Schubert et al.,
1983; Vinkler, 1986). The item oriented ﬁeld normalized citation score average (c̄f ) is an incremental improvement of
the current state of the art indicator (the crown indicator). It differs from the crown indicator in so much as it assigns
equal weight to each included publication. That normalization takes place on the level of individual item also makes

152

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

Fig. 7. Item oriented field normalized logarithm-based citation z-score average 1998–2003 for 1,416,912 publications in 57 life science related
subject categories, co-authored by researchers affiliated with organisations in USA, Germany, France, China and Russia. Data from the citation
indices produced by Thomson Scientific, self citations are included and whole counting is performed.

it possible to calculate total field normalized citation score (cf ) (example in Fig. 8). The more radical improvement
(or complement) is the item oriented ﬁeld normalized logarithm-based citation z-score average (c̄fz[ln] or citation
z-score). This indicator assigns equal weight to each included publication and takes the citation rate variability of
different fields as well as the skewed distribution of citations over publications into account. In comparison with the
crown indicator, which distribution starts to approach normal on aggregated levels (such as distribution of research
groups over classes of crown indicator values), the distribution of field normalized logarithm-based citation z-scores
(cfz[ln] ) over publications starts to approach normal distribution already within low aggregation levels such as research
groups (Fig. 4). cfz[ln] values are approximately normally distributed within department or university level (Fig. 5). On
aggregated levels the citation z-score is approximately normally distributed on the level of research groups (Fig. 6).
Since the citation z-score is based on logarithmic transformations of citation rates, extreme values have less impact
on this indicator. This is both the strength and the weakness of the indicator since one is often interested in just those
‘extreme cases” when assessing research (Tijssen, Visser, & van Leeuwen, 2002). The citation z-score thus provides a
complementary view to other indicators which are more heavily influenced by extreme citation rates (e.g. c̄f ) or solely

Fig. 8. The total field normalized citation score 1998–2003 for 1,416,912 publications in 57 life science related subject categories, co-authored
by researchers affiliated with organisations in USA, Germany, France, China and Russia. Data from the citation indices produced by Thomson
Scientific, self citations are included and whole counting is performed.

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

153

concerns very highly cited publications (e.g. share of publications in top 1% of the citation distribution). It therefore
seems appropriate to provide information on both types of indicators as input to informed peer review.
When calculating bibliometric indicators there are some important issues to consider. First one could discuss whether
to control for research field or consider research fields with high average citation rates as being of higher quality. One
might argue that research performed within fields with high average citation rates, such as cell biology or biochemistry,
on average is of higher quality than research within the fields of nursing or clinical neurology. One should then remember
that the clinical neurologists easily could increase their average citation rate (and thus their ‘quality’ as measured by
raw average citation rate) by starting to write longer reference lists. This would of course not increase the quality of
research in clinical neurology.
Secondly, one should consider the assignment of articles to research fields based on the subject categories of journals
(Glänzel & Schubert, 2003). Currently the only classification scheme that is readily available for evaluative purposes
across scientific disciplines is the ISI subject categories of journals produced by Thomson Scientific. An issue with using
this method is that it has been shown that publications within one field are often published in journals that are categorised
as belonging to another field (e.g. Lewison, 1996; Lundberg, Fransson, Brommels, Skar, & Lundkvist, 2006; Ugolini,
Casilli, & Mela, 2002). Since the categorization is made on journal level, a related issue concerns articles published in
multidisciplinary journals such as Science. Articles in these journals could for example be normalized using the average
citation rate of all other articles published in journals within the ‘multidisciplinary-field’, or they could individually
be assigned to its respective research field. The latter is time-consuming but the number of multidisciplinary journals
is limited, so using a collaborative effort the task of manually categorizing these articles should not be overwhelming.
Other classification schemes are being developed, and hopefully it should not take long before there are standardized
classification schemes on the level of individual publications available across disciplines similar to ones that are
available today within single fields, e.g. Medical Subject Headings (MeSH) for medicine.
A third issue is whether to control for document type. That review and ‘normal’ journal articles differ in average
citation rates as well as in type of research is uncontroversial. A more contentious issue is whether to control for the
document type letter, as these publications could be seen simply as short normal journal articles. Their average citation
rate is, however, much lower than normal articles’. In the suggested indicators, letters and normal articles are separated
in the normalization procedure and the choice of what document types to include is made when calculating the final
indicator of a researcher, group or department. One could simply choose to omit letters from the calculation of the final
indicator values or display indicators for each publication type separately.
Fourth, two of the indicators described here deal with the quality aspect of research performance. In any research
assessment quantity also
 needs to be considered. Calculation of composite indicators has been suggested by, for example,
Lindsey (1978). The cf gives an indication of the total international impact that an assessed group, organisation or
country has had. It should be noted that extreme ‘quality’ indicator values (both low and high) are to be expected for
units with a low number of publications. This is of importance when using the suggested indicators for comparing
units of different size (for example small and large research groups within a university). A related issue that needs to
be dealt with is the decision whether to conduct whole or fractional counting of citation rates. Should each contributor
receive full credit for a paper, or should the credit be distributed according to some formula?
Fifth, a limitation when calculating the suggested normalized citation rates is that the average citation rate (and
standard deviation) of a field cannot be zero. It could even be argued that the lower limit for the average citation rate
should be at least one citation in order for the calculations of indicators to be valid. A second lower limit can also be
(arbitrarily) set for the minimum number of publications in the normalization group—for example that there should
be at least 100 publication of the same type, from same year, published in journals within the same subject category in
order for a publication to be included in the normalization procedure.
The bibliometric community owes the rest of the scientific community not only to deal with the issues mentioned
here, but also to follow Professor Wolfgang Glänzel’s 10 year old recommendations; to work on definitions, reproducibility, validation and compatibility (Glänzel, 1996) and to continue work on new indicators that can contribute to
the development of science world wide.
7. Conclusion
An item oriented ﬁeld normalized logarithm-based citation z-score average (c̄fz[ln] ) is an improvement of available
bibliometric indicators as it assigns equal weight to each included publication, takes the citation rate variability of

154

J. Lundberg / Journal of Informetrics 1 (2007) 145–154

different fields into account and also considers the skewed distribution of citations over publications. Still, it should
not be used as a sole indicator of research performance but rather as one of many indicators used as input for informed
peer review.
Acknowledgements
The author would like to acknowledge the valuable suggestions and feedback provided by Catharina Rehn and Ulf
Kronman at Karolinska Institutet University Library, and by two anonymous reviewers.
References
Glänzel, W. (1996). The need for standards in bibliometric research and technology. Scientometrics, 35(2), 167–176.
Glänzel, W., & Schubert, A. (2003). A new classification scheme of science fields and subfields designed for scientometric evaluation purposes.
Scientometrics, 56(3), 357–367.
Hirsch, J. E. (2005). An index to quantify an individual’s scientific research output. Proceedings of the National Academy of Sciences of the United
States of America, 102(46), 16569–16572.
Kirkwood, B. R., & Sterne, J. A. C. (2003). Essential medical statistics (2nd ed.). Malden, Mass: Blackwell Science.
Lewison, G. (1996). The definition of biomedical research subfields with title keywords and application to the analysis of research outputs. Research
Evaluation, 6(25), 25–36.
Lindsey, D. (1978). The corrected quality ratio: A composite index of scientific contribution to knowledge. Social Studies of Science, 8(3), 349–354.
Lundberg, J., Fransson, A., Brommels, M., Skar, J., & Lundkvist, I. (2006). Is it better or just the same? Article identification strategies impact
bibliometric assessments. Scientometrics, 66(1), 183–197.
Moed, H. F., Debruin, R. E., & Vanleeuwen, T. N. (1995). New bibliometric tools for the assessment of National Research Performance—Database
description, overview of indicators and first applications. Scientometrics, 33(3), 381–422.
Schubert, A., Glänzel, W., & Braun, T. (1983). Relative citation rate: A new indicator for measuring the impact of publications. In Paper presented
at the ﬁrst national conference with international participation on scientometrics and linguistics of scientiﬁc text.
Seglen, P. O. (1992). The skewness of science. Journal of the American Society for Information Science, 43(9), 628–638.
Tijssen, R. J. W., Visser, M. S., & van Leeuwen, T. N. (2002). Benchmarking international scientific excellence: Are highly cited research papers
an appropriate frame of reference? Scientometrics, 54(3), 381–397.
Ugolini, D., Casilli, C., & Mela, G. S. (2002). Assessing oncological productivity: is one method sufficient? European Journal of Cancer, 38(8),
1121–1125.
Vinkler, P. (1986). Evaluation of some methods for the relative assessment of scientific publications. Scientometrics, 10(3–4), 157–177.

