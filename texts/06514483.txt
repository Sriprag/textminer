IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

2675

Multimodal Sparse Representation-Based
Classification for Lung Needle Biopsy Images
Yinghuan Shi, Yang Gao∗ , Member, IEEE, Yubin Yang, Ying Zhang, and Dong Wang

Abstract—Lung needle biopsy image classification is a critical
task for computer-aided lung cancer diagnosis. In this study, a
novel method, multimodal sparse representation-based classification (mSRC), is proposed for classifying lung needle biopsy images.
In the data acquisition procedure of our method, the cell nuclei
are automatically segmented from the images captured by needle
biopsy specimens. Then, features of three modalities (shape, color,
and texture) are extracted from the segmented cell nuclei. After
this procedure, mSRC goes through a training phase and a testing
phase. In the training phase, three discriminative subdictionaries corresponding to the shape, color, and texture information are
jointly learned by a genetic algorithm guided multimodal dictionary learning approach. The dictionary learning aims to select
the topmost discriminative samples and encourage large disagreement among different subdictionaries. In the testing phase, when a
new image comes, a hierarchical fusion strategy is applied, which
first predicts the labels of the cell nuclei by fusing three modalities, then predicts the label of the image by majority voting. Our
method is evaluated on a real image set of 4372 cell nuclei regions
segmented from 271 images. These cell nuclei regions can be divided into five classes: four cancerous classes (corresponding to
four types of lung cancer) plus one normal class (no cancer). The
results demonstrate that the multimodal information is important
for lung needle biopsy image classification. Moreover, compared to
several state-of-the-art methods (LapRLS, MCMI-AB, mcSVM,
ESRC, KSRC), the proposed mSRC can achieve significant improvement (mean accuracy of 88.1%, precision of 85.2%, recall of
92.8%, etc.), especially for classifying different cancerous types.
Index Terms—Dictionary learning, genetic algorithm, lung
cancer image classification, sparse representation-based classification (SRC).

I. INTRODUCTION

A

CCORDING to the report from the World Health Organization [1], lung cancer is the most common cause of

Manuscript received October 23, 2012; revised February 5, 2013, March 13,
2013 and April 20, 2013; accepted April 26, 2013. Date of publication May 7,
2013; date of current version September 14, 2013. The work was supported by
the National 973 Program of China under Grant 2009CB320702, the National
Science Foundation of China under Grants 61035003, 61175042, 61021062,
the 973 Program of Jiangsu, China, under Grant BK2011005, Program for
New Century Excellent Talents in University under Grant NCET-10-0476, and
Jiangsu Clinical Medicine Special Program under Grant BL2013033. Asterisk
indicates corresponding author.
Y. Shi and Y. Yang are with the State Key Laboratory for Novel Software
Technology, Nanjing University, Nanjing 210046, China (e-mail: yinghuan.
shi@gmail.com; yangyubin@nju.edu.cn).
∗ Y. Gao is with the State Key Laboratory for Novel Software Technology,
Nanjing University, Nanjing 210046, China (e-mail: gaoy@nju.edu.cn).
Y. Zhang and D. Wang are with the Bayi Hospital, Nanjing 210046, China
(e-mail: ddddddzy@yahoo.com.cn; wangdong_nj001@126.com).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2262099

NC

SC

AC

SCC

NA

Fig. 1. Typical examples of the normal (NC) images and four types of cancerous ones: SC, AC, SCC, and NA. The images are captured from needle biopsies
specimens by electronic microscopy and digital camera.

cancer-related death worldwide. Conventionally, lung cancer
can be categorized into four types: squamous carcinoma (SC),
adenocarcinoma (AC), small cell cancer (SCC), and nuclear
atypia (NA). Fig. 1 shows several sample images of each of the
four cancerous types, as well as some samples of the normal
type (NC). The key to lung cancer treatment relies on its early
stage diagnosis. Traditionally, the diagnosis of lung cancer is
made by the pathologist. However, the pathologist’s inexperience in clinical practice can cause misdiagnosis. What is worse,
the diagnosing process is time consuming and sometimes tedious when a pathologist is asked to analyze a huge volume of
sample images from patients. Thus, the misdiagnosis can also
be caused by the factors such as fatigue, even for an experienced pathologist. Therefore, the accuracy of the diagnosis is
related to not only the pathologist’s educational background and
clinical experience but also his/her physical and psychological
conditions. In recent years, several computer-aided methods for
lung cancer diagnosis have been developed, such as magnetic
resonance imaging [2], computerized tomography [3]–[5], and
X-ray chest films [6], etc.
In clinical practice, analyzing the images captured from needle biopsy specimens, which routinely follows the X-ray chest
films checking, is one of the most popular and reliable ways to
aid lung cancer diagnosis. However, classifying different types
(SC, AC, SCC, NA) of lung needle biopsy images is still a
significant and challenging problem because the images of different cancerous types sometimes might be very similar to each
other [8] and, hence, are difficult to classify.
To tame the aforementioned challenges, we propose a novel
method, named multimodal sparse representation-based classification (mSRC), for lung needle biopsy image classification.

0018-9294 © 2013 IEEE

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

testing phase

training phase
shape sub-dictionary

data acquisition

training images

cells

discriminative sub-dictionaries

color sub-dictionary

data acquisition

2676

SRC

genetic algorithm guided
multi-modal dictionary
learning

cell-level fusion
testing image

texture sub-dictionary

image-level fusion

Fig. 2. Framework of the proposed mSRC, including a training phase and a testing phase. The training phase outputs the discriminative subdictionaries via
genetic algorithm-based multimodal dictionary learning, and the testing phase outputs the predicted label for each testing image.

It aims to improve the classification performance, especially
in the case that the images of different lung cancer types are
difficult to distinguish. Our method exploits the observation
that the lung cell nuclei have different appearances in different classes. Particularly, we investigate the information of three
cell modalities (shape, color, and texture). For example, the SC
cell nuclei are usually round (shape modality), and the NA cell
nuclei always largely deviate from others in color (color modality). Upon the multimodal information, our method builds a
sparse representation-based classifier for lung needle biopsy
image classification with improved performance.
In recent years, the sparse representation-based classification (SRC) [9] has attracted many research interests due to the
promising results in image and signal processing tasks [9], [12],
[13]. Moreover, in machine learning theory, several theoretical
results, such as sufficient and redundant theorem [14] and large
diversity [10], [11], have demonstrated that the generalization
ability of SRC can be improved when SRC is learned from multimodal data. Our proposed method mSRC puts these theories
into practice and our experimental results validate the soundness
of these theoretical results.
A. Related Work
For lung needle biopsy image analysis, many efforts have
been contributed in recent years. Zhou et al. [15] developed a
lung cancer cell classification system based on a two-level neural network ensemble. In the system, the first-level ensemble
determines whether a testing cell is normal or cancerous, and
the second-level ensemble classifies the types of lung cancer for
the suspected cancerous cells determined in the first level. Zhu
et al. [16] proposed an image-level approach: multiclass multiinstance AdaBoost (MCMI-AdaBoost), which predicts the label of
an image by incorporating the multi-instance distance measurement (Hausdorff distance) under the AdaBoost [17] framework.
However, the similarity measurement ignores local information
of cells. Shi et al. [8] recently introduced a transductive costsensitive learning method for lung needle biopsy image classification, of which the goal is to achieve the best possible results

with only a small number of labeled images. Unfortunately,
the results of classification for different types of lung cancer
is still unreliable, and there is still space for us to improve the
classification performance.
In addition, previous methods for lung needle biopsy image
classification [15], [8], [16] are single-modal based learning
methods, which fail to make full use of the disagreement information among different modalities [18], [19]. It is noteworthy
that, except for the needle biopsy specimens-based lung cancer
diagnosis, many works that focus on cell/nodule/image classification in medical image analysis are highly related to our work.
Works [20]–[22] belong to this category.
For learning with multimodal data, also referred to multiview
learning and ensemble learning in machine learning community,
many algorithms [10], [11], [14], [23], [24] have been developed
recently. Also, multimodal-based methods are promising in the
field of medical image analysis since multimodal information is
naturally available in the data acquisition procedures of various
clinical tasks, such as Alzheimer’s disease diagnosis [13], [25],
prostate cancer prediction [26], and survival prediction for lung
cancer [23].
The rest of the paper is organized as follows. We first present
the framework of our method and data acquisition procedure.
Then, we introduce the problem formulation, training and testing
phase, respectively. Finally, we present our experimental results
and conclude the paper.
II. FRAMEWORK OF OUR METHOD
Generally, our proposed method contains three phases (see
Fig. 2). The first phase is the data acquisition procedure, which
aims to extract the features for cell nuclei in lung needle biopsy
images. After this phase, our method goes through the rest two
ones: training and testing phases.
In the training phase, we introduce the concept of dictionary [9] in pattern recognition/computer vision, where a dictionary means a collection of words/elements/feature vectors.
Traditional SRC belongs to single-modal learning approach,
which only uses one dictionary. Since the information of three

SHI et al.: MULTIMODAL SPARSE REPRESENTATION-BASED CLASSIFICATION FOR LUNG NEEDLE BIOPSY IMAGES

Feature extraction

Image capturing
Electronic
microscopy

Digital camera

2677

Image preprocessing
original images convert to gray scale

smooth

shape feature
segmentation, labeling

color feature
texture feature

Observing
platform

Fig. 3.

Needle biopsy
specimens

Flowchart of the data acquisition procedure in our method, including three sequential steps: image capture, image preprocessing, and feature extraction.

modalities is available, our method employs three corresponding
individual dictionaries (named subdictionaries in this study).
With the features extracted from the three modalities (shape,
color, and texture) of each cell nucleus in the data acquisition
procedure, we build three original subdictionaries on shape,
color, and texture by collecting the corresponding feature vectors of individual cell nuclei. Note that all the subdictionaries mentioned in the following sections only contain the features coming from different individual cell nuclei instead of
images. Moreover for each nucleus, its label is initialized as
the same label of the image it belongs to. The subdictionaries
obtained in this way usually contain several similar samples
coming from different classes, which might be harmful for classification. To learn discriminative subdictionaries, we propose
a genetic algorithm-based multimodal dictionary learning algorithm, which selects the topmost discriminative training cell
nuclei, and encourages large disagreement among different subdictionaries.
In the testing phase, for a new coming image, a hierarchical
fusion strategy is adopted, including cell-level fusion and imagelevel fusion, respectively. In the following sections, we will
discuss the technical details of each phase in our method.

TABLE I
EXTRACTED FEATURES FOR EACH CELL NUCLEUS REGION FROM THREE
MODALITIES: SHAPE, COLOR, AND TEXTURE

shape (9)
height
width
circumference
area
circularity
elongation
Fourier descriptor (3)

color (11)
R, G, B
H, S, I
gray mean
gray variance
feature gray
IOD
central moment

texture (16)
energy (4)
entropy (4)
contrast (4)
divergence (4)

shape-based, 11 color-based, and 16 texture-based features are
extracted. For the Fourier descriptor, the number “3” in the
bracket means that we only use the second, third, and fourth
coefficients. The first coefficient is the mean value of boundary coordinates, which is usually considered as useless in feature representation. In the texture-based features, the number
“4” in a bracket means that we calculate the four directions
(0◦ , 45◦ , 90◦ , 135◦ ) for the four features (energy, entropy, contrast, and divergence), so totally 16 texture-based features are
extracted. These features have demonstrated their robustness
and effectiveness in several medical image analysis applications [8], [15], [16], [27]–[29].

III. DATA ACQUISITION PROCEDURE
With the goal of capturing and preprocessing the lung needle biopsy images, the data acquisition procedure, whose main
pipeline can be referred to Fig. 3, takes the following sequential
steps:
1) An image capturing step aims to capture the images from
the needle biopsy specimens by using an electronic microscopy
and a digital camera.
2) An image preprocessing step aims to obtain the individual cell nuclei from the captured images by segmenting the cell
nuclei from the background (i.e., cell sap). The image preprocessing step is with the following substeps: smoothing the images with Gaussian kernel, segmenting the images using Otsu’s
algorithm, and labeling the connected cell nuclei regions. The
reason of adopting Otsu’s algorithm is that the contrast between
the cell nuclei region and background is large enough to be
easily separated (see sample images in Fig. 1). Basically, the
segmentation results can meet clinical requirements according
to the pathologist’s suggestions.
3) A feature extraction step aims to extract features for individual cell nuclei from three modalities (shape, color, and
texture), respectively. Specifically, as shown in Table I, nine

IV. PROBLEM FORMULATION
In this section, we first review the conventional SRC for single
modal data classification, which is originally introduced in [9].
Then, we formulate our target problem mathematically. In this
paper, the matrices are denoted by the bold upper case letters,
the vectors are denoted by the bold lower case letters, and the
scalars are denoted by the unbold letters, respectively.
A. SRC
The goal of SRC is to represent a new coming sample using sparse linear combination of given dictionary entries. Assuming a dictionary with C different classes: D ∈ Rd×n =
[D(1), D(2), . . . , D(C)], and a testing sample x ∈ Rd are available, where d is the dimension of feature vector and n is the
number of training samples. The sparse representation problem
can be mathematically formulated as follows:
α̂ = {arg min α1

s.t.x − Dα2 ≤ 

(1)

α

where  ∈ R is the parameter to control the tolerance of the
reconstruction error; α ∈ Rn is the sparse linear combination

2678

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

Algorithm 1 SRC
Input: D ∈ Rd×n = [D(1), D(2), ..., D(C)], x ∈ Rd , .
Output: predicted label p(x).
1: Normalize the columns of D to have unit L2 -norm.
2: α̂ ← solution returned by Eq.(1).
3: rj (x) ← solution returned by Eq.(2).
4: p(x) ← arg minj rj (x).

coefficient we want to learn. Equation (1) can be effectively
solved by many algorithms, e.g., Basis Pursuit and Orthogonal
Matching Pursuit [30].
To classify the testing sample x into one of the C classes, we
first compute the reconstruction error rj (x) for each class (j =
1, . . . , C) using the obtained sparse representation coefficient
α̂ as
rj (x) = x − Dδj (α̂)2

(2)

where δj (α̂) ∈ Rn is the class-specific vector, whose only
nonzero entries are the ones in α̂ that are associated with class
j. Finally, to predict the label p(x) ∈ R of the sample x, the
class corresponding to the smallest reconstruction error rj (x)
is selected as the predicted class. Algorithm 1 summarizes the
SRC algorithm. More details about SRC are discussed in the
literature [9].
B. mSRC
We now consider extending SRC to multimodal classification
scenarios. Particularly, there are three available modalities in our
problem. It is noteworthy that although only three modalities are
used in our work, mSRC can be easily extended for the problems
with more than three modalities.
Assuming n training cell nuclei are available, their labels
are the same as the corresponding images’. The kth cell nuT
n
cleus can be denoted as a tuple (xSk , xC
k , xk , yk )k =1 , where
S
dS
C
dC
T
dT
xk ∈ R , xk ∈ R , and xk ∈ R are the corresponding feature vectors of the kth cell nucleus from the different modalities:
shape, color, and texture, respectively. dS , dC , and dT are the
feature dimensionalities of shape, color, and texture, respectively. yk ∈ R is the label of the kth cell. Also, we denote
DS ∈ Rd S ×n , DC ∈ Rd C ×n , and DT ∈ Rd T ×n as the original
subdictionaries corresponding to shape, color, and texture, respectively. DS , DC , and DT are the column-wise collection of
the corresponding modality feature vectors for all the n training
cells, which can be represented as DS = [xS1 , . . . , xSn ], DC =
C
T
T
T
[xC
1 , . . . , xn ], and D = [x1 , . . . , xn ], respectively.
The original subdictionaries may contain several similar
samples coming from different classes; therefore, dictionary
learning is necessary to select the most discriminative samples from the original subdictionaries. As we know, the dictionary plays an essential role when applying SRC for classification. In recent years, there have been a lot of research efforts [31]–[33], [37] contributed to the single-modal dictionary
learning. Since the multimodal data are available, we denote
ES ∈ Rd S ×n S , EC ∈ Rd C ×n C , and ET ∈ Rd T ×n T as the corre-

sponding discriminative subdictionaries on the shape, color, and
texture, respectively, after dictionary learning, where nS , nC ,
and nT (nS , nC , nT < n) are the numbers of the training samples in ES , EC , and ET , respectively. It is noteworthy that
nS , nC , and nT might be different from n. Therefore, the dictionary learning can be considered as applying a mapping function
on the original subdictionaries (DS , DC , DT ) in order to obtain
suitable discriminative subdictionaries (ES , EC , ET ).
After dictionary learning, the task of mSRC is to predict
the unknown labels of testing cell nuclei by using ES , EC ,
and ET . Formally, we denote a new coming cell nucleus as
(zS , zC , zT , y), where zS ∈ Rd S , zC ∈ Rd C , and zT ∈ Rd T are
the corresponding feature vectors in shape, color, and texture, respectively, y is the undetermined label. We can obtain
p(zS ), p(zC ), and p(zT ) as the corresponding predicted labels
on different modalities by applying single-modal SRC (see Algorithm 1). Then, we consider using the majority voting strategy
to get the final prediction on p(zS ), p(zC ), and p(zT ). So, the
core problem of mSRC turns to be how to learn the suitable
discriminative subdictionaries ES , EC , and ET .
V. GENETIC ALGORITHM-BASED MULTIMODAL
DICTIONARY LEARNING: TRAINING PHASE
The most trivial and popular way for multimodal dictionary
learning is to learn each subdictionary independently by selecting the topmost discriminative samples in the corresponding
original subdictionary. However, the method fails to consider the
relationships among different modalities, which can reduce the
generalization error when learning with multimodal data [10],
[11], [14]. In this study, we develop a genetic algorithm-based
multimodal dictionary learning method. Inspired by the accurate and diversity constraint introduced in [11], the goal of our
method is to select several training samples from DS , DC , and
DT to form ES , EC , and ET , respectively. The selection criteria are that 1) each subdictionary after dictionary learning can
train a good classifier independently and 2) the diversity among
different subdictionaries after dictionary learning is encouraged
to be large.
Mathematically, we introduce the binary sample selector
β ∈ B3n , which can be represented as β = [β S β C β T ]. where
β S ∈ Bn , β C ∈ Bn , and β T ∈ Bn are the binary sample selection operator for the different modalities: shape, color, and
texture, respectively. For example, the entries in β S that are
equal to 1 indicate that the corresponding columns in the original subdictionary DS will be selected into ES , while the entries
equal to 0 indicate that the corresponding columns will not be
selected. The explanations for β C and β T are similar. Therefore, for original shape subdictionary DS , we can use β S (DS )
to indicate applying the binary sample selection operator β S
on DS to form the learned shape subdictionary (similar for
β C (DC ), β T (DT )), which can be referred to Fig. 4. Since we
have n labeled training cell nuclei in total, for the kth one, we
denote f (xSk , β S ) as the mapping function, which is the label
of xSk predicted by subclassifier trained on the learned subdictionary β S (DS ). The subclassifier is the single-modal SRC (see
Algorithm 1).

SHI et al.: MULTIMODAL SPARSE REPRESENTATION-BASED CLASSIFICATION FOR LUNG NEEDLE BIOPSY IMAGES

δj (∗) discussed in previous sections whose nonzero entries are
the ones that are associated with class j, each chromosome β i,g
is required to calculate its hm sv (β i,g ) as follows:

sample selection operator

5


original shape sub-dictionary original color sub-dictionary original texture sub-dictionary

hm sv (β i,g ) =



c=1 m ∈{S,C,T}

1
0

discriminative shape
sub-dictionary

discriminative color
sub-dictionary

discriminative texture sub-dictionary

Fig. 4. Illustration of applying the binary sample selection operator β for our
method. The white box (whose value is 1) means that the corresponding sample
will be selected into the learned subdictionaries, while the black box (whose
value is 0) means otherwise.

−

n

k =1



k =1 m ∈{S,C,T}

According to the objective function (3), for each β i,j , the
fitness score function can be defined as follows:

λ 3−



1
+
n


m
m̂
m̂
f (xm
k , β ), f (xk , β )	

n



λ 3−

k =1

(3)

m , m̂ ∈ {S, C, T}
m 
= m̂

where a, b	 is the Kronecker delta function (a, b	 = 1 only if
a = b, otherwise a, b	 = 0). λ ∈ R is the parameter to control
influence caused by the second term. The first term is used
to measure if each subclassifier trained on individual modality
can achieve lower training error, and the second term is used
to encourage the classification results on the training samples
predicted by every two subclassifiers to be different.
Equation (3) is a combinatorial optimization problem, which
is difficult to find a closed-form solution since it is neither
smooth nor continuous. To this end, we consider using genetic algorithm to solve (3). The genetic algorithm is proposed
for optimizing global minimization/maximization problems by
crossover and mutation among a collection of chromosomes
iteratively. Before introducing the technical details applied in
our proposed algorithm, we first introduce the minima support
validation for chromosomes, which is used to validate each
chromosome before applying any genetic algorithm step, such
as crossover and mutation. For clear representation, assuming
that the population size is K, we denote β 1,g , β 2,g , . . . , β K ,g
as the K chromosomes at the generation g in genetic algorithm.
A. Minima Support Validation
If the training samples of a certain class are rarely selected
into the subdictionary, the subclassifier will not be well trained
to classify this class. To avoid this case, for the chromosomes
in each iteration including the initialization, each chromosome
is first required to check the minima support before applying
any step. Minima support validation means that for each class,
at least ρ ∈ R (0 < ρ < 1) proportion training samples should
be selected into each subdictionary. Mathematically, recall the

(4)

B. Fitness Score Function

1
fﬁtness (β i,g ) =
n

Our objective function can be formulated as follows:

n


m
min exp −
yk , f (xm
k , β )	


	

δj (β m
i,g ) 1
sgn
−ρ
Nj

where sgn(∗) is the sign function (sgn(a) = 1 if a > 0 and 0 otherwise), Nj ∈ R is the number of training samples in the original
subdictionary that belongs to class j. For a chromosome β i,g ,
it can be employed for optimization only if the corresponding
hm sv (β i,g ) = 1.

n

β

2679



m
yk , f (xm
k , β i,g )	

k =1 m ∈{S,C,T}




m
m̂
m̂
f (xm
k , β i,g ), f (xk , β i,g )	

.

m , m̂ ∈ {S, C, T}
m 
= m̂

The chromosome β i,j with higher fitness score is the solution
close to the optimal solution. When reaching the termination
condition, the chromosome with the highest fitness score will
be selected as the solution to (3).
C. Optimization
In the initialization step of each population, we randomly
generate K chromosomes one by one until the current generated
chromosome passes the minima support validation.
In this study, we develop two different crossover methods for
optimization: 1) chromosome-level crossover, which is equivalent to the crossover in typical genetic algorithm, and 2)
modality-level crossover, which is a problem-specific crossover
that is adapted for multimodal data. In [34], the author claimed
that problem-specific crossover can improve the results for many
practical problems.
The chromosome-level crossover is also known as one-point
crossover, which is one of the most basic crossover methods in genetic algorithm. The chromosome-level crossover first
chooses two parent chromosomes, which have relatively higher
fitness score in the current generation. Then, it randomly selects a split point for each chromosome. Finally, it swaps
the data segmented by the split point between the two parent
chromosomes. Similar to the chromosome-level crossover, the
modality-level crossover also picks two parent chromosomes
with relatively higher fitness scores. Then, it randomly selects
several split points for each modality (one split point corresponds to one modality). Finally, it swaps the data segmented by
modality-specific split points separately. The difference between
the chromosome-level crossover and modality-level crossover
can be referred to Fig. 5. In our study, we randomly choose
the chromosome-level crossover or modality-level crossover

2680

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

shape

color

texture
split point

crossover

chromosome-level crossover

1
0

shape
split point

color

texture

split point

split point

crossover

modality-level crossover
Fig. 5. Difference between the chromosome-level crossover and modalitylevel crossover. The white box (whose value is 1) means the corresponding
sample will be selected into the learned subdictionaries, while the black box
(whose value is 0) means the corresponding sample will not be selected.

Algorithm 2 Genetic Algorithm based Multi-Modal Dictionary Learning
Input: original sub-dictionaries DS , DC , DT , λ, pm , ρ, K,
G.
Output: learned sub-dictionaries ES , EC , ET .
1: β 1,1 , ..., β K,1 ← Initialize K chromosomes.
2: for g ← 2, ..., G do
3:
Calculate the fitness scores for all the K chromosomes.
4:
Select K/2 chromosomes with the top K/2 highest
fitness scores, and remove the remaining K/2 ones.
5:
while population size < K do
6:
Sample β i1 ,g−1 and β i2 ,g−1 , apply chromosomelevel crossover or modality-level crossover.
7:
Sample β i3 ,g−1 , apply mutation.
8:
Add new chromosomes into current population.
9:
end while
10: end for
11: β opt ← arg max ffitness (β 1,G ), ..., ffitness (β K,G ) .
12:

ES = β Sopt (DS ), EC = β Copt (DC ), ET = β Topt (DT ).

classified as three different labels, it will not be considered in
the later fusion step. Finally, majority voting is applied again on
the cell-level labels to obtain the image-level label. The pipeline
of hierarchical fusion can be referred to Fig. 6.
Algorithm 3 summarizes mSRC. The input is a testing image
T nI
I with nI different cells denoted as (zSq , zC
q , zq )q =1 . The output
is the predicted label for the testing image I.
VII. EXPERIMENTAL RESULTS

for each crossover step, so both the probabilities of applying chromosome-level crossover and applying modality-level
crossover are set as 0.5.
Mutation aims to avoid the case that the population becomes
too similar during optimization. This case usually causes the
local minima problem. Like crossover, mutation is also known
as the basic step when applying genetic algorithm for optimization. We adopt the Bit string mutation: for every chromosome,
each element will be flipped with the probability pm ∈ R (0 <
pm < 1).
When the fitness scores of different chromosomes in the current population become similar, the genetic algorithm is considered as reaching the termination condition. In the later experiments, we will discuss the termination condition used in our
problem. We now summarize the genetic algorithm-based multimodal dictionary learning algorithm (see Algorithm 2). G ∈ R
denotes the number of total generations.
VI. IMAGE CLASSIFICATION BY HIERARCHICAL FUSION:
TESTING PHASE
When a new testing image comes, a hierarchical fusion strategy is used to predict its label. For each cell nucleus, we first
adopt single-modal SRC to predict its labels of different modalities (shape, color, and texture) based on the learned subdictionaries (ES , EC , ET ), respectively. Then, we apply majority
voting on the labels of three modalities for each cell nucleus to
get the cell-level label. It is noteworthy that if a cell nucleus is

A. Experiment Setup
1) Image Set Details: Our method is evaluated on a real lung
needle biopsy image set, provided by Bayi Hospital, Nanjing,
China. The image set includes 271 needle biopsy images of five
different classes: 52 normal (NC) images, 52 SC images, 95
AC images, 40 SCC images, and 28 NA images. Each image
is labeled by experienced pathologists. After the cell nuclei
segmentation, total 4372 cell nuclei (including the overlapping
cell nucleus regions) are extracted from all the images. The
number of segmented cell nuclei in each image varies from 3
to 75 (see Fig. 7). In the experiments, tenfold cross validation
is adopted: the whole image set is divided into ten subsets with
similar sizes, and each subset has relatively similar proportions
of five different classes.
2) Evaluation Metrics: The accuracy, precision, recall, F1
score (a.k.a. F-measure), and true negative rate (TNR) are employed for evaluating the multiclassification results. The accuracy is calculated as the number of correctly classified images
divided by the number of total images. For precision, recall, F1
score, and TNR, we compute not only the class-specific values
of each single class (NA, SC, SCC, AC, and NC), but also the
mean values by averaging all the corresponding class-specific
values.
3) Parameter Setting: In mSRC, the parameter  is chosen by
grid searching from [10−5 , 10−4 , . . . , 104 , 105 ], and  = 10−1 is
found to be the best when applying the leave-one-out testing on
the training images.

SHI et al.: MULTIMODAL SPARSE REPRESENTATION-BASED CLASSIFICATION FOR LUNG NEEDLE BIOPSY IMAGES

2681

label (shape)

NA

cell-level label

data acquisition

SC

label (color)

testing image

SCC

classification

voting

voting

label (texture)

AC
NC
can not
determine

SCC

image-level label

discriminative sub-dictionaries

Image classification by hierarchical fusion. The label of each testing image is determined by voting on the cell-level label.

Algorithm 3 mSRC
Input: original sub-dictionaries DS , DC , DT , a testing image
I
.
I with nI different cells (zSq , zCq , zTq )nq=1
Output: the predicted labels for I.
1: ES , EC , ET ← DS , DC , DT by Algorithm 2.
2: cell-level label pcell ← 0.
3: for q ← 1, ..., nI do
4:
p(zSq ) ← classify zSq using ES by Algorithm 1.
5:
p(zCq ) ← classify zCq using EC by Algorithm 1.
6:
p(zTq ) ← classify zTq using ET by Algorithm 1.
7:
pcell (q) ← voting on p(zSq ), p(zCq ), p(zTq ).
8: end for
9: the predicted labels for I ← voting on pcell .

Number of images

30
20
10

10

20

30

40

50

60

70

80

Number of cell nuclei per image
Fig. 7.

0.64
0.62
0.6
max value
mean value

0.58

0

5

10

15

20

25

30

35

40

45

50

Iteration step
Fig. 8. Average fitness score monotonically increases with the iteration steps
when applying tenfold cross validation. In our experiments, we set the iteration
number to 50.

B. Evaluation on Multimodal Dictionary Learning

40

0
0

Average fitness score

Fig. 6.

Histogram of cells per image. Most images have 5–45 cells.

In genetic algorithm, the population size is set as 50. Since
we use the same mutation step as [35], the mutation rate pm
has been set as 0.01. In the fitness function, λ is automatically
determined by grid searching from [0, 0.01, 0.02, . . . , 0.99, 1] on
the training images. In minima support validation, the parameter
ρ is empirically set as 0.2. However, how to automatically choose
ρ is still an open problem, which will be studied in our future
work.
In order to determine the optimal number of iterations when
applying genetic algorithm for optimization, we run tenfold
cross validation on the image set, calculate the mean and max
values of the fitness score for every fold on each iteration, and
then average the results on each iteration. The average fitness
scores are calculated to reflect the convergence (see Fig. 8).
We found that the average fitness score monotonically increases
with the iteration step. When the iteration step is close to 40,
the mean value is close to the max value, indicating that the
genetic algorithm reaches the termination condition. So in the
experiments, we set the iteration number as 50.

To investigate the advantages of the proposed genetic
algorithm-based multimodal dictionary learning method, the
experiment is designed to answer the two following questions:
1) whether the results of single modality can be improved by
dictionary learning compared to the results before dictionary
learning; 2) whether the multimodal classification results outperform the single-modal classification results via multimodal
feature splicing.
The evaluated methods include SRC on shape, color, texture
modalities before and after dictionary learning, respectively.
For the ease of presentation, we use “shapeonly ” to denote the
method that only employs the original shape subdictionary before dictionary learning to train the SRC classifier. We use
“shapeD.L. ” to denote the method that employs the discriminative shape subdictionary obtained after dictionary learning
to train the SRC classifier (similar explanations for “coloronly ,”
“colorD.L. ,” “textureonly ,” and “textureD.L. ”). Moreover, we use
“SCT”(shape + color + texture) to denote the single-modal classification method via multimodal feature splicing. Specifically,
in SCT, the feature vector is obtained by splicing the feature vectors from three modalities into a longer vector, and the classifier
is trained to classify the spliced feature vectors by adopting SRC.
To evaluate each method, we calculate the accuracy, precision,
recall, F1 score, and TNR by averaging the results calculated on
the five classes (see Table II).
In Table II, there are two observations: 1) The results of
“shapeD.L. ” are better than that of “shapeonly ,” the results of
“colorD.L. ” are better than that of “coloronly ,” and the results
of “textureD.L. ” are better than that of “textureonly .” The observation reveals that the results of each single modality can
be improved by the dictionary learning. Also, the comparison

2682

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

TABLE II
VALIDATION ON THE GENETIC ALGORITHM-BASED MULTIMODAL
DICTIONARY LEARNING

Methods
shapeonly
shapeD.L.
coloronly
colorD.L.
textureonly
textureD.L.
SCT
mSRC

Accuracy
0.511
0.548
0.622
0.627
0.519
0.556
0.726
0.867

Precision
0.376
0.412
0.524
0.551
0.383
0.437
0.613
0.834

Recall
0.394
0.360
0.571
0.543
0.521
0.473
0.755
0.913

F1 score
0.405
0.429
0.525
0.543
0.414
0.428
0.620
0.862

TNR
0.867
0.879
0.895
0.890
0.860
0.878
0.923
0.962

demonstrates that the proposed genetic algorithm-based multimodal dictionary learning method can successfully select the
discriminative samples for each modality independently. 2) The
results of mSRC are better than that of SCT, which indicates that
the multimodal information is helpful to improve the classification performance. Meanwhile, we can infer that considering the
diversity information is helpful for learning with multimodal
data.
In addition, the precision, recall, F1 score, and TNR of SCT
and mSRC on individual class (NA, SC, SCC, AC, NC) are
reported in Fig. 9. We can notice that the results of mSRC outperform those of SCT on almost all the classes (except for the
precision of AC and NC, the recall and TNR of NA). In addition, for classifying NA, mSRC is significantly better than SCT.
To explain why this occurs, we exploit the results of different
modalities before and after dictionary learning. From Fig. 10, we
observe that the color modality information is the most useful
modality for classifying NA, because the color modality information can always achieve better results than both the shape
and texture modalities. In our future work, we will validate this
observation from the perspective of clinical practice.
C. Evaluation on Diversity Measurement
To demonstrate the effectiveness of the large diversity constraint imposed among different modalities, we calculate the
classification accuracy with various λ values, which can be referred to Fig. 11. When λ is 0, it means that no diversity constraint is imposed, and the setting is equivalent to the case that
each subdictionary selects the topmost discriminative samples
independently. In Fig. 11, we can also found that choosing a
suitable λ (λ ∈ [0.1, 0.7]) will improve the classification performance. The observation validates the theoretical assertion
that the generalization ability of SRC can be benefited from the
large diversity constraint [10] when learning with multimodal
data. However, λ should not be too large, otherwise the diversity among different modalities will be overemphasized, which
weakens the discriminative ability of each individual modality.
D. Comparison With Other Methods
To further evaluate the performance of mSRC, we also compare mSRC against other related methods. They are multiclass
SVM (mcSVM) [7], MCMI-AdaBoost [16], LapRLS [36], ensemble sparse classification (ESRC) [13], and kernel sparse

representation-based classifier (KSRC) [37]. Since genetic algorithm belongs to the approximated optimization algorithms,
the solution will be influenced by different population selection
criteria. Besides the original population selection criterion (selecting the chromosomes with the highest K/2 fitness scores
in each iteration), we also consider other population selection
criteria such as roulette wheel selection and tournament selection. The corresponding methods are named as “mSRC (rw)”
and “mSRC (tn),” respectively.
We now explain the experimental settings for the related
methods. For mcSVM, the radial basis function (RBF) kernel
is adopted for similarity measurement. One-versus-all strategy
is employed for cell-level classification, and the image-level label will finally be predicted by majority voting on the obtained
cell-level label. For MCMI-AdaBoost, the parameters and settings are the same as the ones suggested in [16]. For LapRLS,
we degenerate the mCLRLS [8] by applying equal misclassification costs. The parameters of LapRLS are the same as the
ones introduced in [8], i.e., the number of clusters in codebook
learning is set as 7, both the regularized parameters γA and γI
are set as 0.5. For ESRC [13] and KSRC [37], all the parameters
(e.g., the error tolerance weight in ESRC, and the Mercer kernel
parameters in KSRC) are automatically learned via the leaveone-out testing on the training images. Please note that mcSVM,
ESRC, KSRC, and our mSRC adopt the same hierarchical fusion strategy, while the only difference among ESRC, KSRC,
and mSRC is that they recruit different dictionary learning algorithms. The evaluation metrics include the accuracy, mean
precision, recall, F1 score, and TNR. We report the comparison
results in Table III.
In Table III, we found that mSRC successfully achieves significant performance improvement compared to all the illustrated works. Specifically, we discuss the results from following
aspects:
1) Evaluating the Hierarchical Fusion Strategy: We observe
that the results of mcSVM, ESRC, KSRC, and mSRC (using
the hierarchical fusion strategy) outperform those of MCMIAdaBoost and LapRLS (without using hierarchical fusion strategy), because both MCMI-AdaBoost and LapRLS actually belong to image-level analysis methods. In MCMI-AdaBoost, the
distance between every two images is calculated by Hausdorff
distance. In LapRLS, the k-means clustering algorithm is applied to roughly partition the training cell nuclei into several
clusters. The two methods might lose full cell-level information
when performing relatively high-level similarity measurement,
i.e., the Hausdorff distance in MCMI-AdaBoost, the k-means
clustering in LapRLS.
2) Evaluating the Performance of SRC: The same hierarchical fusion strategy is adopted in mcSVM, ESRC, KSRC, and
mSRC. The only difference between ESRC and mcSVM is the
classifiers used for the single-modal classification: ESRC uses
SRC while mcSVM uses SVM. However, ESRC obtains better
classification performance than mcSVM, which validates the
effectiveness of SRC on single-modal classification.
3) Evaluating the Dictionary Learning: According to the
results of ESRC, KSRC, and mSRC, we see that mSRC outperforms both ESRC and KSRC. This comparison also discloses

2683

1

1

0.8

0.8

0.8

0.8

0.6

0.6

0.4

SCT
mSRC

0.2
0

NA

SC

SCC

AC

0.4

SCT
mSRC

0.2
0

NC

NA

SC

SCC

AC

NC

0.6
0.4

SCT
mSRC

0.2
0

NA

SC

SCC

AC

NC

Tr e Ne a i e Ra e

1

F1 Score

1

Recall

Precision

SHI et al.: MULTIMODAL SPARSE REPRESENTATION-BASED CLASSIFICATION FOR LUNG NEEDLE BIOPSY IMAGES

0.6
0.4

SCT
mSRC

0.2
0

NA

SC

SCC

AC

NC

Fig. 9. Results of SCT and mSRC on precision, recall, F1 score, and TNR of different classes (NA, SC, SCC, AC, NC), respectively. Blue bars denote the results
of SCT, and red bars denote the results of mSRC.

Values

0.8
F1 Score
Precision
Recall

0.6
0.4
0.2
0
shape

shape.D.L.

texture

texture.D.L.

color

color.D.L.

Fig. 10. Precision, recall, and F1 score on different modalities. shapeD . L .
indicates that the classifier is built on the shape modality after dictionary learning
(similar for textureD . L . and colorD . L . ).
0.95

Accuracy

0.9
0.85
0.8
0.75
0.7

Fig. 11.

0

0.1

0.2

0.3

0.4

0.5

0.6

value of λ

0.7

0.8

0.9

1.0

Classification accuracy with various λ values.
TABLE III
COMPARISON WITH RELATED METHODS

Methods
LapRLS [36]
MCMI-AB [16]
mcSVM [7]
ESRC [13]
KSRC [37]
mSRC
mSRC (rw)
mSRC (tn)

Accuracy
0.625
0.608
0.674
0.800
0.830
0.867
0.881
0.867

Precision
0.533
0.585
0.598
0.730
0.782
0.834
0.846
0.841

Recall
0.538
0.564
0.577
0.884
0.843
0.913
0.901
0.858

F1 score
0.657
0.563
0.576
0.777
0.804
0.862
0.866
0.846

TNR
0.907
0.899
0.921
0.940
0.953
0.962
0.963
0.967

“MCMI-AB” is short for MCMI-AdaBoost, “mSRC (rw),” and “mSRC (tn)”
mean our method using Roulette wheel, and tournament selection as population
selection criterion, respectively.

that the large diversity constraint among different modalities can
enhance the performance, since both ESRC and KSRC ignore
the diversity information.
4) Different Population Selection Criteria: Adopting different population selection criteria might influence the final results. However, from the results of mSRC, mSRC (rw), and
mSRC (tn), we notice that our method performs better than other
methods. Among three different population selection methods,
mSRC (rw) performs the best.

In addition, we calculate the precision, recall, F1 score, and
TNR of each individual class (SC, AC, SCC, NA, NC) and
report the results in Fig. 12. Obviously, our method outperforms
the related works on almost all the listed measurements. Also
as suggested in [8], the precision of NC is considered as a very
important value, because a higher precision of NC indicates
a lower probability of the case that the system will classify a
cancerous image (NA, SC, AC, SCC) into a normal image. In
Fig. 12, the precision of NC is higher than 0.9 (except for the
mSRC (tn), whose precision is slightly lower than 0.9), which
is a satisfying value in clinical practice.
To validate the performance with different numbers of training images, we randomly sample a certain proportion of lung
needle biopsy images as training images, and the rest ones are
used as testing images. For the training images, the aforementioned parameters can be effectively learned by applying the leave-one-out testing. The obtained parameters will be
used for building an mSRC classifier, and finally we use the
mSRC to classify the testing images. Since the training images
are randomly sampled, we repeat the aforementioned process
200 times, and calculate the mean values of accuracy, precision,
recall, F1-score, and TNR, respectively. We vary the different
proportions (20–90%) of sampled training images in all images,
and calculate the results of different proportions (see Fig. 13).
As the proportion increases, the corresponding result monotonically gets better. Also, when the training images number is
limited (e.g., 20%), the performance is undesirable since the
SRC usually needs enough training samples to learn a good
classifier.
VIII. DISCUSSION AND FUTURE WORK
From the perspective of clinical practice, misclassifying a
cancerous image as a normal one will be much more serious
than misclassifying a normal image as a cancerous one, because
the former case means the patient might lose the best chance
to survive, while the latter case only means that the pathologist
will spend a little more time for final check. Therefore, we can
modify the hierarchical fusion strategy by introducing different
misclassification costs (cost-sensitive learning) or downsampling the normal cells (class imbalance learning), whose goal is
trying to reduce the false negative rate (misclassifying a cancerous image as a normal one).
Due to the quick spread of the lung cancer cells, as well as the
image sampling magnification ratio of the hardware, the cells
in each image mostly belong to one class. So there are seldom

2684

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

Fig. 12.

Comparison with related methods on the precision, recall, F1 score, and TNR of different classes. “MCMI-AB” is short for MCMI-AdaBoost.

ESRC, and KSRC), mSRC can achieve significant improvement
on accuracy, precision, recall, F1 score, and TNR, especially for
the difficult cancerous types classification.

1
0.9

Values

0.8

0.6
0.5
0.4
20%

ACKNOWLEDGMENT

Accuray
Precision
Recall
F1 score
TNR

0.7

30%

40%

50%

60%

70%

80%

The authors would like to thank the anonymous reviewers for
their valuable suggestions, X. Qi (College of William & Mary)
and H. Wang (HKU) for commenting on this paper.
90%

Percentage of training images
Fig. 13.
images.

Performance on testing images with different proportions of training

images with both normal and cancerous cells. Also for a patient,
the possibility of the case that two different cancers occur at the
same time is around 0.4% [38].
Future work will investigate how to implement the method
on the image set with different class ratios, i.e., investigating
the percentage of cancerous nuclei in each image. Also, since
the multimodal information is widely available in medical image analysis due to the various data acquisition techniques,
we will try to adapt an mSRC to related clinical problems,
such as Alzheimer’s disease prediction, multimodal prostate
segmentation.
IX. CONCLUSION
In this paper, we propose a novel method: mSRC for classifying lung needle biopsy images. mSRC aims to improve the
classification performance, especially for the images of different
cancerous types. In mSRC, we design a novel genetic algorithmbased multimodal dictionary learning method to jointly learn the
subdictionaries. The genetic approach aims to select the topmost
discriminative samples for each individual modality as well as
to guarantee the large diversity among different modalities. Finally, a hierarchical fusion strategy is employed to predict the
labels for new coming testing images. We evaluate the performance of mSRC on a real lung cancer image set including 271
images in total. All the images are labeled by the pathologist
from Bayi Hospital. The 271 images consist of normal images
and four types of cancerous images. mSRC can make full use
of the discriminative and diversity information among different
modalities to guide better classification. Compared to several
state-of-the-art methods (mcSVM, LapRLS, MCMI-AdaBoost,

REFERENCES
[1] World Health Organization (WHO), Cancer. (2012). [Online]. Available:
http://www.who.int/mediacentre/factsheets/fs297/en/index.html
[2] H. Huang, L. Shen, J. Ford, L. Gao, and J. Pearlman, “Early lung cancer
detection based on registered perfusion MRI,” J. Oncol. Rep., vol. 15,
pp. 1080–1084, 2005.
[3] A. Depeursinge, D. Racoceanu, J. Iavindrasana, G. Cohen, A. Platon,
P. A. Poletti, and H. Müller, “Fusing visual and clinical information for
lung tissue classification in high-resolution computed tomography,” Artif.
Intell. Med., vol. 50, pp. 13–21, 2011.
[4] S. Diciotti, S. Lombardo, M. Falchini, G. Picozzi, and M. Mascalchi,
“Automated segmentation refinement of small lung nodules in CT scans
by local shape analysis,” IEEE Trans. Biomed. Eng., vol. 58, no. 12,
pp. 3418–3428, Dec. 2011.
[5] Q. Wei, Y. Hu, G. Gelfand, and J. H. MacGregor, “Segmentation of lung
lobes in high-resolution isotropic CT images,” IEEE Trans. Biomed. Eng.,
vol. 56, no. 5, pp. 1383–1393, May 2009.
[6] K. Mori, J. Hasegawa, J. Toriwaki, H. Anno, and K. Katada, “Recognition
of bronchus in three-dimensional X-ray CT images with applications to
virtualized bronchoscopy system,” in Proc. Int. Conf. Pattern Recognit.,
1996, pp. 528–532.
[7] C. C. Chang and C. J. Lin, “LIBSVM: A library for support vector machines,” ACM Trans. Intell. Syst. Technol., vol. 2, pp. 1–27, 2011.
[8] Y. Shi, Y. Gao, R. Wang, Y. Zhang, and D. Wang, “Transductive costsensitive lung cancer image classification,” Appl. Intell., vol. 38, no. 1,
pp. 16–28, 2013.
[9] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face
recognition via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 31, no. 2, pp. 210–227, Feb. 2009.
[10] W. Wang and Z.-H. Zhou, “Analyzing co-training style algorithms,” in
Proc. Eur. Conf. Mach. Learn., 2007, pp. 454–465.
[11] Y. Yu, Y.-F. Li, and Z.-H. Zhou, “Diversity regularized machine,” in Proc.
Int. Joint Conf. Artif. Intell., 2011, pp. 1603–1608.
[12] J. Yang, K. Yu, Y. Gong, and T. Huang, “Linear spatial pyramid matching
using sparse coding for image classification,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., Jun. 2009, pp. 1794–1801.
[13] M. Liu, D. Zhang, and D. Shen, “Ensemble sparse classification of
Alzheimer’s disease,” Neuroimage, vol. 60, no. 2, pp. 1106–1116, 2012.
[14] A. Blum and T. Mitchell, “Combining labeled and unlabeled data with cotraining,” in Proc. Annu. Conf. Comput. Learn. Theory, 1998, pp. 92–100.
[15] Z.-H. Zhou, Y. Jiang, Y.-B. Yang, and S.-F. Chen, “Lung cancer cell
identification based on aritificial neural network ensembles,” Artif. Intell.
Med., vol. 24, no. 1, pp. 25–36, 2002.
[16] L. Zhu, B. Zhao, and Y. Gao, “Multi-class multi-instance learning approach for lung cancer cell classification based on bag feature selection,”
in Proc. Int. Conf. Fuzzy Syst. Knowl. Discovery, Oct. 2008, pp. 487–492.
[17] Y. Freund and R. Schapire, “Experiments with a new boosting algorithm,”
in Proc. Int. Conf. Mach. Learn., 1996, pp. 325–332.

SHI et al.: MULTIMODAL SPARSE REPRESENTATION-BASED CLASSIFICATION FOR LUNG NEEDLE BIOPSY IMAGES

[18] C. M. Christoudias, R. Urtasun, and T. Darrell, “Multi-view learning in the
presence of view disagreement,” in Proc. Conf. Uncertainty Artif. Intell.,
2008, pp. 88–96.
[19] Z.-H. Zhou and M. Li, “Semi-supervised learning by disagreement,”
Knowl. Inf. Syst., vol. 24, pp. 415–439, 2010.
[20] P. Campadelli, E. Casiraghi, and G. Valentini, “Support vector machines
for candidate nodules classification,” Neurocomputing, vol. 68, pp. 281–
288, 2005.
[21] V. Kovalev, N. Harder, B. Neumann, M. Held, U. Liebel, H. Erfle,
J. Ellenberg, R. Ellis, and K. Rohr, “Feature selection for evaluating florescence microscopy images in geneme-wide cell screens,” in Proc. IEEE
Comput. Soc. Conf. Comput. Vis. Pattern Recognit., Jun. 2006, pp. 276–
283.
[22] M. C. Lee, L. Boroczky, K. Sungur-Stasik, A. D. Cann, A. C. Borczuk,
S. M. Kawut, and C. A. Powell, “Computer-aided diagnosis of pulmonary
nodules using a two-step approach for feature selection and classifier
ensemble construction,” Artif. Intell. Med., vol. 50, pp. 43–53, 2010.
[23] S. Yu, B. Krishnapnram, R. Rosales, and R. B. Rao, “Bayesian cotraining,” J. Mach. Learn. Res., vol. 12, pp. 2649–2680, 2011.
[24] M. M. Fraz, P. Remagnino, A. Hoppe, B. Uyyanonvara, A. R. Rudnicka,
C. G. Owen, and S. A. Barman, “An ensemble classification-based approach applied to retinal blood vessel segmentation,” IEEE Trans. Biomed.
Eng., vol. 59, no. 9, pp. 2538–2548, Sep. 2012.
[25] D. Zhang and D. Shen, “Multi-modal multi-task learning for joint prediction of multiple regression and classification variables in Alzheimer’s
disease,” Neuroimage, vol. 59, no. 2, pp. 895–907, 2012.
[26] P. Tiwari, S. E. Viswanath, D. Lee, and A. Madabhushi, “Multi-modal data
fusion schemes for integrated classification of imaging and non-imaging
biomedical data,” in Proc. IEEE Int. Symp. Biomed. Imag., Mar.–Apr.
2011, pp. 165–168.
[27] A. Madabhushi, M. D. Feldman, D. N. Metaxas, J. Tomaszeweski, and
D. Chute, “Automated detection of prostatic adenocarcinoma from highresolution ex vivo MRI,” IEEE Trans. Med. Imag., vol. 24, no. 12,
pp. 1611–1625, Dec. 2005.
[28] G. Dasovich, R. Kim, D. Raicu, and J. Furst, “A model for the relationship
between semantic and content based similarity using LIDC,” in Proc. SPIE
Med. Imag., vol. 7624, pp. 31–43, 2010.

2685

[29] C. Suzuki, J. F. Gomes, A. X. Falcao, J. P. Papa, and S. Hoshino-Shimizu,
“Automatic segmentation and classification of human intestinal parasites
from microscopy images,” IEEE Trans. Biomed. Eng., vol. 60, no. 3,
pp. 803–812, Feb. 2012.
[30] J. A. Tropp, “Greed is good: Algorithmic results for sparse approximation,” IEEE Trans. Inf. Theory, vol. 50, no. 10, pp. 2231–2242, Oct. 2004.
[31] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T. Lee, and
T. J. Sejnowski, “Dictionary learning algorithms for sparse representation,” Neural Comput., vol. 12, no. 2, pp. 349–396, 2003.
[32] J. Mairal, F. Bach, J. Ponce, and G. Sapiro, “Online dictionary learning for
sparse coding,” in Proc. 26th Int. Conf. Mach. Learn., 2009, pp. 689–696.
[33] M. Yang, L. Zang, X. Feng, and D. Zhang, “Fisher discrimination dictionary learning for sparse representation,” in Proc. IEEE Int. Conf. Comput.
Vis., Nov. 2011, pp. 543–550.
[34] M. Mitchell, An Introduction to Genetic Algorithm. Cambridge, MA,
USA: MIT Press, 1998.
[35] J. J. Grefenstette, “Optimization of control parametes for genetic algorithm,” IEEE Trans. Syst. Man Cybern., vol. SMC-16, no. 1, pp. 122–128,
Jan. 1986.
[36] M. Belkin, P. Niyogi, and V. Sindhwani, “Manifold regularization: A
geometric framework for learning from labeled and unlabeled examples,”
J. Mach. Learn. Res., vol. 7, pp. 2399–2434, 2006.
[37] L. Zhang, W. D. Zhou, P. C. Chang, J. Liu, Z. Yan, T. Wang, and F. Z. Li,
“Kernel sparse representation-based classifier,” IEEE Trans. Signal Process., vol. 60, no. 4, pp. 1684–1695, Apr. 2012.
[38] W. D. Travis, E. Brambilla, H. K. Muller-Hermelink, and C. C. Harris,
Pathology and Genetics of Tumours of the Lung, Pleura, Thymus and Hear,
(World Health Organization Classification of Tumours Series). Lyon,
France: IARC Press, 2004.

Author’s photographs and biographies not available at the time of publication.

