IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

1461

Improved Semisupervised Adaptation for a Small
Training Dataset in the Brain–Computer Interface
Jianjun Meng, Xinjun Sheng, Dingguo Zhang, and Xiangyang Zhu∗

Abstract—One problem in the development of brain–computer
interface (BCI) systems is to minimize the amount of subject training on the premise of accurate classification. Hence, the challenge
is how to train the BCI system effectively especially in the scenario with small amount of training data. In this paper, we introduce improved semisupervised adaptation based on common
spatial pattern (CSP) features. The feature extraction and classification are performed jointly and iteratively. In the iteration
step, training data are expanded by part of the testing data with
labels which are predicted by a linear discriminant analysis classifier and/or a Bayesian linear discriminant analysis classifier in
the previous iteration. Then CSP features are reextracted from the
expanded training data, and the classifiers are retrained. Both selftraining and cotraining paradigms are proposed for the improved
semisupervised adaptation. Throughout the investigation on different number of initial training trials, we find that when a small
number of training trials are used, e.g., a training session contains
no more than 30 trials, similar classification performance to that
of large training data items (40–50 trials) can be achieved. Effectiveness of the algorithms is verified by two competition datasets.
Compared with several existing algorithms, the proposed semisupervised algorithms show improvements in classification accuracy
for most of the competition datasets especially in the case of small
training data.
Index Terms—Bayesian linear discriminant analysis (BLDA),
brain–computer interface (BCI), common spatial pattern (CSP),
linear discriminant analysis (LDA), semisupervised adaptation.

I. INTRODUCTION
RAIN–COMPUTER interface (BCI) based on noninvasive
electroencephalography (EEG) signals provides an alternative way to infer the intention of human subjects. The past
decades have witnessed a continuous progress in the noninvasive
BCI technology [1]. Due to the fact that EEG signals are nonstationary in nature, reducing the training effort or annotation cost
without loss of accurate performance is one of the key issues for
making the applicable BCI systems [2]–[5]. In order to initialize

B

Manuscript received January 28, 2013; revised May 23, 2013 and August
20, 2013; accepted September 30, 2013. Date of publication October 9, 2013;
date of current version June 30, 2014. This work was supported by in part
by the National Basic Research Program (973 Program) of China under Grant
2011CB013305, in part by the National Natural Science Foundation of China
under Grant 51075265 and 51375296), and in part by the Science and Technology Commission of Shanghai Municipality under Grants 11JC1406000 and
13430721600).
The authors are with The State Key Laboratory of Mechanical System and
Vibration, Shanghai Jiao Tong University, Shanghai 200240, China (e-mail:
mengjianjunxs008@gmail.com; xjsheng@sjtu.edu.cn; dgzhang@sjtu.edu.cn;
mexyzhu@sjtu.edu.cn).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2285232

parameters of feature extractors and classifiers, a certain amount
of training data items is essential. Since collecting labeled data
to calibrate a classifier every time of experiment is inevitable in
the BCI application, it is valuable to explore how to retain the
accurate performance by the fewest possible training samples.
Raw EEG signals are too high dimensional and highly correlated to be classified directly. The standard BCI system consists
of feature learning and classifier learning [6]. The common
spatial pattern (CSP) algorithm provides an efficient method
for feature learning and dimension reduction [7], [8]. However,
the CSP algorithm is a supervised method for feature extraction.
When the amount of training data items is small, both the feature
extraction, e.g., the learning of CSP transformation matrix [9],
and the classifier are not reliable. Consequently, BCI systems
suffer from deterioration of performance under the condition
of small training data. In both research and real implementation, it is more desirable to improve the performance with less
annotated training data, rather than the abundant one.
To solve the previous problem, several approaches have been
proposed to reduce the training effort. Krauledat et al. propose a
cluster approach based on CSP features to capture generic “invariant” discriminative features of the BCI task. They show that
it is possible to transfer information from prior BCI sessions of
the same subject to a new session [2]. Regularized CSPs provide
an alternative way to overcome the detrimental influence on the
classification performance under the condition of small training
data [10]–[12]. Semisupervised approach has attracted a lot of
attention recently [13]–[16]. Temporal manifold of the testing
samples is used to regularize the covariance of training samples by Tu and Sun [16]. From another point of view, Li et al.
propose an algorithm named semisupervised support vector machine (SSVM) to improve the performance of BCI system with
insufficient training data [13], [14]. Unlabeled testing data are
used to expand the small training dataset. Feature extraction and
classification are jointly and iteratively updated. However, hyperparameters in SSVM are hard to be efficiently selected [14],
[17] and hence this deficiency reduces the effectiveness of this
algorithm. Additionally, the previous methods do not give quantitative analysis on how to reduce the training trials by applying
the semisupervised adaptation.
In this paper, we propose an improved semisupervised adaptation for motor-imagery-based BCI to address the unresolved
problems. First, CSP feature extractor is applied to reduce the
dimensionality of the raw EEG signal and a simple linear classifier is used to label the testing dataset. Then, feature extraction and classification are performed jointly and iteratively on
the expanded training data until the stop condition is satisfied.
Different sizes of initial training data items are investigated to

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1462

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

fed into the classifier after simple transformation. Denote by
 = [w1 , . . . , wp , wN −p+1 , . . . , wN ]T
W

Fig. 1. Methodology of the proposed semisupervised adaptation algorithms.
Instead of applying the weak learner, learning from small training dataset directly, we add an iterative learning step to use this weak learner and parts of
testing data together in order to get a strong learner.

explore the optimal initial size. The efficiency of the proposed
algorithms is evaluated by comparing with several existing algorithms on two competition datasets. The proposed methods show
better performance consistently for a majority of the competition datasets especially for algorithms of Bayesian version. This
study’s main contributions include: first, both the CSP feature
extractor and simple linear classifier without hyperparameters
are simultaneously retrained in order to update the CSP features
and classifiers jointly until the stop condition is satisfied; second, detailed study on EEG signal classification which considers
different number of initial training trials is explored to give guidance on how to use semisupervised adaptation to reduce training
trials on the premise of accurate classification; third, algorithms
of Bayesian version jointly with feature reextraction are used
to give better generalization ability and the possible underlying
reason is explored by the bias variance decomposition.
The remainder of this paper is organized as follows. In
Section II, we describe the methods, including CSP feature extraction, robustness of CSP features, linear discriminant analysis
(LDA) classifier, Bayesian linear discriminant analysis (BLDA)
classifier, and the improved semisupervised adaptation algorithms. Section III describes the BCI competition datasets and
the results, followed by discussions in Section IV. Finally,
Section V concludes the paper.
II. METHODOLOGY
The conventional method (i.e., CSP feature extraction with
LDA classifier) often obtains a weak learner in small sample setting. Instead of applying this weak learner directly, we propose
semisupervised adaptation which combines this weak learner
with parts of the large testing dataset (without labels) to produce a strong learner (see Fig. 1). Detailed descriptions of the
proposed algorithms are presented in the next section.
A. CSP Feature Extraction and Robustness
The common spatial filter (CSP) is frequently used for extracting features from motor imagery-based EEG signals [8].
The CSP algorithm generates features, which are ready to be

(1)

the matrix of selected CSP filters, which is derived from eigenvectors corresponding to eigenvalues from both ends of the
eigenvalue spectrum. The feature used in this paper is the normalized log band power, which is obtained by



Xk XkT  T

f (k) = log diag W
W
= [f1 (k), . . . , f2p (k)]T
tr(Xk XkT )
(2)
where diag(·) and tr(·) denote the diagonal elements and the
trace of a matrix, respectively. Xk ∈ C ×S , C is the number of
channels, and S is the number of time points. k is the trial number
and k = 1, . . . , N1 + N2 . N1 and N2 denote the numbers of
training trials and testing trials, respectively. The log operation
serves as to approximate normal distribution of the data. We
named the feature f (k) as the CSP feature.
In the supervised case, where all the labels are known, the
 can be calculated accurately by enough
projection matrix W
training dataset. While supervised adaptation seems impractical
for motor-imagery-based BCI implementation [16], fortunately
semisupervised learning provides a promising way. The necessary condition for a semisupervised learning algorithm is that
the applied feature set has sufficient consistency [18]. The consistency of CSP features depends on the robustness of CSP
features, as shown in [9]. A sketch of the proof is shown as
follows. Suppose 1 and 2 are symmetric matrices related to
testing trials being labeled incorrectly, which are defined as
follows:
 Xk X T
 Xk X T
k
k
, 2 =
.
(3)
1 =
T
tr(Xk Xk )
tr(Xk XkT )
k ∈{−}

k ∈{+}

If we expand the training dataset by adding the testing data
items with predicted labels, the covariance matrices are updated
(1)
(1)
(1)
(1)
to R+ + 1 and R− + 2 , respectively, where R+ , R− correspond to the new covariance matrices by adding the trials
labeled correctly in the testing dataset. Similarly, a projection
 () can be derived through joint diagonalmatrix denoted as W
ization.
Then, the CSP feature for the kth trial is



T
 () Xk Xk W
 ()T
f (, k) = log diag W
,
tr(Xk XkT )
k = 1, . . . , N1 , N1 + 1, . . . , N1 + N2 . (4)
In ideal circumstances, the number of incorrectly classified trials
tends to be infinitesimal compared to large amount of correctly
 () converges to the
labeled trials. Then, the projection matrix W

true CSP projection matrix W ; the derived CSP features f (, k)
tend to the true features f (k):
lim f (, k) = f (k).

→0

(5)

In real application, the number of trials is limited and classification error is inevitable; hence, we should carefully examine the
efficiency of the robustness of CSP features.

MENG et al.: IMPROVED SEMISUPERVISED ADAPTATION FOR A SMALL TRAINING DATASET IN THE BRAIN–COMPUTER INTERFACE

B. LDA Classifier

1463

mean (m) and covariance (C) are tractable. They are given by

The first classifier used in this study is the Fisher LDA [19].
Suppose the features are a set of 2p-dimensional samples
x1 , . . . , xN 1 , where N1 is the number of training examples.
A discriminant function that is a linear combination of the components of x can be written as y = wT x + b, where w is the
weight vector, wT means transpose of vector w, and b is the
bias. Hence, the parameters of LDA classifier is definitely determined by Θ = {w, b}.

T

m = β(βXX + I  (α))−1 Xy
T

C = (βXX + I  (α))−1 .

(10)

The marginal likelihood function p(DI |β, α) is the denominator
of (9). In BLDA, the hyperparameters α and β are efficiently
resolved in a Bayesian evidence framework. By maximizing the
marginal likelihood p(DI |β, α), the values of α and β can be
obtained in a closed form by
2p
2
i=1 cii + mi

α = 	2p

C. BLDA Classifier
The BLDA is naturally described as a Bayesian version of
regularized LDA, where the regularization parameters are estimated automatically by the evidence framework. Since the
BLDA uses a different approach for optimizing parameters of
classifier, the diversity of predicted labels between LDA and
BLDA is guaranteed. The BLDA implementation is similar to
those described in [20] and [21].
Suppose DI = {(x1 , y1 ), . . . , (xN 1 , yN 1 )} to be the training data composed of feature vectors xi and corresponding
class labels yi . X is the matrix resulting from stacking all the
training feature vectors, i.e., X = [x1 , . . . , xN 1 ] ∈ 2p×N 1 . Let
X = ( X1 ) ∈ (2p+1)×N 1 ; here; the X is introduced for convenience of allowing any fixed offset in the data (also called a bias
parameter) [22]. y is a vector containing all the corresponding
class labels. The basic assumption of BLDA is that the targets
y and feature matrix X are linearly related, and contaminated
by Gaussian noise . Hence, we have
y = wT · X + .

(6)

From (6), the likelihood function for weight vector w can be
obtained as
  N21


β
β
T
2
p(DI |β, w) =
· exp − w X − y
(7)
2π
2
where DI is the training data, N1 is the number of training
examples, and β denotes the inverse variance of the noise. In the
framework of Bayesian inference, a Gaussian prior distribution
of weight vector w is assumed as


 α  22p  δ  12
1 T 
p(w|α) =
· exp − w I (α)w
2π
2π
2

(8)

where α is the hyperparameter which specifies an isotropic,
zero-mean Gaussian prior distribution with covariance α1 for
the weights. Note that the last entry of w is the bias term and
its variance 1δ is separately defined. Usually, a small value is
evaluated to δ for overcoming the danger of overfitting. I  (α) is a
diagonal matrix with 2p + 1 dimension. According to Bayesian
theorem, the posterior distribution is derived by
p(w|β, α, DI ) = 

p(DI |β, w)p(w|α)
.
p(DI |β, w)p(w|α)dw

(9)

Since the likelihood distribution (7) and prior distribution (8)
are Gaussian, the posterior distribution is also Gaussian and the

β=

N1
T

tr(XX C) + wT X − y2

(11)

where cii denotes the ith diagonal element of the covariance
matrix C, mi is the ith element of the mean vector m. These are
implicit solutions for α and β, because values of α and β depend
on m and C. They can be solved by choosing an initial value
and then using this to calculate m and C according to (10). This
procedure is repeated until convergence. Once the computation
of m and C is complete, the optimum weight values of w is
taken as mean of the posterior (m).
Then given a new test feature vector, x∗ , prediction for corresponding class label can be obtained by the predictive distribution,


p(y∗ |β, α, DI , x∗ ) = p(y∗ |β, x∗ , w)p(w|β, α, DI )dw
(12)
where p(y∗ |β, x∗ , w) denotes a Gaussian distribution. Since
p(y∗ |β, x∗ , w) and p(w|β, α, DI ) are both Gaussian distributions, the predictive distribution is also Gaussian. It can be characterized by its mean (μ) and variance (σ 2 ) as
μ = m T x∗
σ2 =

1
+ xT∗ Cx∗ .
β

(13)

The mean μ can be used for class prediction. If μ > 0, the feature
vector x∗ belongs to class 1, otherwise, it belongs to class 2.
D. Expanding Training Dataset
by Semisupervised Adaptation
We denote the initial parameter set of LDA classifier as
(1)
Θ0 = {w0 , b0 }, which can be fully determined by {μ1 , μ2 , Σ1 ,
(2)
Σ2 }. Let Θ0 = {m0 , C0 } be the initial parameters of BLDA
classifier, which can be determined by the training dataset DI
0 stands for the initial projecand hyperparameters α and β. W
tion matrix for CSP features. The main work of this paper is
(l)
 (l)
to update the parameter set Θi = {wi , bi }, l = 1, 2, and W
i
iteratively by expanding the training dataset. Here, i is the iteration number and l = 1 stands for parameters of LDA classifier
and l = 2 stands for parameters of BLDA classifier. In the following, we take self-training LDA as an example to describe in
detail. Procedures for self-training BLDA are similar to those
of self-training LDA and are omitted consequently.

1464

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

1) Self-Training Algorithm: Initial step. Denote DI as the
initial training dataset and DT as the testing dataset. The initial
0 are
CSP transformation matrix W0 and projection matrix W
derived by the training dataset. Then, extract CSP features on the
initial training dataset and testing dataset. The initial parameter
(1)
Θ0 of LDA is received by the CSP features from the training
dataset.
The iteration step. The ith iteration (i = 1, 2, . . .) performs
the following steps:
Step 1 (expanding the training dataset): Define the expanded
training dataset Di as DI + DT  , where DT  is part of DT
and the labels of DT  are the predicted labels [yi−1 (1), . . . ,
yi−1 (N2 )] derived by the LDA classifier in the (i − 1)th iteration. Note that N2 = Int(c · N2 ) where c ∈ (0, 1] is a predetermined percentage and DT  are the part with largest absolute
values of DT , the values of which (there is a classification score
yk = wT xk + b for each feature xk ) are predicted by the LDA
classifier. Int(c · N2 ) defines the largest integer, which is smaller
than c · N2 .
Step 2 (feature reextraction): Retrain the CSP transformation
i based on expanded training dataset Di , where the
matrix W
labels of DT  are the predicted labels in the (i − 1)th iteration.
Then, reextract the CSP features on both DI and DT . The CSP
feature vector for the kth trial is updated to f(i) (k), where i refers
to the ith iteration, and k = 1, . . . , N1 + N2 .
Step 3 (retrain LDA classifier): Retrain LDA classifier with
the reextracted CSP features of dataset Di , which contains the
dataset of DI and DT  . The labels of DT  are the predicted labels
(1)
in (i − 1)th iteration. The parameter set is updated to Θi .
Step 4 (termination judgment): Define α as a preset positive
constant, and find out the number of trials from the dataset DT
with different labels in the current and previous iteration, such
that
dy(i) =

N2


| yi (k) − yi−1 (k) | .

(14)

k =1

If dy(i) < α or i = 20, the algorithm stops after the ith iteration, and the predicted labels [y(i) (1), . . . , y(i) (N2 )] are the final
results.
2) Cotraining Algorithm: The classic cotraining algorithm
proposed by Blum and Mitchell [23], deals with tasks whose
input features have two different views (i.e., two sufficient and
independent sets of features) and works in an iterative manner. One variation of cotraining proposed in [24] relaxes the
constraint of two views of feature sets and shows that labels
generated from two different classifiers can also be used to
perform cotraining style learning and to improve the classification. Recent research explores the robust cotraining with labels
inspection by which the reliability of labels provided by the
classifiers is examined and the results suggest that the efficiency
of cotraining can be further improved [25].
In the following, we introduce the two-classifier cotrainingbased approach to reduce the training effort. Similarly, denote
DI as the initial training dataset and DT as the unlabeled testing
0 are the initial CSP transformation
dataset. Matrices W0 and W
matrix and projection matrix, respectively, derived by the train-

ing dataset. Then, extract CSP features on the initial training
data and testing data, denoted as f(0) (k), k = 1, . . . , N1 + N2 .
(1)

(1)

The initial parameters Θ0 of LDA (denoted as classifier h0 )
(2)
(2)
and Θ0 of BLDA (denoted as classifier h0 ) are obtained by
the CSP features from the training dataset, respectively.
(1)
Initial step. Derive initial CSP transformation matrix W0
(2)
and W0 from the training dataset, respectively. Note that
(1)
(2)
(1)
(2)
(1)
(2)
W0 = W0 . Obtain Θ0 and Θ0 for h0 and h0 ,
respectively.
The iteration step. The ith iteration (i = 1, 2, . . .) performs
the following steps:
(1)
(2)
Step 1: Label the testing dataset DT by hi−1 and hi−1 ,
respectively.
Step 2 (expanding the training dataset): Construct a new la(1)
(2)
(2)
beled set Di as DI + DT  , where DT  is part of DT and la(2)
(2)
(2)
bels of DT  are predicted labels [yi−1 (1), . . . , yi−1 (N2 )] given
(2)
by hi−1 . Set N2 = Int(c · N2 ). Similarly, construct the labeled
(2)
(1)
(1)
set Di as DI + DT  , where the labels of DT  are predicted
(1)
(1)
(1)
labels [yi−1 (1), . . . , yi−1 (N2 )] given by hi−1 .
Step 3 (feature reextraction): Retrain the CSP transformation
(1)
(1)
matrix Wi based on the new labeled set Di . Then reextract
(1)
 . The CSP feature
CSP features on both DI and DT by W
i
(1)
vector for the kth trial is updated to fi (k), where i refers to the
 (2)
ith iteration, and k = 1, . . . , N1 + N2 . Similarly, update W
i
(2)
(2)
based on Di and obtain the new CSP features fi (k), k =
1, . . . , N1 + N2 .
(1)
Step 4 (retrain LDA and BLDA classifiers): Retrain hi with
(1)
reextracted CSP features of dataset Di , which contains dataset
(2)
(2)
of DI + DT  . Meanwhile, retrain hi based on reextracted
(2)
(l)
CSP features of dataset Di . The labels of DT  , (l = 1, 2) are
(l)
the predicted labels of hi , (l = 1, 2) in the (i − 1)th iteration.
(l)
The parameter set is updated to Θi , (l = 1, 2).
Step 5 (termination judgment): Define α as a preset positive
constant, and find out the number of trials from the dataset DT
(1)
(2)
with different labels predicted by hi and hi , such that
dy(i) =

N2


(1)

(2)

| yi (k) − yi (k) | .

(15)

k =1

If dy(i) < α or i = 20, the algorithm stops after the ith iteration,
(1)  (1)
(2)  (2)
and hi , W
and hi , W
are the final parameters for LDA
i
i
and BLDA cotraining, respectively. The cotraining paradigm is
summarized in Fig. 2.
Remark: We choose LDA and BLDA as individual classifiers since these two computationally simple methods give
comparable good results. Meanwhile, they output reasonable
diversity of classification results, which is critical in cotraining
style algorithms. Depending on the semisupervised strategy described previously, we have the following four different learning
algorithms:
1) self-training LDA (SLDA),
2) self-training BLDA (SBLDA),

MENG et al.: IMPROVED SEMISUPERVISED ADAPTATION FOR A SMALL TRAINING DATASET IN THE BRAIN–COMPUTER INTERFACE

(a)

Fig. 2. Block diagram of the proposed semisupervised cotraining paradigm. In
each iteration, parts of testing dataset with predicted labels obtained by the other
classifiers are added to the training data and this procedure facilitates labeling
information exchange. Then, CSP features are reextracted and sent back to its
own classifier. The iteration stops until the termination condition is satisfied.

3) cotraining LDA (CoLDA)—where the output is given by
the LDA classifier which is cotrained with BLDA,
4) cotraining BLDA (CoBLDA)—where the output is given
by the BLDA classifier which is cotrained with LDA.
Performance analysis of the four algorithms is given in the
following section.
III. EXPERIMENT EVALUATION
This section presents experiments on two competition
datasets to investigate the following objectives. 1) Investigate
how the performance of semisupervised adaptation algorithms
are affected by the number of initial training samples. 2) Evaluate the performance of the proposed algorithms as well as other
competing CSP-based algorithms on EEG signal classification.
A. Dataset IV-a in BCI Competition III
1) Data Description and Preprocessing: These EEG
datasets [26] were recorded from five healthy subjects (aa, al,
av, aw, and ay). A total of 118 electrodes were placed for each
subject and the sampling rate in this experiment was 1000 Hz.
During each trial, the subject was given visual cues for 3.5 s,
during which the three motor imageries should be performed:
left hand, right hand, and right foot. Only EEG trials for righthand and right-foot movements were provided for analysis. The
presentation of target cues was intermitted by periods of random
length, the value of which was in the range of 1.75 to 2.25 s. In
the random periods, the subject could relax. A total of 140 trials
were collected for each subject and each task.
The EEG data we used are down sampled to 100 Hz. Then,
they are bandpass filtered to the 8–30 Hz wide-frequency band.
Although visual cues were given for 3.5 s, only signals in the
time interval of [0.5, 3.5] s are analyzed for each trial. We select
20 EEG channels [see Fig. 3(a)] instead of all the 118 channels
for the CSP procedure. The reason is that in the case of small

1465

(b)

Fig. 3. (a) Placement of the 20 selected EEG electrodes. The three electrodes
C3, Cz, and C4 are marked, respectively. (b) Dataset for one subject is divided
into two parts according to the size of initial training trials (M). The first part is
used to perform fivefold CV. In each fold, M trials are used as the initial training
dataset, the remaining 4 × M trials are used to extend the training dataset.
We call it “extended testing dataset”. The second part is used to evaluate the
generalization ability of the learning algorithms (bias-variance decomposition
is also performed on this dataset).

training dataset, the covariance matrix with large number of
channels contains much more unknown parameters which are
difficult to be estimated accurately. However, choosing only
a small number of relevant channels helps to overcome the
harmful deterioration and to improve the classification accuracy.
p = 3 is used for dimensional reduction of CSP features.
2) Experimental Design for Competition Dataset: Two
kinds of experiments are carried out as detailed in the following.
a) Experiment Protocol I: To study EEG signal classification
under the condition of a small training data set, the following
nine values of M (the number of initial training trials) are tested
for each subject:
M ∈ {10, 15, 20, 25, 30, 35, 40, 45, 50}.
The dataset for each subject is divided into two parts. The
first part is used to perform fivefold cross-validation (CV) and
the second part of remaining trials is used as an independent
testing dataset. To ensure the significance of the studies, the CV
is repeated five times. The number of trials in first and second
parts of dataset is determined by the initial training trials (e.g.,
if M = 50, then 250 trials are selected to be the first part of the
dataset for CV and 30 trials are left to be independent testing
datasets). We increase the number of M from 10 to 50 in the
interval of 5, in order to investigate the possible best choice
for the proposed semisupervised algorithms. This procedure is
described in Fig. 3(b).
b) Experiment Protocol II: To evaluate the proposed semisupervised algorithms against existing competing solutions, four
proposed algorithms (SLDA,SBLDA,CoLDA, and CoBLDA)
are compared against the following algorithms.
i) Static: CSP features are extracted by the initial small training dataset and parameters of LDA classifier are estimated from
the CSP features derived by the small training dataset. This is
the benchmark method.
ii) OnlyLDA: The CSP transformation matrix is derived from
the initial small training dataset. Then, the CSP features are extracted and parameters of the LDA classifier are estimated from
the CSP features of both an initial small training data set and an
extended training data set. Here, the labels of extended training

1466

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

(a)

(b)

(c)

(d)

(e)

(f)

Fig. 4. Illustration of the improvement achieved over several existing algorithms by the proposed improved semisupervised adaptation algorithms. The left
column shows comparison among the proposed algorithms and the conventional algorithms. The middle column shows the results of semisupervised adaptation.
The right column gives the comparative results among the regularized approaches and the proposed SBLDA, CoBLDA. The training trials in the first part of
dataset vary from 10 to 50. A total 90% of the testing dataset is used to expand the initial small training dataset (c = 90%) for the proposed algorithms. Mean
classification accuracies of 5 × 5-fold CV on the testing dataset and on the independent testing dataset are placed in the first row [(a)–(c)] and the second row
[(d)–(f)], respectively.

dataset are self-predicted labels. This adaptation is a selftraining paradigm. Infact, several papers focus only on the adaptation of common classifiers (e.g., LDA classifier) [27]–[29].
Here, the idea of “onlyLDA” is similar to those.
iii) SSVM: CSP features are reextracted by the initial small
training dataset and extended training dataset. CSP feature extraction and parameters of SVM are adapted jointly and iteratively as proposed in paper [14]. Since the authors report that
their iterative SSVM algorithms perform better than the earlier
extended EM algorithm [9], we compare our results with the better one, SSVM algorithm. Here, we adopt fine-tuned parameters
of kernel and regularization for SSVM rather than parameter setting proposed by the authors, since we find no improvement over
our fine-tuned parameters by adopting the parameter selection
strategy [14].
iv) STSEER: Semisupervised temporally smooth extreme energy ratio (STSEER) utilizes features extracted from the EER,
which has been proved to be theoretically equivalent to the CSP
method [30]. The STSEER applies the regularization term to
preserve the temporal manifold embedded in the testing trials.
There are two hyperparameters to be preset in advance. In this
study, we use the optimal selection of r = 10 and α = 0.3 as
suggested by the authors [16].
v) SIWEER: Semisupervised importance weighted EER (SIWEER) regularize the estimation of covariance matrix by taking

into consideration the intertrial importance and intratrial importance of training samples. These importance weights are estimated by density ratio estimation. The single hyperparamter σ
of this method is set to 8 according to the previous results [16].
vi) TRCSP: The CSP with Tikhonov regularization (TRCSP)
belongs to the family of regularized CSP (R-CSP) [12]. TRCSP
was proved to be valuable in small training dataset compared
with 11 different RCSP algorithms. For TRCSP, the tuning
parameter of the regularized coefficient α is selected from
{1 × 10−10 , 1 × 10−9 , . . . , 1 × 10−1 } by the nested CV as in
[12].
vii) R-CSP-A: R-CSP with aggregation shrinks the covariance matrix for each class toward both the identity and generic
covariance matrices which can be derived from other subjects. Then the final outputs are computed from the ensemblebased learning. The hyperparameters for β and γ are selected
from {0, 0.01, 0.1, 0.2, 0.4, 0.6} and {0, 0.001, 0.01, 0.1, 0.2},
respectively as in [11].
3) Classification Results: In the following, the experimental
results are presented for the aforementioned two experimental
settings, respectively.
a) Results for Experiment Protocol I: The complete experimental results for the first set of experiments are illustrated in
Fig. 4. Mean accuracies of 5 × 5 fold CV for the five subjects
and for the nine values of M using the proposed algorithms

MENG et al.: IMPROVED SEMISUPERVISED ADAPTATION FOR A SMALL TRAINING DATASET IN THE BRAIN–COMPUTER INTERFACE

1467

TABLE I
MEAN ACCURACIES OF 5 × 5-FOLD CV ON DATASET IV-A OF BCI COMPETITION III

Paired Student’s t-test has been performed between static classification and each of the comparative algorithms. Statistically significant p-value (< 0.05) and p-value
(< 0.01) have been marked with “*” and “**,” respectively.

against the competing algorithms are plotted. The horizontal
axis presents a different number of M in the interval of five
trials. Since there are total 280 trials for each subject, the maximum number of initial training dataset can be 50 for fivefold
CV. The vertical axis shows mean classification accuracies for
5 × 5-fold CV of five subjects. The comparative results among
11 algorithms are divided into three groups for visual convenience according to the characteristic of each algorithm. The
left column compares the proposed algorithms and the conventional algorithms including “static” and “onlyLDA”. The middle
column gives comparative results among methods of semisupervised adaptation including SSVM, SEMEER-LDA, SIWEERLDA, and the proposed algorithms (SBLDA and CoBLDA).
The right column contains the regularized approaches including
R-CSP-A and TRCSP-LDA. Note that 90% of testing data items
(N2 = Int(c · N2 ), here, c = 90%) are used for all the proposed
algorithms and SSVM. Throughout experimental analysis, we
find that the proposed algorithms are not very sensitive to this
parameter when c ∈ {0.7, 0.8, 0.9}; hence, we choose c = 90%
for all the proposed algorithms tentatively in this study. From
the results of Fig. 4, it is clear that static classification performs
worse than all the other ones especially with insufficient amount
of training data. SSVM [see Fig. 4(b) and (e)] and R-CSP-A [see
Fig. 4(c) and (f)] performs slightly better than all the other algorithms except our proposed SBLDA and CoBLDA on both
testing and independent testing datasets when the number of
initial training set is smaller than 30. However, the performance
of SSVM and R-CSP-A deteriorates quickly on the independent
testing dataset when the number of initial training data is larger
than 30. The proposed semisupervised algorithms especially
for SBLDA and CoBLDA perform better than other competing algorithms consistently on both extended and independent
testing datasets and are particularly powerful when the number
of training samples is small. After the size of training samples
increases, it is reasonable that all the algorithms perform better,
while, their differences tend to decrease. However, the SBLDA
and CoBLDA give almost the best results when the size of initial training set is 25 to 30 trials, whereas another 20 trials are
needed for other competing algorithms to obtain the same performance. Note that one of the reasons for deterioration of all
the algorithms on the independent testing dataset is that when

the amount of training data is varied, the amount of independent
testing data also varies. This introduces a bias when comparing
different values for M and this bias is clear in Fig. 7 when we
analyze the performance of all the algorithms in Section IV. One
possible solution to alleviate the bias is to increase the number
of random tests. However, more testing trials and total trials are
needed to eliminate this bias.
b) Results for Experiment Protocol II: In Table I, the mean
accuracies of 5 × 5-fold CV on extended testing dataset and independent testing dataset when M = 30 are listed, respectively.
Paired Student’s t-test has been performed between static
classification and each of the algorithms listed in the table.
For each method and each subject, the statistically significant pvalue (<0.05) and p-value (<0.01) are marked with “∗ ” and “∗∗ ”,
respectively. From Table I, it may be concluded that SBLDA and
CoBLDA perform moderately better than the other algorithms
and the superiority compared to static classification is statistical
significant for at least three out of five subjects.
To further evaluate the performance of SBLDA and CoBLDA,
we perform paired t-test between SBLDA/CoBLDA and
SSVM/R-CSP-A on this dataset (M = 30) because SSVM and
R-CSP-A are slightly superior than other algorithms (see Fig. 4
and Table I). The paired t-test shows no significant difference
between SSVM and R-CSP-A for all the five subjects except for
the subject “av” whose performance by SSVM is significantly
inferior to R-CSP-A (p-value < 0.05). There are significant improvements (p-value < 0.01) over both SSVM and R-CSP-A
by SBLDA and CoBLDA for subjects “aw” and “ay”. Additionally, SBLDA and CoBLDA are significantly superior to SSVM
for subject “av” on independent testing set and they are significantly superior to R-CSP-A for subject “aa” on both extended
and independent testing data. Totally, the performance obtained
by SBLDA and CoBLDA is significantly superior to that by
SSVM and R-CSP-A for at least two out of five subjects on
both extended and independent testing data.
From the previous results, it is clear that the improvement of
classification accuracy is attributed to both feature reextraction
(the proposed algorithms) and parameter adaptation of classifier (“onlyLDA”). To see why CSP feature reextraction make
a difference, Fig. 5 shows the effects of the semisupervised
CSP transformation matrix reextraction on six random small

1468

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

(a)

(b)

(c)

(d)

Fig. 5. Illustration of the effects of semisupervised CSP transformation matrix
reextraction on six random small training datasets for two particular subjects
“aw” and “ay.” Eigenvalue distribution of the corresponding CSP transformation
matrix W derived by the initial small training dataset for subject (a) “aw”
and (b) “ay,” respectively. Eigenvalue distribution of the corresponding CSP
transformation matrix W () derived after combining the initial small training
dataset with part of extended testing dataset for subject (c) “aw” and (d) “ay,”
respectively.

training datasets for two particular subjects “aw” (left column)
and “ay” (right column), respectively. From the distribution of
eigenvalues for the CSP transformation matrix of subjects “aw”
and “ay”, it is observed that the variance of eigenvalues for
initial CSP transformation matrices (upper row of Fig. 5) is
much higher than those when CSP transformation matrices are
reextracted by including extended testing dataset (lower row of
Fig. 5). From this point, the additional extended testing dataset
has the effect of regularizing the initial transformation matrix
closer to the true CSP transformation matrix. As a result, the performance of semisupervised algorithms is improved compared
to no feature reextraction methods like “static” and “onlyLDA”.
To further evaluate the efficiency of the proposed semisupervised algorithms, we extend our validation to the dataset II-a
provided by the Graz University of Technology in BCI competition IV.
B. Dataset II-a in BCI Competition IV
1) Data Description and Preprocessing: These datasets
consisted of EEG data from nine subjects of four motor imagery tasks [31]. At the beginning of a trial (t = 0 s), a fixation
cross appeared on the black screen along with a short acoustic
warning tone. After 2 s (t = 2 s), a cue in the form of an arrow
pointing either to the left, right, down or up appeared and stayed
on the screen for 1.25 s. This prompted the subjects to perform
the desired motor imagery task. No feedback was provided. The
subjects were asked to carry out the motor imagery task until the
fixation cross disappeared from the screen at t = 6 s. A short
break followed and the screen was black again.

Two sessions are recorded on different days for each subject.
Each session is comprised of six runs separated by short breaks.
One run consists of 48 trials (12 for each class), yielding a total
of 288 trials per session. Only EEG trials for left hand (class 1)
and both feet (class 3) are analyzed in this study. Although two
sessions are recorded on two different days, they are combined
together to investigate the effect of different sizes of initial
training data items. In ideal situation, there are 288 trials for
two motor imagery tasks and for each subject. However, there
are different amounts of trials that are contaminated by artifacts;
hence, the available trials for the two specific tasks are different
for each subject. The EEG data used are sampled with 250 Hz
and bandpass filtered between 8 and 30 Hz. Since one channel
contains invalid signals in session two for some subjects, only
21 out of 22 EEG channels are used in the following analysis.
Signals in the time interval of [3, 6]s are analyzed for each trial.
p = 3 is used for dimensional reduction of CSP features.
2) Classification Results: In the following, the similar experimental results to those of dataset IV-a are presented.
a) Results for Experiment Protocol I: The complete experimental results for the first set of experiments are illustrated in
Fig. 6. Mean accuracies of 5 × 5-fold CV for the nine subjects
and for the seven values of M using the proposed algorithms
against the competing algorithms are plotted. Because some
subjects contain less than 225 valid trials without artifacts, only
7 values of M are chosen. Similarly, the comparative results are
divided into three groups, which are placed in the left, middle,
and right columns of Fig. 6, respectively. Clearly, the performance of all the algorithms is enhanced with more training
samples, which is consistent with the previous results and our
common belief. The proposed semisupervised algorithms especially for SBLDA and CoBLDA are particularly powerful
when M is small. Although their superiority against other competing algorithms decreases when M gets larger, SBLDA and
CoBLDA still give almost the best results when the size of M
gets 25 to 30, which is smaller than other competing algorithms
to obtain the same level of performance. The results are similar
to those in Fig. 4.
b) Results for Experiment Protocol II: The mean accuracies
of 5 × 5-fold CV on extended testing and independent testing
datasets are listed in the upper and lower parts of Table II, respectively. Paired Student’s t-test has been performed between
static classification and each of the algorithms listed in the table.
For each method and each subject, the statistically significant pvalue (<0.05) and p-value (<0.01) are marked with “∗ ” and “∗∗ ”,
respectively. For one subject “A06T”, the performance of proposed method CoBLDA is inferior to static classification. This
inferior performance is statistically significant (p-value < 0.01),
and is hence, marked with a red “∗ ”. Altogether, in this dataset,
SBLDA and CoBLDA perform better than the others and the
superiority compared to static classification is statistically significant for at least six out of nine subjects on both the extended
testing dataset and independent testing dataset. It is worth noting that there is more than 10% improvement on average for
these six subjects compared to static classification. Specifically,
the improvement of classification accuracy is almost 20% by
CoBLDA for subject “A02T” on extended testing dataset.

MENG et al.: IMPROVED SEMISUPERVISED ADAPTATION FOR A SMALL TRAINING DATASET IN THE BRAIN–COMPUTER INTERFACE

(a)

(b)

(c)

(d)

(e)

(f)

1469

Fig. 6. Illustration of the improvement achieved over several existing algorithms by the proposed improved semisupervised adaptation algorithms on dataset
II-a of BCI competition IV. The left column shows comparison among the proposed algorithms and the conventional algorithms. The middle column shows the
results of semisupervised adaptation. The right column gives the comparative results among the regularized approaches and the proposed SBLDA, CoBLDA.
The training trials in the first part of dataset vary from 10 to 40. 80% of testing dataset is used to expand the initial small training dataset (c = 80%). Mean
classification accuracies of 5 × 5-fold CV on the testing dataset and on the independent testing dataset are placed in the first row [(a)–(c)] and the second row
[(d)–(f)], respectively.

TABLE II
MEAN ACCURACIES OF 5 × 5-FOLD CV ON DATASET II-A OF BCI COMPETITION IV

Paired Student’s t-test has been performed between static classification and each of the comparative algorithms. Here, c = 80% is used. Statistically significant p-value (< 0.05)
and p-value (< 0.01) have been marked with “*” and “**”, respectively. The “*” with red color means the proposed method is inferior to the static classification.

1470

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

Similarly, paired t-test has been performed between
SBLDA/CoBLDA and SSVM/R-CSP-A on this specific dataset
(M = 10). There is no significant difference between SSVM
and R-CSP-A except that R-CSP-A performs significantly better than SSVM for two subjects “A04” and “A05”. Note that discrimination for subjects like “A04”, “A05,” and “A06” is hard
since their classification accuracy is lower than 70%, which
is a threshold of acceptable classification performance for the
BCI system [32]. Without considering these subjects, SBLDA
and CoBLDA are significantly superior to both SSVM and RCSP-A for at least three out of six subjects. Although R-CSP-A
performs better for those subjects whose separability is poor, the
performance of all three subjects (“A04”, “A05,” and “A06”) is
still lower than 60% even by R-CSP-A.
IV. DISCUSSION
A. Why SBLDA/CoBLDA (Bayesian Version)
Outperforms Other Algorithms?
From the results of the previous section, we can see SBLDA
and CoBLDA are stronger than other algorithms especially for
small training data sets. The underlying possible reason for
success of SBLDA and CoBLDA is further explored by biasvariance (BV) decomposition on the comparing algorithms.
Since binary classifications are made in motor imagery-based
BCI, the routinely applied BV decomposition approach proposed by Kohahi and Wolpert for zero-one loss function is
used [33]. In summary, this technique decomposes the expected
loss of one learning algorithm (under fixed training set size)
into three parts, i.e., bias2 , variance, and the intrinsic noise of
the target. Since the intrinsic noise is hard to know and hence is
elected to adopt the strategy of defining this noise to be zeros as
proposed in [33]. For a particular problem, the smaller the values
of bias and variance, the better the performance of the learning
algorithm. However, the predicted error of one specific learning
algorithm is restricted by the bias and variance dilemma.
The bar plots of bias and variance decomposition for comparing algorithms on different sizes of training data items are
illustrated in Fig. 7 (for dataset IV-a of BCI competition III).
For brevity, results on another dataset are not included here but
do not affect the major conclusions. Each bar in the plots is
derived based on 100 runs of learning algorithms and the bias
and variance is decomposed on the independent testing dataset.
As shown in the upper plot of Fig. 7, the bias of the proposed
SBLDA, CoBLDA, SSVM, and R-CSP-A is relatively larger
than the rest of comparative algorithms, indicating that “static,”
“onlyLDA,” STSEER, SIWEER, and TRCSP perform better
in term of bias. On the contrary, as shown in the lower panel
of Fig. 7, the variance of SBLDA and CoBLDA is relatively
smaller than all the other algorithms, indicating that SBLDA
and CoBLDA perform better in term of variance. Compared
to all the other algorithms, SBLDA and CoBLDA almost have
the lowest variance and only increase the bias slightly (This is
most prominent when the size of M is smaller than 40. The
bias of SBLDA, CoBLDA, SSVM, and R-CSP-A is relatively
larger compared to other algorithms after the size of M is larger
than 40, and this explains the decrease of performance for this

Fig. 7. Bar plots of bias2 (first row) and varaince (second row) decomposition
for Dataset IV-a of BCI competition III on the comparing algorithms. The
horizontal axis represents different sizes of initial training number (varying M ).
The vertical axis shows the value of bias2 and variance.

dataset on independent testing data from another aspect of view,
see Fig. 4, which has been mentioned previously). From the
previous analysis, we may conclude that SBLDA and CoBLDA
increase performance by decreasing the variance of learning
algorithms (due to the regularization of BLDA classifier and
reextract CSP transformation matrix) at the cost of increasing
the bias slightly.
B. Performance Deterioration
The deterioration of semisupervised adaptation algorithms
are observed in both of the two real datasets. For these subjects,
their separability is very low which means these datasets are difficult to be discriminated. Usually, their classification accuracies
are lower than 70%. There might be two reasons for the deterioration. First, the robustness of CSP features is based on the
assumption that the number of incorrectly labeled trials is relatively small. When the classification accuracy is very low, the
number of wrong labeled trials is inevitably large. This contradicts with the basic assumption of robustness of CSP features. In
this circumstance, reextract CSP features only by self-predicted
labels may not be helpful; since reextracting CSP transformation matrix by adding self-predicted labels plays a similar role
in regularizing CSP transformation matrix (recall Fig. 5). To
address this problem, a possible solution might be combining
class covariance information derived by self-predicted labels
together with additional regularization parameters like those
proposed in [11] and [12]. We will investigate this issue in the
future. Second, the classification performance is also influenced
by individual classifiers. That unlabeled data can degrade classification performance of model-based generative classifiers has
been reported by Cohen [34], [35]. LDA and BLDA are both
model-based classifiers, hence, when the distribution of features

MENG et al.: IMPROVED SEMISUPERVISED ADAPTATION FOR A SMALL TRAINING DATASET IN THE BRAIN–COMPUTER INTERFACE

deviate greatly from the proposed distribution of classifiers, the
deterioration may happen.
V. CONCLUSION
To reduce the training time while retaining accurate performance in real application is our main objective in this paper. The
proposed semisupervised adaptation algorithms employ both
self-training and cotraining style learning paradigm to reextract the CSP features and update the used classifiers simultaneously. Algorithms of Bayesian version including SBLDA and
CoBLDA perform better than the other two proposed algorithms
and all the competing algorithms throughout almost all the sizes
of initial training data. They enhance the classification accuracy by decreasing the variance of learning algorithms at the
cost of increasing the bias slightly. We would recommend that
training data can be reduced to as few as 30 trials by these two
algorithms, whereas another 20 trials are needed for the other
competing algorithms to achieve the same level of performance.
When the number of training trials is even smaller (<30), our
analysis in the experiments shows that the proposed SBLDA
and CoBLDA are particularly powerful and outperform all the
other algorithms. And for most of the subjects with acceptable
classification accuracies (>75%), the superiority of SBLDA
and CoBLDA is statistically significant (p-value < 0.01) compared to static classification and even statistically significant
(p-value < 0.05) compared to SSVM and R-CSP-A. This implies that the proposed SBLDA and CoBLDA may provide an
alternative way to reduce the training effort.
ACKNOWLEDGMENT
The authors would like to thank the Berlin BCI group and the
Graz BCI group for sharing their datasets. They would also like
to thank U. Hoffmann from EPFL for providing the MATLAB
code of p300soft. They appreciate helpful suggestions of H. Liu,
P. Shull, and the anonymous reviewers’ comments to improve
the quality of this paper.
REFERENCES
[1] G. Dornhege, B. Blankertz, G. Curio, and K. R. Müller, “Boosting bit rates
in noninvasive EEG single-trial classifications by feature combination and
multiclass paradigms,” IEEE Trans. Biomed. Eng., vol. 51, no. 6, pp. 993–
1002, Jun. 2004.
[2] M. Krauledat, M. Schroder, B. Blankertz, and K. R. Müller, “Reducing
calibration time for brain-computer interfaces: A clustering approach,”
Adv. Neural Inf. Process. Syst., vol. 19, pp. 753–760, 2007.
[3] Y. Li, H. Li, C. Guan, and Z. Chin, “A self-training semi-supervised
support vector machine algorithm and its applications in brain computer
interface,” in Proc. IEEE Int. Conf. Acoust., Speech Signal Process., 2007,
vol. 1, pp. 1385–1388.
[4] J. Wang, E. Pohlmeyer, B. Hanna, Y.-G. Jiang, P. Sajda, and S.-F. Chang,
“Brain state decoding for rapid image retrieval,” in Proc. 17th ACM Int.
Conf. Multimedia, 2009, pp. 945–954.
[5] P. Sajda, E. Pohlmeyer, J. Wang, L. C. Parra, C. Christoforou,
J. Dmochowski, B. Hanna, C. Bahlmann, M. K. Singh, and S.-F. Chang,
“In a blink of an eye and a switch of a transistor: Cortically coupled
computer vision,” Proc. IEEE, vol. 98, no. 3, pp. 462–478, Mar. 2010.
[6] X. Hu, F. Deng, K. Li, T. Zhang, H. Chen, X. Jiang, J. Lv, D. Zhu,
C. Faraco, D. Zhang, A. Mesbah, J. Han, X. Hua, L. Xie, S. Miller,
L. Guo, and T. Liu, “Bridging low-level features and high-level semantics
via fMRI brain imaging for video classification,” in Proc. ACM Int. Conf.
Multimedia, 2010, pp. 451–460.

1471

[7] G. Blanchard and B. Blankertz, “BCI competition 2003-data set IIa: Spatial patterns of self-controlled brain rhythm modulations,” IEEE Trans.
Biomed. Eng., vol. 51, no. 6, pp. 1062–1066, Jun. 2004.
[8] H. Ramoser, J. Müller-Gerking, and G. Pfurtscheller, “Optimal spatial
filtering of single trial EEG during imagined hand movement,” IEEE
Trans. Rehabil. Eng., vol. 8, no. 4, pp. 441–446, Dec. 2000.
[9] Y. Li and C. Guan, “An extended EM algorithm for joint feature extraction
and classification in brain-computer interfaces,” Neural Comput., vol. 18,
no. 11, pp. 2730–2761, 2006.
[10] H. Kang, Y. Nam, and S. Choi, “Composite common spatial pattern for
subject-to-subject transfer,” IEEE Signal Process. Lett., vol. 16, no. 8,
pp. 683–686, Aug. 2009.
[11] H. Lu, H. Eng, C. Guan, K. Plataniotis, and A. Venetsanopoulos, “Regularized common spatial pattern with aggregation for EEG classification
in small-sample setting,” IEEE Trans. Biomed. Eng., vol. 57, no. 12,
pp. 2936–2946, Dec. 2010.
[12] F. Lotte and C. Guan, “Regularizing common spatial patterns to improve
BCI designs: Unified theory and new algorithms,” IEEE Trans. Biomed.
Eng., vol. 58, no. 2, pp. 355–362, Feb. 2011.
[13] Y. Li and C. Guan, “A semi-supervised SVM learning algorithm for joint
feature extraction and classification in brain computer interfaces,” in Proc.
IEEE Annu. Int. Conf. Eng. Med. Biol. Soc., 2006, vol. 1, pp. 2570–2573.
[14] Y. Li and C. Guan, “Joint feature re-extraction and classification using
an iterative semi-supervised support vector machine algorithm,” Mach.
Learn., vol. 71, no. 1, pp. 33–53, 2008.
[15] W. Tu and S. Sun, “A subject transfer framework for EEG classification,”
Neurocomputing, vol. 82, pp. 109–116, 2012.
[16] W. Tu and S. Sun, “Semi-supervised feature extraction for EEG classification,” Pattern Anal. Appl., vol. 16, pp. 1–10, 2012.
[17] M. Zhang and Z. Zhou, “Cotrade: Confident co-training with data editing,”
IEEE Trans. Syst., Man, Cybern. B: Cybernet., vol. 41, no. 6, pp. 1612–
1626, Jun. 2011.
[18] D. Zhou, O. Bousquet, T. Lal, J. Weston, and B. Schölkopf, “Learning
with local and global consistency,” Adv. Neural Inf. Process. Syst., vol. 16,
pp. 321–328, 2004.
[19] R. Duda, P. Hart, and D. Stork, Pattern Classification. New York, NY,
USA: Wiley, 2001.
[20] U. Hoffmann, J. Vesin, T. Ebrahimi, and K. Diserens, “An efficient p300based brain-computer interface for disabled subjects,” J. Neurosci. Methods, vol. 167, no. 1, pp. 115–125, 2008.
[21] P. Xu, P. Yang, X. Lei, and D. Yao, “An enhanced probabilistic LDA for
multi-class brain computer interface,” PloS one, vol. 6, no. 1, p. e14634,
2011.
[22] C. Bishop, Pattern Recognition and Machine Learning. vol. 4, New
York, NY, USA: Springer-Verlag, 2006.
[23] A. Blum and T. Mitchell, “Combining labeled and unlabeled data with cotraining,” in Proc. ACM 11th Annu. Conf. Comput. Learn. Theory, 1998,
pp. 92–100.
[24] S. Goldman and Y. Zhou, “Enhancing supervised learning with unlabeled
data,” in Proc. 17th Conf. Mach. Learn., 2000, pp. 327–334.
[25] S. Sun and F. Jin, “Robust co-training,” Int. J. Pattern Recognit. Artif.
Intell., vol. 25, no. 7, pp. 1113–1126, 2011.
[26] F. F. I. D. A. Group and C. B. F. of the Charité University Medicine Berlin
Neurophysics Group. “BCI competition III data set IVa,” (2005). [Online].
Available: http://www.bbci.de/competition/iii/desc IVa.html
[27] G. Liu, G. Huang, J. Meng, D. Zhang, and X. Zhu, “Improved GMM with
parameter initialization for unsupervised adaptation of brain–computer
interface,” Int. J. Numer. Methods Biomed. Eng., vol. 26, no. 6, pp. 681–
691, 2010.
[28] C. Vidaurre, M. Kawanabe, P. von Bunau, B. Blankertz, and K. R. Müller,
“Toward unsupervised adaptation of LDA for brain–computer interfaces,”
IEEE Trans. Biomed. Eng., vol. 58, no. 3, pp. 587–597, Mar. 2011.
[29] G. Liu, D. Zhang, J. Meng, G. Huang, and X. Zhu, “Unsupervised adaptation of electroencephalogram signal processing based on fuzzy c-means
algorithm,” Int. J. Adaptive Control Signal Process., vol. 26, pp. 482–495,
2012.
[30] S. Sun, “The extreme energy ratio criterion for EEG feature extraction,”
in Artificial Neural Networks. New York, NY, USA: Springer, 2008,
pp. 919–928.
[31] G. U. O. T. Institute for Knowledge Discovery (Laboratory of BrainComputer Interfaces). “BCI competition IV data set IIa,” (2008). [Online].
Available: http://www.bbci.de/competition/iv/desc 2a.pdf
[32] G. Müller-Putz, R. Scherer, C. Brunner, R. Leeb, and G. Pfurtscheller,
“Better than random? A closer look on BCI results,” Int. J. Bioelectromagnet., vol. 10, no. 1, pp. 52–55, 2008.

1472

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

[33] R. Kohavi and D. Wolpert, “Bias plus variance decomposition for zero-one
loss functions,” in Proc. 13th Int. Conf. Mach. Learn., 1996, pp. 275–283.
[34] F. Cozman, I. Cohen, and M. Cirelo, “Unlabeled data can degrade classification performance of generative classifiers,” in Proc. 15th Int. Florida
Artif. Intell. Soc. Conf., 2002, pp. 327–331.
[35] I. Cohen, F. Cozman, N. Sebe, M. Cirelo, and T. Huang, “Semisupervised
learning of classifiers: Theory, algorithms, and their application to humancomputer interaction,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 26,
no. 12, pp. 1553–1566, Dec. 2004.

Jianjun Meng received the Bachelor’s degree from
the School of Mechanical Engineering, Shanghai Jiao
Tong University, Shanghai, China, in 2005, where he
is currently working toward the Ph.D. degree.
His research interests include BCI technology, EEG signal processing, feature extraction, and
classification.

Xinjun Sheng received the B.Sc. and M.Sc. degrees
in mechanical engineering from Shanghai Jiao Tong
University, Shanghai, China, in 2001 and 2003, respectively. Now, he is pursuing the Ph.D. degree in
the School of Mechanical Engineering, Shanghai Jiao
Tong University, China.
In 2012, he was a visiting scientist in Concordia University, Canada. He is currently a Lecturer in
the School of Mechanical Engineering at Shanghai
Jiao Tong University. His current research interests
include mechatronics and biorobotics.

Dingguo Zhang received the B.Eng. degree in electrical engineering from Jilin University, Changchun,
China, and the M.Eng. degree in control engineering from the Harbin Institute of Technology, Harbin,
China, in 2000 and 2002, respectively. He received
the Ph.D. degree from Nanyang Technological University, Nanyang Ave, Singapore, in 2007.
From 2006 to 2007, he was a Research Fellow in
Biorobtoics Lab of Nanyang Technological University. In 2008, he was a Postdoctoral Fellow in LIRMM
of CNRS, France. He is currently an Associate Professor in the Institute of Robotics at Shanghai Jiao Tong University, Shanghai,
China. His research interests include biorobotics, biomechatronics, biological
cybernetics, and BCI/EMG technique.

Xiangyang Zhu received the B.S. degree from the
Department of Automatic Control Engineering, Nanjing Institute of Technology, Nanjing, China, in 1985,
the Master’s degree in instrumentation engineering
and the Ph.D. degree in automatic control engineering both from Southeast University, Nanjing, China,
in 1989 and 1992, respectively.
He joined the Department of Mechanical Engineering, Southeast University in 1995, after two years
work as a Postdoctoral Fellow in Huazhong University of Science and Technology, Wuhan, China. Since
June 2002, he has been a Professor at Shanghai Jiao Tong University, Shanghai, China, with a joint appointment in the Robotics Institute and the State
Key Laboratory of Mechanical Systems and Vibrations. His current research interests include robotic manipulation planning, manufacturing automation, and
biomechatronics.
Dr. Zhu received the National Science Fund for Distinguished Young Scholars in 2005, and was appointed as a “Cheung Kong” Chair Professor in 2007.

