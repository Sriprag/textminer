IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 4, JULY 2015

1209

Big Data, Big Knowledge: Big Data
for Personalized Healthcare
Marco Viceconti, Peter Hunter, and Rod Hose

Abstract—The idea that the purely phenomenological knowledge that we can extract by analyzing large amounts of data can
be useful in healthcare seems to contradict the desire of VPH researchers to build detailed mechanistic models for individual patients. But in practice no model is ever entirely phenomenological
or entirely mechanistic. We propose in this position paper that big
data analytics can be successfully combined with VPH technologies to produce robust and effective in silico medicine solutions.
In order to do this, big data technologies must be further developed to cope with some specific requirements that emerge from
this application. Such requirements are: working with sensitive
data; analytics of complex and heterogeneous data spaces, including nontextual information; distributed data management under
security and performance constraints; specialized analytics to integrate bioinformatics and systems biology information with clinical
observations at tissue, organ and organisms scales; and specialized
analytics to define the “physiological envelope” during the daily
life of each patient. These domain-specific requirements suggest a
need for targeted funding, in which big data technologies for in
silico medicine becomes the research priority.
Index Terms—Big data, healthcare, virtual physiological human.

I. INTRODUCTION
HE birth of big data, as a concept if not as a term, is usually associated with a META Group report by Doug Laney
entitled “3-D Data Management: Controlling Data Volume,
Velocity, and Variety” published in 2001 [1]. Further developments now suggest big data problems are identified by the
so-called “5V”: volume (quantity of data), variety (data from
different categories), velocity (fast generation of new data), veracity (quality of the data), and value (in the data) [2].
For a long time the development of big data technologies was
inspired by business intelligence [3] and by big science (such
as the Large Hadron Collider at CERN) [4]. But when in 2009
Google Flu, simply by analyzing Google queries, predicted flulike illness rates as accurately as the CDC’s enormously complex
and expensive monitoring network [5], some analysts started to
claim that all problems of modern healthcare could be solved
by big data [6].
In 2005, the term virtual physiological human (VPH) was
introduced to indicate “a framework of methods and technologies that, once established, will make possible the collaborative

T

Manuscript received September 29, 2014; revised December 14, 2014; accepted February 6, 2015. Date of publication February 24, 2015; date of current
version July 23, 2015.
M. Viceconti is with the VPH Institute for Integrative Biomedical Research,
and the Insigneo Institute for In Silico Medicine, University of Sheffield,
Sheffield S1 3JD, U.K. (e-mail: m.viceconti@sheffield.ac.uk).
P. Hunter is with the Auckland Bioengineering Institute, University of
Auckland, 1010 Auckland, New Zealand (e-mail: p.hunter@auckland.ac.nz).
R. Hose is with the Insigneo institute for In Silico Medicine, University of
Sheffield, Sheffield S1 3JD, U.K. (e-mail: d.r.hose@sheffield.ac.uk).
Digital Object Identifier 10.1109/JBHI.2015.2406883

investigation of the human body as a single complex system”
[7], [8]. The idea was quite simple:
1) To reduce the complexity of living organisms, we decompose them into parts (cells, tissues, organs, organ systems)
and investigate one part in isolation from the others. This
approach has produced, for example, the medical specialties, where the nephrologist looks only at your kidneys,
and the dermatologist only at your skin; this makes it very
difficult to cope with multiorgan or systemic diseases, to
treat multiple diseases (so common in the ageing population), and in general to unravel systemic emergence due
to genotype-phenotype interactions.
2) But if we can recompose with computer models all the
data and all the knowledge we have obtained about each
part, we can use simulations to investigate how these parts
interact with one another, across space and time and across
organ systems.
Though this may be conceptually simple, the VPH vision
contains a tremendous challenge, namely, the development of
mathematical models capable of accurately predicting what will
happen to a biological system. To tackle this huge challenge,
multifaceted research is necessary: around medical imaging
and sensing technologies (to produce quantitative data about
the patient’s anatomy and physiology) [9]–[11], data processing to extract from such data information that in some cases
is not immediately available [12]–[14], biomedical modeling
to capture the available knowledge into predictive simulations
[15], [16], and computational science and engineering to run
huge hypermodels (orchestrations of multiple models) under
the operational conditions imposed by clinical usage [17]–[19];
see also the special issue entirely dedicated to multiscale
modeling [20].
But the real challenge is the production of that mechanistic knowledge, quantitative, and defined over space, time and
across multiple space-time scales, capable of being predictive
with sufficient accuracy. After ten years of research this has produced a complex impact scenario in which a number of target
applications, where such knowledge was already available, are
now being tested clinically; some examples of VPH applications
that reached the clinical assessment stage are:
1) The VPHOP consortium developed a multiscale modeling technology based on conventional diagnostic imaging
methods that makes it possible, in a clinical setting, to
predict for each patient the strength of their bones, how
this strength is likely to change over time, and the probability that they will overload their bones during daily life.
With these three predictions, the evaluation of the absolute
risk of bone fracture in patients affected by osteoporosis
will be much more accurate than any prediction based on

2168-2194 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1210

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 4, JULY 2015

external and indirect determinants, as it is in current clinical practice [21].
2) More than 500 000 end-stage renal disease patients in
Europe live on chronic intermittent haemodialysis treatment. A successful treatment critically depends on a wellfunctioning vascular access, a surgically created arteriovenous shunt used to connect the patient circulation to
the artificial kidney. The ARCH project aimed to improve
the outcome of vascular access creation and long-term
function with an image-based, patient-specific computational modeling approach. ARCH developed patientspecific computational models for vascular surgery that
makes possible to plan such surgery in advance on the
basis of the patient’s data, and obtain a prediction of the
vascular access function outcome, allowing an optimization of the surgical procedure and a reduction of associated
complications such as nonmaturation. A prospective study
is currently running, coordinated by the Mario Negri Institute in Italy. Preliminary results on 63 patients confirm
the efficacy of this technology [22].
3) Percutaneous coronary intervention (PCI) guided by fractional flow reserve (FFR) is superior to standard assessment alone to treat coronaries stenosis. FFR-guided PCI
results in improved clinical outcomes, a reduction in the
number of stents implanted, and reduced cost. However,
currently FFR is used in few patients, because it is invasive and it requires special instrumentation. A less invasive FFR would be a valuable tool. The VirtuHeart project
developed a patient-specific computer model that accurately predicts myocardial FFR from angiographic images alone, in patients with coronary artery disease. In a
phase 1 study the methods showed an accuracy of 97%,
when compared to standard FFR [23]. A similar approach,
but based on computed tomography imaging, is even at a
more advanced stage, having recently completed a phase 2
trial [24].
While these and some other VPH projects have reached the
clinical assessment stage, quite a few other projects are still in
the technological development, or preclinical assessment phase.
But in some cases the mechanistic knowledge currently available
simply turned out to be insufficient to develop clinically relevant
models.
So it is perhaps not surprising that recently, especially in the
area of personalized healthcare (so promising but so challenging) some people have started to advocate the use of big data
technologies as an alternative approach, in order to reduce the
complexity that developing a reliable, quantitative mechanistic
knowledge involves.
This trend is fascinating from an epistemological point of
view. The VPH was born around the need to overcome the
limitations of a biology founded on the collection of a huge
amount of observational data, frequently affected by considerable noise, and boxed into a radical reductionism that prevented
most researchers from looking at anything bigger than a single
cell [25], [26]. Suggesting that we revert to a phenomenological
approach where a predictive model is supposed to emerge not
from mechanistic theories but by only doing high-dimensional

big data analysis, may be perceived by some as a step toward
that empiricism the VPH was created to overcome.
In the following, we will explain why the use of big data methods and technologies could actually empower and strengthen
current VPH approaches, increasing considerably its chances of
clinical impact in many “difficult” targets. But in order for that
to happen, it is important that big data researchers are aware that
when used in the context of computational biomedicine, big data
methods need to cope with a number of hurdles that are specific
to the domain. Only by developing a research agenda for big
data in computational biomedicine can we hope to achieve this
ambitious goal.
II. DOCTORS AND ENGINEERS: JOINED AT THE HIP
As engineers who have worked for many years in research
hospitals, we recognize that clinical and engineering researchers
share a similar mind-set. Both in traditional engineering and in
medicine, the research domain is defined in terms of problem
solving, not of knowledge discovery. The motto common to both
disciplines is “whatever works.”
But there is a fundamental difference: Engineers usually deal
with problems related to phenomena on which there is a large
body of reliable knowledge from physics and chemistry. When
a good reliable mechanistic theory is not available, engineers
resort to empirical models, as far as they can solve the problem
at hand. But when they do this, they are left with a sense of
fragility and mistrust, and they try to replace them as soon as
possible with theory-based mechanistic models, which are both
predictive and explanatory.
Medical researchers deal with problems for which there is
a much less well-established body of knowledge; in addition,
this knowledge is frequently qualitative or semiquantitative, and
obtained in highly controlled experiments quite removed from
clinical reality, in order to tame the complexity involved. Thus,
not surprisingly, many clinical researchers consider mechanistic
models “too simple to be trusted,” and in general the whole idea
of a mechanistic model is looked upon with suspicion.
But in the end “whatever works” remains the basic principle.
In some VPH clinical target areas where we can prove convincingly that our mechanistic models can provide more accurate predictions than the epidemiology-based phenomenological
models, the penetration into clinical practice is happening. On
the other hand, when our mechanistic knowledge is insufficient,
the predictive accuracy of our models is poor, and models based
on empirical/statistical evidences are still preferred.
The true problem behind this story is the competition between
two methods of modeling nature that are both effective in certain
cases. Big data can help computational biomedicine to transform
this competition into collaboration, significantly increasing the
acceptance of VPH technologies in clinical practice.
III. BIG DATA VPH: AN EXAMPLE IN OSTEOPOROSIS
In order to illustrate this concept we will use as a guiding
example the problem of predicting the risk of bone fracture in a
woman affected by osteoporosis, a pathological reduction of her
bone mineralized mass [27]. The goal is to develop predictors

VICECONTI et al.: BIG DATA, BIG KNOWLEDGE: BIG DATA FOR PERSONALIZED HEALTHCARE

that indicate whether the patient is likely to fracture over a given
time (typically in the following ten years). If a fracture actually
occurs in that period, this is the true value used to decide if the
outcome prediction was right or wrong.
Because the primary manifestation of the disease is quite
simple (reduction of the mineral density of the bone tissue) not
surprisingly researchers found that when such mineral density
could be accurately measured in the regions where the most
disabling fractures occurred (hip and spine), such measurement
was a predictor of the risk of fracture [28]. In controlled clinical
trials, where patients are recruited to exclude all confounding
factors, the bone mineral density (BMD) strongly correlated
with the occurrence of hip fractures. Unfortunately, when BMD
is used as a predictor, especially over randomized populations,
the accuracy drops to 60–65% [29]. Given that fracture is a
binary event, tossing a coin would give us 50%, so this is considered not good enough.
Epidemiologists run huge international, multicentre clinical
trials where the fracture events are related to a number of observables; the data are then fitted with statistical models that provide
phenomenological models capable of predicting the likelihood
of fracture; the most famous, called FRAX, was developed by
John Kanis at the University of Sheffield, U.K., and is considered
by the World Health Organisation the reference tool to predict
risk of fractures. The predictive accuracy of FRAX is comparable to that of BMD, but it seems more robust for randomised
female cohorts [30].
In the VPHOP project, one of the flagship VPH projects
funded by the Seventh Framework Program of the European
Commission, we took a different approach: We developed a
multiscale patient-specific model informed by medical imaging
and wearable sensors, and used this model to predict the actual
risk of fracture of the hip and at the spine, essentially simulating
ten years of the patient’s daily life [18]. The results of the first
clinical assessment, published only a few weeks ago, suggest
that the VPHOP approach could increase the predictive accuracy
to 80–85% [31]. Significant but not dramatic: No information
is available yet on the accuracy with fully randomized cohorts,
although we expect the mechanistic model to be less sensitive
to biases.
The goal of the VPHOP project was to replace FRAX; in doing this, the research consortium took a considerable risk, common to most VPH projects, when a radically new and complex
technology aims to replace an established standard of care. The
difficulty arises from the need to step into the unknown, with the
outcome remaining unpredictable until the work is complete. In
our opinion big data technologies could change this high-risk
scenario, allowing a progressive approach to modeling where
predictions are initially generated only using the available data,
and then progressively a priori knowledge is introduced about
the physiology and the specific disease, captured into mechanistic predictive models.
FRAX uses Poisson processes to define an epidemiological
predictor for the risk of bone fracture in osteoporotic patients,
which consider only the BMD and a few personal and clinical
information on the patient [32]. It has already been proposed
that this could be extended to include also data related to the

1211

propensity to fall, such as stability tests, or wearable sensor
recordings [33]. On the other hand, a vital piece of this puzzle
is the ability of a patient’s bone (as depicted in CT scan dataset)
to withstand a given mechanical load without fracturing; this is
something we can predict mechanistically with great accuracy
[34]. The future are of technologies that make possible (and
easy) to combine statistical, population-based knowledge with
mechanistic, patient-specific knowledge; in the case at hand, we
could keep a stochastic representation of the fall, and of the
resulting load, and model mechanistically the fracture event in
itself.
Can this example be considered a big data problem, in the
light of the “5V” definition?
1) Volume: This is probably the big data criterion that current VPH research fits least well. Although the community
wishes to exploit the vast entirety of clinical data records,
often there is simply not the level of detail, or depth, that
supports the association of parameters in the mechanistic
models with the data in the clinical record. The datasets
that support these analyses are often very expensive to
acquire, and currently the penetration is limited. Nevertheless, this is an important area of research, in which the
VPH community could learn from, and exploit, existing
technology from the big data community.
2) Variety: The variety is very high. In the example at hand
we would have clinical data, data from medical imaging,
data from wearable sensors, lab exams, and simulation
results. This would include both structured and nonstructured data, with 3-D imaging posing specific problems of
data treatment such as automated voxel classification (see
as examples [35]–[37]).
3) Velocity: Osteoporosis is a chronic condition; as such all
patients are expected to undergo a full specialist control
every two years, where the totality of the examinations
is repeated. Regarding growth rate: If to this we add that
the ageing of the population is constantly increasing the
number of patients affected, we face growth rates in the
order of 55–60% every year.
4) Veracity: Here there is a big divide between clinical research and clinical practice. While data collected as part
of clinical studies are in general of good quality, clinical
practice tends to generate low quality data. This is due in
part to the extreme pressure medical professionals face,
but also to a lack of “data value” culture; most medical
professionals see the logging of data a bureaucratic need
and a waste of time that distracts them from the care of
their patients.
5) Value: The potential value associated with these data is
very high. The cost of osteoporosis, including pharmacological intervention in the EU in 2010 was estimated at
€37 billion [38]. Moreover, in general, healthcare expenditure in most developed countries is astronomical: the
2013/2014 budget for NHS England was £95.6 billion,
with an increase over the previous year of 2.6%, at a time
when all public services in the UK are facing hard cuts.
In OECD countries, we spend on average USD$3395 per
year per inhabitant in healthcare (source: OECD 2011).

1212

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 4, JULY 2015

But the real value of big data analytics in healthcare still
remains to be proven. We believe this is largely due to
the need for much more sophisticated analytics, which
incorporate a priori knowledge of pathophysiology; this
is exactly what the VPH has to offer to big data analytics
in healthcare.
IV. FROM DATA TO THEORY: A CONTINUUM
Modern big data technologies make it possible in a short time
to analyze a large collection of data from thousands of patients,
identify clusters and correlations, and develop predictive models
using statistical or machine-learning modeling techniques [39],
[40]. In this new context it would be feasible to take all the
data collected in all past epidemiology studies - for example,
those used to develop FRAX - and continue to enrich them with
new studies where not only new patients are added, but different
types of information are collected.
Another mechanism that in principle very high-throughput
technologies make viable for exploration is the normalization
of digital medical images to conventional space-time reference
systems, using elastic registration methods [41]–[43], followed
by the treatment of the quantities expressed by each voxel value
in the image as independent data quanta. The voxel values of
the scan then become another medical dataset, potentially to be
correlated with average blood pressure, body weight, age, or
any other clinical information.
Using statistical modeling or machine learning techniques we
may obtain good predictors valid for the range of the datasets
analyzed; if a database contains outcome observables for a subset of patients, we will be able to compute automatically the
accuracy of such a predictor. Typically the result of this process would be a potential clinical tool with known accuracy; in
some cases the result would provide a predictive accuracy sufficient for clinical purposes, in others a higher accuracy might
be desirable.
In some cases there is need for an explanatory theory, which
answers the “how” question, and which may be used in a wider
context than that a statistical model normally is. As a second
step, one could use the correlation identified by the empirical
modeling to elaborate possible mechanistic theories. Given that
the available mechanistic knowledge is quite incomplete, in
many cases we will be able to express a mathematical model
only for a part of the process to be modeled; various “grey-box”
modeling methods have been developed in the last few years
that allow one to combine partial mechanistic knowledge with
phenomenological modeling [44].
The last step is where physiology, biochemistry, biomechanics, and biophysics mechanistic models are used. These models
contain a large amount of validated knowledge, and require only
a relatively small amount of patient-specific data to be properly
identified.
In many cases these mechanistic models are extremely expensive in terms of computational cost; therefore input-output
sets of these models may also be stored in a data repository in
order to identify reduced-order models (also referred as “surrogate” models and “meta-models”) that accurately replace a

computationally expensive model with a cheaper/faster simulation [45]. Experimental design methods are used to choose
the input and output parameters or variables with which to run
the mechanistic model in order to generate the meta-model’s
state space description of the input-output relations—which is
often replaced with a piecewise partial least-squares regression approximation [46]. Another approach is to use Nonlinear
AutoRegressive Moving Average model with eXogenous inputs
in the framework of nonlinear systems identification [47].
It is interesting to note that no real model is ever fully “whitebox.” In all cases, some phenomenological modeling is required
to define the interaction of the portion of reality under investigation with the rest of the universe. If we accept that a model
describes a process at a certain characteristic space-time scale,
everything that happens at any scale larger or smaller than that
must also be accounted for phenomenologically. Thus, it is possible to imagine a complex process being modeled as an orchestration of submodels, each predicting a part of the process (for
example at different scales), and we can expect that, while initially all submodels will be phenomenological, more and more
will progressively include some mechanistic knowledge.
The idea of a progressive increase of the explanatory content
of a hypermodel is not fundamentally new; other domains of
science already pursued the approach described here. But in
the context of computational biomedicine this is an approach
used only incidentally, and not as a systematic strategy for the
progressive refinement of clinical predictors.
V. BIG DATA FOR COMPUTATIONAL
BIOMEDICINE: REQUIREMENTS
In a complex scenario such as the one described above, are
the currently available technologies sufficient to cope with this
application context?
The brief answer is no. A number of shortcomings that need
to be addressed before big data technologies can be effectively
and extensively used in computational biomedicine. Here we
list five of the most important.
A. Confidential Data
The majority of big data applications deal with data that do
not refer to an individual person. This does not exclude the
possibility that their aggregated information content might not
be socially sensitive, but very rarely is it possible to reconnect
such content to the identity of an individual.
In the cases where sensitive data are involved, it is usually
possible to collect and analyze the data at a single location; so
this becomes a problem of computer security; within the secure
box, the treatment of the data is identical to that of nonsensitive
data.
Healthcare poses some peculiar problems in this area. First,
all medical data are highly sensitive, and in many developed
countries are considered legally owned by the patient, and the
healthcare provider is required to respect patient confidentiality.
The European parliament is currently involved in a complex
debate about data protection legislation, where the need for

VICECONTI et al.: BIG DATA, BIG KNOWLEDGE: BIG DATA FOR PERSONALIZED HEALTHCARE

individual confidentiality can be in conflict with the needs of
society [48].
Second, in order to be useful for diagnosis, prognosis or
treatment planning purposes the data analytics results must
in most cases be relinked to the identity of the patient. This
implies that the clinical data cannot be fully and irreversibly
anonymized before leaving the hospital, but requires complex
pseudoanonymization procedures. Normally the clinical data are
pseudoanonymized so as to ensure a certain k-anonymity [49],
which is considered legally and ethically acceptable. But when
the data are, as part of big data mash-ups, relinked to other data,
for example from social networks or other public sources, there
is a risk that the k-anonymity of the mash-up can be drastically
reduced. Specific algorithms need to be developed that prevent
such data aggregation when the k-anonymity could drop below
the required level.
B. Big Data: Big Size or Big Complexity?
Consider two data collections:
1) In one case, we have 500 TB of log data from a popular
web site: a huge list of text strings, typically encoding
7–10 pieces of information for transaction.
2) In the other, we have a full VPHOP dataset for 100 patients, a total of 1 TB; for each patient, we have 122 textual
information items that encode the clinical data, three medical imaging datasets of different types, 100 signal files
from wearable sensors, a neuromuscular dynamics output database, an organ-level model with the results, and a
tissue-scale model with the predictions of bone remodeling over ten years. This is a typical VPH data folder; some
applications require even more complex data spaces.
Which one of these two data collections should be considered
big data? We suggest that the idea held by some funding agencies, that the only worthwhile applications are those targeting
data collections over a certain size, trivializes the problem of
big data analytics. While the legacy role of big data analysis is
the examination of large amounts of scarcely complex data, the
future lies in the analysis of complex data, eventually even in
smaller amounts.
C. Integrating Bioinformatics, Systems Biology,
and Phenomics Data
Genomics and postgenomics technologies produce very large
amounts of raw data about the complex biochemical processes
that regulate each living organism; nowadays a single deepsequencing dataset can exceed 1 TB [50]. More recently, we
have started to see the generation of “deep phenotyping” data,
where biochemical, imaging, and sensing technologies are used
to quantify complex phenotypical traits and link them to the genetic information [51]. These data are processed with specialised
big data analytics techniques, which come from bioinformatics,
but recently there is growing interest in building mechanistic
models of how the many species present inside a cell interact
along complex biochemical pathways. Because of the complexity and the redundancy involved, linking this very large body
of mechanistic knowledge to the higher-order cell-cell and cell-

1213

tissue interactions remains very difficult, primarily for the data
analytics problems it involves. But when this is possible, genomics research results finally link to clinically relevant pathological signs, observed at tissue, organ, and organism scales,
opening the door to a true systems medicine.
D. Where are the Data?
In big data research, the data are usually stored and organised
in order to maximize the efficiency of the data analytics process.
In the scenario described here, however, it is possible that parts
of the simulation workflow require special hardware, or can be
run only on certain computers because of licence limitations.
Thus, one ends up trading the needs of the data analytics part
with those of the VPH simulation part, always ending up with a
suboptimal solution.
In such complex simulation scenarios, data management becomes part of the simulation process; clever methods must be
developed to replicate/store certain portions of the data within
organisations and at locations that maximize the performance
of the overall simulation.
E. Physiological Envelope and the Predictive Avatar
In the last decade, there has been a great deal of interest in the
generation and analysis of patient-specific models. Enormous
progress has been made in the integration of image processing and engineering analysis, with many applications in healthcare across the spectrum from orthopaedics to cardiovascular
systems and often multiscale models of disease processes, including cancer, are included in these analyses. Very efficient
methods, and associated workflows, have been developed that
support the generation of patient-specific anatomical models
based on exquisite three and four-dimensional medical images
[52], [53]. The major challenge now is to use these models
to predict acute and longer-term physiological and biological
changes that will occur under the progression of disease and
under candidate interventions, whether pharmacological or surgical. There is a wealth of data in the clinical record that could
support this, but its transformation into relevant information is
enormously difficult.
All engineering models of human organ systems, whether focused on structural or fluid flow applications, require not only
the geometry (the anatomy) but also constitutive equations and
boundary conditions. The biomedical engineering community
is only beginning to learn how to perform truly personalized
analysis, in which these parameters are all based on individual
physiology. There are many challenges around the interpretation
of the data that is collected in the course of routine clinical investigation, or indeed assembled in the Electronic Health Record
or Personal Health Records. Is it possible to predict the threat or
challenge conditions (e.g., limits of blood pressure, flow waveforms, joint loads), and their frequency or duration, from the
data that is collected? How can the physiological envelope of
the individual be described and characterized? How many analyses need to be done to characterize the effect of the physiological envelope on the progression of disease or on the effectiveness of treatment? How are these analyses best formulated

1214

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 4, JULY 2015

and executed computationally? How is information on disease
interpreted in terms of physiology? As an example, how (quantitatively) should we adapt a patient-specific multiscale model
of coronary artery disease to reflect the likelihood that a diabetic patient has impaired coronary microcirculation? At a
more generic level, how can the priors (in terms of physical
relationships) that are available from engineering analysis be
integrated into machine learning operations in the context of
digital healthcare, or alternatively how can machine learning be
used to characterize the physiological envelope to support meaningful diagnostic and prognostic patient-specific analyses? For
a simple example, consider material properties: Arteries stiffen
as an individual ages, but diseases such as moyamoya syndrome
can also dramatically affect arterial stiffness; how should models be modified to take into account such incidental data entries
in the patient record?
VI. CONCLUSION
Although sometimes overhyped, big data technologies
do have great potential in the domain of computational
biomedicine, but their development should take place in combination with other modeling strategies, and not in competition.
This will minimize the risk of research investments, and will
ensure a constant improvement of in silico medicine, favoring
its clinical adoption.
We have described five major problems that we believe need
to be tackled in order to have an effective integration of big data
analytics and VPH modeling in healthcare. For some of these
problems there is already an intense on-going research activity,
which is comforting.
For many years the high-performance computing world was
afflicted by a one-size-fits-all mentality that prevented many
research domains from fully exploiting the potential of these
technologies; more recently the promotion of centres of excellence, etc., targeting specific application domains, demonstrates
that the original strategy was a mistake, and that technological
research must be conducted at least in part in the context of each
application domain.
It is very important that the big data research community
does not repeat the same mistake. While there is clearly an important research space examining the fundamental methods and
technologies for big data analytics, it is vital to acknowledge
that it is also necessary to fund domain-targeted research that
allows specialized solutions to be developed for specific applications. Healthcare, in general, and computational biomedicine,
in particular, seems a natural candidate for this.
REFERENCES
[1] D. Laney, “3D data management: Controlling data volume, velocity, and
variety,” META Group, 2001.
[2] O. Terzo, P. Ruiu, E. Bucci, and F. Xhafa, “Data as a service (DaaS) for
sharing and processing of large data collections in the cloud,” in Proc. Int.
Conf. Complex Intell. Softw. Intensive Syst., 2013, pp. 475–480.
[3] B. Wixom, T. Ariyachandra, D. Douglas, M. Goul, B. Gupta, L. Iyer,
U. Kulkarni, J. G. Mooney, G. Phillips-Wren, and O. Turetken, “The
current state of business intelligence in academia: The arrival of big data,”
Commun. Assoc. Inform. Syst., vol. 34, no. 1, p. 1, 2014.

[4] A. Wright, “Big data meets big science,” Commun. ACM, vol. 57, no. 7,
pp. 13–15, 2014.
[5] J. Ginsberg, M. H. Mohebbi, R. S. Patel, L. Brammer, M. S. Smolinski,
and L. Brilliant, “Detecting influenza epidemics using search engine query
data,” Nature, vol. 457, no. 7232, pp. 1012–1014, 2009.
[6] J. Manyika, M. Chui, B. Brown, J. Bughin, R. Dobbs, C. Roxburgh, and
A. H. Byers, “Big data: The next frontier for innovation, competition, and
productivity,” Mckinsey Global Inst., 2011.
[7] J. W. Fenner, B. Brook, G. Clapworthy, P. V. Coveney, V. Feipel,
H. Gregersen, D. R. Hose, P. Kohl, P. Lawford, K. M. McCormack,
D. Pinney, S. R. Thomas, S. Van Sint Jan, S. Waters, and M. Viceconti,
“The EuroPhysiome, STEP and a roadmap for the virtual physiological human,” Philos. Trans. A Math. Phys. Eng. Sci., vol. 366, no. 1878,
pp. 2979–99, Sep. 2008.
[8] (2014). Virtual physiological human [Online]. Available: http://en.
wikipedia.org/w/index.php?title=Virtual_Physiological_Human&oldid=
626773628
[9] F. C. Horn, B. A. Tahir, N. J. Stewart, G. J. Collier, G. Norquay,
G. Leung, R. H. Ireland, J. Parra-Robles, H. Marshall, and J. M. Wild,
“Lung ventilation volumetry with same-breath acquisition of hyperpolarized gas and proton MRI,” NMR Biomed., vol. 27, pp. 1461–1467, Sep.
2014.
[10] A. Brandts, J. J. Westenberg, M. J. Versluis, L. J. Kroft, N. B. Smith,
A. G. Webb, and A. de Roos, “Quantitative assessment of left ventricular
function in humans at 7 T,” Magn. Reson. Med., vol. 64, no. 5, pp. 1471–
1477, Nov. 2010.
[11] E. Schileo, E. Dall’ara, F. Taddei, A. Malandrino, T. Schotkamp,
M. Baleani, and M. Viceconti, “An accurate estimation of bone density improves the accuracy of subject-specific finite element models,”
J. Biomech., vol. 41, no. 11, pp. 2483–91, Aug. 2008.
[12] L. Grassi, N. Hraiech, E. Schileo, M. Ansaloni, M. Rochette, and
M. Viceconti, “Evaluation of the generality and accuracy of a new mesh
morphing procedure for the human femur,” Med. Eng. Phys., vol. 33,
no. 1, pp. 112–20, 2011.
[13] P. Lamata, S. Niederer, D. Barber, D. Norsletten, J. Lee, R. Hose, and
N. Smith, “Personalization of cubic Hermite meshes for efficient biomechanical simulations,” Med. Image. Comput. Comput. Assist. Interv.,
vol. 13, no. Pt 2, pp. 380–387, 2010.
[14] S. Hammer, A. Jeays, P. L. Allan, R. Hose, D. Barber, W. J. Easson, and
P. R. Hoskins, “Acquisition of 3-D arterial geometries and integration with
computational fluid dynamics,” Ultrasound Med. Biol., vol. 35, no. 12,
pp. 2069–2083, Dec. 2009.
[15] A. Narracott, S. Smith, P. Lawford, H. Liu, R. Himeno, I. Wilkinson,
P. Griffiths, and R. Hose, “Development and validation of models for the investigation of blood clotting in idealized stenoses and cerebral aneurysms,”
J. Artif. Organs, vol. 8, no. 1, pp. 56–62, 2005.
[16] P. Lio, N. Paoletti, M. A. Moni, K. Atwell, E. Merelli, and M. Viceconti,
“Modeling osteomyelitis,” BMC Bioinformatics, vol. 13, p. S12,
2012.
[17] D. J. Evans, P. V. Lawford, J. Gunn, D. Walker, D. R. Hose, R. H.
Smallwood, B. Chopard, M. Krafczyk, J. Bernsdorf, and A. Hoekstra,
“The application of multiscale modelling to the process of development
and prevention of stenosis in a stented coronary artery,” Philos. Trans. A
Math. Phys. Eng. Sci., vol. 366, no. 1879, pp. 3343–3360, Sep. 2008.
[18] M. Viceconti, F. Taddei, L. Cristofolini, S. Martelli, C. Falcinelli, and
E. Schileo, “Are spontaneous fractures possible? An example of clinical
application for personalised, multiscale neuro-musculo-skeletal modeling,” J. Biomech., vol. 45, no. 3, pp. 421–426, Feb. 2012.
[19] V. B. Shim, P. J. Hunter, P. Pivonka, and J. W. Fernandez, “A multiscale
framework based on the physiome markup languages for exploring the
initiation of osteoarthritis at the bone-cartilage interface,” IEEE Trans.
Biomed. Eng., vol. 58, no. 12, pp. 3532–3536, Dec. 2011.
[20] S. Karabasov, D. Nerukh, A. Hoekstra, B. Chopard, and P. V. Coveney,
“Multiscale modeling: Approaches and challenges,” Philos. Trans. A
Math. Phys. Eng. Sci., vol. 372, Aug. 2014.
[21] M. Viceconti, Multiscale Modeling of the Skeletal System. Cambridge,
U.K.: Cambridge Univ. Press, 2011.
[22] A. Caroli, S. Manini, L. Antiga, K. Passera, B. Ene-Iordache, S. Rota,
G. Remuzzi, A. Bode, J. Leermakers, F. N. van de Vosse, R. Vanholder,
M. Malovrh, J. Tordoir, A. Remuzzi, and A. p. Consortium, “Validation of
a patient-specific hemodynamic computational model for surgical planning of vascular access in hemodialysis patients,” Kidney Int., vol. 84,
no. 6, pp. 1237–1245, Dec. 2013.
[23] P. D. Morris, D. Ryan, A. C. Morton, R. Lycett, P. V. Lawford, D. R. Hose,
and J. P. Gunn, “Virtual fractional flow reserve from coronary angiography:

VICECONTI et al.: BIG DATA, BIG KNOWLEDGE: BIG DATA FOR PERSONALIZED HEALTHCARE

[24]

[25]
[26]
[27]
[28]
[29]
[30]

[31]

[32]
[33]
[34]

[35]

[36]

[37]

Modeling the significance of coronary lesions: Results from the VIRTU-1
(virtual fractional flow reserve from coronary angiography) study,” JACC
Cardiovasc. Interv., vol. 6, no. 2, pp. 149–157, Feb. 2013.
B. L. Norgaard, J. Leipsic, S. Gaur, S. Seneviratne, B. S. Ko, H. Ito,
J. M. Jensen, L. Mauri, B. De Bruyne, H. Bezerra, K. Osawa,
M. Marwan, C. Naber, A. Erglis, S. J. Park, E. H. Christiansen, A. Kaltoft,
J. F. Lassen, H. E. Botker, S. Achenbach, and N. X. T. T. S. Group, “Diagnostic performance of noninvasive fractional flow reserve derived from
coronary computed tomography angiography in suspected coronary artery
disease: The NXT trial (analysis of coronary blood flow using CT angiography: Next steps),” J. Am. Coll. Cardiol., vol. 63, no. 12, pp. 1145–1155,
Apr. 2014.
D. Noble, “Genes and causation,” Philos. Trans. A Math. Phys. Eng. Sci.,
vol. 366, no. 1878, pp. 3001–3015, Sep. 2008.
D. Noble, E. Jablonka, M. J. Joyner, G. B. Muller, and S. W. Omholt,
“Evolution evolves: Physiology returns to centre stage,” J. Physiol.,
vol. 592, no. Pt 11, pp. 2237–2244, Jun. 2014.
O. Johnell and J. A. Kanis, “An estimate of the worldwide prevalence and
disability associated with osteoporotic fractures,” Osteoporos Int., vol. 17,
no. 12, pp. 1726–1733, Dec. 2006.
P. Lips, “Epidemiology and predictors of fractures associated with osteoporosis,” Am. J. Med., vol. 103, no. 2A, pp. 3S–8S, Aug. 1997.
M. L. Bouxsein and E. Seeman, “Quantifying the material and structural
determinants of bone strength,” Best Pract. Res. Clin. Rheumatol., vol. 23,
no. 6, pp. 741–753, Dec. 2009.
S. K. Sandhu, N. D. Nguyen, J. R. Center, N. A. Pocock, J. A. Eisman, and
T. V. Nguyen, “Prognosis of fracture: Evaluation of predictive accuracy
of the FRAX algorithm and garvan nomogram,” Osteoporos Int., vol. 21,
no. 5, pp. 863–871, May 2010.
C. Falcinelli, E. Schileo, L. Balistreri, F. Baruffaldi, B. Bordini,
M. Viceconti, U. Albisinni, F. Ceccarelli, L. Milandri, A. Toni, and
F. Taddei, “Multiple loading conditions analysis can improve the association between finite element bone strength estimates and proximal femur fractures: A preliminary study in elderly women,” Bone, vol. 67,
pp. 71–80, Oct. 2014.
E. V. McCloskey, H. Johansson, A. Oden, and J. A. Kanis, “From relative
risk to absolute fracture risk calculation: The FRAX algorithm,” Curr.
Osteoporos. Rep., vol. 7, no. 3, pp. 77–83, Sep. 2009.
Z. Schechner, G. Luo, J. J. Kaufman, and R. S. Siffert, “A poisson process
model for hip fracture risk,” Med. Biol. Eng. Comput., vol. 48, no. 8,
pp. 799–810, Aug. 2010.
L. Grassi, E. Schileo, F. Taddei, L. Zani, M. Juszczyk, L. Cristofolini,
and M. Viceconti, “Accuracy of finite element predictions in sideways
load configurations for the proximal human femur,” J. Biomech., vol. 45,
no. 2, pp. 394–399, Jan. 2012.
A. Karlsson, J. Rosander, T. Romu, J. Tallberg, A. Gronqvist, M. Borga,
and O. Dahlqvist Leinhard, “Automatic and quantitative assessment of
regional muscle volume by multi-atlas segmentation using whole-body
water-fat MRI,” J. Magn. Reson. Imag., Aug. 2014.
D. Schmitter, A. Roche, B. Marechal, D. Ribes, A. Abdulkadir,
M. Bach-Cuadra, A. Daducci, C. Granziera, S. Kloppel, P. Maeder,
R. Meuli, G. Krueger, “An evaluation of volume-based morphometry
for prediction of mild cognitive impairment and Alzheimer’s disease,”
Neuroimage. Clin., vol. 7, pp. 7–17, 2015.
C. Jacobs, E. M. van Rikxoort, T. Twellmann, E. T. Scholten, P. A. de Jong,
J. M. Kuhnigk, M. Oudkerk, H. J. de Koning, M. Prokop,
C. Schaefer-Prokop, and B. van Ginneken, “Automatic detection of subsolid pulmonary nodules in thoracic computed tomography images,” Med.
Image. Anal., vol. 18, no. 2, pp. 374–384, Feb. 2014.

1215

[38] E. Hernlund, A. Svedbom, M. Ivergard, J. Compston, C. Cooper,
J. Stenmark, E. V. McCloskey, B. Jonsson, and J. A. Kanis, “Osteoporosis
in the European Union: Medical management, epidemiology and economic burden. A report prepared in collaboration with the international
osteoporosis foundation (IOF) and the European federation of pharmaceutical industry associations (EFPIA),” Arch Osteoporos, vol. 8, no. 1–2,
p. 136, 2013.
[39] W. B. Nelson, Accelerated Testing: Statistical Models, Test Plans, and
Data Analysis. New York, NY, USA: Wiley, 2009.
[40] C. M. Bishop, Pattern Recognition and Machine Learning. New York,
NY, USA: Springer, 2006.
[41] J. Ide, R. Chen, D. Shen, and E. H. Herskovits, “Robust brain registration
using adaptive probabilistic atlas,” Med. Image Comput. Comput. Assist.
Interv., vol. 11, no. Pt 2, pp. 1041–1049, 2008.
[42] M. Rusu, B. N. Bloch, C. C. Jaffe, N. M. Rofsky, E. M. Genega, E. Feleppa,
R. E. Lenkinski, and A. Madabhushi, “Statistical 3D prostate imaging
atlas construction via anatomically constrained registration,” Proc. SPIE,
vol. 8669, 2013.
[43] D. C. Barber and D. R. Hose, “Automatic segmentation of medical images
using image registration: Diagnostic and simulation applications,” J. Med.
Eng. Technol., vol. 29, no. 2, pp. 53–63, 2005.
[44] A. K. Duun-Henriksen, S. Schmidt, R. M. Roge, J. B. Moller, K. Norgaard,
J. B. Jorgensen, and H. Madsen, “Model identification using stochastic differential equation grey-box models in diabetes,” J. Diabetes Sci. Technol.,
vol. 7, no. 2, pp. 431–440, 2013.
[45] E. E. Vidal-Rosas, S. A. Billings, Y. Zheng, J. E. Mayhew, D. Johnston,
A. J. Kennerley, and D. Coca, “Reduced-order modeling of light transport
in tissue for real-time monitoring of brain hemodynamics using diffuse
optical tomography,” J. Biomed. Opt., vol. 19, no. 2, p. 026008, Feb. 2014.
[46] S. Wold, M. Sjöström, and L. Eriksson, “PLS-regression: A basic tool
of chemometrics,” Chemometrics Intell. Laboratory Syst., vol. 58, no. 2,
pp. 109–130, 2001.
[47] S. A. Billings, Nonlinear System Identification: NARMAX Methods in the
Time, Frequency, and Spatio-Temporal Domains. New York, NY, USA:
Wiley, 2013.
[48] P. Casali, “Risks of the new EU Data protection regulation: An ESMO
position paper endorsed by the European oncology community,” Annals
Oncology, vol. 25, no. 8, pp. 1458–1461, 2014.
[49] L. Sweeney, “k-anonymity: A model for protecting privacy,” Int. J. Uncertain. Fuzziness Knowl.-Based Syst., vol. 10, no. 5, pp. 557–570, 2002.
[50] N. Shomron, Deep Sequencing Data Analysis. New York, NY, USA:
Springer, 2013.
[51] R. P. Tracy, “Deep phenotyping: Characterizing populations in the era
of genomics and systems biology,” Curr. Opin. Lipidol., vol. 19, no. 2,
pp. 151–157, Apr. 2008.
[52] C. Canstein, P. Cachot, A. Faust, A. F. Stalder, J. Bock, A. Frydrychowicz,
J. Kuffer, J. Hennig, and M. Markl, “3D MR flow analysis in realistic rapidprototyping model systems of the thoracic aorta: Comparison with in vivo
data and computational fluid dynamics in identical vessel geometries,”
Magn. Reson. Med., vol. 59, no. 3, pp. 535–546, Mar. 2008.
[53] T. I. Yiallourou, J. R. Kroger, N. Stergiopulos, D. Maintz, B. A. Martin, and
A. C. Bunck, “Comparison of 4D phase-contrast MRI flow measurements
to computational fluid dynamics simulations of cerebrospinal fluid motion
in the cervical spine,” PLoS One, vol. 7, no. 12, p. e52284, 2012.

Authors’ photographs and biographies not available at the time of publication.

