458

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

Active Data Selection for Motor Imagery
EEG Classification
Naoki Tomida, Student Member, IEEE, Toshihisa Tanaka∗ , Senior Member, IEEE,
Shunsuke Ono, Student Member, IEEE, Masao Yamagishi, Member, IEEE, and Hiroshi Higashi, Member, IEEE

Abstract—Rejecting or selecting data from multiple trials of
electroencephalography (EEG) recordings is crucial. We propose a
sparsity-aware method to data selection from a set of multiple EEG
recordings during motor-imagery tasks, aiming at brain machine
interfaces (BMIs). Instead of empirical averaging over sample covariance matrices for multiple trials including low-quality data,
which can lead to poor performance in BMI classification, we
introduce weighted averaging with weight coefficients that can
reject such trials. The weight coefficients are determined by the
1 -minimization problem that lead to sparse weights such that almost zero-values are allocated to low-quality trials. The proposed
method was successfully applied for estimating covariance matrices for the so-called common spatial pattern (CSP) method, which
is widely used for feature extraction from EEG in the two-class
classification. Classification of EEG signals during motor imagery
was examined to support the proposed method. It should be noted
that the proposed data selection method can be applied to a number
of variants of the original CSP method.
Index Terms—Brain–machine interfaces, electroencephalography (EEG), 1 -norm, motor imagery, sparsity-aware signal
processing.

I. INTRODUCTION
HE brain–machine interface (BMI) is a challenging application of signal processing, machine learning, and neuroscience [1]. Such interfaces capture brain activities associated
with mental tasks and external stimuli and enable nonmuscular
communication and a control channel for conveying messages
and commands to the external world [1]–[5]. A noninvasive BMI
uses recordings of brain activities such as electroencephalogram
(EEG), magnetoencephalogram (MEG), and functional magnetic response imaging. Because of its simplicity of device and
high temporal resolution, using EEG is the most practical for
engineering applications [6], [7].

T

Manuscript received March 3, 2014; revised August 5, 2014; accepted
September 4, 2014. Date of publication September 16, 2014; date of current
version January 16, 2015. This work was supported in part by JSPS KAKENHI
Grant 24360146. Asterisk indicates corresponding author.
N. Tomida, S. Ono, and M. Yamagishi are with the Department of Communications and Computer Engineering, Tokyo Institute of Technology, Tokyo
152-8550, Japan (e-mail: tomida@sp.ce.titech.ac.jp; ono@sp.ce.titech.ac.jp;
myamagi@sp.ce.titech.ac.jp).
∗ T. Tanaka is with the Department of Electrical and Electronic Engineering,
Tokyo University of Agriculture and Technology, Tokyo 184-8588, Japan, and
also with the RIKEN Brain Science Institute, Saitama 351-0198, Japan (e-mail:
tanakat@cc.tuat.ac.jp).
H. Higashi is with the Department of Computer Science and Engineering, Toyohashi University of Technology, Aichi, 441-8580, Japan, and also with RIKEN
Brain Science Institute, Saitama 351-0198, Japan (e-mail: higashi@tut.jp).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2358536

A crucial technique for enabling BMIs associated with motor
imagery (MI-BMI) [8], [9] is efficient decoding around the motor cortex, which leads to practical biomedical applications in
rehabilitation and neuroprosthesis [10]–[13]. For instance, real
and imaginary movements of hands and feet evoke a change
in the so-called mu rhythm in different brain regions [2], [3].
Therefore, by accurately capturing these changes from EEG in
the presence of measurement noise and spontaneous components related to other brain activities, we can classify the EEG
signal associated with imagination of different motor actions
such as hand, arm, or foot movement.
A well-known method to extract the brain activity for the MIBMI is the common spatial pattern (CSP) [1], [14], [15]. The
CSP is a set of spatial weight coefficients corresponding to each
electrode in a multichannel EEG. These coefficients are determined from measured EEG data in such a way that the variances
of the signal extracted by the spatial weights differ greatly between two tasks (e.g., left and right hand movement imageries).
These weights can also be regarded as a spatial filter that projects
observed EEG signals onto the optimal space used to classify
the observed data to a class corresponding to a subject’s cerebral status. Several variants of the CSP have been proposed
such as common spatio-spectral pattern (CSSP) [16], common
sparse spectral spatial pattern [17], spectrally weighted CSP
[18], [19], iterative spatio-spectral patterns learning [20], filter
bank CSP [21], discriminative filter bank CSP [22], common
spatio-time-frequency patterns [23], divergence-based method
[24], and augmented complex CSP [25].
A common manipulation for this CSP family is to estimate
the true covariance matrices in two different tasks of observed
signals. To increase estimation accuracy, EEG signals (training
data) are observed several times (called trials) for the same
task, which yields empirical covariance matrices called withintrial covariance matrices. These matrices of all trials are then
simply averaged. This is due to an implicit assumption that an
EEG corresponding to the same task should be a (wide sense)
stationary process. However, simply averaging all trials can lead
to poor estimation of the covariance matrices mainly due to the
following reasons. First, the feature signal can be influenced
by the user’s concentration. Second, the observed EEG can be
contaminated by nonstationary artifacts such as eye and muscle
movement. We call a trial leading to heavily contaminated EEG
a low-quality trial. It is crucial to eliminate low-quality trials
from a dataset used for obtaining a more accurate CSP.
In this paper, we propose a method for estimating the true
covariance matrix of each task not by the simple average of but
by a weighted average of within-trial covariance matrices. To

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

TOMIDA et al.: ACTIVE DATA SELECTION FOR MOTOR IMAGERY EEG CLASSIFICATION

evaluate quality of trials, within-trial covariance matrices are
approximately jointly diagonalized. The underlying assumption
behind this diagonalization is that the residue resulting from
the diagonalization with respect to a low-quality trial is large.
This idea of weighted averaging is related to our previous work
[26], where a weighted 2 -norm minimization with residue improved classification accuracy. Moreover, to increase estimation
accuracy of the covariance matrices, efficient approaches are covariance shrinkage and reduced rank estimation (see [27] and
[28], for instance). However, our aim with this paper is to determine trial weights to reject low-quality trials. To this end, the
residue is involved in an 1 -norm term to design sparse weights
in such a way that a larger residue yields almost zero weight to
reject low-quality trials. A convex optimization problem to find
the sparse weights is introduced and an iterative algorithm for
solving this problem is developed.
Notations: The following terminology, notations, and mathematical operations are used throughout the paper. A matrix is
denoted by a capital bold letter, e.g., A and the (i, j)th entry of
matrix A and the jth column vector are, respectively, denoted by
[A]i,j and [A]:,j . A matrix A ∈ RM ×M is called positive (semi)
definite if u Au() > 0 for all nonzero u ∈ RM . The p -norm

p 1/p
of x ∈ RN is defined as xp := ( N
, where xi is
i=1 |xi | )
the ith entry of x.

II. CSP IN TERMS OF JOINT DIAGONALIZATION
Before discussing the proposed method, we summarize the
CSP, which is obtained as a generalized eigenvectors of a pair
of two covariance matrices. In other words, the CSP is a result
of joint diagonalization.
Let X k ∈ RM ×N be a matrix consisting of M channel signals with N samples at the kth trial. The CSP is a topological
pattern derived from scalp EEG given as vector v ∈ RM , which
minimizes the in-class variance of a signal extracted by a linear
combination of X k [14], [15]. In general, each channel signal in
X k is band limited by a bandpass filter that passes the frequency
components related to the target brain activity. The components
of X k are denoted by X k = [xk1 , . . . , xkN ], where xkn ∈ RM ,
and n is the time index (n = 1, . . . , N ). The
sample mean of
k
the observed signal is given by μk = (1/N ) N
n =1 xn . Then,
k
the sample variance of the extracted signal of X is given by

459

where S d (d = 1, 2) is given as
Sd =

1  k
S
Kd

(3)

k ∈Cd

and S k ∈ RM ×M is the within-trial covariance matrix for the
kth trial given as
S k :=

N
1  k
(x − μk )(xkn − μk ) .
N n =1 n

(4)

Note that the solution of (2) is given by the generalized eigenvector corresponding to the smallest generalized eigenvalue of
the generalized eigenvalue problem described as
S 1 v = λS 2 v.

(5)

It should be noted that solving (5) is equivalent to finding a
matrix, denoted by V , jointly diagonalizing both S 1 and S 2 .
V  S 1 V = Λ1 , V  S 2 V = Λ2

(6)

where Λ1 and Λ2 are diagonal matrices.
III. TRIAL SELECTION WITH SPARSE WEIGHTS
FOR COVARIANCE MATRICES
Ideally, S k in (4) is invariant over trials up to noise since it is
a result of the same mental task. This motivates the simple arithmetic averaging given in (3). However, as mentioned earlier, the
observed EEG is highly trial variant even for the same mental
task. Moreover, the measurement environment of EEG (electronic noise, electrode impedance, etc.) always varies. Thus, we
soften (3) and consider the weighted average defined as

wk S k
(7)
S ∗d =
k ∈Cd

where wk is the weight coefficient at the kth trial and holds

wk = 1, wk ≥ 0.
(8)
k ∈Cd

We define the weight vector consisting of the weights of all
trials as
w := [w1 , . . . , wK d ] ∈ RK d .
Note that, w is included by CH ∩ CN , where
CH := {w ∈ RK d | w 1K d = 1}

σ 2 (X k , v) =

N
1   k
|v (xn − μk )|2 .
N n =1

(1)

Let C1 and C2 be the training data containing the signals
observed at all trials belonging to classes (tasks) 1 and 2, respectively, such that C1 ∩ C2 = ∅. Let Kd be the number of
elements in class d (d = 1, 2). The CSP of class 1 (resp. 2)
is given as the maximizer (resp. minimizer) of the following
generalized Rayleigh quotient [14], [15]:

J(v) =

v S 1 v
v S 2 v

(2)

CN := {w ∈ RK d | wk ≥ 0 ∀k}
and 1K d is the vector of one of size Kd . Note that, in the CSP,
wk = 1/Kd in the aforementioned equation. The underlying
idea behind the weighted average is illustrated in Fig. 1. Under the constraints, the positive semidefiniteness, which is the
inherent property of covariance matrices, is guaranteed.
A. Cost Function Promoting Sparsity
Thus, the underlying problem is to find a sparse set of wk that
can reject low-quality trials.
Since the CSP is designed for binary classification, we have
to determine the weights in (7) corresponding to each class. For

460

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

B. Trial Quality Deduced from Approximate
Joint Diagonalization
1) Quantification of Trial Quality: If the observed EEG is
stationary over the trial up to noise, S k (k ∈ C1 ∪ C2 ) should
be diagonalized in the same way, even though S d is substituted with any S k in (6). However, as mentioned earlier, this
assumption is not true. Hence, we do not consider the exact
joint diagonalization but an approximate joint diagonalization
given by
S k = U Σk U  + E k (k ∈ C1 ∪ C2 )
k

Fig. 1. Covariance matrix of EEG for the motor-imagery task is estimated by
weighted averaging over within-trial covariance matrices.

simplicity, we design the weight of Kd trial matrices S k (k ∈
Cd ) for a single class d.
To find trial weights eliminating low-quality trials, we introduce a weighted 1 -norm term with positive scalar qk .

1
1
D 1 w1
qk |wk | =
J 1 (w) := 
tr(D 1 )
k ∈Cd qk
k ∈Cd

where D 1 is defined as
D 1 := diag [q1 , . . . , qK d ] ∈ RK d ×K d .
Note that, parameter qk is chosen to evaluate the quality of each
trial. The process of choosing this parameter is discussed in the
next section.
Assume that the ideal covariance matrix exists close to the
covariance matrix obtained by simple averaging. To evaluate
this, we define

 
  1
2
1

k
− wk S 
J 2 (w) := 


2 k ∈Cd S k 2F k ∈C Kd
F

d

=

1
(w − 1K d /Kd ) G(w − 1K d /Kd )
2tr(G)

where G is defined as
⎡ 
 1 1 
tr S S
⎢
⎢
⎢
..
G := ⎢
.
⎢
⎣



tr S K d S 1




· · · tr S S
..

.

1

Kd 




· · · tr S K d S K d 

⎥
⎥
⎥
⎥ ∈ RK d ×K d .
⎥
⎦

w∈CH ∩CN

αJ 1 (w) + J 2 (w).

qk := E k F .
2) Approximate Joint Diagonalization by Fast Frobenius
Diagonalization: Various approaches to the approximate joint
diagonalization algorithm can be considered. In this paper, we
use the fast Frobenius diagonalization (FFDIAG) algorithm
[29]. This iterative algorithm attempts to solve the following
optimization problem:
 

2
BS k B  i,j .
min F(B) :=
B∈RM

×M

k ∈C1 ∪C2 i
= j

The FFDIAG algorithm for the aforementioned minimization
problem is summarized in Algorithm 1, which yields a common
factor B such that
S k = B −1 Λk (B −1 ) + B −1 Rk (B −1 )
where Λk and Rk are, respectively, a diagonal matrix and an
off-diagonal matrix at the kth trial. We adopt this decomposition
as the joint diagonalization in (10), i.e., we adopt

C. Iterative Optimization Method

Following the aforementioned discussion, the proposed optimization problem is given, with positive parameter α to control
the sparsity, as
min

where U is a common factor and Σ and E are, respectively, a
diagonal matrix and an error matrix at the kth trial. If the desired
EEG is not observed at some trial, the covariance matrix of the
EEG should be the outliers. In this situation, residues resulting from the diagonalization of the covariance matrices should
be large.
Therefore, we detect trials, which are not jointly diagonalized
well, to assign those trials to smaller weights. That is, we regard
trials, where the Frobenius norms of E k , E k F are large,
as low-quality trials and impose small weights on those trials.
Thus, we simply choose

U = B −1 , Σk = Λk , E k = B −1 Rk (B  )−1 .

 ⎤

..
.

(10)

k

(9)

The optimization problem described in (13) can be solved
using projected gradient methods with a simplex projection
P CH ∩CN [30]. However, in this case, it takes a great deal of
time to converge to the optimal solution due to the ill-condition
of G in J 1 . To avoid this situation, we apply the alternatingdirection method for multipliers (ADMM) [31]–[33].
In the ADMM, we consider the following optimization
problem:
min

w∈RN ,z∈RM

f (w) + g(z)

subject to Lw − z = 0

(11)

TOMIDA et al.: ACTIVE DATA SELECTION FOR MOTOR IMAGERY EEG CLASSIFICATION

Algorithm 1 Approximate Joint Diagonalization by using
FFDIAG
Input S k (k ∈ C1 ∪ C2 ).
A(1) = 0, B (1) = I.
repeat
1. Compute A(n ) as follows:
yij =



[S k(n ) ]j,j [S k(n ) ]i,j ,

k

zij =



[S k(n ) ]i,i [S k(n ) ]j,j ,

k

[A(n ) ]i,j =

zij yj i − zii yij
2 ,
zj j zii − zij

[A(n ) ]j,i =

zij yij − zj j yj i
.
2
zj j zii − zij

if A(n ) F > θ, then
A(n ) = A( θn ) F A(n ) .
end if
2. B (n +1) = (I + A(n ) )B (n ) .
3. Normalize columns of B (n ) .
4. S k(n +1) = (I + A(n ) )S k(n ) (I + A(n ) ) .
until converged.
B = B (n +1) .
Store the diagonal part of S k(n +1) in Λk .
Store the offdiagonal part of S k(n +1) in Rk .
E k = B −1 Rk (B −1 ) .
Output B and E k (k ∈ C1 ∪ C2 ).
where f and g are proper lower semicontinuous convex,1
i.e., f ∈ Γ0 (RN ), g ∈ Γ0 (RM ), and a linear operator L ∈
RM ×N \{O} satisfies a mild condition [33].
Assume that L in (14) has full column rank. For (14), the
ADMM consists of minimizing Lγ over w and over z, and
updating the Lagrange multiplier d.
⎧
w
= arg min Lγ (w, z (n ) , d(n ) )
⎪
⎪
⎪ (n +1)
w∈RN
⎪
⎨
z (n +1) = arg min Lγ (w(n +1) , z, d(n ) )
(12)
⎪
z∈RM
⎪
⎪
⎪
⎩
d(n +1) = d(n ) + (Lw(n +1) − z (n +1) )
where Lγ is the augmented Lagrangian of index γ ∈ (0, ∞)
defined by
Lγ (w, z, d) = f (w) + g(z) +

1 
d (Lw − z)
γ

1
+ Lw − z22
2γ
function f : RN → (−∞, ∞] is called proper lower semicontinuous convex if dom(f ) := {x ∈ RN | f (x) < ∞} 
= ∅, lev ≤α (f ) := {x ∈
RN | f (x) ≤ α} is closed for every α ∈ R, and f (λx + (1 − λ)y) ≤
λf (x) + (1 − λ)f (y) for every x, y ∈ RN and λ ∈ (0, 1), respectively [34].
The set of all proper lower semicontinuous convex functions in RN is denoted
by Γ 0 (RN ).
1A

461

where d ∈ RM and γ are a Lagrange multiplier and a positive
scalar parameter, respectively. With the ADMM, the effect of
the ill condition can be reduced to a certain degree by properly
tuning a parameter γ appearing in the steps given as in (12).
To apply the ADMM to the constrained minimization problem, we rewrite (9) with the indicator function as the following
unconstrained optimization problem.
D1
1K + J 2 (w) + ιCH (w) + ιCN (w)
tr(D 1 ) d
w∈R
(13)
where ιCH and ιCN denote the indicator functions.2 Note
that in the aforementioned problem, the second linear term
D1
1K d is a replacement of the 1 -norm in J 1 (w) since
w tr(D
1)
w is constrained in CN . The steps of the aforementioned algorithm are shown in Algorithm 2, which is derived by adopting
the ADMM algorithm with
min αw
Kd

f (w) := αw

D1
1K + J 2 (w) + ιCH (w)
tr(D 1 ) d

g(z) := ιCN (z)
and L = I ∈ RK d ×K d in (11).
IV. EXPERIMENTAL RESULTS
Two experiments are conducted to support the proposed
method. The first one is an experiment in artificial situation
to confirm whether weights corresponding to low-quality trials
(nonstationary data) become relatively small values or zeros by
using the proposed method. The other one is an experiment of
classification of EEG signals during motor imagery to show
performance in accuracy with the proposed method.
A. EEG Data Description
We used dataset IVa from BCI competition III and dataset 1
from BCI competition IV, which were public datasets provided
by Fraunhofer FIRST (Intelligent Data Analysis Group) and
Campus Benjamin Franklin of the Charité - University Medicine
Berlin (Department of Neurology, Neurophysics Group) [35],
[36], respectively. Aside from the public datasets, we recorded
the EEG of motor imagery (called dataset JK-HH 1). The experiment for obtaining JK-HH 1 was approved by the research
ethics committee of the Tokyo University of Agriculture and
Technology, Tokyo, Japan.
1) Dataset IVa: This public dataset consists of EEG signals
during right hand and right foot motor imageries. The EEG
signals from 118 channels at positions of the extended international 10/20-system were recorded from five subjects assigned
2 A subset C ⊂ RN is called convex if for every x, y ∈ C and λ ∈ (0, 1),
λx + (1 − λ)y ∈ C. For a given nonempty closed convex subset C ⊂ RN , the
indicator functionιC ∈ Γ 0 (RN ) is defined by



ιC (x) :=

0 (x ∈ C)
∞ (x ∈
/ C)

and the metric projection onto C is the mapping P C : RN → C : x →
arg miny∈C x − y2 . The metric projection is also described, for any γ ∈
(0, ∞), as P C (x) = arg miny∈RN ιC (y) + 21γ x − y22 , which is a particular case of the proximity operator [34].

462

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

Algorithm 2 Solver for optimization problem in (13)
Initialize w(1) , z (1) , and d(1) .
repeat



G1 K d
D1 1 K d
G
−1
(z
−1
(γ
+
I)
−
d
)
+
γ
−
α
1
(n )
(n )
Kd
tr(G)
K d tr(G)
tr(D 1 )
.
1a. ξ =
G
−1
γ1
K d (γ tr(G) + I) 1K d
−1 



G1K d
D 1 1K d
G
+I
−α
− ξ1K d .
1b. w(n +1) = γ
z (n ) − d(n ) + γ
tr(G)
Kd tr(G)
tr(D 1 )


2. z (n +1) = P CN w(n +1) + d(n ) . (See (20) for a specific form.)

(14)

(15)
(16)

3. d(n +1) = d(n ) + w(n +1) − z (n +1) .

(17)

until converged.
labels aa, al, av, aw, and ay. The measured signal was bandpass
filtered with a passband of 0.05–200 Hz, and then, digitized
at 1000 Hz with 16 bits (0.1 μV). In the experiment, visual
cues told the subject which imagery task (left hand, right hand,
or right foot) should be performed. The cue was indicated for
3.5 s and the subject performed the motor imagery for this period. The resting interval between two trials was randomized
from 1.75 to 2.25 s. Only EEG trials for right hand and right
foot were provided.
We also applied a bandpass filter whose passband was 7–30
Hz and downsampled to 100 Hz to this dataset. The dataset for
each subject consisted of signals of 140 trials per class. The
signal in each trial was extracted 3.5 s after the visual cue.
2) Dataset 1: This public dataset consists of EEG signals
during two motor imageries, which were selected from three
classes; left hand, right hand, and foot (side chosen by the subject; optionally also both feet). The EEG signals were recorded
from four subjects assigned labels a, b, f, and g. The signals from
59 EEG channels were measured, which were most densely
distributed over sensorimotor areas. The measured signal was
bandpass filtered with a passband of 0.05–200 Hz, and then, digitized at 1000 Hz with 16 bits (0.1 μV). Additionally, the data
passed through the low-pass filter (Chebyshev Type II filter of
order 10 with stopband ripple of 50-dB down and stopband edge
frequency of 49 Hz) then downsampled at 100 Hz (calculating
the mean of blocks of 10 samples). During each experiment,
visual cues were displayed for a period of 4.0 s during which
the subject was instructed to perform the cued motor imagery
task (left hand, right hand, or right foot). These periods were
interleaved with 2.0 s of blank screen and 2.0 s with a fixation
cross shown in the center of the screen.
We also applied a bandpass filter whose passband was 7–30
Hz to this data. The dataset for each subject consisted of signals
of 100 trials per class. The signal in each trial was extracted 4.0
s after the visual cue.
3) Dataset JK-HH 1: This original dataset consists of EEG
signals during two motor imageries, right hand and foot. They
were recorded from five (five males; averaged age 23.2 with
SD 1.6) subjects assigned labels sa, sb, sc, sd, and se. During the recording, the subjects performed the motor-imagery
tasks instructed by a visual cue. The cue was given by an arrow
on an LCD screen. The right and down arrows instructed the

TABLE I
NUMBER OF TRIALS OF WHICH COEFFICIENTS WERE ZERO, 	(w k = 0)
IN BOTH CLASSES WHEN K 0 = 111 (ONE NONSTATIONARY TRIAL)
AND K 0 = 102 (TEN NONSTATIONARY TRIALS)

	(w k = 0) in class 1
	(w k = 0) in class 2

K 0 = 111

K 0 = 102

1
1

10
10

subjects to perform the motor imagery tasks of the right hand
and the foot, respectively. The subjects performed the tasks
repeatedly with an interval of around 3 s. The EEG signals
were recorded with Ag/AgCl active electrodes (g.LADYbird,
g.LADYbirdGND, and g.GAMMAearclip produced by Guger
Technologies) and a power supply (g.GAMMAbox produced
by Guger Technologies). There were 29 electrodes, which were
placed at F3, Fz, F4, FC5, FC3, FC6, FCz, FC2, FC4, FC6, T7,
C5, C3, C1, Cz, C2, C4, C6, T8, CP5, CP3, CP1, CPz, CP2,
CP4, CP6, P3, Pz, and P4 (the positions are represented by the
notation of the International 10–10 system [37]). The signals observed from the electrodes were amplified using a bioamplifier
(MEG-6116 produced by Nihon Kohden). The amplifier analog
filtered the signals with a passband of 0.5–100 Hz. The signals through the amplifier were sampled using an A/D converter
(AIO-163202F-PE produced by Contec) with a sampling rate
of 256 Hz. The converted signals were recorded with the Data
Acquisition Toolbox, which is one of the toolboxs of MATLAB
(MathWorks). We also applied to this dataset a Butterworth lowpass filter, whose cutoff frequency was 50 Hz and filter order
was 4, and downsampled to 128 Hz.
We also applied to this dataset a bandpass filter whose passband was 7–30 Hz. The dataset for each subject consisted of
signals of 100 trials per class. The signal in each trial was extracted 4.0 s after the visual cue.
B. Confirmation of Sparsity in Artificial Situation
The following numerical experiments were conducted to confirm whether the weights corresponding to the low-quality trials
(nonstationary data) are almost zeros when the proposed method
is applied.

TOMIDA et al.: ACTIVE DATA SELECTION FOR MOTOR IMAGERY EEG CLASSIFICATION

463

TABLE II
CLASSIFICATION ACCURACY [%] FROM 5-FOLD CROSS VALIDATION
Subject
(w k = 1/K d )

Common Spatial
(CSP) Method with several weighting techniques
 Pattern

(w k ∼ 1/  E k  F )
(w k :  1 sparse weights)
	(w k 
= 0) (K 1 + K 2 )

dataset IVa

aa
al
av
aw
ay
Ave.

75.71 ±12.66
93.57 ±2.99
63.21 ±5.14
97.86 ±1.96
92.86 ±3.79
84.64

76.43 ±11.32
93.93 ±5.14
65.36 ±2.71
95.71 ±2.04
93.21 ±4.07
84.93

80.36 ±12.81
95.36 ±4.48
71.07 ±7.08
97.86 ±1.96
93.57 ±2.99
87.64

94.4
143.8
98.2
224.0
214.8
155.0

(224)
(224)
(224)
(224)
(224)
(224)

dataset 1

a
b
f
g
Ave.

66.00 ±9.78
71.50 ±5.18
88.50 ±6.75
89.00 ±4.87
78.75

66.50 ±6.75
67.50 ±4.33
89.50 ±6.94
79.50 ±6.47
75.75

71.50 ±9.75
75.00 ±4.68
89.50 ±4.47
90.00 ±3.54
81.25

133.2
160.0
62.8
159.8
129.0

(160)
(160)
(160)
(160)
(160)

JK-HH 1

sa
sb
sc
sd
se
Ave.

83.50 ±6.02
56.50 ±3.79
47.50 ±10.00
49.00 ±9.12
85.50 ±10.37
64.40

78.50 ±7.62
54.00 ±6.27
49.00 ±6.75
48.00 ±15.45
85.50 ±9.25
63.00

83.50 ±6.02
62.00 ±5.42
56.50 ±6.98
56.50 ±7.42
87.00 ±11.00
69.10

160.0
148.0
105.0
85.6
131.8
126.1

(160)
(160)
(160)
(160)
(160)
(160)

Highest classification accuracies for each subject among the accuracies obtained with several α are listed. The figures with ± denote standard deviation. 	(w k 
= 0) stands
for average number of trials of which coefficients were not zero. K 1 + K 2 denotes number of trials in both classes. We regard trial weight coefficients of less than 10 −5
as zeros.

1) Simulation Scenario: Suppose in this simulation that the
observed dataset of an EEG in class d consists of Kd trials,
where K0 trials out of Kd trials (i.e., K0 < Kd ) are widesense stationary observed with white Gaussian noise and the
remaining Kd − K0 trials are nonstationary with different covariance matrices.
This scenario was implemented similar to [38] and [39] as
follows. As a reference signal, we used a signal corresponding to
each class that was chosen randomly out of the dataset of subject
al. We assumed X d ∈ RM ×N as the pure EEG signal in class
d. Based on the signal, we produced Kd trials Y kd ∈ RM ×N
(k = 1, . . . , Kd ) as follows:

Y kd =

⎧
⎨X d + N k1

(k = 1, . . . , K0 )

⎩
X d + N k1 + N k2

(k = K0 + 1, . . . , Kd )

where N k1 ∈ RM ×N denotes Gaussian noise N (0, σ12 I) and
N k2 ∈ RM ×N stands for outlier noise generated from a normal
mixture distribution [40] such as
[N k2 ]:,n ∼ (1 − 
)δ0 + 
N (0, σ22 I)
where n (n = 1, . . . , N ) is a time index, δ0 denotes a point
mass distribution located at zero, and 
 > 0 is the occurrence
probability [39].
2) Results: We set Kd = 112, and chose σ1 = 1.0, σ2 =
1.0 × 103 for the noise distributions and 
 = 1.0 × 10−1 for the
occurrence probability. Table I lists the resulting zero weight
coefficients for both classes when K0 = 111 (one nonstationary
trial) and K0 = 102 (ten nonstationary trials). In the parameter
settings of Algorithm 2, we chose α = 2.0 × 10−1 and γ =
1.0 × 10−3 in both cases.

C. Two-Class EEG Classification
1) Parameter Settings: The following three types of CSP are
used for feature extraction of motor-imagery EEG.
i) A CSP with the empirically averaged covariance matrix,
as in (3).
ii) A CSP with the weight-averaged covariance matrix, as
in (7), with a simple weighting technique
 −1
wk = η E k F (k ∈ Cd )

(18)

where
η is a constant for normalization such that 1 =

k ∈Cd wk . The underlying idea is to simply give a small
weight corresponding to a large residue (a low-quality
trial).
iii) A CSP with the weight-averaged covariance matrix, as in
(7), with the proposed sparsity-aware estimation method.
It should be noted that more recent CSP-based methods could
be used in the experiments; however, the aim with this study was
to show the effectiveness of the proposed data selection/rejection
method, and that the choice of CSP was not the issue.
We defined the following feature vector as the output of feature extraction using CSP. Although the solution of (2) is given
by the eigenvector corresponding to the largest eigenvalue in (5),
we can use the other eigenvectors for classification [41]. The M
eigenvectors can be obtained by solving (5) as v̂ 1 , . . . , v̂ M ,
where v̂ i is the eigenvector corresponding to the ith smallest
eigenvalue of (5). We used the 2r eigenvectors to form the
feature vector, denoted by y, for classification of unlabeled
data, X.
y = [σ 2 (X, v̂ 1 ), . . . , σ 2 (X, v̂ r )
σ 2 (X, v̂ M −r +1 ), . . . , σ 2 (X, v̂ M )] ∈ R2r . (19)
The feature is classified with linear discriminant analysis [42].

464

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

among the accuracies obtained with several α. The results were
obtained by conducting 5-fold cross validation (CV). In the table, 	(wk 
= 0) stands for the average number of trials of which
coefficients were not zero. In other words, 	(wk 
= 0) was the
average number of selected trials from the dataset. In all cases,
for simplicity of comparison, the number of the associated spatial weights r in (19) was fixed to 3. For every parameter α, we
chose γ = 1.0 × 105 .
To see the sensitivity of parameter α, we measured the classification accuracy for varying sparsity parameter α for each
subject in each dataset, as shown in Fig. 2. The more α increased, the more sparsity was promoted. In the case of the
smallest α in the figures was 10−7 , the term of the 1 -norm in
the cost function could be virtually ignored; therefore, we observed that the resulting weights were identical. As also shown
in Fig. 2, there was no common trend in the change in classification accuracy by the parameter among the subjects. The results
suggest that the number of low-quality samples was different
among the subjects.
V. DISCUSSION AND CONCLUSION

Fig. 2. Classification accuracy for varying parameter α for each subject in
datasets IVa, 1, and JK-HH 1. (a) Dataset IVa (w k : 1 sparse weights). (b)
Dataset 1 (w k : 1 sparse weights). (c) JK-HH 1 (w k : 1 sparse weights).

2) Results: In Table II, we list the classification accuracy by CSP with the following weighting techniques: i) the
simple average (wk = 1/Kd ), ii) the weighted average with
 −1
wk ∼ E k F , and iii) the weighted average with the proposed sparse weights. The results of the proposed data selection
method are the highest classification accuracy for each subject

The main contribution of this paper was to establish new
methods for selecting or rejecting trials. A weight coefficient
was assigned to each within-trial covariance matrix to measure
the quality of the trial, and a sparse set of weights was determined by the 1 optimization problem.
The experiments to confirm sparsity in an artificial situation
have shown that the trials, where the residues yielded by joint diagonalization were large, correspond to the low-quality trials. As
expected, only nonstationary trials (assumed to be low-quality
trials) were weighted with (almost) zero, and the others were
quite uniquely weighted, as shown in Table I.
The results of classification accuracy shown in Table II exhibit
the advantage of the proposed method. Detailed discussion is
given in the following. First of all, the simple nonsparse weights
determined by the error matrices obtained in joint diagonalization do not help to improve the classification accuracy. This
implies that the error matrices should be utilized for designing
weight coefficients in more sophisticated ways.
In contrast, the proposed sparse weights led to noticeable
classification results. Subject av showed a large improvement
in accuracy by more than 7 % with the proposed trial rejection
method. It is well known in the BCI community that this dataset
of av always shows poor classification performance with variants of CSP. From this table, the average number of nonzero
weights was 98.2, which implies that 126 trials out of 224
were rejected by the 1 optimization. This fact suggests that
the dataset of Subject av contains many low-quality trials.
Next, note that even with the standard CSP, Subject aw
showed a high accuracy of 97.66%, and no trials were rejected
with the proposed method. This implies that the dataset of this
subject includes stationary signals.
On the other hand, Subject f showed a small improvement
of 1.00%, even though a large number of trials was rejected,
i.e., only 62.8 trials out of 160 were selected on average. This
may contradict the aforementioned argument that a dataset consists of stationary trials. However, as observed in Fig. 2(b), the

TOMIDA et al.: ACTIVE DATA SELECTION FOR MOTOR IMAGERY EEG CLASSIFICATION

accuracies of Subject f appeared inconsistent over parameter α.
In other words, the value of 	(wk 
= 0) did not mean the quality
of trials in the dataset. Unlike Subject f, some subjects exhibited
a clear relation between α and accuracy. For instance, Subject
aw showed that increased sparsity led to decreased accuracy.
The experiment of two EEG classification showed that the
proposed method is effective. What we would like to emphasize
is that the proposed method can be applied to variants of CSP. By
introducing the 1 norm to the cost function, we can obtain the
sparse weights, which lead to the rejection of low-quality trials.
Even though introducing sparse weight coefficients improved
classification accuracy, an important question arose: Does a
zero weight really correspond to a low-quality trial? This paper
established how to select or reject trials from a dataset based on
the sparse 1 optimization. The established method should be
verified through a psychophysiological experiment in which a
subject is randomly distracted during a motor-imagery mental
task and the distribution of weight coefficients derived based
on the proposed method is evaluated. This important problem
will be addressed in the future. The analysis with such EEG
data in which low-quality trials are on purpose might help in
developing a model for evaluating the quality of EEG signals.
We then can discuss the issue of bias caused by 1 regularization
in the solution [43].
The limitation of the proposed method is that we have to
choose the regularization parameter. The simplest way to choose
the parameter is using a CV method with a dataset. When the
learning and test data are separated out of the dataset for CV
and the number of low-quality data in the learning data is significantly different in each CV, the choice of the parameter by
CV might not work well. Therefore, we need a method for estimating an appropriate regularization parameter for each dataset
or subject.
In this paper, we did not discuss the problem of how to reject
a low-quality test sample to be classified. However, how to
reject low-quality samples by real-time processing is crucial
for practical use of the BMI. We will address this problem by
expanding the proposed method.
Moreover, the concept of the proposed method could be extended as a method for rejecting each sample instead of each
trial. To reject samples, we need to design N × (K1 + K2 )
coefficients as the weights. This can lead to the additional computational cost compared to that for finding the trial weights.
Additionally, a joint diagonalization with matrices whose rank
is 1 would be unstable. In this case, we need another method to
N ×(K +K )
estimate {qk }k =1 1 2 . Although we should solve these problems for extending the proposed method, the proposed method
can be used as a framework for rejecting samples not only for
rejecting trials.
APPENDIX A
DERIVATION OF STEPS IN ALGORITHM 2
We derive Steps 1 and 2 in Algorithm 2. Step 1 is derived by
solving the following optimization problem (also see (15)):
w(n +1) = arg min Lγ (w, z (n ) , d(n ) )
w∈RK d

465

= arg min αw
w∈CH

+

D1
1K + J 2 (w)
tr(D 1 ) d

1
z (n ) − w − d(n ) 22 .
2γ

Using a multiplier, we define the Lagrangian as
L(w) :=

1
G
(w − 1K d /Kd )
(w − 1K d /Kd )
2
tr(G)
+ αw

D1
1
1K +
z (n ) − w − d(n ) 22
tr(D 1 ) d
2γ

+ ξ(w 1K d − 1)
with ξ being the Lagrange multiplier for the constraint set CH .
Taking a gradient of L(w) with respect to w, we obtain the
requirement
∇L(w) =

D 1 1K d
G
(w − 1K d /Kd ) + α
tr(G)
tr(D 1 )
1
+ (w + d(n ) − z (n ) ) + ξ1K d
γ

= 0.
Thus, the solution is obtained as (18). Plugging this solution
into the constraint w 1K d = 1 leads to
1
K d w (n +1)
−1

G

+I
= 1K d γ
tr(G)



G1K d
D 1 1K d
−α
− ξ1K d
× z (n ) − d(n ) + γ
Kd tr(G)
tr(D 1 )
−1

G
−γξ1
+I
1K d
γ
Kd
tr(G)
= 1
which readily results in (17). Note that strict discussion about
the Lagrange method is written in, e.g., [34, Proposition 26.11].
Step 2 is derived as follows:
z (n +1) = arg min Lγ (w(n +1) , z, d(n ) )
z∈RK d

= arg min ιCN (z) +
z∈RK d

1
z − (w(n +1) + d(n ) )22
2γ

= P CN (w(n +1) + d(n ) )
where P CN is the metric projection onto CN defined by
⎧
if [x]i ≤ 0,
⎨ 0,
Kd
P CN : R → CN : [x]i →
⎩
[x]i , otherwise.

(20)

ACKNOWLEDGMENT
The authors would like to thank the reviewers for their insightful comments and constructive suggestion. The authors would
also like to thank Prof. I. Yamada of the Tokyo Institute of Technology and members of his laboratory for their useful comments
and proofreading on this paper.

466

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

REFERENCES
[1] G. Dornhege, J. D. R. Millan, T. Hinterberger, D. McFarland, and K.-R.
Müller, Toward Brain-Computer Interfacing. Cambridge, MA, USA: MIT
Press, 2007.
[2] S. Sanei, Adaptive Processing of Brain Signals. Hoboken, NJ, USA: John
Wiley & Sons, Apr. 2013.
[3] J. R. Wolpaw, N. Birbaumer, D. J. McFarland, G. Pfurtscheller, and T. M.
Vaughan, “Brain–computer interfaces for communication and control,”
Clin. Neurophysiol., vol. 113, no. 6, pp. 767–791, 2002.
[4] B. He, S. Gao, H. Yuan, and J. R. Wolpaw, “Brain–computer interfaces,” in Neural Engineering, New York, NY, USA: Springer, 2013,
pp. 87–151.
[5] H. Yuan and B. He, “Brain-computer interfaces using sensorimotor
rhythms: Current state and future perspectives,” IEEE Trans. Biomed.
Eng., vol. 61, no. 5, pp. 1425–1435, May 2014.
[6] D. J. McFarland and J. R. Wolpaw, “Brain-computer interface operation
of robotic and prosthetic devices,” Computer, vol. 41, no. 6, pp. 52–56,
2008.
[7] C. Zhang, Y. Kimura, H. Higashi, and T. Tanaka, “A simple platform
of brain–controlled mobile robot and its implementation by SSVEP,” in
Proc. 2012 Int. Joint Conf. Neural Netw., 2012, pp. 1–7.
[8] D. J. McFarland and J. R. Wolpaw, “Brain-computer interfaces for communication and control,” Commun. ACM, vol. 54, no. 5, pp. 60–66,
2011.
[9] J. R. Wolpaw and D. J. McFarland, “Control of a two-dimensional movement signal by a noninvasive brain-computer interface in humans,” Nat.
Acad. Sci., vol. 101, no. 51, pp. 17849–17854, 2004.
[10] Y. Liu, M. Li, H. Zhang, H. Wang, J. Li, J. Jia, Y. Wu, and L. Zhang,
“A tensor-based scheme for stroke patients’ motor imagery EEG analysis in BCI–FES rehabilitation training,” J. Neurosci. Methods, vol. 222,
pp. 238–249, 2014.
[11] K. K. Ang and C. Guan, “Brain-computer interface in stroke rehabilitation,” J. Comput. Sci. Eng., vol. 7, no. 2, pp. 139–146, 2013.
[12] K. K. Ang, C. Guan, K. Sui Geok Chua, B. T. Ang, C. Kuah, C. Wang, K.
S. Phua, Z. Y. Chin, and H. Zhang, “Clinical study of neurorehabilitation
in stroke using EEG-based motor imagery brain-computer interface with
robotic feedback,” in Proc. 32nd Annu. Int. Conf. IEEE Eng. Med. Bio.
Soc., 2010, pp. 5549–5552.
[13] G. Prasad, P. Herman, D. Coyle, S. McDonough, and J. Crosbie, “Using
motor imagery based brain-computer interface for post-stroke rehabilitation,” in Proc. 4th Int. IEEE/EMBS Conf. Neural Eng., Apr. 2009,
pp. 258–262.
[14] J. Müller-Gerking, G. Pfurtscheller, and H. Flyvbjerg, “Designing optimal
spatial filters for single-trial EEG classification in a movement task,” Clin.
Neurophysiol., vol. 110, no. 5, pp. 787–798, 1999.
[15] H. Ramoser, J. Müller-Gerking, and G. Pfurtscheller, “Optimal spatial
filtering of single trial EEG during imagined hand movement,” IEEE
Trans. Neural Syst. Rehabil. Eng., vol. 8, no. 4, pp. 441–446, Dec. 2000.
[16] S. Lemm, B. Blankertz, G. Curio, and K.-R. Müller, “Spatio-spectral filters
for improving the classification of single trial EEG,” IEEE Trans. Biomed.
Eng., vol. 52, no. 9, pp. 1541–1548, Sep. 2005.
[17] G. Dornhege, B. Blankertz, M. Krauledat, F. Losch, G. Curio, and
K.-R. Müller, “Combined optimization of spatial and temporal filters
for improving brain-computer interfacing,” IEEE Trans. Biomed. Eng.,
vol. 53, no. 11, pp. 2274–2281, Nov. 2006.
[18] R. Tomioka, G. Dornhege, G. Nolte, B. Blankertz, K. Aihara, and K. R.
Müller, “Spectrally weighted common spatial pattern algorithm for single
trial EEG classification,” Department of Mathematical Informatics, The
University of Tokyo, Tokyo, Japan, Tech. Rep., 2006.
[19] R. Tomioka and K.-R. Müller, “A regularized discriminative framework
for EEG analysis with application to brain–computer interface,” NeuroImage, vol. 49, no. 1, pp. 415–432, 2010.
[20] W. Wu, X. Gao, B. Hong, and S. Gao, “Classifying single-trial EEG during
motor imagery by iterative spatio-spectral patterns learning (ISSPL),”
IEEE Trans. Biomed. Eng., vol. 55, no. 6, pp. 1733–1743, Jun. 2008.
[21] K. K. Ang, Z. Y. Chin, H. Zhang, and C. Guan, “Filter bank common
spatial pattern (FBCSP) in brain-computer interface,” in Proc. 2008 Int.
Joint Conf. Neural Netw., 2008, pp. 2390–2397.
[22] H. Higashi and T. Tanaka, “Simultaneous design of FIR filter banks and
spatial patterns for EEG signal classification,” IEEE Trans. Biomed. Eng.,
vol. 60, no. 4, pp. 1100–1110, Apr. 2013.
[23] H. Higashi and T. Tanaka, “Common spatio-time-frequency patterns for
motor imagery-based brain machine interfaces,” Comput. Intell. Neuroscie., vol. 2013, art. no. 537218, p. 12, 2013.

[24] W. Samek, M. Kawanabe, and K.-R. Müller, “Divergence-based framework for common spatial patterns algorithms,” IEEE Rev. Biomed. Eng.,
vol. 7, pp. 50–72, Nov. 2013.
[25] C. Park, C. C. Took, and D. P. Mandic, “Augmented complex common
spatial patterns for classification of noncircular EEG from motor imagery
tasks,” IEEE Trans. Neural Syst. Rehab. Eng., vol. 22, no. 1, pp. 1–10,
Jan. 2014.
[26] N. Tomida, H. Higashi, and T. Tanaka, “A joint tensor diagonalization
approach to active data selection for EEG classification,” in Proc. 2013
IEEE Int. Conf. Acoust., Speech, Signal Process., 2013, pp. 983–987.
[27] D. Bartz and K.-R. Müller, “Generalizing Analytic Shrinkage for Arbitrary
Covariance Structures,” in Proc. Adv. Neural Inform. Process. Syst. 26,
2013, pp. 1869–1877.
[28] N. Tomida, M. Yamagishi, I. Yamada, and T. Tanaka, “A reduced rank
approach for covariance matrix estimation in EEG signal classification,” in
Proc. IEEE 36th Annu. Int. Conf. Eng. Med. Bio. Soc., 2014, pp. 668–671.
[29] A. Ziehe, P. Laskov, G. Nolte, and K.-R. Müller, “A fast algorithm for joint
diagonalization with non-orthogonal transformations and its application
to blind source separation,” J. Mach. Learning Res., vol. 5, pp. 801–818,
2004.
[30] J. C. Duchi, S. Shalev-Shwartz, Y. Singer, and T. Chandra, “Efficient
projections onto the l1-ball for learning in high dimensions,” in Int. Conf.
Mach. Learning, 2008, vol. 307, pp. 272–279.
[31] D. Gabay and B. Mercier, “A dual algorithm for the solution of nonlinear
variational problems via finite element approximation,” Comput. Math.
Appl., vol. 2, no. 3, pp. 19–40, 1976.
[32] J. Eckstein and D. P. Bertsekas, “On the Douglas-Rachford splitting
method and the proximal point algorithm for maximal monotone operators,” Math. Program., vol. 55, no. 1-3, pp. 293–318, 1992.
[33] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein, “Distributed
optimization and statistical learning via the alternating direction method
of multipliers,” Found. Trends Mach. Learn., vol. 3, no. 1, pp. 1–122,
2011.
[34] H. H. Bauschke and P. L. Combettes, Convex Analysis and Monotone
Operator Theory in Hilbert Spaces. New York, NY, USA: Springer, 2011.
[35] B. Blankertz, K.-R. Müller, D. J. Krusienski, G. Schalk, J. R. Wolpaw, A.
Schlögl, G. Pfurtscheller, J. del R. Millan, M. Schröder, and N. Birbaumer,
“The BCI competition III: Validating alternative approaches to actual
BCI problems,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 14, no. 2,
pp. 153–159, Jun. 2006.
[36] G. Dornhege, B. Blankertz, G. Curio, and K.-R. Müller, “Boosting bit rates
in noninvasive EEG single-trial classifications by feature combination
and multiclass paradigms,” IEEE Trans. Biomed. Eng., vol. 51, no. 6,
pp. 993–1002, Jun. 2004.
[37] R. Oostenveld and P. Praamstra, “The five percent electrode system for
high-resolution EEG and ERP measurements,” Clinical Neurophysiol.,
vol. 112, no. 4, pp. 713–719, 2001.
[38] H. Wang, Q. Tang, and W. Zheng, “L1-norm-based common spatial patterns,” IEEE Trans. Biomed. Eng., vol. 59, no. 3, pp. 653–662, Mar. 2012.
[39] X. Yong, R. K. Ward, and G. E. Birch, “Robust common spatial patterns
for EEG signal preprocessing,” in Proc. IEEE 30th Annu. Int. Conf. Eng.
Med. Bio. Soc., 2008, pp. 2087–2090.
[40] R. Maronna, D. Martin, and V. Yohai, Robust Statistics: Theory and Methods ser. Wiley Series in Probability and Statistics., New York, NY, USA:
Wiley, 2006.
[41] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Müller,
“Optimizing spatial filters for robust EEG single-trial analysis,” IEEE
Signal Process. Mag., vol. 25, no. 1, pp. 41–56, 2008.
[42] C. M. Bishop, Pattern Recognition and Machine Learning. New York,
NY, USA: Springer, 2006.
[43] D. H. Dini and D. P. Mandic, “Exploiting sparsity in widely linear
estimation,” in Proc. 10th Int. Symp. Wireless Commun. Syst., 2013,
pp. 1–5.
Naoki Tomida (S’13) received the B.E. degree in
electrical and electronic engineering from the Tokyo
University of Agriculture and Technology, Tokyo,
Japan, in 2013. He has been working toward the master degree in the Department of Communication and
Computer Engineering, Tokyo Institute of Technology, Tokyo, since 2013.
His research interests include signal processing, brain and biomedical signal processing, machine
learning, matrix and tensor factorization, and convex
optimization.

TOMIDA et al.: ACTIVE DATA SELECTION FOR MOTOR IMAGERY EEG CLASSIFICATION

467

Toshihisa Tanaka (S’98–M’02–SM’10) received
the B.E., M.E., and Ph.D. degrees from the Tokyo
Institute of Technology, Tokyo, Japan, in 1997, 2000,
and 2002, respectively.
From 2000 to 2002, he was a Research Fellow at
the Japan Society for the Promotion of Science. From
October 2002 to March 2004, he was a Research
Scientist at RIKEN Brain Science Institute, Saitama,
Japan. In April 2004, he joined Department of Electrical and Electronic Engineering, the Tokyo University
of Agriculture and Technology, Tokyo, where he is
currently an Associate Professor. In 2005, he was a Royal Society Visiting
Fellow at the Communications and Signal Processing Group, Imperial College
London, U.K. From June 2011 to October 2011, he was a Visiting Faculty Member in the Department of Electrical Engineering, University of Hawaii at Manoa,
Honolulu, HI, USA. His research interests include image and signal processing, statistical signal processing and machine learning, brain and biomedical
signal processing, and adaptive systems. He is a coeditor of Signal Processing
Techniques for Knowledge Extraction and Information Fusion (with Mandic,
Splinger), 2008.
Dr. Tanaka served as a Guest Editor of special issues in journals including
Neurocomputing. He served as an Associate Editor of the IEICE Transactions
on Fundamentals. He was a Chair of the Technical Committee on Biomedical
Signal Processing, Asia-Pacific Signal and Information Processing Association
(APSIPA). He is a member of the Institute of Electronics, Information and Communication Engineers and APSIPA.

Masao Yamagishi (M’12) received the B.E., M.E.,
and Ph.D. degrees from the Tokyo Institute of Technology, Tokyo, Japan, in 2007, 2008, and 2012, respectively.
From April 2009 to March 2012, he was a Research Fellow at the Japan Society for the Promotion
of Science. From September to December 2012, he
was a Visiting Researcher at the Technical University of Munich, Munich, Germany. He is currently an
Assistant Professor in the Department of Communications and Computer Engineering, Tokyo Institute
of Technology. His research interests include mathematical signal processing,
adaptive filtering, convex optimization, and inverse problems.
He received the Young Researcher Award from the Institute of Electrical,
Information and Communication Engineers of Japan in 2010.

Shunsuke Ono (S’11) received the B.E. degree in
computer science and the M.E. degree in communications and computer engineering from the Tokyo
Institute of Technology, Tokyo, Japan, in 2010 and
2012, respectively, where he is currently working toward the Ph.D. degree with the Department of Communications and Computer Engineering.
He is a Research Fellow with the Japan Society
for the Promotion of Science. His current research interests include signal and image processing, convex
optimization, and inverse problems.
Mr. Ono received the Best Paper Award in 2014 and the Young Researchers
Award in 2013 from the Institute of Electronics, Information and Communication Engineers.

Hiroshi Higashi (S’10–M’14) received the B.E.,
M.E., and Ph.D. degrees from the Tokyo University of
Agriculture and Technology, Tokyo, Japan, in 2009,
2011, and 2013, respectively.
From 2011–2012, he was a Junior Research Associate with the Laboratory for Advanced Brain Signal
Processing, Brain Science Institute, RIKEN, Saitama,
Japan. From 2012–2014, he was a Research Fellow
of the Japan Society for the Promotion of Science,
Japan. He is currently an Assistant Professor in the
Department of Computer Science and Engineering,
Toyohashi University of Technology, Toyohashi, Japan, and a Visiting Researcher in Brain Science Institute, RIKEN. His research interests include brain
and biomedical signal processing.

