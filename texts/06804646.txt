430

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Fall Detection Based on Body Part Tracking
Using a Depth Camera
Zhen-Peng Bian, Student Member, IEEE, Junhui Hou, Student Member, IEEE, Lap-Pui Chau, Senior Member, IEEE,
and Nadia Magnenat-Thalmann

Abstract—The elderly population is increasing rapidly all over
the world. One major risk for elderly people is fall accidents, especially for those living alone. In this paper, we propose a robust
fall detection approach by analyzing the tracked key joints of the
human body using a single depth camera. Compared to the rivals
that rely on the RGB inputs, the proposed scheme is independent
of illumination of the lights and can work even in a dark room.
In our scheme, a pose-invariant randomized decision tree algorithm is proposed for the key joint extraction, which requires low
computational cost during the training and test. Then, the support vector machine classifier is employed to determine whether a
fall motion occurs, whose input is the 3-D trajectory of the head
joint. The experimental results demonstrate that the proposed fall
detection method is more accurate and robust compared with the
state-of-the-art methods.
Index Terms—Computer vision, fall detection, head tracking,
monocular, video surveillance, 3-D.

I. INTRODUCTION
HE proportion of the elderly population is rising rapidly
in most countries. In 2010, the elderly population (60+
years old) is 759 million (11% of the total population) all over
the world [1]. Many studies have indicated that falls in elderly
people are one of the most dangerous situations at home [2].
Approximately 28–35% of elderly people fall one time or more
per year [3].
When an elderly person is living alone and has a fall accident, he/she may be lying on the floor for a long time without
any help. This scenario mostly will lead to a serious negative
outcome. Therefore, a fall accident detection system, which can
automatically detect the fall accident and call for help, is very
important for elderly people, especially for those living alone.
In [2], [4], and [5], the authors reviewed principles and methods used in existing fall detection approaches. Nowadays, fall
detection approaches could be classified as two main categories:
nonvision-based method and vision-based method. Most methods of fall detection employ inertial sensors, such as accelerom-

T

Manuscript received October 25, 2013; revised February 28, 2014; accepted
April 15, 2014. Date of publication April 23, 2014; date of current version
March 2, 2015. Date of publication; Date of current version. This work was
supported by the Institute for Media Innovation, Nanyang Technological University, Singapore, under Ph.D. Grant.
Z.-P. Bian, J. Hou, and L.-P. Chau are with the School of Electrical and
Electronics Engineering, Nanyang Technological University, Singapore 639798
(e-mail: zbian1@e.ntu.edu.sg; houj0001@e.ntu.edu.sg; elpchau@ntu.edu.sg).
N. Magnenat-Thalmann is with the Institute for Media Innovation, Nanyang
Technological University, Singapore 639798 (e-mail: nadiathalmann@
ntu.edu.sg).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org
Digital Object Identifier 10.1109/JBHI.2014.2319372

eters, since they are low costs. However, the methods based on
inertial sensors are intrusive. As the vision technologies developed fast during the past few years, the vision-based methods,
which are nonintrusive, have become a focal point in the research of the fall detection. They can capture the object’s motion and analyze the object environment and their relationship,
such as the human lying on the floor. The feature of the visionbased systems can be posture [6]–[9], shape in-activity/change
[10], [11], spatio-temporal [10], 3-D head position [12], [13],
and 3-D silhouette vertical distribution [14]. To improve the
accuracy, some researchers combined nonvision-based method
and vision-based method [15]. Most of the fall detection methods based on vision try to execute in real time using standard
computers and low cost cameras. The fall motion is very fast,
taking few hundred milliseconds, and the image processing is
high computational complexity. Thus, most vision-based existing methods cannot capture the specific motion during the
fall phase. Recent researches on fall detection based on computer vision showed some practical frameworks. However, the
robustness as well as accuracy of vision-based methods still
leave a wide open room for further fall detection research and
development.
The depth camera, such as Kinect [16], was used for fall
detection [17]–[20]. Thanks to the infra-red LED, the depth
camera is independent of illumination of lights and can work
well in weak light condition even in a dark room. It can also
work well when the light condition significantly changes such
as switching ON or OFF the lights. As we know, some falls are
caused by the weak light condition. In the depth image, each
pixel value represents the depth information instead of the traditional color or intensity information. The depth value is the
distance between the object and the camera. The depth information can be used to calibrate each pixel to a real world 3-D
location point. Compared with the traditional intensity or color
camera, the depth camera provides several useful advantages in
the object recognition. Depth cameras are useful for removing
ambiguity in size scale. The object size in the color or intensity
image is changed according to the distance between the object
and the camera. That introduces ambiguity in size scale since the
distance is unknown. In color or intensity images, the shadow
greatly reduces the quality of background subtraction. Depth
cameras can resolve silhouette ambiguity of the human body.
The depth cameras simplify the tasks of background subtraction and floor detection. That can improve the robustness of the
object recognition and can offer some useful information about
the relationship between the human and the environment, such
as the human hitting the floor. Furthermore, the realistic depth

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

BIAN et al.: FALL DETECTION BASED ON BODY PART TRACKING USING A DEPTH CAMERA

Fig. 1.

431

Flowchart of proposed fall detection.

images of human can be much easier to synthesize. Therefore, a
large and reliable training dataset can be built up for markerless
motion capture [21].
In this paper, a robust fall detection system based on human
body part tracking using a depth camera is proposed. To capture
the fall motion, an improved randomized decision tree (RDT)
algorithm is employed to extract the 3-D body joints. By tracking the 3-D joint trajectory, a support vector machine (SVM)
classifier for fall detection is proposed. The 3-D joint trajectory pattern is the input of the SVM classifier. Fig. 1 shows the
flowchart of proposed fall detection algorithm.
The structure of this paper is as follows. Section II introduces
the joint extraction method. Section III introduces the fall detection methods based on SVM. Experimental results are presented
in Section IV. Conclusions are presented in Section V.

Fig. 2.

Randomized decision tree.

where (x0 , y0 ) is the test pixel of the depth image, (Δx, Δy)
is the offset related to the test pixel (x0 , y0 ), (Δx1 , Δy1 ), and
(Δx2 , Δy2 ) are two different offset values, z(x, y) is the depth
value of the pixel (x, y)·1/z(x, y) is used to normalize the offset value, so that the feature can resolve the ambiguity in depth
variation. As shown in (1), this feature is 3-D translation invariant.
The depth information around the test pixel describes the geometric surface around this pixel. The geometric surface around
the test pixel can be described well enough by the depth differences between the neighbor pixels and the test pixel. Thus, the
following feature can be used to describe the same geometric
surface as (1) to recognize the test pixel


(Δx, Δy)
f ((x0 , y0 )|(Δx, Δy)) = z(x0 , y0 ) − z (x0 , y0 ) +
.
z(x0 , y0 )

II. JOINT EXTRACTION
To capture the human fall motion, a markerless joint extraction is employed. The joint extraction is based on the proposed
RDT algorithm, which is trained by large depth images dataset.
A. Feature for Joint Extraction
In [22], the recognized feature is based on the difference of
intensities of two pixels taken in the neighborhood of a key
point. This feature was further developed in the depth image by
Shotton et al. in [21]. They employed a simple depth comparison
feature instead of the intensity comparison feature, resulting in
a wonderful success.
Only one or two comparison features are very weak for discriminating objects, such as discriminating body parts. However,
an RDT based on these comparison features are sufficient to discriminate objects. It can handle the noise of the depth image and
can even work using the 2-D silhouette [21]. The formulation
of comparison feature in [21] can be described as
f ((x0 , y0 )|(Δx1 , Δy1 ), (Δx2 , Δy2 ))




(Δx1 , Δy1 )
(Δx2 , Δy2 )
= z (x0 , y0 ) +
−z (x0 , y0 ) +
z(x0 , y0 )
z(x0 , y0 )

(1)

(2)
Compared with (1), this feature is with a higher computational
efficiency: there are only one division, one addition, and one
subtraction; it only looks up two depth pixels. That can save
time and leave more time for real-time fall detection.
Fig. 2 shows an RDT consisting of split and leaf nodes. A
randomized decision forest includes several RDTs. In many applications, the task of multiclass classifiers can be implemented
in a high efficiency and high speed by RDT [21], [22]. In each
tree, each split node n has a parameter pn = ((Δxn , Δyn ), τn ).
(Δxn , Δyn ) is the offset value. τn is a scalar threshold for comparing with the feature value of the test pixel. The evaluating
function of comparison is
E((x0 , y0 ); pn ) = B (f ((x0 , y0 )|(Δxn , Δyn )) − τn )

(3)

where B(·) is a binary function. When the value of
(f ((x0 , y0 )|(Δxn , Δyn )) − τn ) is greater than or equal to 0,
there will be B(f ((x0 , y0 )|(Δxn , Δyn )) − τn ) = 1; otherwise,
B(f ((x0 , y0 )|(Δxn , Δyn )) − τn ) = 0. When E((x0 , y0 ); pn )
is one, the test pixel is split to the left branch child of node n.
When E((x0 , y0 ); pn ) is zero, the test pixel is split to the right
branch child of node n. The operation is repeated, and stop when
it meets the leaf node l. There are some classification information in the leaf node l, such as the probability of body parts. To

432

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Fig. 4.
map.

Entropy map and 3-D view. (a) Map of entropy. (b) 3-D view of the

of wic is
wic = P (c| (xi , yi ), I) · z(xi , yi )2 .

(5)

This mean shift method result is more accurate than the result
of the global center of the same body part since there are some
outlying pixels as shown in Fig. 3.
C. Training

Fig. 3. Classification result of each depth pixel, each color stands for the
respective part of the human body with the highest probability. Blue: head, red:
hip, green: other part.

classify a pixel (x0 , y0 ), one starts at the root and repeatedly
evaluates (3), branching left or right according to the value of
E until reaching the leaf node.
After classifying each pixel, the joint position can be predicted
by body part classification [21]. In the body part classification
method, the joint position is presented by the mean of the same
class pixels. Since the head and hip, which are used in the
proposed fall detection, are the most visible body parts, we
can use body part classification to extract them. Fig. 3 shows
a classification result of the human body. Each color in Fig. 3
stands for the respective part of the human body with the highest
probability.

In order to obtain an optimized parameter pn of each split
node n of RDT, the computation complexity was very high in
[21] and [23]. Based on (2), we propose a fast training algorithm
which can reduce the number of candidate offsets significantly
in the training phase.
The training pixels with labels for each synthesized depth
image are randomly down sampled. The tree is trained using the
smallest Shannon entropy to split each node.
At each node, a weak learner parameter p((Δx, Δy), τ )
((Δx, Δy) ∈ (ΔX, ΔY ) is the pixel offset, τ ∈ T is the threshold value in (3)) induces a partition of input example set
Q = {I, (X, Y )} into left and right subsets by (3)
QL ((Δx, Δy), τ ) = {E ((x0 , y0 ); p((Δx, Δy), τ )) = 1}

(6)

QR ((Δx, Δy), τ ) = {E((x0 , y0 ); p((Δx, Δy), τ )) = 0}.

(7)

For each offset (Δx, Δy), compute the τ giving the smallest
Shannon entropy
τ ∗ = arg min S(Q((Δx, Δy), τ ))

B. Aggregating Predictions
The result of RDT algorithm is the classification of each pixel.
Then, the joint position is found by a mean shift method. The
mean shift is based on a Gaussian kernel and a weighting factor.
For the mean shift, the density estimator [21] of each body part
c is defined as
 
 
N

 L − Li 2

wic exp − 
(4)
Jc (L) ∝
 bc 
i=1
where c is the body part label, L is the 3-D location in the 3-D
real world, wic is a pixel weighting, N is the number of total test
pixels in the test image I, and Li is the 3-D location of the test
pixel (xi , yi ). bc is a bandwidth for body part c. wic includes two
factors: 1) the probability P from the RDT algorithm and 2) the
world surface area related to the depth value z. The formulation

τ ∈T

(8)

S(Q((Δx, Δy), τ ))
 |Qsub ((Δx, Δy), τ )|
H(Qsub ((Δx, Δy), τ ))
=
Q
sub∈L,R

(9)
where H(Q) is the Shannon entropy (computed on the probability of body part labels) of set Q. S(Q) is the sum of Shannon
entropy.
Fig. 4(a) is a map in a node by drawing Shannon entropy (given by p((Δx, Δy), τ ∗ )) on the corresponding location (Δx, Δy). Fig. 4(b) is a mesh view of Fig. 4(a).
From Fig. 4(b), it can be noted that the Shannon entropy
surface is smooth. The smallest Shannon entropy in this
surface can be efficiently searched out by some search algorithms. Thus, just a few offsets need to be trained by (6), (7)

BIAN et al.: FALL DETECTION BASED ON BODY PART TRACKING USING A DEPTH CAMERA

433

TABLE I
COMPARISON OF ALGORITHMS BASED ON [21] AND OURS

[21]
Ours

Training time per tree (hour)

Test time per frame (ms)

Mean error (cm)

310
3.9

5.0
2.8

3.2
3.1

for each node. Equations (6) and (7) should be tested by the
whole set Q = {I, (X, Y )} of input examples in a node, and
this operation takes a long training time. Therefore, using (2)
and a suitable search algorithm, it can dramatically save training
cost. In contrast, the feature in (1) requires two offsets, which
require randomly sampling 2000 candidate offset pairs among
(2M + 1)4 candidate offset pairs in [21] and [23], where M is
the range of Δx, Δy, i.e., Δx, Δy ∈ [−M, M ].
The parameter p with the smallest Shannon entropy in all
candidate offsets and thresholds is
p((Δx∗ , Δy ∗ ), τ ∗ ) =

arg min

Fig. 5.

Joint extraction results.

Fig. 6.

After rotation, the head and hip center joints can be extracted correctly.

S (Q((Δx, Δy), τ ))

(Δ x,Δ y )∈(Δ X ,Δ Y ),τ ∈T

=

arg min

{min S(Q((Δx, Δy), τ ))}.

(Δ x,Δ y )∈(Δ X ,Δ Y ) τ ∈T

(10)

If the depth of the tree is not too large, the training algorithm
recurs for the left and right child nodes with example subsets
QL ((Δx∗ , Δy ∗ ), τ ∗ ) and QR ((Δx∗ , Δy ∗ ), τ ∗ )), respectively,
according to p((Δx∗ , Δy ∗ ), τ ∗ ).
Table I demonstrates the performances of the algorithms
based on [21] and ours. Based on (2) and a search algorithm, it
takes less than 4 h to train one tree with 20 levels from 10 000
images on a standard PC using one core by MATLAB. There
are 1000 training example pixels in each image, 33 candidate
thresholds per offset, and 24 dynamic candidate offsets. It takes
310 h with 2000 pairs of candidate offset based on (1). 2000
images with ground truth of the head joint are used to test the
performance of these two algorithms. The test is implemented
on a standard PC using one core by C++. Based on (2), the test
time per frame is 2.8 ms. Based on (1), the test time per frame
is 5.0 ms, which is 79% more than 2.8 ms. The mean error is
measured on the head joint. Their mean errors, i.e., 3.2 cm (1)
and 3.1 cm (2), are very similar without substantial difference.
D. Pose Correction
In order to track the joint trajectory well, the frame rate of
the camera output should be high and the joint extraction should
be fast. The frame rate of Kinect is 30 frames/s and the joint
extraction based on RDT takes a few milliseconds. Thus, the
joint trajectories can be tracked well.
Based on RDT algorithm, an open license software has been
released, i.e., Kinect for windows SDK [16]. Fig. 5(a) shows
that the human joint extraction from the SDK is correct when
the person is standing even moving a chair at the same time.
However, the SDK joint extraction cannot work well under
some scenarios, such as the person lying on a sofa as shown
in Fig. 5(b). The degraded accuracy problem also appears in

the moment of falling down due to the human body orientation
changes dramatically while falling.
We propose to rotate the depth image in Fig. 5(b) by 90◦
clockwise, and the key joint positions can be extracted accurately as shown in Fig. 6. As inspired from this, the human torso
is always rotated to be vertical in the image before the joint extraction process in the proposed fall detection system. Thanks to
the high output frame rate of the depth camera, it is fast enough
to track the human motion during falls. The torso orientation
can be defined by the straight line through the hip center and
the head. The person’s torso orientation changes very little per
frame even during the fall. Thus, the torso orientation can be
corrected well enough by rotating the torso based on the pose
in the previous frame. Therefore, the joint extraction is pose
invariant.
When the person walks into the view field at the first time, the
head and hip center are extracted. This information is used to
rotate the torso orientation for the next frame, and the rotation
angle is updated frame by frame. As shown in Fig. 7, when
the person is falling down [see Fig. 7(a)], the torso orientation
can be always rotated to be vertical in the image [see Fig. 7(b)]
based on the previous frame.
The reinitialization is required when the torso orientation
tracking is lost. As shown in Fig. 8, the input image after the
subject segmentation was rotated by several angles, such as 0◦ ,
120◦ , and 240◦ , to generate several images. For each image,
the head and the hip center are extracted to obtain the torso
orientation, and then the extracted torso orientation is corrected

434

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

Fig. 9. Two patterns of head distance trajectory from a same video sequence.
(a) Fall pattern. (b) Nonfall pattern.

A. Fall Motion Analysis Based on SVM
Fig. 7. Simulated fall sequence. (a) Original poses. (b) After correction by
proposed method.

“Inadvertently coming to rest on the ground, floor or other
lower level, excluding intentional change in position to rest in
furniture, wall or other objects” is the definition of fall by World
Health Organizations [24]. After the joint extraction, “coming
to” “the ground” can be described in a technical word, i.e., some
key joints coming to the ground. This feature can be used for
fall detection.
Referring to the above feature, the fall is an activity related
to the floor. The floor information should be measured. In [17],
they assume that the floor occupies a sufficiently large part in
the image. However, this assumption is not always true such as
a small bedroom with a bed. In the proposed system, the floor
plane is defined by choosing three points from the input depth
image when the depth camera is setup. The floor plane equation
can be described as
Axw + Byw + Czw + D = 0
2

2

2

A +B +C =1

Fig. 8.

Example of reinitialization.

to be vertical. The extraction and correction are repeated several
times. After the extractions and corrections, three candidate
final rotation angles are obtained. It needs to select the best one
from the three candidates. The selection metric is defined by the
density estimator (4) during the processing of joint extraction.
For simplicity, the decision can be made by using the density
estimator of the head portion.
To speed up the joint extraction, the rotation of the depth
image can be taken place by rotating the offset parameter
(Δxn , Δyn ) of each split node n of tree. This rotation of parameters can be done offline after training. The rotation angles
are fixed, such as 10◦ , 20◦ , . . . , 350◦ .
III. FALL DETECTION BASED ON SVM
This section describes the fall detection based on SVM, which
employs the head joint distance trajectory as input feature vector.

(11)
(12)

where A, B, C, and D are coefficients, xw , yw , and zw are
real-world coordinate variables. The coefficients A, B, C, and
D will be determined after choosing three points.
The joint distance trajectory is defined as the trajectory of the
distance between the joint and the floor. This signal includes
the fall motion information, such as the acceleration, the joint
velocity, the distance between the human joint and the floor
and other hidden information. Fig. 9 shows a fall pattern and a
nonfall pattern of the head distance trajectory. The fall motion
can be classified by the joint distance trajectory pattern.
A d-dimensional feature vector is formed by the distance trajectory in d consecutive frames. d should be large enough to
cover all the phases of falling including rapid movement period
during the fall, the period before the fall, and the period after
the fall. The fall detection can be seen as two classes classification problem. SVM is very suitable for two classes classification problem. It can automatically learn the maximum-margin
hyperplane for classification. The feature vector as aforementioned described can be used as the input feature vector of SVM
classifier.

BIAN et al.: FALL DETECTION BASED ON BODY PART TRACKING USING A DEPTH CAMERA

435

After the fall motion, if the person cannot recover within a
certain time, fall detection will be confirmed. Without a fall
confirmation, the fall alert will not be triggered. This stage can
avoid the false alert.
IV. EXPERIMENTAL RESULTS
To test the proposed method, some normal activities (like
crouching down, standing up, sitting down, walking) and falls,
which are simulated by the human, have been tested.
Fig. 10. Free fall body trajectory fits the head distance trajectory. (a) Without
noise. (b) Free fall body with noises.

Fig. 11.

A. Performance Evaluation Metric
The following parameters suggested by [2] are used to analyze
the detection results of the proposed algorithm.
1) True positives (TP): the number of fall events detected
correctly.
2) True negatives (TN): the number of nonfall events detected
correctly.
3) False positives (FP): the number of nonfall events detected
as fall events.
4) False negatives (FN): the number of fall events detected
as nonfall events.
5) Sensitivity (Se): the capacity to detect fall events

Nonfall patterns simulated by computer.

Se =

B. Training Dataset
Since the 3-D head joint trajectory has been tracking, the
head joint motion can be analyzed by the physics mechanics
principle. During the falling phase, the joint motion can be seen
as a free fall body. The free fall body is described as a simple
formula
1
h(t) = h0 + a(t − t0 )2
2

C. Fall Confirmation
In order to confirm the fall detection, the recover motion
analysis after fall motion is required.
There are two recover metrics: a) the heights of hip and head
are higher than a recover threshold value Trecover1 for a certain
time; and b) the height of the head is higher than a high recover
threshold value Trecover2 for a certain time. If one of these two
metrics is satisfied, it means that the person is recovered. In our
experiment, we set Trecover1 = 0.5 m and Trecover2 = 0.8 m.

(14)

6) Specificity (Sp): the capacity to detect nonfall events
Sp =

TN
.
TN + FP

(15)

7) Accuracy (Ac): the correct classification rate

(13)

where h(t) is the height at the time t, h0 is the height at the
beginning of fall, a is the acceleration, t is the current time,
and t0 is the beginning time. The free fall body can be used
to simulate the joint fall motion to generate fall patterns by
computer. Fig. 10(a) shows a free fall body trajectory fitting
the head distance trajectory of a falling person. It can be noted
that the free fall body curve fits well. The difference can be
considered as Gaussian white noises. In order to improve the
robustness, Gaussian white noises are added into the free fall
body curve, as shown in Fig. 10(b). Some nonfall patterns can
also be simulated by computer, as shown in Fig. 11. Based on
the free fall body simulation, a large fall and nonfall patterns
dataset can be built up.

TP
.
TP + FN

Ac =

TP + TN
.
TP + TN + FP + FN

(16)

8) Error rate (Er): the incorrect classification rate
Er =

FP + FN
.
TP + TN + FP + FN

(17)

A high sensitivity means that most falls are detected. A high
specificity means that most nonfalls are detected as nonfall. A
good method for fall detection should have a high sensitivity
and a high specificity. Besides, the accuracy should be high and
the error rate should be low.
B. Dataset
The nonfall and fall activities were simulated as shown in
Table II. They are suggested by [2], but with more detailed
description. During an impactive fall, the elderly person cannot
keep the transient pose kneeling or sitting on the floor. Therefore,
the person will lie on the floor after fall as shown in the first
row and fifth row of scenarios in Table II. In Table II, “Positive”
and “Negative” means fall and nonfall, respectively. In total,
there are 20 scenarios 50% are positive and 50% are negative.
Each scenario is simulated several times. Totally, there are 380
samples. There are four subjects, and their heights, ages, and
weights are 159–182 cm, 24–31 years, and 48–85 kg. Three are
male and one is female. The experiments are in a real bedroom,

436

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

TABLE II
SCENARIOS FOR THE EVALUATION OF FALL DETECTION [2]
Category
Backward fall

Forward fall

Lateral fall to the right
Lateral fall to the left
Syncope
Neutral

Description

Outcome

On the hip, then lying
Ending lying
Ending in lateral position
With recovery
On the knees, then lying
With forward arm protection
Ending lying
With rotation, ending in the lateral right position
With rotation, ending in the lateral to the left position
With recovery
Ending lying
With recovery
Ending lying
With recovery
Vertical slipping against a wall finishing in sitting position
To sit down on a chair then to stand up
To lie down on the bed then to rise up
Walk a few meters
To bend down, catch something on the floor, then to rise up
To cough or sneeze

Positive
Positive
Positive
Negative
Positive
Positive
Positive
Positive
Positive
Negative
Positive
Negative
Positive
Negative
Negative
Negative
Negative
Negative
Negative
Negative

TABLE III
RESULTS OF FALL MOTION DETECTION BASED ON SVM

Fig. 12.

TP

TN

FP

FN

Se(%)

Sp(%)

Ac(%)

Er(%)

182

190

0

8

95.8

100

97.9

2.1

Room plan for the evaluation.

as shown in Fig. 12. The camera is mounted 2.3 m height on the
wall.
The research was approved by the Institutional Review Board
of Nanyang Technological University, Singapore.

Fig. 13. Head distance trajectory during the fall of an FN sample of fall motion
detection by SVM classifier. The circle indicates the rebound behavior.

C. Performance Validation
To test the performance of SVM classifier method, a large
training dataset, which includes about 100k nonfall and fall patterns of head distance trajectory, is generated by computer. After
training, the SVM classifier is used to detect falls in the dataset
of human simulated scenarios in Table II. The experimental results of fall motion analysis (detection) of SVM classifier is
shown in Table III. For the fall motion detection, there is only
eight error results, which are FN errors. The head distance trajectory during fall of an FN sample is shown in Fig. 13. There
is an air mattress on the floor to protect the subject, as shown
in Fig. 14. In this fall event, when the subject falls backward,
the air mattress bounces the body of the subject quickly that the

Fig. 14.

There is an air mattress on the floor to protect the subject.

BIAN et al.: FALL DETECTION BASED ON BODY PART TRACKING USING A DEPTH CAMERA

437

TABLE IV
COMPARISON OF DIFFERENT APPROACHES FOR FALL DETECTION

[17]
[18]
Proposed

TP

TN

FP

FN

Se(%)

Sp(%)

Ac(%)

Er(%)

188
175
181

104
151
190

86
39
0

2
15
9

98.9
92.1
95.3

54.7
79.5
100

76.8
85.8
97.6

23.2
14.2
2.4

head distance trajectory is indicated by a circle in Fig. 13. The
SVM classifier cannot have a correct decision on this head distance trajectory. If there is no air mattress, the dramatic rebound
would not happen. In the fall confirmation stage, there is one
fall event sample lost tracking the subject. In this sample, when
the subject is lying on the floor, the system cannot segment the
subject and recognizes that there is no subject in the view field.
The fall motion in this sample has been detected correctly by the
SVM classifier. However, the system fails to confirm this fall
event in the fall confirmation stage. It is due to failing in subject
segmentation operated by Kinect SDK. In the fall confirmation
stage, all the confirmation results are correct, except this lost
tracking sample. Without a fall confirmation, the fall alert will
not be triggered. Though the fall confirmation stage misses the
lost tracking sample, this stage can avoid the false alert effectively. Thus, the system misses nine fall alerts, and there is no
false alert.
A video demonstration of fall detection based on SVM is
available in the website1 .
D. Comparison
To further evaluate the proposed algorithm, we compared it
with two state-of-the-art approaches based on depth camera.
The approach of [17] is based on human silhouette center
height relative to the floor. The human silhouette center is obtained by the whole foreground (with morphological filtering).
When the silhouette center is lower than a threshold, a fall is
detected. The experimental results are shown in row [17] of
Table IV. This algorithm can detect the most of fall events, but
with a lot of false positives (FP). It cannot distinguish the fall
accident and the nonimpact initiative activities well since it does
not consider the motion together. When most part of the foreground object is near to the floor, including slowly lying down
or sitting down on the floor or bad segmentation, it is detected
as fall. The center location is easily distorted by moving object
and bad segmentation. To these events, the two key joints, head
and hip, are still tracked well by the proposed method in our
experiment. The proposed joint tracking method has a better
robustness.
The approach of [18] makes use of the orientation of the body,
which is based on the joints extracted from Kinect SDK, and the
height information of the spine. If the orientation of the body is
parallel to the floor and the spine distance to the floor is smaller
than a threshold, a fall is detected. The experimental results are
shown in row [18] of Table IV. The main disadvantage of this
1 http://www.ntu.edu.sg/home/elpchau/FallDetect2013.wmv

Fig. 15. Two examples of the angle between the orientation extracted from
Kinect SDK and the floor when the subject walked and fell down ending lying.
(a) Two depth image samples. (b) Angle waveforms corresponded to the two
sequences of (a).

approach is the unreliable joints extraction. When the subject
falls and is lying on the floor, the joint extraction is inaccurate
and the orientation of the body obtained from inaccurate joints
provides false information. Fig. 15 shows two examples of the
angle between the orientation extracted from Kinect SDK and
the floor when the subject walked and fell down ending lying.
From Fig. 15, it can be noted that the orientations extracted from
Kinect SDK were wrong when the subject fell and was lying
on the floor. Based on unrobust joint extraction and predefined
empirical thresholds, this method’s capacity of prediction is
limited.
Combined with the fall confirmation, the error rate of the
proposed method is 2.4%. As shown in Table IV, the proposed
approach outperforms the existing state-of-the-art approaches.
Furthermore, compared with [18], which is required to extract
at least six joints, only two joints are required in the proposed
algorithm. Combined with the higher efficiency feature in (2),
the proposed algorithm has a lower computational complexity.
V. CONCLUSION
The proposed fall detection approach uses the infra-red-based
depth camera, so the approach can operate even in the dark condition. The depth camera can measure the human body motion
and the relationship between the body and the environment. The
floor plane can be extracted from the depth images. To capture
the human motion, an enhanced RDT algorithm, which reduces
the computational complexity based on the one offset feature,
is employed to extract the human joints. Existing fall detection
method based on joint extraction cannot extract human joints
correctly when the subject lies down. The proposed rotation
of the person torso orientation increases the accuracy of the
joint extraction for fall detection. After extracting the joints, an

438

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 2, MARCH 2015

SVM classifier is proposed to detect the fall based on the joint
trajectory. The proposed approach is based on a single depth
camera. Because the proposed motion analysis is based on the
head tracking and the depth camera can be mounted close to
the ceiling, it can avoid most occlusion situations. In the case
of occlusion problem, multiple depth cameras based on the proposed approach can solve the problem. However, the proposed
approach cannot detect the fall ending lying on furniture, for
example, a wooden sofa, since the distance between the body
and the floor is too high.
The proposed RDT training algorithm based on the one offset
feature reduces the number of candidate offsets significantly
from 2000 to 24. Therefore, the computational complexity of
building RDT for fall detection can be reduced by 83 times
under the same training condition.
Based on the depth image sequence, by extracting and tracking the joints of the human body as well as investigating the
joints’ behavior after fall, the proposed approach can detect and
confirm the human fall accurately. Experimental results show
that the accuracy of the proposed algorithm is improved by
11.8% compared with the most recent state-of-the-art fall detection algorithm.

REFERENCES
[1] United Nations, Dept. Econ. Social Affairs, Population Division (2009).
World Population Prospects: The 2008 Revision, Highlights, Working
Paper No. ESA/P/WP.210.
[2] N. Noury, A. Fleury, P. Rumeau, A. K. Bourke, G. O. Laighin, V. Rialle,
and J. E. Lundy, “Fall detection-principles and methods,” in Proc. IEEE
29th Int. Conf. Eng. Med. Biol. Soc., 2007, pp. 1663–1666.
[3] WHO (World Health Organization), Good health adds life to years: Global
brief for World Health Day 2012, April 2012, WHO reference number:
WHO/DCO/WHD/2012.2.
[4] X. Yu, “Approaches and principles of fall detection for elderly and patient,”
in Proc. IEEE 10th Int. Conf. e-Health Netw., Appl. Serv., 2008, pp. 42–47.
[5] M. Mubashir, L. Shao, and L. Seed, “A survey on fall detection: Principles
and approaches,” Neurocomputing, vol. 100, pp. 144–152, 2013.
[6] N. Thome, S. Miguet, and S. Ambellouis, “A real-time, multiview fall
detection system: A LHMM-based approach,” IEEE Trans. Circuits Syst.
Video Technol., vol. 18, no. 11, pp. 1522–1532, Nov. 2008.
[7] M. Yu, A. Rhuma, S. Naqvi, L. Wang, and J. Chambers, “A posture
recognition-based fall detection system for monitoring an elderly person in a smart home environment,” IEEE Trans. Inf. Technol. Biomed.,
vol. 16, no. 6, pp. 1274–1286, Nov. 2012.
[8] D. Brulin, Y. Benezeth, and E. Courtial, “Posture recognition based on
fuzzy logic for home monitoring of the elderly,” IEEE Trans. Inf. Technol.
Biomed., vol. 16, no. 5, pp. 974–982, Sep. 2012.
[9] M. Yu, Y. Yu, A. Rhuma, S. Naqvi, L. Wang, and J. Chambers, “An online
one class support vector machine based person-specific fall detection system for monitoring an elderly individual in a room environment,” IEEE J.
Biomed. Health Informat., vol. 17, no. 6, pp. 1002–1014, Nov. 2013.
[10] H. Foroughi, B. S. Aski, and H. Pourreza, “Intelligent video surveillance
for monitoring fall detection of elderly in home environments,” in Proc.
IEEE 11th Int. Conf. Comput. Inf. Technol., 2008, pp. 219–224.
[11] C. Rougier, J. Meunier, A. St-Arnaud, and J. Rousseau, “Robust video
surveillance for fall detection based on human shape deformation,” IEEE
Trans. Circuits Syst. Video Technol., vol. 21, no. 5, pp. 611–622, May
2011.
[12] C. Rougier, and J. Meunie, “Fall detection using 3D head trajectory extracted from a single camera video sequence,” in Proc. First Int. Work-shop
Video Process. Security, 2006.
[13] C. Rougier, J. Meunier, A. St-Arnaud, and J. Rousseau, “3D head tracking
for fall detection using a single calibrated camera,” Image Vis. Comput.,
vol. 31, no. 3, pp. 246–254, Mar. 2013.
[14] E. Auvinet, F. Multon, A. Saint-Arnaud, J. Rousseau, and J. Meunier, “Fall
detection with multiple cameras: An occlusion-resistant method based on

[15]

[16]
[17]

[18]
[19]
[20]
[21]

[22]
[23]

[24]

3-D silhouette vertical distribution,” IEEE Trans. Inf. Technol. Biomed.,
vol. 15, no. 2, pp. 290–300, Mar. 2011.
C. Doukas and I. Maglogiannis, “Emergency fall incidents detection in
assisted living environments utilizing motion, sound, and visual perceptual
components,” IEEE Trans. Inf. Technol. Biomed., vol. 15, no. 2, pp. 277–
289, Mar. 2011.
Microsoft. (2012). [Online]. Available: http://www.microsoft.com/en-us/
kinectforwindows/
C. Rougier, E. Auvinet, J. Rousseau, M. Mignotte, and J. Meunier, “Fall
detection from depth map video sequences,” in Proc. 9th Int. Conf. Toward
Useful Serv. Elderly People Disabilities: Smart Homes Health Telematics,
2011, pp. 121–128.
R. Planinc and M. Kampel, “Introducing the use of depth data for fall
detection,” Personal Ubiquitous Comput., vol. 17, pp. 1063–1072, 2012.
Z. P. Bian, L. P. Chau, and N. Magnenat-Thalmann, “A depth video approach for fall detection based on human joints height and falling velocity,”
in Proc. Int. Conf. Comput. Animation Social Agents, May 2012.
Z.-P. Bian, L.-P. Chau, and N. Magnenat-Thalmann, “Fall detection based
on skeleton extraction,” in Proc. 11th ACM SIGGRAPH Int. Conf. VirtualReality Continuum Appl. Ind., 2012, pp. 91–94.
J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finocchio, R. Moore, A.
Kipman, and A. Blake, “Real-time human pose recognition in parts from
single depth images,” in Proc. IEEE Conf. Comput. Vis. Pattern Recog.,
Jun. 2011, pp. 1297–1304.
P. Lagger and P. Fua, “Randomized trees for real-time keypoint recognition,” in Proc. IEEE Comput. Soc. Conf. Comput. Vision Pattern Recog.,
2005, pp. 775–781.
K. Buys, C. Cagniart, A. Baksheev, T. D. Laet, J. D. Schutter, and C.
Pantofaru, “An adaptable system for RGB-D based human body detection
and pose estimation,” J. Visual Commun. Image Representation., vol. 25,
pp. 39–52, 2014.
W.H.O., WHO global report on falls prevention in older age World Health
Organization (WHO) Library Cataloguing-in-Publication Data, Geneva,
Switzerland, 2007.

Zhen-Peng Bian (S’13) received the B.Eng. degree
in microelectronics from South China University of
Technology, Guangzhou, China, in 2007. He is currently working toward the Ph.D degree at the School
of Electrical and Electronic Engineering, Nanyang
Technological University, Singapore.
His current research interests include fall detection, motion capture, human–computer interaction,
and image processing.

Junhui Hou (S’13) received the B.Eng. degree
in information engineering (Talented Students Program) from South China University of Technology,
Guangzhou, China, and the M.Eng. degree in signal and information processing from Northwestern
Polytechnical University, Xi’an, China, in 2009 and
2012, respectively. He is currently working toward
the Ph.D. degree at the School of Electrical and Electronic Engineering, Nanyang Technological University, Singapore.
His current research interests include video compression, image processing, and computer graphics processing.

BIAN et al.: FALL DETECTION BASED ON BODY PART TRACKING USING A DEPTH CAMERA

Lap-Pui Chau (SM’03) received the B.Eng. degree
(first class Hons.) in electronic engineering from
Oxford Brookes University, Oxford, U.K., and the
Ph.D. degree in electronic engineering from Hong
Kong Polytechnic University, Hong Kong, in 1992
and 1997, respectively.
In June 1996, he joined Tritech Microelectronics
as a Senior Engineer. In March 1997, he joined Centre for Signal Processing, National Research Centre,
Nanyang Technological University as a Research Fellow, subsequently he joined School of Electrical and
Electronic Engineering, Nanyang Technological University as an Assistant Professor and currently, he is an Associate Professor. His research interests include
fast signal processing algorithms, scalable video and video transcoding, robust
video transmission, image representation for 3-D content delivery, and imagebased human skeleton extraction.
Dr. Chau was involved in organization committee of international conferences
including the IEEE International Conference on Image Processing (ICIP 2010,
ICIP 2004), and IEEE International Conference on Multimedia & Expo (ICME
2010). He is a Technical Program Co-Chairs for Visual Communications and
Image Processing (VCIP 2013) and 2010 International Symposium on Intelligent Signal Processing and Communications Systems (ISPACS 2010). He was
the Chair of Technical Committee on Circuits and Systems for Communications
(TC-CASC) of IEEE Circuits and Systems Society from 2010 to 2012, and the
Chairman of IEEE Singapore Circuits and Systems Chapter from 2009 to 2010.
He served as an Associate Editor for IEEE TRANSACTIONS ON MULTIMEDIA,
IEEE SIGNAL PROCESSING LETTERS, and is currently serving as an Associate
Editor for IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY, IEEE TRANSACTIONS ON BROADCASTING, and IEEE CIRCUITS AND
SYSTEMS SOCIETY NEWSLETTER. Besides, he is IEEE Distinguished Lecturer
for 2009–2013, and a steering committee member of IEEE TRANSACTIONS FOR
MOBILE COMPUTING from 2011–2013.

439

Nadia Magnenat-Thalmann received several Bachelor’s and Master’s degrees in various disciplines
(psychology, biology, and biochemistry) and the
Ph.D. degree in quantum physics from the University of Geneva, Geneva, Switzerland, in 1977.
She has pioneered various aspects of research of
virtual humans for more than the last 30 years. From
1977 to 1989, she was a Professor at the University of Montreal, Canada. In 1989, she moved to the
University of Geneva where she founded the interdisciplinary research group MIRALab. She is an Editorin-Chief of The Visual Computer Journal published by Springer Verlag, and
Editor of several other journals.
Dr. Magnenat-Thalmann has received more than 30 Awards in her career.
Among the recent ones, two Doctor Honoris Causa (Leibniz University of
Hanover, Germany, and University of Ottawa, Canada), the Distinguished Career
Award from the Eurographics in Norrkoping, Sweden, and a Career Achievement Award from the Canadian Human Computer Communications Society,
Toronto. Very recently, she received the prestigious Humboldt Research Award
in Germany. Besides directing her research group MIRALab in Switzerland,
she is presently a Visiting Professor and the Director of the Institute for Media
Innovation (IMI), Nanyang Technological University, Singapore.

