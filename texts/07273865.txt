IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

991

Computer-Vision-Assisted Palm Rehabilitation
With Supervised Learning
K. M. Vamsikrishna, Debi Prosad Dogra∗ , Member, IEEE, and Maunendra Sankar Desarkar

Abstract—Physical rehabilitation supported by the computerassisted-interface is gaining popularity among health-care fraternity. In this paper, we have proposed a computer-vision-assisted
contactless methodology to facilitate palm and finger rehabilitation. Leap motion controller has been interfaced with a computing
device to record parameters describing 3-D movements of the palm
of a user undergoing rehabilitation. We have proposed an interface
using Unity3D development platform. Our interface is capable of
analyzing intermediate steps of rehabilitation without the help of
an expert, and it can provide online feedback to the user. Isolated
gestures are classified using linear discriminant analysis (DA) and
support vector machines (SVM). Finally, a set of discrete hidden
Markov models (HMM) have been used to classify gesture sequence
performed during rehabilitation. Experimental validation using a
large number of samples collected from healthy volunteers reveals
that DA and SVM perform similarly while applied on isolated gesture recognition. We have compared the results of HMM-based
sequence classification with CRF-based techniques. Our results
confirm that both HMM and CRF perform quite similarly when
tested on gesture sequences. The proposed system can be used for
home-based palm or finger rehabilitation in the absence of experts.
Index Terms—Finger tracking, gesture recognition, human–
computer interface, physical rehabilitation, palm tracking.

I. INTRODUCTION
OMPUTER-AIDED systems in health-care are becoming
popular among doctors, agencies providing health-care
services, patients, and researchers. Sensors used to design such
systems are becoming more accurate and cheaper. This has triggered a huge paradigm shift. Therefore, various types of sensors
are used to design low-cost home-based physical rehabilitation
systems. For example, IMU [6] sensor, tilt sensor [24], and camera [34] based systems are becoming popular. Even, people have
started designing software tools for hand-held devices, such as
smart phones, tablets [19], [25], to support health-care services,
since these devices are equipped with additional sensors.
Usually, health-care services are availed in various modes,
i.e., at outdoor unit of hospitals, inside critical care units or
ambulatory setup, inhouse arrangement, etc. However, trained
manpower is necessary irrespective of the service modalities. As

C

Manuscript received May 4, 2015; revised August 11, 2015; accepted September 13, 2015. Date of publication September 22, 2015; date of current version
May 19, 2016. Asterisk indicates corresponding author.
K. M. Vamsikrishna is with the School of Electrical Sciences, Indian Institute
of Technology.
∗ D. P. Dogra is with the School of Electrical Sciences, Indian Institute of
Technology, Bhubaneswar 751013, India (e-mail: dpdogra@iitbbs.ac.in).
M. S. Desarkar is with the Department of Computer Science and Engineering,
Indian Institute of Technology Hyderabad.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2015.2480881

reported by Wong and Hsien [33], even a well-developed country like Singapore also faces similar problem. The country is suffering from acute shortage of experts (specialist-to-population
ratio 1:1740) and nurses (nurse-to-population ratio 1:230). On
the other hand, fast developing nations, such as India, face similar problem, but at a larger scale. As per the report published
by Garg et al. [23], hospitals and health centers across India are
suffering from an acute shortage of well-trained specialists in
various domains. Therefore, solutions to address above problem
can be considered as a significant contribution in this field of
research.
People suffering from neurological disorders, stroke, or paralysis often require various kinds of exoskeleton rehabilitation and
physiotherapy to recover quickly [11]. Even muscular or bone
injuries are treated with follow-up rehabilitation. However, such
rehabilitation must be carried out under the strict supervision of
experts to avoid complications that may arise during the process.
Therefore, experts issue strict guidelines before home-based
treatments are allowed. With the help of electronic gadgets and
computing devices, inhouse treatments are becoming safe and
accepted by medical fraternity. However, in such cases, periodic
or offline monitoring is adopted to minimize the risk. For example, parameters recorded during offline rehabilitation can be
periodically delivered to the experts for analysis and feedback.
There are two important factors that influence motor recovery during physical therapy, namely intensity of motor output
(subject’s voluntary participation) and intensity of movementcorrelated sensory information [13]. However, using artificial
therapy device, such as robotic equipment, these two factors
may be in conflict. For example, if the robotic device completes
therapeutic movements independent of patient output, then the
motor output may reduce. On the other hand, the correlated sensory information is limited when the subject cannot complete
the movement without any assistance. Therefore, solutions with
minimal hardware setup are preferred. Contact-based palm or
upper limb rehabilitation systems are already in use and they
have certain advantages as well as disadvantages [7], [14]. For
example, the pattern recognition-based method to classify user
motion intent based on surface electromyography developed
by Amsuss et al. [21] has been tested in well-controlled laboratory conditions. However, it has not been tested for clinical
applications for upper limb prosthesis control due to lack of robustness. People have also shown commercial interest on such
devices. For example, the patent granted to Brassil and Brassil [2] presents a hardware design of such a device (hand glove)
that can be used for finger and palm rehabilitation. Similar systems for full body rehabilitation are also available [4]. Even
virtual reality-based systems proposed in [9] and[29] can be
used for rehabilitation purposes.

0018-9294 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

992

Though above systems are accurate, some inherent problems
with them are as follows: 1) These apparatus are mainly contactbased systems which are a burden to the patients and often they
feel uncomfortable with such artificial attachments [7]. 2) Many
of the existing systems require external power through battery
which is considered as a body hazard, especially to the patients [9]. 3) Robotic attachments may suffer due to the possibility of reduced motor output for lack of synchronization between
actual hand movement and forced movement [14]. Therefore,
researchers have focused on designing contactless systems with
the help of computer vision techniques [10], [18], [34]. However,
existing vision-based systems are limited in scopes. Accuracy
of such systems depends on various factors such as background
scene, illumination variation, and presence of environmental
noise. For example, the method proposed by Sucar et al. [18]
takes the help of the color ball attached with the hand gripper.
The model cannot support palm or finger rehabilitation since the
user needs to hold the gripper during rehabilitation. Therefore,
the advantage of the contactless approach through computer
vision gets lost in the process. Also, color-based tracking techniques may fail in case of complex backgrounds. On the other
hand, though the method proposed by Zariffa and Steeves [34]
does not require external support; however, it suffers from segmentation errors in alien environments. Recently, a game-based
human–computer interface has been proposed by Tan et al. [10].
However, they have assumed a simple background to extract the
palm area which is not realistic.
We have proposed a low-cost computer-vision-assisted setup
using Leap motion controller to overcome some of the limitations of the aforementioned techniques. Guna et al. [15] have
evaluated the device using several parameters and found that
its accuracy is quite high (error of 1.2 mm for dynamic scene)
when the readings are taken within 25 cm above the surface of
the device. Though devices such as Intel RealSense 3-D camera
or Kinect sensor can be used for similar tasks, however, Leap
motion controller has the added advantage of being extremely
portable. In addition to that, being relatively new to the community as compared to Leap controller, RealSense has not gone
through a time-tested rigorous evaluation process. Kinect sensor
is expensive and unable to detect small movements. Therefore,
we found this device to be suitable for the present application.
Our proposed Leap motion controller-based methodology can
detect precise movements of palm or finger within the field of
view of the sensors. The device came into use in 2012, and has
been used for designing applications related to human–computer
interfaces [16], serious gaming [3], etc. The device can be useful for computer-assisted limb rehabilitation. For example, this
can be used for palm or upper limb rehabilitation [12], stroke
rehabilitation [20], etc. Therefore, we have used this device as
the base hardware in our computer-vision-assisted palm rehabilitation setup. We have used machine learning techniques to
learn parameters that describe palm movements. After training
the system with adequate samples, we can successfully predict
gestures performed by a user undergoing rehabilitation. The
method is less susceptible to illumination variation, environmental noises due to the use of infrared sensors inside the Leap
motion controller.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

The rest of this paper is organized as follows. Problem formulation and novel contributing aspects are presented in Section II.
We present filtering, segmentation, and feature extraction steps
in Section III. Proposed methodology of palm and finger rehabilitation is presented in Section IV. Results and discussions are
presented in Section V. Conclusion and possible future extensions are presented in Section VI.
II. PROBLEM FORMULATION AND CONTRIBUTIONS
Automatic palm gesture recognition to facilitate upper arm
rehabilitation has been proposed in this paper. Palm movement
can be represented using the first-order Markov system. Such
assumptions are common in sequence detection, especially in
the context of augmented reality [26], human–computer interaction [10], and health-care related applications [28]. Our model
captures feature variations to train the parameters of the proposed Bayesian framework.
A. Problem Formulation
Palm movements during rehabilitation can be described using the following framework. Assume a set of k independent
gestures, say G = {g1 , g2 , g3 , . . . , gk }, can represent the position and orientation of the palm at a given point of time.
A user undergoing the rehabilitation process may require to
change his/her palm position in a predefined order of sequence
as suggested by experts. Let a gesture be represented using a set
of parameters describing the geometrical structure of the palm
and these parameters can be measured through visual features
captured using Leap motion controller. However, such measurements are prone to environmental and process noises. Therefore,
a Bayesian framework guided by the first-order Markov system
is used to predict the state of the palm from a given set of
observations. Let, x = {x1 , x2 , . . . , xn } be an n-dimensional
feature vector describing the orientation of the palm. Now,
given an instance, say x(i), represented by the vector x. We
need to find a suitable gj ∈ G that maximizes the probability
p(gj |x) = p(gj |x1 , x2 , x3 , . . . , xn ). Assuming the dimensions
of x are independent to each other, a Bayesian formulation of
the conditional probability p(gj |x) can be given as
p(gj |x) =

p(gj )p(x|gj )
.
p(x)

(1)

In the present context, we can estimate the likelihood, i.e.,
p(x|gj ), with the help of supervised learning techniques using a set of labeled data. A user undergoing the rehabilitation
process is asked to perform a set of predefined sequence of
gestures and feature vectors corresponding to those gestures
that are recorded. Therefore, following two distinct problems
must be solved to achieve our goals. 1) Let low-level features
representing a sequence of gestures performed by a user undergoing palm rehabilitation be given by [y1 , y2 , . . . , yt ]. Assume
the low-level feature set represents a sequence of l independent
gestures, i.e., γ1 → γ2 → . . . → γl , where γi ∈ G. One problem can be, given a time series representation of a sequence
using above low-level features, segment it into homogeneous
regions and map them to relevant isolated gestures. 2) The

VAMSIKRISHNA et al.: COMPUTER-VISION-ASSISTED PALM REHABILITATION WITH SUPERVISED LEARNING

993

Fig. 1. Set of gestures used as a vocabulary of symbols to facilitate palm or
finger rehabilitation. (a) g1 : Open hand, palm facing down. (b) g2 : Open hand,
middle finger down. (c) g3 : Open hand, palm facing left. (d) g4 : Closed feast,
thumb open. (e) g5 : Closed feast. (f) g6 : Middle, ring, and thumb closed. (g) g7 :
Open hand, index bend. (h) g8 : Middle, ring, and little closed. (i) Leap motion
controller-based setup that has been used for recording parameters related to
palm or finger rehabilitation.

second problem is as follows. Given a sequence of gestures
say Γ = γ1 → γ2 → . . . → γl , where γi ∈ G, our objective is
to determine whether Γ is valid or not with respect to a given
set of valid sequences. Mathematically, it can be represented
using (2), where Δ denotes the set of valid gesture sequences

valid,
if Γ ∈ Δ
Γ=
(2)
invalid, otherwise.

Fig. 2. (a) Raw signal representing average velocity of all fingers. (b)
Smoothed version of the signal. (c) Binary pattern generated using the threshold
defined in (3) to localize stable states representing gestures related to palm or
finger rehabilitation.

samples/s is adequate for our purpose and the computational
overhead is acceptable for online processing.

B. Contributions
The main contributions of our paper are as follows: 1) segmentation of the continuous signal acquired through Leap motion controller into isolated gestures; 2) representation of the
gestures using a set of features in 3-D; 3) learning various parameters of linear discriminant analysis (DA) and support vector
machines (SVM) classifiers; 4) training of hidden Markov models (HMMs) for recognition of gesture sequences and prediction
of test sequences; and 5) designing of a graphical user interface
to facilitate recording of gestures, train the system, and to support self-assessment of palm injury through rehabilitation.
III. FEATURE EXTRACTION
Palm rehabilitation can be facilitated by instructing users to
perform sequences of predefined gestures within the purview
of the sensor attached with the computing device. Depending
on the type and the grade of injury, number of gestures and
their sequences may vary. However, it is possible to prepare a
universal set of gestures that can cover major types of palm or
finger rehabilitation. To begin with, we have started with a set
of eight gestures as shown in Fig. 1. New gestures can be added
with the set to address more complex rehabilitation as and when
necessary. However, feature extraction process as described in
the following sections remains the same.
A. Signal Acquisition
In order to get the effective classification of the gestures,
one needs to have accurate knowledge about the location and
position of a user’s hand. A typical Leap motion controllerbased setup that has been used to record rehabilitation sessions
is presented in Fig. 1(i). The controller can deliver upto 200
samples/s. However, we have observed that recording at 150

B. Segmentation of Continuous Signal
Raw signals captured using the device cannot be used directly to extract features because of environmental noises and
unintentional motion artifacts. Though the quality of the raw
signal shown in Fig. 2(a) is reasonably good; however, the signal may not be as smooth as the example shown in the figure
due to unintentional finger movements or error in the tracking
process. We have applied a smoothing filter to remove such
noises. It is done as follows. We1 estimate
instantaneous vev +v 2 +v 3 +v 4 +v 5
locity of the palm as vipalm = i i 5i i i , where vik =

k )2 + (z k − z k )2 represents the
(xki − xki+1 )2 + (yik − yi+1
i
i+1
displacement of the kth finger-tip between two successive
frames. The triplet (xki , yik , zik ) represents the location of the
kth finger-tip at frame i.
Next, the mean of vipalm over a window of w frames is calculated and all samples within this window are replaced by the
mean. After several experiments with volunteers, we have decided the value of w = 20. In the next step, a velocity-guided
segmentation has been applied to demarcate regions corresponding to various states of the palm. It is necessary to localize start
and end points of the gestures across the time axis. It is done by
grouping samples corresponding to stable state (rest position).
The palm is said to be in stable state if the velocity of the palm
(vipalm ) is less than a predefined threshold value vth as given in

0, if|vipalm | < vth
(3)
B(i) =
1, otherwise.
Threshold (vth ) is estimated empirically using the following
heuristic. At the beginning of the training, every user is asked to
keep his or her palm at a stable condition (with less movement)
for a while. Next, we determine the maximum instantaneous

994

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

Fig. 3. (a) Geometrical description of the human palm where d1 , d2 , d3 , d4 , and d5 represent distance of finger tips from the center of palm. (b) Estimation
 4 ), and little: e(θ5 ). (c) Geometric
of various angles between p and unit vectors toward finger tips, i.e., thumb: a (θ1 ), index: b(θ2 ), middle: c(θ3 ), ring: d(θ
representation of the palm showing the normal vector (n ) and vector parallel to the palm surface (
p ) pointing toward the fingers. (d) Grab strength is minimum or
0. (e) Intermediate grab strength, e.g., when g = 0.5. (f) Situation when grab strength is maximum or 1.0.

velocity amongst all five fingers during the stable condition
which is then multiplied by a factor of 1.2 to get the value of vth .
Aforementioned heuristic has worked reasonably well for the
present application covering all 30 normal users and two users
with upper limb injury. We construct a binary pattern say B(i)
using the smoothed version of the signal where zeroes represent
stable states and ones represent movements or unstable states.
Example of a raw signal, corresponding smoothed version, and
segment boundaries are shown in Fig. 2. Given a binary segmentation of the signal, features corresponding to stable states
can be computed as described in the following section.
C. Description of the Feature Vector
In this section, a method to extract features that can represent stable states of the palm during rehabilitation is presented.
Various parameters related to the orientation of a user’s hand
such as grab strength (g), angle between fingers, normal vector
to the palm surface (n), unit vectors pointing toward finger tips
 e), and location of the palm center, are included in
(a, b, c, d,
the feature vector (f ). These parameters can be explained using
the diagram shown in Fig. 3.
Grab strength (g) is defined as the strength of a user’s grab
force. It indicates how close a hand is to being a fist. Any
finger that is not curled will reduce grab strength. Its value
varies between 0 (open hand) and 1 (complete grabbing). These
situations are depicted in Fig. 3(d)–(f). The normal vector to
the palm (n) as depicted in Fig. 3(c) represents the vector that

points downward, or “out” of the front surface when the hand is
flat. The direction is expressed as a unit vector pointing in the
same direction as the palm normal (that is, a vector orthogonal
to the palm). We have included x and y components of this
normal vector into the feature set. The magnitude and direction
of the vector along each finger can be computed from the 3D location of the finger-tip (xi (t), yi (t), zi (t)) at time t with
respect to the center of the palm. The palm vector p as depicted
in Fig. 3(c) is computing the resultant of all vectors along the
direction of the fingers. We have also included angles between
p and independent vectors along the fingers in the feature set.
Finally, finger-tip distance from the palm center completes f =
[g, x, y, θ1 , θ2 , θ3 , θ4 , θ5 , d1 , d2 , d3 , d4 , d5 ].
Other parameters, e.g., intermediate distance of Proximal
Phalanges, Proximal Phalanx, and Intermediate Phalanx between consecutive finger-pairs can be considered as features.
However, our empirical evaluation suggests that including such
features does not improve the recognition accuracy. This has
been elaborated in Section V. Therefore, we have restricted the
feature vector to a set of 13 parameters as mentioned earlier.
IV. PROPOSED METHODOLOGY OF REHABILITATION
Solutions of the two problems discussed in Section II-A are
proposed in this section. First problem deals with the identification of isolated gestures performed during rehabilitation. We
solve this with the help of supervised classifiers such as DA and
SVMs. The second problem deals with the recognition of gesture

VAMSIKRISHNA et al.: COMPUTER-VISION-ASSISTED PALM REHABILITATION WITH SUPERVISED LEARNING

Fig. 4.

995

Block diagram of the proposed methodology of palm and finger rehabilitation.

sequences which has been solved with HMM-based sequence
recognizer. Fig. 4 presents a block diagram of the proposed
methodology.
A. Analysis of Isolated Gestures

X ∈g i

Palm or finger rehabilitation usually involves performing a
set of predefined gestures. The set as given in Fig. 1 can be used
to represent these gestures. However, basic requirements remain
the same, i.e., demarcating gesture boundaries (segmentation),
feature extraction, training, and recognition of isolated gestures,
followed by recognition of gesture sequences. Once the first two
phases are over, feature vectors representing isolated segments
can be used to train the classifiers. We present DA- and SVMbased methodologies adopted in this paper to train classifiers and
recognize isolated gestures performed during rehabilitation.
1) Recognition Using DA: DA is a method of multivariate
analysis. It uses independent variables to distinguish among
groups or categories of dependent variables [31]. The method
creates discriminant functions that are necessary to predict the
group to which a test sample belongs. Since the number of
classes (e.g., gestures in the present context) is more than two,
we have adopted a multiclass linear DA system. The steps involved in multiclass analysis are described henceforth [31].
Step I: The algorithm begins with finding intraclass and interclass scatters from the elements of, say k, independent classes
or gestures available in the dictionary. Let these parameters be
represented using Sintra and Sinter . These values as given in (4)
and (5) can be estimated by training
Sintra =

|G |



(X − X̄i )(X − X̄i )T

(4)

i=1 X ∈g i

Sinter =

|G |

i=1

(X̄i − X̄)(X̄i − X̄)T

where mean (X̄i ) of each class i and overall mean (X̄) can be
computed using (6) and (7), such that mi and m denote number
of samples in class i and total samples in all classes
1 
X̄i =
X
(6)
mi

(5)

1 
mi X̄i .
m i=1
n

X̄ =

(7)

Step II: Once we obtain Sintra and Sinter , we can find the
transformation Φ that maximizes
(Φ) =

|ΦT Sinter Φ|
.
|ΦT Sinter Φ|

(8)

The transformation Φ can be obtained by solving the generalized
eigenvalue problem given in
Sinter Φ = λSintra Φ.

(9)

Step III: Finally, using the transformation Φ, classification can
be performed in the transformed space using Euclidean distance
and any new instance, say z, can be classified using (10), where
X̄k is the centroid of the kth class
arg min d(zΦ, X̄k Φ).

(10)

k

During training, a user is asked to perform each gesture (gi ∈ G)
multiple times. Assuming, gestures are repeated τ times by every
user, a total of τ and (|G| − 1)τ positive and negative samples
are generated for training.
2) Recognition Using SVM: We have also applied SVMs
for classification. It is an efficient classifier widely used in pattern recognition tasks [1]. Although SVM actually does a binary
classification, however, several classifiers can be combined to do
perform multiclass separation [27]. Principle behind the SVM
is to map the input data on to higher dimensional feature space
that is not linearly related to the input space and determine

996

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

a hyperplane separating the two classes of feature space with
maximum margin. The algorithm finds an optimal hyperplane
w · X + b = 0 that separates the training samples X1 , . . . , Xm
labeled using Y1 , . . . Ym such that Yi ∈ {−1, +1} [1]. The
optimization problem can be expressed using (11), such that
yi (w · Xi + b) ≥ 1 for i = 1, . . . m, where w and b define the
separating hyperplane w · X + b = 0
1
min ||w||2 .
w ,b 2

(11)

In order to enable nonlinear decision surfaces, it is possible
to map the input vectors to a higher dimensional space where
two classes are linearly separable. The dot product in this vector space can computed using a chosen kernel function. These
functions allow the computations (learning the classifier and
predictions using the classifier) to be performed efficiently in
high-dimensional feature space, without the need of actually
transforming the sample to the high dimension [1]. Now, considering a nonlinear mapping, the Lagrangian dual
 of the above
problem can be written as in (12), such that m
i=1 Yi αi = 0
and αi ≥ 0 for i = 1, . . . m, where α1 , . . . , αm are Lagrange
multipliers and K is a chosen kernel function
min
α

m m
m

1 
Yi Yj αi αj K(Xi , Xj ) −
αi .
2 i=1 j =1
i=1

(12)

Some commonly used kernel functions are linear (XiT Xj ), polynomial (γXiT Xj + r)d , γ > 0), radial basis function (RBF)
2
j ||
), γ > 0), and sigmoid (tanh(γXiT Xj +
(exp ( −γ ||X2σi −X
2
r)). Larger margin between the classes makes the classifier
robust, and less susceptible to noises. Therefore, in problems
where the input data are not linearly separable, larger margin
can be achieved by allowing the classifier to wrongly classify
some training samples [1]. As a result, soft margin classifiers,
where misclassification is allowed while training, often perform
better than hard margin classifiers where misclassification is not
allowed during training. In order to increase margin allowing
some misclassification, a slack variable ξi ≥ 0, i = 0, . . . m is
introduced in the optimization formulation as given below (13)
subject to Yi (w.Xi + b) ≥ 1 − ξi , ξi ≥ 0

1
min ||w||2 + C
ξi
w ,b 2
i=1

Classification using one against all or one against one approach can be adopted for a multiclass problem. In our application, one against all approach has been used to classify multiclass gestures. We have tested with all four types of kernels
and found that RBF performs most consistently, thus, selected
for classification of gestures in our application. This has been
verified through results presented in Section V.
B. Analysis of Gesture Sequences
The second problem described in Section II-A cannot be addressed alone by recognizing independent gestures. Errors in
isolated recognition can easily propagate through sequences.
However, if the sequence of gestures are fixed and known
in advance, contextual information can be used to minimize
such errors. In such cases, HMM- or conditional random field
(CRF)-based classifiers can be used. HMMs is extensively being
used in pattern classification tasks, such as speech and gesture
recognition since early 1980s [30]. However, first time, Yamato
et al. [17] have shown that discrete HMM can be quite handy
in vision-based applications such as gesture recognition. Since
then, several modifications have been proposed [5], [32]. We
have used a discrete representation of HMMs to train models
and classify gesture sequences. A description of the proposed
methodology of HMM-guided gesture-sequence classification
is presented in the following section.
1) Classification of Continuous Gestures Using HMM: Let
the set of gesture sequences (Δ) used in the rehabilitation be
known in advance. Let Γ be a sequence of gestures as given
in (15), such that γi ∈ G. Assuming time series representation
of each symbol say γi is known in advance. A discrete version
of the HMM has been used to estimate the parameters of the
Bayesian classifier
Γ = (γ1 )(γ2 ) . . . (γl ).

(15)

Every gesture (γi ) of a sequence is represented using a discrete time varying function as given in (16), where f (nT ) denotes the feature vector at time nT as described in Section III.
This is further used for training HMM parameters
γi = f (nT ),

n = 0, 1, . . . k.

(16)

m

(13)

where C is a penalty parameter that controls the relative importance of maximizing the margin and minimizing the amount
of slack. By choosing a moderate value of C, the chances of
the classifier overfitting the training data can be reduced without significant perturbation in the accuracy. Also, in real-world
problems, boundaries generally separate two classes that are
highly nonlinear. The training process involves solving a convex quadratic programming problem with equality and inequality constraints. Once trained, classification of a new pattern or
gesture say Xi can be solved by evaluating the decision function
given in
 m


αi Yi K(xi , X) + b .
f (X) = sgn
(14)
i=1

HMM is a well-known tool to describe joint probability of
a collection of hidden states observed through discrete random
variables. It assumes that the present observation variables depend only on the current hidden state. However, learning is very
important to predict correct sequence during recognition. We
have used Baum–Welch training algorithm to estimate maximum likelihood of the parameters of the HMM from a given set
of observed feature vectors. The algorithm uses EM technique
for maximization of the likelihood. A hidden Markov chain can
be described by θ = (A, B, π), where A, B, and π represent
state transition matrix, output matrix, and initial probability distribution, respectively. The algorithm finds a local maximum for
θ as given in (17), where Y represents the observation sequence.
The detailed method can be found in [30]
θ∗ = max P (Y |θ).
θ

(17)

VAMSIKRISHNA et al.: COMPUTER-VISION-ASSISTED PALM REHABILITATION WITH SUPERVISED LEARNING

The parameter θ that maximizes the probability of the observation can be used to predict observation given a set of test
vector [17]. Therefore, given a time series representation of
the feature vector describing a test sequence say Γ, we can
compute the probability of observing Γ using (17), (18), where
θi represents the parameters of the ith HMM that are learned
through training mentioned earlier and X represents the state of
observation

P (Γ|θi ) =
P (Γ|X, θi )P (X|θi ).
(18)
X

Since our aim is to classify a given test sequence to the best
possible model, we have constructed separate HMM for each
sequence and estimated P (Γ, θi ) for i = 1, 2, . . . , S, assuming
S number of sequences are available in the dataset. Finally,
the given test sequence is classified to one of the S classes as
mentioned in
arg max P (Γ|θi ) i = 1, 2, . . . S.
i

997

TABLE I
SEQUENCE OF GESTURES USED IN THIS PAPER
sequence #
S1
S2
S3
S4
S5
S6
S7
S8
S9
S1 0

gesture sequence
g1 → g2 → g4
g1 → g4 → g8
g1 → g2 → g5 → g6
g1 → g3 → g2 → g7
g1 → g5 → g8 → g5 → g6
g1 → g5 → g8 → g5 → g4
g1 → g4 → g6 → g2 → g8 → g2
g1 → g7 → g1 → g3 → g2 → g7 → g8
g1 → g2 → g3 → g4 → g5 → g6 → g7 → g8
g1 → g3 → g5 → g7 → g2 → g4 → g6 → g8

Each gesture was performed five times by every user.

(19)

V. RESULTS AND DISCUSSIONS
Methods described in the preceding sections have been tested
with a large set of data. Results of experiments are presented and
analyzed in this section. Experiments are conducted in multiple
phases to highlight key novelties of our proposed method. To
begin with, we present a set of protocols that were used during
experiments. Thirty healthy volunteers participated during the
study and each user performed every gesture multiple times. Experiments were carried out using the following eight different
configurations: E1: train the DA module with individual user’s
data and perform recognition of isolated gestures using test data
of the same user; E2: train of the DA module with all user’s
data and recognition of isolated gestures using test data of independent user; E3: training of the SVM module with individual
user’s data and perform recognition of isolated gestures using
test data of the same user; E4: training of the SVM module
with all user’s data and recognition of isolated gestures using
test data of independent user; E5: train the HMMs with labels
of isolated gestures obtained by DA-based classification and
recognition of gesture sequences using independent user’s test
data; E6: train the HMMs with labels of isolated gestures obtained by SVM-based classification and recognition of gesture
sequences using independent user’s test data; E7: recognition
of gesture sequence by concatenating the isolated gestures recognized using DA-based classification; and E8: recognition of
gesture sequence by concatenating the isolated gestures recognized using SVM-based classification. First four configurations
deal with isolated gesture recognition. Remaining four represent
the setups for gesture sequence recognition.
A. Dataset for Experiments
In this section, we present experimental data created and used
during validation of the proposed palm rehabilitation methodologies. Thirty healthy volunteers participated in our study. As
per our knowledge, none of them were suffering from upper
limb disabilities at the time of data collection. We collected

Fig. 5. Results of classification using DA and SVM obtained with E1 and
E3 experimental setups across various gestures with SVM parameters such as
C = 9.3 × 10 −5 and γ = 1.2.

data in two phases, i.e., training and test. During training on
isolated gestures, every user was asked to perform each gesture
100 times. Therefore, a total of 24 000 (8 × 30 × 100) samples
were recorded. In the next phase, volunteers were asked to perform every sequence listed in Table I. This set was found to be
exhaustive as it represents a wide range of movements during
finger or palm rehabilitation. However, we kept option in our
design to add more sequences for future expansion. It may be
observed that sequences vary in length. Some of these gestures
were repeated within a sequence and across various sequences to
make the dataset robust. Each sequence was repeated five times
by every user. Therefore, a total of 150 samples were collected
for each sequence with an overall collection of 1500 sequences.
On an average, every sequence contained 5.3 gestures, making
a dataset of 7950 independent segments. In addition, we have
also collected data from two persons having right-arm injury.
B. Results of Isolated Gesture Recognition
First, we present results obtained using DA and SVMs. We
have applied both methods independently. Fig. 5 summarizes a
comparative analysis between DA and SVM using experimental
setups E1 and E3. F1 score of each gesture has been estimated
by averaging scores across all users.
It may be observed that both DA and SVM perform quite similarly except two cases, i.e., gesture #1 and gesture #3. However,
SVM does not perform as well as DA on these two cases. One of
the reasons can be attributed to close similarity between gesture
#1 and gesture #3. Our investigation reveals that these two gestures are quite similar except a 90◦ rotation as it can be verified
from Fig. 1. On the other hand, both algorithms perform similarly on unambiguous gestures, such as gesture #5 or gesture
#8. We have also analyzed the results across users and observed

998

Fig. 6. Performance of DA across all users on ambiguous gestures (#1 and
#3) and unambiguous gestures (#5 and #8) using E1 setup.

Fig. 7. Performance of SVM across all users on ambiguous gestures (#1 and
#3) and unambiguous gestures (#5 and #8) using E3 setup with SVM parameters
such as C = 9.3 × 10 −5 and γ = 1.2.

Fig. 8. Results of classification using DA and SVM obtained with E2 and
E4 experimental setups across various gestures with SVM parameters such as
C = 9.3 × 10 −5 and γ = 1.2.

interesting behavior. Our comparisons across users reveal that
DA is more consistent as compared to SVM when independent
gestures are taken into consideration. Results of such comparisons are presented in Figs. 6 and 7. Thus, DA is a better choice
than SVM when user-specific training is performed.
Next, we present comparative analysis between DA and SVM
using E2 and E4 setups. We have analyzed the variations in average F1 scores across various gestures and the results are reported
in Fig. 8. It may be observed that when training is performed
using data of all users, intermethodology variation across gestures is reduced significantly. Both DA and SVM perform quite
similarly and results have improved as compared to E1 and E3
setups. Therefore, it may be stated that training with large samples collected from multiple users actually helps to improve the
recognition accuracy. Increasing the training dataset essentially
increases the number of positive and negative samples; therefore, E2 and E4 setups performed reasonably better than E1 and
E3 setups.
Similar analysis has also been done on ambiguous gestures,
i.e., #1 and #3 as well as unambiguous gestures, i.e., #5 and #8
across all users to compare results with E1 and E3 setups. Such
comparisons are presented in Figs. 9 and 10. It can be verified
from the figures that both DA and SVM perform consistently
across all 30 users.
1) Effect of Kernel Parameters on Classification: In this section, we present results by varying the kernel of the SVM-based

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

Fig. 9. Performance of DA across all users on ambiguous gestures (#1 and
#3) and unambiguous gestures (#5 and #8) using E2 setup.

Fig. 10. Performance of SVM across all users on ambiguous gestures (#1 and
#3) and unambiguous gestures (#5 and #8) using E4 setup with SVM parameters
such as C = 9.3 × 10 −5 and γ = 1.2.

Fig. 11. Performance of SVM by varying kernel, e.g., linear, polynomial,
RBF, and sigmoid with SVM parameters such as C = 9.3 × 10−5 and γ = 1.2,
d = 2.05, and r = 10 5 .

classifier and its various parameters. SVM is quite sensitive to
a choice of kernel and other parameters. Hsu et al. [8] have
presented a comprehensive guide on selection of a suitable kernel and related parameters to get optimum results. We have
followed their methodology to initialize some of the important
parameters of the SVM-based classifier.
Several experiments have been carried out with three different
kernels, namely linear, polynomial, and RBF. Results of such
experiments on isolated gesture recognition are presented in
Fig. 11. It may be observed that RBF performs consistently
better than other three kernels with an average F 1-score of
0.89. This is consistent with the observations obtained by Hsu
et al. However, polynomial kernel with a degree of 2 has also
performed quite satisfactorily for the present application. We
present the summary of such analysis in the following part of
the discussion.
We have tested with four parameters, namely penalty parameter (C), degree of the polynomial used in a polynomial kernel
(d), γ of the RBF kernel, and coefficient (r). Out of these four,
C is the parameter of the classifier, whereas remaining three are
mainly kernel parameters. We have varied the penalty parameter (C) of (13) between 8.9 × 10−8 and 10. Results of some
of those variations are presented in Fig. 12. It can be observed
that results get saturated and no further improvement is possible
when C ≥ 9.3 × 10−5 .

VAMSIKRISHNA et al.: COMPUTER-VISION-ASSISTED PALM REHABILITATION WITH SUPERVISED LEARNING

Fig. 12. Results of isolated gesture recognition by varying C of the SVM
classifier with γ = 1.2.

999

Fig. 15. Results of isolated gesture recognition by varying r with a fixed value
of γ = 1.2 using polynomial kernel with d = 2.05 and C = 9.3 × 10−5 .
TABLE II
COMPARATIVE PERFORMANCE OF THE FEATURE VECTOR
Isolated Gesture Recognition
(Mean F1 Score Over Eight Gestures)
set 1:{g , x, y , θ 1 − θ 5 }, set 2: {g , x, y , θ 1 − θ 5 , d 1 − d 5 },
set 3: {(set 2) ∪ additional eight}, set 4: {θ 1 − θ 5 , d 1 − d 5 } ,

Fig. 13. Results of isolated gesture recognition by varying degree of the
polynomial used in the kernel of the SVM classifier with γ = 1.2.

Fig. 14. Results of isolated gesture recognition by varying γ of the RBF kernel
with C = 9.3 × 10 −5 .

Order of the polynomial is important in the context of polynomial kernel-based classifiers. Our experiments reveal that a
second-order polynomial gives satisfactory results. Increasing
the degree can marginally improve results; however, computational overhead also gets increased substantially. Results of such
comparisons are presented in Fig. 13.
RBF kernel is mainly sensitive to two parameters, i.e., C and
γ. Since it is not known beforehand which C and γ are best
for a given problem. Therefore, some kind of model selection
(parameter search) must be done as suggested in [8]. Keeping
the value of C = 9.3 × 10−5 , we have varied γ between 10−6
and 2.5. Optimum results are obtained when γ ≥ 0.5 as depicted
in Fig. 14.
Both polynomial and sigmoid kernels are sensitive to r. We
have varied r between 0.2 and 1010 keeping γ = 1.2 of the
polynomial kernel. Results of such experiments are presented
in Fig. 15. Best results have been recorded when 105 ≤ r ≤ 107 .
The above set of experiments reveals that the RBF kernel is
probably the best choice keeping value of the other parameters
fixed. Therefore, we have used this kernel in our experiments
with parameters as described earlier.
2) Effect of Feature Set on Classification: Selection of a
good set of features as mentioned in Section III must be judiciously done. We have experimented with various combinations
of features and compared them against DA- and SVM-based

DA
SVM

set 5: additional eight, set 6: {(g , x, y ) ∪ additional eight}
SET 1
SET 2
SET 3
SET 4
SET 5
SET 6
0.97
0.99
0.99
0.75
0.71
0.86
0.98
0.98
0.99
0.76
0.74
0.79

isolated gesture recognition modules. Moreover, we have included eight additional features that mainly represent interfinger
and intrafinger distances, e.g., distance between proximal phalanges of side-by-side fingers and distance between proximal
and intermediate phalanx of the same finger.
Results of such experiments are presented in Table II. It can
be verified from the table that the proposed set of features
(thirteen) performs quite satisfactorily when compared against
other combinations. Though, additional eight features comprising of interfinger and intrafinger distances can improve the results marginally for SVM-based classifier, it does not affect the
performance of DA-based classification. However, results also
reveal that (first three set of parameters) eight parameters, i.e.,
g, x, y, and θ1 –θ5 are quite important in the present context.
C. Results of Gesture Sequence Recognition
In this section, sequence recognition results are presented. In
practice, users are asked to perform gesture sequences during
rehabilitation. This can reactivate various muscle fibers during
change in postures. However, classifying a sequence as a whole
may not be accurate due to spatiotemporal variations. Therefore,
continuous movements or sequences are segmented into independent gestures using the method described in Section III-B.
Next, these segments are used to train isolated gesture recognition modules (DA and SVM independently). Recognizing discrete gestures and then concatenating the recognized gestures
to obtain a sequence may not be a wise choice because these
methods do not exploit inter-gesture relationship in a sequence.
However, results of isolated gesture recognition using DA or
SVM can be used to prepare training data for HMM-based classification. It is done as follows. Out of the total 1500 sequences,
1000 sequences have been processed during training. Remaining 500 sequences are used for evaluating the performance of
the trained model. After segmentation and feature extraction,

1000

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 5, MAY 2016

Fig. 16. Comparative performance of sequence detection using four setups
(E5–E8) with parameters such as RBF kernel with C = 9.3 × 10−5 , γ = 1.2,
number of states in HMM = 8.
TABLE III
COMPARATIVE STUDY OF SEQUENCE CLASSIFICATION TECHNIQUES
Fig. 17. Unity3-D-based user interface to facilitate human palm or finger
rehabilitation.
Classification
Techniques

S1

S2

DA+HMM
SVM+HMM
DA+CRF
SVM+CRF
Only DA
Only SVM

99.4
98.2
99.8
98.2
90.4
92.6

99.8
99.4
99.8
99.4
91.2
90.3

S3

Gesture Sequences
(Accuracy in %)
S4
S5
S6
S7

S8

S9

S1 0

99.6
99.4
100
99.2
90.5
91.5

99.6
99.4
100
99.4
87.3
88.7

99.6
99.6
100
100
84.3
83.7

97.4
97.2
98.2
97.6
81.2
82.4

96.2
96.6
97.6
97.8
80.5
83.8

99.8
98.8
99.6
98.6
93.1
92.4

99.6
98.6
99.6
98.6
94.4
94.8

98.6
98.4
99.6
98.6
83.8
84.2

isolated gestures of the training sequences are labeled using DA
or SVM. Next, these sequences (e.g., g1 → g2 → g4 ) along with
their ground truth labels are fed to the HMM module to estimate
the model parameters (θ∗ ). The learned HMM model is then
used to identify the labels for the remaining 500 test sequences.

1) Comparison of HMM with CRF: Sequence classification
can also be done using CRF-based models [22]. Therefore, our
proposed HMM-based gesture classification should be compared with CRF-based classification to justify the application of
HMM in similar applications. We have applied CRF-based classification after preparing the training data using DA- and SVMbased isolated gesture recognition discussed earlier. Fig. 16
presents such comparisons. It may be noted from the figure
and Table III that both CRF- and HMM-based sequence classification techniques perform almost similarly with average accuracy of 99.09% and 98.76%, respectively. Therefore, it can
be concluded that our system performs with very high accuracy
irrespective of the choice of the classifier.
E. Proposed Rehabilitation Interface and Comparison

D. Comparative Analysis of Sequence Classification
We have performed a comparative analysis of results obtained
using E5–E8 setups. E5 and E6 setups represent HMM-based
classification when the training data are prepared using DA and
SVM. In Fig. 16, results of sequence detection are presented. It
can be verified that HMM-based sequence detection performs
consistently across various sequences. However, as the length
of the sequence increases, performance may degrade, which is
natural, because larger the length, greater the chance of error accumulation in sequential processing. After several experiments
and knowledge from the state of the art, we have fixed the number of states of HMM to eight.
We have also applied DA- and SVM-based classification on
segmented gestures as per the E7 and E8 setups and compared
them against HMM-based setups (E5 and E6). It can be verified
from the figure that HMM combined with DA or SVM performs
way better than concatenation-based classification using DA or
SVM. This is because HMM can correct errors even if some of
the gestures within a sequence are classified wrongly. On the
contrary, DA- or SVM-based classifiers with the help of simple
concatenation fail in several occasions. These algorithms usually fail to predict a sequence correctly even when any one of
the isolated gestures is recognized wrongly. Sequence detection
accuracy using aforementioned setups are summarized in Table III. It is evident that HMM-based classifier is a good choice
as compared to concatenation-based classifiers with the help of
DA or SVM only.

We have developed an interface to record various gestures.
It facilitates the recording of raw signals, training the classifiers, and use trained models to recognize gesture sequences
performed during palm rehabilitation. The interface has been
designed using Unity3D platform, which is a popular opensource tool often used for developing games, virtual reality
applications, architectural visualizations, and various other UI
designing applications. Our interface also provides visual feedback to the users during rehabilitation. For example, when a
user deviates from its prescribed sequence, the system highlights such cases through visual feedback and the events are
logged for report generation. The interface can record all predicted sequences including timing information. This can be sent
to the experts for offline analysis for evaluation. A snapshot of
the proposed interface in operation is shown in Fig. 17. We have
used the proposed interface to facilitate rehabilitation for two
persons suffering from right-arm injury. In both cases, a trained
system with the data of 30 normal subjects was able to predict
correct gesture sequences with accuracy of 80% or above. However, due to unavailability of sufficient number of subjects with
upper limb injury, we could not confirm its robustness.
The method proposed by Khademi et al. [20] also discusses
an alternate methodology of stroke rehabilitation. However, they
did not discuss about any new design to facilitate complex gesture sequences. They have actually combined the popular Fruit
Ninja game with the Leap motion interface. This can be viewed
as a customization of the game interface using a contactless

VAMSIKRISHNA et al.: COMPUTER-VISION-ASSISTED PALM REHABILITATION WITH SUPERVISED LEARNING

platform. On the contrary, ours is a new game-like methodology
that can facilitate various types of palm or finger rehabilitation.
Therefore, a direct comparison may not be possible with such
systems that exist in the literature.
VI. CONCLUSION AND FUTURE SCOPES
Postinjury physical rehabilitation is very important for quick
healing. Patients suffering from cerebral strokes, physical injury,
or any other developmental disabilities usually benefit a lot by
undergoing a methodological rehabilitation process depending
on the type and grade of injury. However, expert’s supervision is
necessary to avoid complications that may arise during rehabilitation. In this paper, we have proposed machine learning-based
solutions to undergo home-based palm or finger rehabilitation
with the help of computer vision. Our proposed system takes
the help of a Leap motion controller to record various parameters related to palm movement and analyzes them to extract
valuable information. The information can be delivered offline
to the experts for assessment of the recovery process. We have
designed a visual interface which guides a user to follow necessary steps during palm rehabilitation and logs events for future
analysis. We have found that HMM-based classifier is the best
choice when tested on a moderately large dataset as compared
to DA- and SVM-based classifiers. However, we have observed
that CRF-based classifier can also be used for sequence classification.
The set of eight gesture used in our methodology may not be
optimum for all types of rehabilitation. It is desirable to extend
this set for complex rehabilitation. In addition to that, the system
must be verified and certified by experts before it can be used
for clinical purposes. Therefore, experiments with large number
patients suffering from upper limb disabilities should be carried
out to test the robustness and usability of the proposed system.
ACKNOWLEDGMENT
The authors would like to thank D. D. Chandra for his tremendous help during data collection and experiments.
REFERENCES
[1] S. Abe, Support Vector Machines for Pattern Classification, New York,
NY, USA: Springer-Verlag, 2005, vol. 53.
[2] T. Brassil and J. Brassil, Hand rehabilitation glove, U.S. Patent 6 454 681,
Sep. 24, 2002.
[3] O. Cho and S. Lee, “A study about honey bee dance serious game for kids
using hand gesture,” Int. J. Multimedia Ubiquitous Eng., vol. 9, no. 6,
pp. 397–404, 2014.
[4] A. Dowling et al., “An adaptive home-use robotic rehabilitation system
for the upper body,” IEEE J. Transl. Eng. Health Med., vol. 2, pp. 1–10,
Apr. 2014.
[5] B. Min et al.“Hand gesture recognition using hidden Markov models” in
Proc. IEEE Int. Conf. Syst. Man Cybern., Oct. 1997, vol. 5, pp. 4232–4235.
[6] C. Cifuentes et al., “Development of a wearable zigbee sensor system for
upper limb rehabilitation robotics,” in Proc. IEEE RAS/EMBS Int. Conf.
Biomed. Robot. Biomechatron., Jun. 2012, pp. 1989–1994.
[7] C. Hao, et al., “Design of the workstation for hand rehabilitation based
on data glove,” in Proc. IEEE Int. Conf. Bioinformat. Biomed. Workshops,
Dec. 2010, pp. 769–771.
[8] C.-W. Hsu, C.-C. Chang, and C.-J. Lin, “A practical guide to support
vector classification,” 2010. Department of Computer Science National
Taiwan University, Taipei 106, Taiwan, [Online]. Available: http://www.
csie.ntu.edu.tw/∼cjlin, Accessed on 18/04/2015.

1001

[9] C. Schonauer, et al., “Chronic pain rehabilitation with a serious game
using multimodal input,” in Proc. Int. Conf. Virtual Rehabil., Jun. 2011,
pp. 1–8.
[10] C. Tan et al., “Game-based human computer interaction using gesture
recognition for rehabilitation,” in Proc. IEEE Int. Conf. Control Syst.
Comput. Eng., Nov. 2013, pp. 344–349.
[11] D. Bercht et al., “ARhT: A portable hand therapy system,” in Proc. IEEE
34th Annu. Int. Conf. Eng. Med. Biol. Soc., Aug. 2012, pp. 264–267.
[12] D. Charles et al., “An evaluation of the leap motion depth sensing camera
for tracking hand and fingers motion in physical therapy,” in Proc. Int.
Tech. Gam., 2013.
[13] F. Wolbrecht et al., “Single degree-of-freedom exoskeleton mechanism
design for finger rehabilitation,” in Proc. IEEE Int. Conf. Rehabil. Robot.,
Jun. 2011, pp. 1–6.
[14] H. Yamaura et al., “Development of hand rehabilitation system for paralysis patient-universal design using wire-driven mechanism,” in Proc. IEEE
31st Annu. Int. Conf. Eng. Med. Biol. Soc., Sep. 2009, pp. 7122–7125.
[15] J. Guna et al., “An analysis of the precision and reliability of the leap
motion sensor and its suitability for static and dynamic tracking,” IEEE
Sensors, vol. 14, no. 2, pp. 3702–3720, Feb. 2014.
[16] J. Palacios et al., “Human-computer interaction based on hand gestures
using RGB-D sensors,” IEEE Sensors, vol. 13, no. 9, pp. 11842–11860,
Sep. 2013.
[17] J. Yamato et al. “Recognizing human action in time-sequential images
using hidden Markov model,” in Proc. IEEE Comput. Vis. Pattern Recog.,
Jun. 1992, pp. 379–385.
[18] L. Sucar et al., “Gesture therapy: A vision-based system for upper extremity stroke rehabilitation,” in Proc. IEEE 32nd Annu. Int. Conf. Eng.
Med. Biol. Soc., Aug. 2010, pp. 3690–3693.
[19] L. Wang et al. “Smartphone-based wound assessment system for patients
with diabetes,” IEEE Trans. Biomed. Eng., vol. 62, no. 2, pp. 477–488,
Feb. 2015.
[20] M. Khademi et al., “Free-hand interaction with leap motion controller
for stroke rehabilitation,” in Proc. CHI Extended Abstr. Human Factors
Comput. Syst., 2014, pp. 1663–1668.
[21] S. Amsuss et al., “Self-correcting pattern recognition system of surface
EMG signals for upper limb prosthesis control,” IEEE Trans. Biomed.
Eng., vol. 61, no. 4, pp. 1167–1176, Apr. 2014.
[22] S. Chatzis et al., “A conditional random field-based model for joint
sequence segmentation and classification,” Pattern. Recog., vol. 46,
pp. 1569–1578, 2013.
[23] S. Garg et al. “India’s health workforce: Current status and the way forward,” Nat. Med. J. India, vol. 25, no. 2, pp. 111–113, Mar. 2012.
[24] S. Salim et al., “Integration of tilt sensors as a device for monitoring
rehabilitation process,” in Proc. IEEE Int. Conf. Control Syst. Comput.
Eng., Nov. 2014, pp. 232–235.
[25] W. Lee et al., “A smartphone-centric system for the range of motion
assessment in stroke patients,” IEEE J. Biomed. Health Informat., vol. 18,
no. 6, pp. 1839–1847, Nov. 2014.
[26] A. Gorbunov, “3d+stereo augmented reality for landing systems feasibility
and experiment design,” Sci. Technol., vol. 2, no. 1, pp. 21–24, 2012.
[27] Z. Jiang, “Support vector machines for multi-class pattern recognition
based on improved voting strategy,” in Proc. Chin. Control Des. Conf.,
May 2010, pp. 517–520.
[28] A. Mannini and A. Sabatini, “Machine learning methods for classifying
human physical activity from on-body accelerometers,” IEEE Sensors,
vol. 10, no. 2, pp. 1154–1175, Jan. 2010.
[29] P. Ofner and G. Muller-Putz, “Using a noninvasive decoding method to
classify rhythmic movement imaginations of the arm in two planes,” IEEE
Trans. Biomed. Eng., vol. 62, no. 3 pp. 972–981, Mar. 2015.
[30] L. Rabiner, “A tutorial on hidden Markov models and selected applications
in speech recognition,” Proc. IEEE, vol. 77, no. 2, pp. 257–286, Feb. 1989.
[31] M. Welling, “Fisher linear discriminant analysis,” Dept. Comp. Sci., Univ.
Toronto, Toronto, ON, Canada, vol. 3, 2005.
[32] A. Wilson and A. Bobick, “Recognition and interpretation of parametric
gesture,” in Proc. Int. Conf. Comput. Vis., Jan. 1998, pp. 329–336.
[33] C. Wong and C. Hsien, “Healthcare in singapore: Challenges and management,” Jpn. Med. Assoc. J., vol. 51, no. 5, pp. 343–346, 2008.
[34] J. Zariffa and J. Steeves, “Computer vision-based classification of hand
grip variations in neurorehabilitation,” in Proc. IEEE Int. Conf. Rehabil.
Robot., Jun. 2011, pp. 1–4.

Authors’ photographs and biographies not available at the time of publication.

