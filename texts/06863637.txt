IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 1, JANUARY 2015

145

Dynamic Low-Level Context for the Detection
of Mild Traumatic Brain Injury
Anthony Bianchi∗ , Bir Bhanu, Fellow, IEEE, and Andre Obenaus

Abstract—Mild traumatic brain injury (mTBI) appears as low
contrast lesions in magnetic resonance (MR) imaging. Standard
automated detection approaches cannot detect the subtle changes
caused by the lesions. The use of context has become integral for
the detection of low contrast objects in images. Context is any
information that can be used for object detection but is not directly due to the physical appearance of an object in an image.
In this paper, new low-level static and dynamic context features
are proposed and integrated into a discriminative voxel-level classifier to improve the detection of mTBI lesions. Visual features,
including multiple texture measures, are used to give an initial estimate of a lesion. From the initial estimate novel proximity and
directional distance, contextual features are calculated and used
as features for another classifier. This feature takes advantage of
spatial information given by the initial lesion estimate using only
the visual features. Dynamic context is captured by the proposed
posterior marginal edge distance context feature, which measures
the distance from a hard estimate of the lesion at a previous time
point. The approach is validated on a temporal mTBI rat model
dataset and shown to have improved dice score and convergence
compared to other state-of-the-art approaches. Analysis of feature
importance and versatility of the approach on other datasets are
also provided.
Index Terms—Dynamic context, low contrast, magnetic resonance imaging (MRI), traumatic brain injury.

I. INTRODUCTION
WARENESS of mild traumatic brain injury (mTBI) has
increased dramatically in recent years. Causes of mTBI
include sports injuries, automobile accidents, blast injuries in the
military, and falls in the workplace [1]. The long-term effects
of mTBI are just being recognized, leading to the need for
quantitative techniques to characterize and measure the injured
tissue.
Clinical evaluation of mTBI has been too qualitative by relying on the Glasgow Coma Scale, to assess loss of consciousness, loss of memory, alteration in mental status, and focal
neurological deficits. When imaging is used to assist in diagnosis, MR imaging or computed tomography (CT), a quantitative

A

Manuscript received January 22, 2014; revised May 19, 2014; accepted June
25, 2014. Date of publication July 24, 2014; date of current version December
18, 2014. This work was supported by National Science Foundation Integrative
Graduate Education and Research Traineeship (IGERT) in Video Bioinformatics
(DGE-0903667) and Department of Defense (DCMRP-DR080470). Asterisk
indicates corresponding author.
∗ A. Bianchi is with the Center for Research in Intelligent Systems, University
of California, Riverside, CA 92521 USA (e-mail: abianchi@vislab.ee.ucr.edu).
B. Bhanu is with the Center for Research in Intelligent Systems, University
of California, Riverside, CA 92521 USA (e-mail: bhanu@cris.ucr.edu).
A. Obenaus is with the Department of Pediatrics, Loma Linda University,
Loma Linda, CA 92354 USA (e-mail: aobenaus@llu.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2342653

Fig. 1. T2 weighted MR image of half a coronal slice from the rat model
dataset. (a) Original T2 weighted image. (b) Manual detection of the mTBI
lesion (highlighted in red). This illustrates the low contrast appearance of mTBI
lesions. The manual lesions have been verified by histology in [13].

measurement of the size and location of injury is typically obtained. The primary focus of imaging is only to assess the presence of a hematoma [1]. Various clinical studies have used
manual quantitative analysis to evaluate TBI in MRI [2]–[4].
These studies identified the size and location of the lesions and
linked them to long-term neurological effects. Manual analysis
has some major downfalls, hours per scan of ROI extraction, a
trained operator is required and difficulty in interpreting multiple
modalities. Inter- and intraoperator errors can become significant in low contrast images. Interoperator error has been shown
to be significant even in higher contrast tumors [5]. However,
manual segmentation is considered the “gold standard” and is
used as the groundtruth.
Some computational approaches have been proposed for
quantifying lesions in moderate to severe TBI [6], which on
MRI have high contrast, but these have been unsuccessful when
attempting to evaluate the subtle MR signature of mTBI. Lesions caused by mTBI appear as small low contrast regions (see
Fig. 1) in both T2 weighted images and T2 maps. The T2 values within these lesions often fall within the range of normal
tissue values. Therefore, the T2 value of a voxel cannot be used
by itself. In [7], it was shown that there are significant texture
changes in brain tissues as a result of mTBI, which provides a
measure of the underlying structure of the tissue. Therefore, our
proposed approach uses multiple texture measures to improve
the discriminative ability of mTBI detection from MR images.
There has been research into advanced MR techniques for
detecting mTBI, mainly susceptibility-weighted imaging (SWI)
[8]–[10] and diffusion tensor imaging (DTI) [10]–[12]. SWI
utilizes the phase image to estimate the magnetic susceptibility
in tissue. It has been shown to be more sensitive to blood than CT.
However, SWI tends to overestimate a lesion and can give too
sharp a contrast leading to hard determination of low-contrast
brain structures [9]. DTI measures the “flow” of water in the
brain, and has been used to estimate white matter (WM) tracts.
Significant changes in WM tracts have been found in patients

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

146

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 1, JANUARY 2015

with mTBI, but it requires multiple imaging time points or a
baseline image [14]. It becomes difficult to evaluate it due to
interpatient variations in tractography. This rules out DTI for
voxel-based classification. We utilize T2 weighted imaging, a
traditional imaging technique as described above.
A measure of contrast within the lesion can be obtained by
calculating the contrast-to-noise ratio (CNR). The manually segmented regions are used to calculate the CNR, and a CNR of
six on average is obtained. Rajan et al. [15] made noise measurements from MRI to estimate the noise variance.
To estimate the mTBI lesion, we use a discriminative approach comprised of voxel level classification utilizing visual
and contextual features. Both static and dynamic contextual features are proposed. Static contextual features, proximity and directional, are calculated from local neighborhood information of
the posterior marginal probability estimated by a preceding classifier. The dynamic context (posterior marginal edge distance) is
calculated from the maximum posterior marginal probability of
the cascade of classifiers at a previous time point and then used
to estimate the current lesion. Adding these contextual features
overcomes the low contrast nature of the lesions.
The paper is organized as follows. Section II presents the
related work and the contributions. Section III discusses the
technical approach. Section IV describes the experiments and
finally Section V provides the conclusion.
II. RELATED WORK AND CONTRIBUTIONS
A. Related Work
Context has been an active area of research in computer vision
and can be used to overcome low-contrast detection problems.
Context is defined [16] as, “any information that might be relevant to object detection, categorization, and classification tasks,
but not directly due to the physical appearance of the object, as
perceived by the image acquisition system.” Context types that
have been described include: local pixels, 2-D scene gist, 3-D
geometric, semantic, photogrammetric, illumination, weather,
geographic, temporal, and cultural [17]. Context can be split
into two types local and global: 1) Local context includes spatial relationships learned at the pixel/region level. It is used in
conditional random fields (CRFs) [18] and autocontext [19].
2) Global context can be thought of as an estimation of the spatial location of an object. In this paper, we exploit low (voxel)
level context. Our previous approach [20] uses a contextually
driven generative model to estimate the lesion location.
As a starting point, algorithms for multiple Sclerosis (MS)
are examined due to similar low contrast lesions being present
in MR images. The approaches for detecting MS lesions [21],
[22] use texture features and take advantage of the knowledge
that MS occurs in a specific tissue type, which is consistently
located in similar anatomical regions. This is a type of context
(anatomical context) that can be exploited easily for MS, but
mTBI lesions do not have this advantage since mTBI can occur
in different tissue types.
Recently, Tu and Bai [19] proposed autocontext is a way
to model context at the voxel level. The main premise is to
estimate an object with a discriminative classifier and use a

sampling of the estimated posterior probability as an additional
feature to a subsequent classifier. It is able to take information
from far away (across the brain) compared to other methods
like CRFs [18] which are local. The context features in this
case are a sparse sampling of a distant neighborhood around
every pixel. This can lead to overfitting due to the very specific
locations and this can be seen in one of the examples in [19].
Other methods that exploit this type of context include [23],
which has similar pitfalls as autocontext. These methods have
been shown to segment rigidly positioned objects well, such
as organs or body parts. In this paper, we adopted the idea
of cascading classifiers, but developed features that are more
generalizable and integrate temporal information, which has
not been addressed in [19].
B. Contributions
The contributions of our paper are 1) development of three
new contextual features to be used with a cascade of classifiers. These features include a proximity feature, a directional
feature, and a maximum a posteriori edge distance feature. 2)
Use of a temporal sequence of MR images to provide context
from a previous time point, capturing the dynamics of the injury
progression automatically. 3) Analysis of multiple contextual
feature configurations on a rat controlled cortical impact (CCI)
mTBI model dataset. This work extends the conference proceedings [37].
III. TECHNICAL APPROACH
A. System Overview
A discriminative approach is taken, where classifiers are used
to directly estimate the posterior probability of lesion and normal
appearing brain matter (NABM) voxels (note the groundtruth
is obtained from segmentation of the mTBI lesion by a domain
expert). The discriminative approach performs well when there
is a large amount of training data and it can be used for a complex
decision space. A voxel level classifier has a large amount of data
considering the 3-D nature of MR images. The appearance of
lesions in MRIs can be very complex, which leads to a complex
decision space.
A cascade of classifiers is used for estimating the detected
lesion at each time point (shown in Fig. 2), where information
from a previous time point is propagated forward. The first classifier in the cascade estimates the lesion using only the visual
features. Then, context features are formed from the posterior
probability map estimated by the classifier. These features are
recalculated for each iteration in the process, for a given number
of classifiers shown as a column of classifier for a single time
point in Fig. 2. Spatial information is propagated by the contextual features leading to improved classification. This process
was introduced by [19] for static images. The contextual features
used by [19] were point samplings of the posterior estimates.
Their features demonstrated good segmentation performance on
objects that were rigid in shape, when that shape was distorted
their performance faltered. We propose two new features that
generalize the static contextual information so it will work with
amorphous objects.

BIANCHI et al.: DYNAMIC LOW-LEVEL CONTEXT FOR THE DETECTION OF MTBI

147

C. Static Contextual Features

Fig. 2. Overview of the proposed system. Context information is sent from
one classifier to the next. After each classifier the static contextual features are
extracted. The output of the Nth classifier is the final estimate of the lesion for
the given time point. The dynamic contextual features are estimated from the
final posterior marginal of the preceding time point.

Dynamic contextual features are calculated from the final
classifier at a single time point. These features also in our approach are used by the classifier at the subsequent time point, and
they make use of spatial and lesion growth information. Note
that [19] proposed using spatiotemporal volumes with their basic
point features in the higher dimensional space. Their approach
would require extreme amounts of data and the entire sequence
to be known before segmentation. Our approach only considers
pairs of brain volumes at a time, which allows for estimation at
every time point.
B. Visual Features
Due to the low contrast nature of the unimodal MR images and
the mild nature of the TBI, texture features are used to increase
the discrimination in our approach. Four types of texture features
are used: uniform local binary pattern (ULBP) local histograms
[24] in the coronal plane (59 features), local statistics (mean,
variance, skewness, kurtosis) of a Gabor filter bank [25] with
eight orientations and four scales in the coronal plane (128
features), basic histogram of oriented gradients in the coronal
plane [26] (nine features), and local neighborhood statistical
features (mean, variance, skewness, kurtosis, range, entropy,
gradient magnitude xyz) (nine features). This gives a total of
205 visual features. This wide variety of features provides many
different characteristics without being too specific (i.e., they will
generalize well). The size of the local neighborhood used is 5 × 5
voxels [33], where the feature size is chosen to be the one that
maximizes the Bhattacharyya distance between the two classes
and gives a bound on the Bayesian error [27]. Examples of the
visual features are shown in Fig. 3.

The contextual features come from the posterior probability
estimated by an already learned classifier. Previous approaches
[13] have directly sampled a dense neighborhood around an observed voxel, making each location a potential feature. This
method can lead to large feature sizes and can cause overfitting due to the specific locations that are learned. In this
paper, two new static features are proposed to overcome this
problem. One incorporates a sense of the surrounding without
a known direction, while the other gives a general sense of
direction.
The first feature, shown in Fig. 4(a), gives the average posterior probability at various distances around the observed voxel.
This can be thought of as a proximity feature, where what is
a close, medium, and far away in distance are estimated. The
distance function used here is the Manhattan distance allowing
for a cuboidal region. These features are directionally invariant
and can lead to better generalization since they describe a general area. By having a nesting of boxes the integral image can
be utilized for quick computation of the features. In 3-D, only
eight points are needed to find the average of a cuboidal region,
using integral images [28]. Equation (1) provides these features,
where fxy z is the proximity feature, R1xy z , R2xy z are square
neighborhoods around the voxel at xyz, and size(∗ ) is the size of
the bounding box
fxy z =



×

R1xy z −




R2xy z

1
size(R1xy z ) − size(R2xy z )


.

(1)

Directional information is important for classification since
the objects are rigidly registered to a naı̈ve animal. The second
contextual feature describes the posterior probability in various
directions from the observed voxel. Rays are sampled at various distance ranges and angles from the observed voxel [see
Fig. 4(b)]. From the distance ranges along the rays the mean
is calculated. This gives a refined sense of the surrounding. An
example would be what is close and above the observed voxel.
The integral image is also used to calculate these features. Both
features can be computed at coarse or fine distance bins without
a significant increase in computational time. Equation (1) can
also be used for the direction context features, the shape of the
features are changed to be that of Fig. 4(b). The features are
rectangles with width one and the 45° features can be estimated
using the rotated image.
D. Dynamic Contextual Feature
We propose the posterior marginal edge distance (PMED)
feature as a measure of the distance a voxel is from the perimeter
of objects of a class found by the maximum posterior marginal
(MPM) estimate. To create this feature, first the MPM at a voxel
is obtained from the output of a classifier. This gives a binary
image for each class. The distance transform is applied to the

148

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 1, JANUARY 2015

Fig. 3. Examples of the visual features on coronal slices. The features from left to right, top to bottom: (A) local mean, (B) variance, (C) skewness, (D) kurtosis,
(E–F) a bin from the histogram of oriented gradients, (G) a bin from the LBP histogram, and (H–J) example gabor features.

ALGORITHM I
ADABOOST
1:
2:
3:

Fig. 4. (A) Illustration of the proximity feature. V is the observed voxel and the
feature is the average probability of the regions (R 1 , R 2 , R 3 ). (B) Illustration
of the distance features. V is the observed voxel and an example feature is the
average probability between P 1 and P 2 along the 45° ray.

4:
5:

1
2

1 −ε t
// Classifier Weight
εt
D t ∗e x p ( −α t y h t ( x ) )
// Reweight
Z

6:

αt =

7:

Dt + 1 =
the examples
Where Z is a normalization term.
Exit if |0.5 − ε t | < threshold //exit if error is small

8:

Fig. 5. (A) Example MPM estimate. (B) Corresponding PMED feature. Note
that the values become more negative toward the center of the object and more
positive farther away from the object.

Input(x,y) where x is the set of all training features and features and y is the class
label.
T = number of weak classifiers
Initialize D1 = 1/m where D is the weight of each training example and m is
the number of training examples.
For t = 1 to T
h t = argm ax h t ∈H |0.5 − ε t | // Base Classifier


where ε t = D ∗ C ost M atr ix ∗ (y = h t (x)) //Error
ln

α. During the training process a cost matrix (4) is used, where
the top row represents the cost of normal appearing brain matter
(NABM) and the bottom row the cost of Lesion voxels, such
that the priors are offset to be even. This is done to account for
the large disparity, in the number of training samples between
the classes. Algorithm 1 describes the flow of adaboost
⎡

0

Cost Matrix = ⎣ NABM voxels

Lesion voxels

image and the inverse image and the feature are given by
PMED = d (MPM) − d(∼ MPM)
MPM = argmax p (ω = c|F ).

1
0

⎤
⎦.

(4)

(2)
(3)

c

Where, d(∗ ) is the Euclidean distance transform. This gives an
image that is increasing as the voxels become farther away from
the edge and smaller (more negative) as the voxels get further
into the object. In (3), ω is the estimated class, c is a specific
class (lesion or normal brain in our case), and F is the feature
set at a given voxel (see Fig. 5).
E. Classifier Cascade Selection
The base classifier in the cascade of classifiers has to be able to
deal with a large number of features, a complex decision space,
and be able to give a posterior estimate. The primary classifier
being used is adaboost [29] with small decision trees as base
classifiers. Using small trees as a weak classifier (h(∗ )) allows
for inherent feature selection, meaning erroneous features are
disregarded. It is also not sensitive to feature normalization. In
each iteration (t), the best classifier is selected and weighted with

It has been shown that the posterior marginal can be estimated
using logistic regression [30] given by
eH c (f x y z )
p(ωxy z = c|fxy z ) = C
H c (f x y z )
c=1 e
T
αt ht (fxy z ).
Hc (fxy z ) =
t=1

(5)
(6)

Other classifiers such as the popular support vector machine
(SVM) approach are also able to estimate the posterior marginal
[31]. SVM is a strong classifier, but it does not have inherent
feature selection and is sensitive to feature normalization. SVM
tends to perform well with a small number of strong features.
An example of the iterative training process for a single time
point is shown in Fig. 6. Notice that the estimate improves
the greatest between classifier 1 [see Fig. 6(b)] and classifier 2
[see Fig. 6(c)]. During training, each successive classifier will
have a performance greater than or equal to its predecessor,
since the posterior marginal is one of the features.

BIANCHI et al.: DYNAMIC LOW-LEVEL CONTEXT FOR THE DETECTION OF MTBI

Fig. 6. Example probability maps after each classifier. (a) T2 map. (b–e)
Classifier output probability map for the training of classifiers 1–4, respectively.
(f) Manual Segmentation where yellow denotes the lesion. This shows the
convergence of the algorithm to the manually segmented injury.

149

Fig. 7. Dice coefficient after thresholding the posterior probability map at the
end of each cascade of classifier (i.e., at each time point). This is the average of
all the tests in the leave one out validation.

IV. EXPERIMENTAL RESULTS
A. Contribution of Features in a Temporal mTBI Dataset
Our temporal dataset uses Sprague Dawley rats subjected to
a single impact CCI as an animal model of mTBI [13], [32].
The CCI model results in a focal injury. Briefly, a craniectomy
(5 mm) was performed over the right hemisphere (3 mm lateral,
3 mm posterior to bregma), where a mild CCI was induced using
an electromagnetically driven piston (4 mm diameter, 0.5 mm
depth, 6.0 m/s). The incision made for the craniectomy is sutured
after the injury is induced.
The animals were imaged in vivo at three time points post injury: acute (1st day), subacute (8th day), and chronic (14th day).
There are a total of six sequences, each with three time points.
MRI data were acquired using a Bruker Advance 4.7T for T2
weighted images (T2WI; TR/TE/FA = 3453 ms/20 ms/20°,
25 × 1 mm slices) with a 256 × 256 matrix and 3 cm field of
view. Each voxel has a dimension of 0.12 × 0.12 × 1 mm. The
T2 weighted images were converted to T2 maps. ROIs were
manually segmented using Cheshire image processing software
(Hayden Image/Processing Group, Waltham, MA, USA) and
included the right and left hemispheres and injured tissue volumes that were defined as abnormal (hyper/hypointense) signal
intensities within the cortex with the remaining tissues designated as NABM. The injuries have been verified with histology
in [32] at the end points of experiments. A direct comparison to
the results of histology is not possible due to the deformations
that occur in the histological staining process.
We examine the effect of the proposed features and effect of
the dynamic information. For the training/testing split, leaveone-out validation is used where a whole sequence is left out
(resulting in six crossfolds). Three temporally consecutive volumes were left for testing and the rest were used for training.
The parameters used were: 300 weak learners, learning rate 1,
and 4 cascaded classifiers. Three approaches were tested: the
original autocontext features [19], the proposed approach with
only the proposed static features, and the proposed approach
with the static and dynamic features.

To evaluate the segmentation results of the proposed approach
the dice coefficient is used, given by
Dice =

2TP
(TP + FP) + (TP + FN)

(7)

where TP is true positive, FP is false positive, and FN is false
negative. The dice coefficient gives the ratio of the intersection
between the detected object and the target object to the size of the
objects. The dice gives a better idea of the intersection between
the two objects because it does not consider the true negatives,
which will be large with the unbalanced class size. The output
of the classifier at each time point (acute, subacute, and chronic)
shown in Fig. 2 is thresholded to get a hard classification.
From Fig. 7, it is clear that the proposed dynamic approach
outperforms the other methods. This shows it is important to
use the dynamic information. The original autocontext tends
to overfit due to the specific locations the features represent.
The same features locations proposed in [19] were used for the
original autocontext testing. During the training phase it obtains
a dice score above 0.9, but it does not generalize well to the
testing data. From the testing, our proposed static features give
a good generalization compared to the original autocontext.
The output from the original autocontext approach is patchy
with many false negatives leading to an underestimation of the
injury. This is due to the fact that it looks at specific locations
for the context features.
The proposed approach has a very flat dice curve, so it is
not sensitive to a chosen threshold on the output probability
map. This makes the selection of a threshold less critical. From
the qualitative results (see Fig. 8), it is clear that the results of
proposed approach work on small to medium size lesions. The
qualitative results also show that false positives are only close
to the majority of the lesion mass without having erroneous
spatial outliers. It is also apparent that the approach has some
difficulties at the edges of the lesions. This could be rectified by
using shape constraints on the thresholded probability map.

150

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 1, JANUARY 2015

Fig. 8. Qualitative results of the proposed approach using dynamic and static contextual features. Each coronal slice is from a separate volume. Color code:
yellow = true positive, black = true negative, green = false negative, red = false positive.

In [20], a similar dataset was used for analysis except repeated
contralateral injuries were also considered. The average dice
score using a high-level contextual model was 0.35 and 0.07
using probabilistic SVM alone. This is similar to our dice score
using the proposed static features with an average dice score
of 0.36 and it far exceeds the results of a probabilistic SVM.
The dynamic features increase the dice up to 0.41. The method
proposed here does not use any extra patient information as it
has been done in [20].

B. Convergence Rate of the Features
We evaluate the strength of the individual features. The same
dataset from Section IV-A is used in this section. To determine
the strength of the features, the rate of convergence of the cost
function during training is observed.
In Fig. 9, the training cost after each tree during the training of
adaboost is given. Both the autocontext and the proposed static
features have the same results from the initial classifier utilizing

BIANCHI et al.: DYNAMIC LOW-LEVEL CONTEXT FOR THE DETECTION OF MTBI

151

Fig. 9. Training cost after each tree for the proposed static features versus the
original autocontext. This is the training cost of the second classifier, i.e., the
input to both methods is the results after using visual features only. Lower cost
is better.

only visual features. This is due to the proposed static features
being able to generalize in the surrounding area better.
To understand the convergence of each classifier in the cascade, one must observe the output training cost after adding each
tree during the training of adaboost. In Fig. 10, the convergence
of the training using only the static features is observed. Convergence using only the visual features takes a while compared
to the static contextual features. After the second classifier, the
improvement over the number of trees is less pronounced. Not
much information is gained after the second classifier.
Also in Fig. 10, we can observe the convergence of the classifier using the dynamic and static context features. It is apparent
that the convergence is very fast for Classifier 1 when the dynamic context features are used, compared to the case when no
dynamic information is used in the first iteration. Information is
still gained by the static features but at a slower rate.
C. Feature Importance Evaluation
An important question to ask when using adaboost is, “which
features are being chosen and when?” Here, those questions are
answered by looking at the percent of features chosen throughout the adaboost training process. In the case of the static features
alone, Fig. 11 shows first 50 trees are almost exclusively context
features. This shows that the contextual features hold the most
information for discrimination. The visual features start being
used after the information from the contextual features is nearly
all used. In the end, 40–50% of all the features are contextual.
A similar pattern occurs when the dynamic features are used
(shown in Fig. 11). The contextual features in Classifier 1 are
used early in the learning process, and then the visual features
almost exclusively take over. In Classifier 3, nearly 60% of the
features are chosen as contextual features. This shows that the
dynamic information plays a vital role in the classification.
D. Static Feature Performance on the BRATS Dataset
To show the versatility of the proposed approach the static
features were tested on a brain tumor dataset. The dataset is

Fig. 10. Top: Convergence of classifier training using only the static contextual features. Bottom: Convergence of classifier training using both dynamic
and static contextual features. Lower cost is better.

from the MICCAI 2012 Multimodal brain tumor segmentation
(BraTS) challenge. Here, a subset of the data was used that
included 15 High grade glioma patients. The volumes included
the following MR modalities, T1, post-Gadolinium T1, T2, and
FLAIR. Each modality has been coregistered, skull-stripped,
and resampled to be 1 mm isometric voxels. A mode shift was
used to normalize the histogram across subjects. For this testing,
the edema and core tumor are considered to be a single class.
The visual features used were the individual modalities, the
same local neighborhood statistics as the mTBI dataset on the
FLAIR channel, and LBP on the FLAIR channel. Since there is
a single time point database, only the static contextual features
are used. Testing was done using 5 fold-cross validation.
For this testing, an average dice score of 0.71 was obtained.
The best scores reported during the BraTS challenge were between 0.7–0.8, for the high grade glioma cases. Note that the
other approaches [33], [34] used symmetry features. These features can be directly used by the proposed approach. This is
another type of context that can be exploited. Another feature
used in the challenge were long-range context features [35].
These features sample the value around an observed voxel.
The parameter space for these features is extremely large and

152

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 1, JANUARY 2015

REFERENCES

Fig. 11. Top: The percentage of the static contextual features chosen versus
the current number of trees. Bottom: The percentage of dynamic and static
contextual features chosen up to a number of trees. Note in Classifier 1 only the
dynamic contextual features are used.

requires random sampling of the space via decision forests [36].
The other features used by the winners of the challenge [35]
were based on generative modeling of the tissue types. These
features can also directly be used by the approach proposed
here.
V. CONCLUSIONS
A fully automated method for detecting mTBI lesions that
integrates low-level static and dynamic contexts is proposed.
Three novel features are proposed that describe the posterior
probability of classifier outputs in a cascade. These features
were shown to have good qualitative and quantitative results
on an mTBI dataset. The proposed approach outperformed the
original autocontext [19], by being able to generalize and integrate dynamic information. The context features were shown to
be the most important features in the training process. Adding
the dynamic context features lead to fast convergence during
training leading to a smaller number of trees that are needed.
The generality and flexibility of the proposed approach allows
it to be applied to other brain lesion problems.

[1] T. Morris, “Traumatic brain injury,” in Handbook of Medical Neuropsychology, C. L. Armstrong and L. Morrow, Eds. New York, NY, USA:
Springer, 2010, ch. 2, pp. 17–32.
[2] N. C. Colgan, M. M. Cronin, O. L. Gobbo, S. M. O’Mara, W. T. O’Connor,
and M. D. Gilchrist, “Quantitative MRI analysis of brain volume changes
due to controlled cortical impact,” Neurotrauma, vol. 27, no. 7, pp. 1265–
1274, Jul. 2010.
[3] A. Obenaus, M. Robbins, G. Blanco, N. R. Galloway, E. Snissarenko,
E. Gillard, S. Lee, and M. Currás-Collazo. “Multi-modal magnetic resonance imaging alterations in two rat models of mild neurotrauma,” Neurotrauma, vol. 24, no. 7, pp. 1147–1160, Jul. 2007.
[4] Z. Metting, L. A. Rodiger, J. De Keyser, and J. Van Der Naalt, “Structural
and functional neuroimaging in mild-to-moderate head injury,” Lancet
Neurol., vol. 6, no. 8, pp. 699–710, Aug. 2007.
[5] G. P. Mazzara, R. P. Velthuizen, J. L. Pearlman, H. M. Greenberg, and
H. Wagner, “Brain tumor target volume determination for radiation treatment planning through automated MRI segmentation,” Int. J. Radiation
Oncol. Biol. Phys., vol. 59, no. 1, pp. 300–312, May 2004.
[6] A. Irimia, M. C. Chambers, J. R. Alger, M. Filippou, M. W. Prastawa,
B. Wang, D. A. Hovda, G. Gerig, A. W. Toga, R. Kikinis, P. M. Vespa,
and J. D. Van Horn, “Comparison of acute and chronic traumatic brain
injury using semi-automatic multimodal segmentation of MR volumes,”
J. Neurotrauma, vol. 28, no. 11, pp. 2287–2306, Nov. 2011.
[7] K. Holli, L. Harrison, P. Dastidar, M. Waljas, S. Liimatainen, T. Luukkaala,
J. Ohman, S. Soimakallio, and H. Eskola, “Texture analysis of MR images
of patients with mild traumatic brain injury,” Biomed. Central Med. Imag.,
vol. 10, no. 8, May 2010.
[8] G. Spitz, J. J. Maller, A. Ng, R. O’Sullivan, N. J. Ferris, and J. L.
Ponsford, “Detecting lesions after traumatic brain injury using susceptibility weighted imaging: A comparison with fluid-attenuated inversion recovery and correlation with clinical outcome,” J. Neurtrama, vol. 20, no. 24,
pp. 2038–2050, Dec. 2013.
[9] M. P Quinn, J. S. Gati, L. M. Klassen, A. W. Lin, J. R. Bird, S. E. Leung, and
R. S. Menon, “Comparison of multiecho postprocessing schemes for SWI
with use of linear and nonlinear mask functions,” Amer. J. Neuroradiol.,
vol. 35, no. 1, pp. 38–44, Jan. 2014.
[10] R. R. Benson, R. Gattu, B. Sewick, Z. Kou, N. Zakariah, J. M. Cavanaugh,
and E. M. Haacke, “Detection of hemorrhagic and axonal pathology in
mild traumatic brain injury using advanced MRI: Implications for neurorehabilitation,” NeuroRehabilitation, vol. 31, no. 3, pp. 261–279, 2012.
[11] T. Madhyastha, S. Merillat, S. Hirsiger, L. Bezzola, F. Liem, T. Grabowski,
and L. Jancke, “ Longitudinal reliability of tract-based spatial statistics in
diffusion tensor imaging,” Human Brain Mapp., vol. 35, no. 9, pp. 4544–
4555, Apr. 2014.
[12] S. N. Niogi and P. Mukherjee, “ Diffusion tensor imaging of mild traumatic
brain injury,” J. Head Trauma Rehabil., vol. 25, no. 4, pp. 241–255, Jul.
2010.
[13] V. Donovan, A. Bianchi, R. Hartman, B. Bhanu, M. J. Carson, and
A. Obenaus, “Computational analysis reveals increased blood deposition
following repeated mild traumatic brain injury,” NeuroImage: Clin., vol.
1, no. 1, pp. 18–28, 2012.
[14] E. D. Bigler and W. L. Maxwell, “Neuropathology of mild traumatic
brain injury: Relationship to neuroimaging findings,” Brain Imag. Behav.,
vol. 6, no. 2, pp. 108–136, Jun. 2012.
[15] J. Rajan, D. Poot, J. Juntu, and J. Sijbers, “Noise measurement from
magnitude MRI using local estimates of variance and skewness,” Phys.
Med. Biol., vol. 55, no. 16, pp. N441–N449, Aug. 2010.
[16] A. Bianchi, B. Bhanu, V. Donovan, and A. Obenaus, “Contextual and
visual modeling for detection of mild traumatic brain injury in MRI,”
in Proc. IEEE Int. Conf. Image Process., Orlando, FL, USA, Oct. 2012,
pp. 1261–1264.
[17] O. Marques, E. Barenholtz, and V. Charvillat, “Context modeling in
computer vision: Techniques, implications, and applications,” Multimedia Tools Appl., vol. 51, no. 1, pp. 303–339, Jan. 2011.
[18] J. Lafferty, A. McCallum, and F. Pereira, “Conditional random fields:
Probabilistic models for segmenting and labeling sequence data,” in Proc.
10th Int. Conf. Mach. Learning, 2001, pp. 282–289.
[19] Z. Tu and X. Bai, “Auto-context and its application to high-level vision
tasks and 3D brain image segmentation,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 32, no. 10, pp. 1744–1757, Oct. 2010.
[20] A. Bianchi, B. Bhanu, and V. Donovan, “Visual and contextual modeling
for the detection of repeated mild traumatic brain injury,” IEEE Trans.
Med. Imag., vol. 33, no. 1, pp. 1–12, Jan. 2014.

BIANCHI et al.: DYNAMIC LOW-LEVEL CONTEXT FOR THE DETECTION OF MTBI

[21] P. Anbeek, K. L. Vincken, M. J. P. Van Osch, R. H. C. Bisschops, and
J. Van Der Grond, “Automatic segmentation of different-sized white matter lesions by voxel probability estimation,” Med. Image Anal., vol. 8, no.
3, pp. 205–215, Sep. 2004.
[22] F. Kruggel, J. S. Paul, and H.-J. Gertz, “Texture-based segmentation of
diffuse lesions of the brain’s white matter,” NeuroImage, vol. 39, no. 3,
pp. 987–996, Feb. 2008.
[23] A. Montillo, J. Shotton, J. Winn, J. E. Iglesias, D. Metaxas, and
A. Criminisi, “Entangled decision forests and their application for semantic segmentation of CT images,” Inf. Process. Med. Imag., vol. 6801,
pp. 184–196, 2011.
[24] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and
rotation invariant texture classification with local binary patterns,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, Aug. 2002.
[25] B. S. Manjunath and W.Y. Ma, “Texture features for browsing and retrieval
of image data,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 18, no. 8,
pp. 837–842, Aug. 1996.
[26] N. Dalal and B. Triggs, “Histograms of oriented gradients for human
detection,” in Proc. IEEE Congr. Comput. Vis. Pattern Recog., San Diego,
CA, USA, Jun. 2005, pp. 886–893.
[27] X. Guorong, C. Peiqi, and W. Minhui, “Bhattacharyya distance feature
selection,” in Proc. Int. Conf. Pattern Recog., Vienna, Austria, 1996,
pp. 195–199.
[28] P. Viola and M. J. Jones, “Rapid object detection using a boosted cascade
of simple features,” in Proc. IEEE Congr. Comput. Vis. Pattern Recog.,
Kauai, HI, USA, Dec. 2001, pp. 511–518.
[29] Y. Freund and R. E. Schapire, “A decision-theoretic generalization of
on-line learning and an application to boosting,” J. Comput. Syst. Sci.,
vol. 55, pp. 119–139, 1997.
[30] J. Friedman, T. Hastie, and R. Tibshirani, “Additive Logistic Regression:
A Statistical View of Boosting,” Ann. Statist., vol. 38, no. 2, pp. 337–407,
2000.
[31] J. C. Platt, “Probabilistic outputs for support vector machines and comparison to regularized likelihood methods,” in Advances in Large Margin
Classifiers. Cambridge, MA, USA: MIT Press, 2000, pp. 61–74.
[32] A. Obenaus, M. Robbins, G. Blanco, N. R. Galloway, E. Snissarenko,
E. Gillard, S. Lee, and M. Currás-Collazo, “Multi-modal magnetic resonance imaging alterations in two rat models of mild neurotrauma,”
J. Neurotrauma, vol. 24, no. 7, pp. 1147–1160, Jul. 2007.
[33] B. Menze, K. Van Leemput, D. Lashkari, M. Weber, N. Ayache, and
P. Golland, “Segmenting glioma in multi-modal images using a generativediscriminative model for brain lesion segmentation,” presented at the Med.
Image Comput. Comput. Assisted Intervention BraTS Challenge, Nice,
France, 2012.
[34] E. Geremia, B. H. Menze, and N. Ayache, “Spatial decision forest for
glioma segmentation in multi-channel MRI,” presented at the Med. Image
Comput. Comput. Assisted Intervention BraTS Challenge, Nice, France,
2012.
[35] D. Zikic, B. Glocker, E. Konukoglu, J. Shotton, A. Criminisi, D. H. Ye,
C. Demiralp, O. M. Thomas, T. Das, R. Jena, and S. J. Price, “Contextsensitive classification forests for segmentation of brain tumor tissues,”
presented at the Med. Image Comput. Comput. Assisted Intervention,
Nice, France, 2012.
[36] A. Bianchi, J. V. Miller, E. T. Tan, and A. Montillo, “Brain tumor segmentation with symmetric texture and symmetric intensity-based decision
forests,” presented at the IEEE Int. Symp. Biomed. Imag., San Francisco,
CA, USA, Apr. 2013.
[37] A. Bianchi, B. Bhanu, V. Donovan, and A. Obenaus, “Detecting mild
traumatic brain injury using dynamic low level context,” presented at the
IEEE Int. Conf. Image Process., Melbourne, Australia, Sep. 15–18, 2013.
Anthony Bianchi (S’06) received the B.S. degree
in electrical engineering from the California State
Polytechnic University, Pomona, CA, USA, in 2008
and the M.S. degree in electrical engineering from
the University of California, Riverside, CA, USA,
in 2010. He completed his Ph.D. degree in electrical engineering at the Center for Research in Intelligent Systems, University of California, Riverside in
September 2014.
He is currently working toward the Ph.D. degree
in electrical engineering at the Center for Research
in Intelligent Systems, University of California, Riverside.
His research interests include image processing, computer vision, pattern
recognition, and machine learning. His current research interests also include
mild traumatic brain injury and contextual feature generation.

153

Bir Bhanu (S’72–M’82–SM’87–F’95) received the
S.M. and E.E. degrees in electrical engineering and
computer science from the Massachusetts Institute
of Technology, Cambridge, MA, USA, the Ph.D. degree in electrical engineering from the University of
Southern California, Los Angeles, and the M.B.A.
degree from the University of California, Irvine, CA.
He is currently a Distinguished Professor of electrical engineering, a Cooperative Professor of computer science and engineering, mechanical engineering, and bioengineering, and the Director of the Center for Research in Intelligent Systems and the Visualization and Intelligent
Systems Laboratory with the University of California, Riverside. In addition, he
serves as the Director of National Science Foundation (NSF), The Integrative
Graduate Education and Research Traineeship program on Video Bioinformatics and the Interim Chair of the Department of Bioengineering at UCR. He has
been the Principal Investigator of various programs for NSF, Defense Advanced
Research Project Agency, National Aeronautics and Space Administration, Air
Force Office of Scientific Research, Office of Naval Research, Army Research
Office, and other agencies and industries in the areas of video networks, video
understanding, video bioinformatics, learning and vision, image understanding,
pattern recognition, target recognition, biometrics, autonomous navigation, image databases, and machine vision applications. He is the author or coauthor
of 450 reviewed technical publications, including more than 125 journal papers
and 43 book chapters, seven published books, and three edited books. He is
the holder of 18 (five pending) patents. His current research interests include
computer vision, pattern recognition and data mining, machine learning, artificial intelligence, image processing, image and video database, graphics and
visualization, robotics, human computer interactions, and biological, medical,
military and intelligence applications.
Dr. Bhanu is Fellow of American Association for the Advancement of Science, International Association of Pattern Recognition, and The International
Society for Optics and Photonics.

Andre Obenaus received the B.S. degree in biophysics from La Sierra University, Riverside, CA,
USA, in 1984, and the Ph.D. dgree in neurophysiology from the University of British Columbia,
Vancouver, BC, Canada, focused on understanding
the mechanisms of epilepsy in 1989. Subsequent
postdoctoral fellowship in continued his studies of
epilepsy were undertaken at University of California,
Los Angeles (UCLA), CA.
In 1995, he received a Research Associate Position
in the Department of Neurology at UCLA. From 1996
to 2002, he became an Assistant Professor within the Academic Department of
Medical Imaging, the University of Saskatchewan, SK, Canada, where he applied neuroimaging techniques, principally magnetic resonance imaging (MRI)
to study epilepsy. These early studies in MRI led to recruitment to Loma Linda
University within the Department of Radiation Medicine in 2001, where he
was instrumental in establishing the Non-Invasive Imaging Laboratory (NIL).
He became the Director of the NIL in 2007 to 2012 overseeing acquisition
and management of several MRIs, computed tomography (CT, and positron
emission tomography (PET) imagers. His research interests include to identify
the biological mechanisms underlying normal and abnormal brain function using imaging modalities. He has 118 peer-reviewed journal articles, including
Journal of Neuroscience and Annals of Neurology. He has also contributed 10
book chapters including methods of computational analysis of MR images.
Dr. Obenaus received several awards including in 1999 a prestigious Medical
Research Council Fellowship for 5 consecutive years. He was also nominated
to the Faculty of 1000 in 2011 based on work related to tracking and monitoring
stem cells within the brain using MRI. In 2012, he became a Member in the
Department of Pediatrics, the School of Medicine at LLU.

