IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

415

Exploiting Spatial Redundancy of Image Sensor
for Motion Robust rPPG
Wenjin Wang∗ , Sander Stuijk, and Gerard de Haan

Abstract—Remote photoplethysmography (rPPG) techniques
can measure cardiac activity by detecting pulse-induced color variations on human skin using an RGB camera. State-of-the-art rPPG
methods are sensitive to subject body motions (e.g., motion-induced
color distortions). This study proposes a novel framework to improve the motion robustness of rPPG. The basic idea of this paper
originates from the observation that a camera can simultaneously
sample multiple skin regions in parallel, and each of them can
be treated as an independent sensor for pulse measurement. The
spatial redundancy of an image sensor can thus be exploited to distinguish the pulse signal from motion-induced noise. To this end,
the pixel-based rPPG sensors are constructed to estimate a robust
pulse signal using motion-compensated pixel-to-pixel pulse extraction, spatial pruning, and temporal filtering. The evaluation of this
strategy is not based on a full clinical trial, but on 36 challenging benchmark videos consisting of subjects that differ in gender,
skin types, and performed motion categories. Experimental results
show that the proposed method improves the SNR of the state-ofthe-art rPPG technique from 3.34 to 6.76 dB, and the agreement
(±1.96σ) with instantaneous reference pulse rate from 55% to
80% correct. ANOVA with post hoc comparison shows that the improvement on motion robustness is significant. The rPPG method
developed in this study has a performance that is very close to that
of the contact-based sensor under realistic situations, while its computational efficiency allows real-time processing on an off-the-shelf
computer.
Index Terms—Biomedical monitoring, motion analysis, photoplethysmography, remote sensing.

I. INTRODUCTION
ARDIAC activity is measured by medical professionals
to monitor patients’ health and assist clinical diagnosis.
The conventional contact-based monitoring methods, i.e., electrocardiogram (ECG) and photoplethysmography (PPG), are
somewhat obtrusive and may cause skin irritation in sensitive
subjects (e.g., skin-damaged patients, neonates). In contrast,
camera-based vital signs monitoring triggers a growing interest
for noninvasive and nonobtrusive healthcare monitoring.

C

Manuscript received April 25, 2014; revised August 18, 2014; accepted August 31, 2014. Date of publication September 8, 2014; date of current version
January 16, 2015. Asterisk indicates corresponding author.
*W. Wang is with the Electronic Systems Group, Department of Electrical
Engineering, Eindhoven University of Technology, Einhoven 5612 AZ, The
Netherlands (e-mail: W.Wang@tue.nl).
S. Stuijk is with the Electronic Systems Group, Department of Electrical
Engineering, Eindhoven University of Technology, Einhoven 5612 AZ, The
Netherlands (e-mail: S.Stuijk@tue.nl).
G. de Haan is with the Philips Innovation Group, Philips Research, Eindhoven
5612 AZ, The Netherlands (e-mail: G.de.Haan@philips.com).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2356291

Earlier progress made in camera-based vital signs monitoring can be categorized into two trends: 1) detecting the minute
optical absorption variations of the human skin induced by
blood volume changes during the cardiac cycle, i.e., remotePPG (rPPG) [1]–[3]; 2) detecting the periodic head motions
caused by the blood pulsing from heart to head via the abdominal aorta and carotid arteries [4]. However, both the color-based
and motion-based approaches are sensitive to body motions,
since these can dramatically change the light reflected from the
skin surface and also corrupt the subtle head motion driven by
the cardiovascular pulse. Although significant progress has been
reported in the rPPG category for a fitness setting recently [3],
the signal-to-noise ratio (SNR) of the pulse signals obtained by
all existing methods are still reduced when the subject is moving
relative to the camera.
The goal of this paper is to significantly improve the SNR
of the rPPG pulse signal by better exploiting the spatial redundancy of the image sensor. To some extent, the spatial redundancy of the image sensor has already been exploited in previous
rPPG methods [1]–[3] as they extract the pulse signal from the
averaged pixel value in a skin region. Such averaging of independent sensors is optimal only if the (temporal) noise level
in skin pixels is comparable and has a Gaussian distribution.
However, the image-to-image variations in skin pixels from a
face may be very strong in the mouth region of a talking subject,
while relatively low on the stationary forehead. If the outliers
(pixels near the mouth) could be removed from the average, the
quality of the extracted pulse signal is expected to be improved
significantly.
To this end, a motion robust rPPG method is proposed to
treat each skin pixel in an image as an independent rPPG sensor
and extract/combine multiple rPPG signals in a way that is
immune to noise. The proposed method consists of three steps:
1) creating pixel-based rPPG sensors from motion-compensated
image pixels, 2) rejecting motion-induced spatial noise, and
3) optimizing temporally extracted pulse-traces into a single
robust rPPG signal. To demonstrate the effectiveness, it has
been evaluated on 36 challenging videos with an equal number
of male and female subjects in three skin-type categories and
six motion-type categories.
The contributions of this paper are threefold: 1) a new strategy
is proposed to track pixels in the region of interest (e.g., a
subject’s face) for rPPG measurement using global and local
motion compensation; 2) exploiting the spatial redundancy of
an image sensor, i.e., pixel-based rPPG sensors, is proved to
lead to a considerable gain in accuracy as compared to the
common approach that takes a single-averaged color trace, and
3) a novel algorithm is introduced to optimize the pixel-based
rPPG sensors in spatial and temporal domain.

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

416

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

Fig. 1. Flowchart of the proposed motion robust rPPG framework: (a) A video sequence with a manually selected RoI is the input to the framework. The global
and local motions of the RoI are compensated, and pixel-based rPPG sensors between adjacent frames are constructed using motion-compensated pixel-to-pixel
correspondences. (b) The outliers among the pixel-based rPPG sensors, i.e., the ones without skin information or distorted by motion noise, are pruned in the
spatial domain. (c) The spatially pruned inliers are chained up in the temporal domain as multiple pulse traces, which are filtered and further optimized into a
single robust rPPG signal.

The rest of this paper is organized as follows. Section I
provides an overview of the related work. Section II analyzes
the problem concerning this study and describes the proposed
method. The experimental setup is discussed in Section I while
the proposed method is evaluated and discussed in Section V.
Finally, the conclusions are drawn in Section VI.

nal as its output. There are three main steps in the processing
chain: motion-compensated pixel-to-pixel pulse extraction, spatial pruning, and temporal filtering. Each step is discussed in
detail in the following sections.

II. RELATED WORK

To extract parallel pulse signals from spatial-redundant pixels, the pixels belonging to the same part of skin should be
concatenated temporally. So this method compensates for the
subject motion and relates temporally corresponding pixels.
1) Global Motion Compensation: In previous rPPG methods [1]–[3], the subject’s face is typically used as the region of
interest (RoI) for pulse measurement. The motion of the face
can be interpreted as a linear combination of global rigid motion (head translation and rotation) and local nonrigid motion
(e.g., eye blinking and mouth talking). The common approach
to compensate for the global motion of a face is using the Viola–
Jones face detector to locate the face in consecutive frames with
a rectangular bounding-box [7]. However, a classifier that has,
for example, been trained with only the frontal-face samples
cannot detect the side-view faces. This fundamental limitation
may lead to a discontinuous face localization across subsequent
video frames.
As an alternative, a “tracking-by-detection” approach, which
enables the online updating of the target appearance model while
tracking the object, demonstrates the capability of adapting to
occasional appearance changes of the target as well as handling
the challenging environmental noise (e.g., partial occlusions and
background clutter). According to the latest benchmark results
of online object tracking presented in 2013 [8], the circulant
structure of tracking-by-detection with kernels (CSK) developed
by Henriques et al. [9] has the highest tracking speed among
the top ten accurate trackers, which can achieve hundreds of
frames per second [8]. Considering that no significant accuracy
difference can be observed among the state-of-the-art trackers
in the setting of this study, the fastest CSK method is chosen to
compensate for the global motion of the subject’s face instead
of a Viola–Jones face detector.
2) Local Motion Compensation: Based on the globally
tracked face, the pixels’ displacements can be more precisely

In the cardiovascular system, the blood pulse propagating
throughout the body changes the blood volume in the vessels.
Given the fact that the optical absorption of hemoglobin varies
across the light spectrum, a specific cardiovascular event can be
revealed by measuring the color variations of skin reflections
[1]. In 2008, Verkruysse et al. found that in an ambient light
condition, the PPG signal has different relative strength in three
color channels of an RGB camera that senses the human skin
[5]. Based on this finding, Poh et al. proposed a linear combination of RGB channels defining three independent signals with
independent component analysis using non-Gaussianity as the
criterion for separating independent resource signals [1]. As an
alternative, Lewandowska et al. suggested a principal component analysis (PCA)-based solution to define three independent
linear combinations of RGB channels [2]. In 2012, MIT developed a method called “Eulerian video magnification” to amplify
the subtle color changes through band-pass filtering the temporal
pyramidal image differences [6]. However, any motion-induced
color distortions within the same frequency band as that of the
pulse are unfortunately amplified. More recently, de Haan et al.
introduced the chrominance-based rPPG method (CHROM) to
consider the pulse as a linear combination of three color channels under a standardized skin-tone assumption [3]. This method
demonstrates the highest accuracy of all existing rPPG methods.
Based on a comparison of the state-of-art rPPG methods, this
study relies on the CHROM method as the baseline to develop
a motion robust rPPG method.
III. METHOD
The overview of the proposed motion robust rPPG framework
is shown in Fig. 1, which takes a video sequence containing a
subject’s face as the input and returns the extracted pulse sig-

A. Motion-Compensated Pixel-to-Pixel Pulse Extraction

WANG et al.: EXPLOITING SPATIAL REDUNDANCY OF IMAGE SENSOR FOR MOTION ROBUST rPPG

estimated in this step. The implementation of the Farneback
dense optical flow algorithm [10] in OpenCV 2.4 [11] is utilized to measure the translational displacement of each image
pixel between adjacent frames. In addition, the idea of forward–
backward flow tracking proposed by Kalal et al. [12] is adopted
to detect the pixel-based tracking failures: in a bidirectional
tracking procedure, the motion vectors with larger spatial errors
yielded by abrupt motion are removed as noise, whereas the
consistent motion vectors are retained to associate the temporal
corresponding pixels via spatial bilinear interpolation.
3) Pixel-to-Pixel Pulse Extraction: After global and local
motion compensation, the pixels between adjacent frames have
been aligned into pairs. By concatenating them in a longer frame
interval, multiple pixel trajectories can be generated. However,
there is a problem in creating such longer pixel trajectories:
pixels belonging to the same trajectory may disappear due to
occlusions (e.g., face rotation).
In fact, under a constant lighting environment, the pixels
in different locations of the skin show the same relative PPG
amplitude. It implies that if the pulse-induced color changes in
each aligned pixel pair are temporally normalized, they can be
concatenated in an arbitrary order to derive a long-term signal.
Since the pixel-based motion vectors only need to be estimated
between two frames (the smallest possible interval), it minimizes
the occlusion problem and also prevents the propagation of
errors in local motion estimation.
The temporally normalized RGB differences of the ith pixel
t→t+1
, which
between frame t and t + 1 is denoted by a vector C i
is defined as
⎛ t→t+1 ⎞
Ri
⎜ t→t+1 ⎟
t→t+1
t+1
t
⎟.
⎜
(1)
Ci
= C i − C i = ⎝ Gi
⎠
t→t+1

Bi

Assuming the spatial displacement of the ith pixel from frame
→
−
t to t + 1 is d (dx , dy ), (1) can be written as
⎛

t→t+1
Ci

⎜
⎜
⎜
⎜
=⎜
⎜
⎜
⎜
⎝

Rit+1 (x + dx , y
Rit+1 (x + dx , y
Gt+1
(x + dx , y
i
t+1
Gi (x + dx , y
Bit+1 (x + dx , y
Bit+1 (x + dx , y

+ dy ) − Rit (x, y)
+ dy ) + Rit (x, y)
+ dy ) − Gti (x, y)
+ dy ) + Gti (x, y)
+ dy ) − Bit (x, y)
+ dy ) + Bit (x, y)
t→t+1

Fig. 2. Histograms of temporally normalized RGB differences between frame
t and t + 1 of three skin types in a homogeneous lighting condition. The
histogram distributions show that all skin pixels describe the similar pulseinduced RGB changes after temporal normalization.

bination of RGB channels as
t→t+1

Xi

t→t+1

Yi

t→t+1

= 3Ri

t→t+1

− 2Gi

t→t+1

= 1.5Ri

t→t+1

+ Gi

t→t+1

− 1.5B i

t→t+1

.

(3)

t→t+1

,Y i
) estimated
By temporally concatenating (X i
from pixel pairs between adjacent frames and integrating them,
multiple chrominance traces can be derived as
X̃it→t+l = 1 +

l


t→t+1

Xi

0

Ỹit→t+l = 1 +

l


t→t+1

Yi

(4)

0

where l is the interval length of the chrominance trace defined
by a temporal sliding window. In line with [3], l is specified as
64 frames in case of a 20 FPS video recording rate. The pulse
trace in the temporal window can be calculated as
P̃it→t+l = X̃it→t+l − αỸit→t+l

⎞
⎟
⎟
⎟
⎟
⎟.
⎟
⎟
⎟
⎠

417

(5)

with
(2)

Fig. 2 shows the histogram distribution of C i
on three dift→t+1
,
ferent skin tones: the Gaussian-shaped distribution of Ri
t→t+1
t→t+1
Gi
, and B i
on different skin tones are all within the
range [−0.02, 0.02], which is very concentrated compared to its
theoretical variation range [−1, 1]. Thus, it can be concluded that
in all skin pixels, pulse-induced color variations roughly exhibit
the same strengths in temporally normalized color channels.
After that, the temporally normalized RGB differences are
projected onto the chrominance plane using the CHROM
method [3], which defines the pulse signal as a linear com-

α=

σ(X̃it→t+l )
σ(Ỹit→t+l )

(6)

where σ(·) corresponds to the standard deviation operator. In
order to avoid the signal drifting/explosion in a long-term accumulation, the pulse traces estimated from the sliding window
are overlap—added together with a Hann window [3].
Note that the spatial averaging of local pixels can reduce
quantization errors during the temporal color normalization.
The face RoI is downsampled starting from the local motion
compensation step, which not only reduces the noise sensitivity
of pixel-based rPPG sensors, but also increases the processing
speed of the dense optical flow. There is a tradeoff in selecting
the optimal downscaling size considering the accuracy and efficiency. Since the size of all subjects’ face used in this study are
approximately 200 × 250 pixels, the RoI is uniformly downsampled to 36 × 36 pixels.

418

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

Fig. 3. Example of skin/nonskin pixel classification on three subjects with
different skin colors. The red bounding box is the tracked face, and the nonskin
pixels inside the bounding box are masked by black color.

Fig. 4. Example of spatial pruning in the temporally normalized RGB space.
The distribution of pixel-based rPPG sensors in this space is different between
the stationary and motion scenarios. This step removes the sensors containing
explicit motion-induced color distortions.

B. Spatial Pruning
Since the temporal noise level in pixel-based rPPG sensors
is not Gaussian distributed, the next step is to optimally select
the inliers (reliable sensors) from a set of spatially redundant
sensors for a robust rPPG-signal measurement. In practice, there
are mainly two kinds of noise degrading the quality of rPPG
sensors: 1) nonskin pixels (e.g., eyebrow, beard, and nostril) that
do not present pulse signals; 2) skin pixels that contain motioninduced color distortions. Based on this observation, a spatial
pruning method including skin/nonskin pixel classification and
color space pruning is designed to preselect the reliable sensors.
1) Skin/Nonskin Pixel Classification: Most skin segmentation methods use predefined thresholds of skin color composition or model a binary boundary between foreground
and background. However, these approaches suffer from
dilemmas in choosing suitable thresholds or defining foreground/background. As a matter of fact, most of the pixels inside
a well-tracked face region represent the skin while only a small
number of them are not skin. Since the skin pixels that share
some similarities are bound in one cluster, a clustered featurespace can be constructed to detect the pixels that are further
away from the cluster center as novelties (nonskin pixels). In
this method, the one class support vector machine (OC-SVM)
[13] is employed to estimate such a hyperplane, which encircles
most of the pixel samples as a single class (skin class) without
any prior skin color information.
In order to train an OC-SVM, a list of feature descriptors
x1 , x2 , x3 , . . . , xn should be created to represent the skin pixels.
Inspired by [14] that using the intensity-normalized RGB and
YCrCb to discriminate skin and nonskin regions, this method
represents each vector xi with four components: r − g, r − b,
Y − Cr, and Y − Cb. The OC-SVM is only trained with the
first few frames to adapt to the subject skin tone; then, it is used
to predict the skin pixels in the subsequent frames, i.e., the pixels
with the positive and negative responses for f (x) are classified
as skin and nonskin pixels, respectively. This step significantly
removes the pixel-based rPPG sensors that are not pointing at
the subject’s skin, and its performance is invariant to different
skin tones, as shown in Fig. 3.
2) Color Space Pruning: As explained before, the pulset→t+1
uninduced color variations exhibit similar changes in C i
der a homogeneous lighting environment, i.e., in temporally nor-

t

t

t

malized color space, the transformation between (Ri , Gi , B i )
t+1
t+1
t+1
and (Ri , Gi , B i ) should ideally be the translation. However, motion-induced color distortions enter this translation by
adding additional residual transformations, such as rotation.
Therefore, by checking the geometric transformation of pixelbased rPPG sensors in the temporally normalized color space,
a number of unreliable sensors distorted during the transformation can be found and pruned. To realize this step, the inner
product φ of the unit color vectors between frame t and t + 1 is
simply calculated as


	 t
t+1
Ci
Ci
t→t+1
(7)
=
φi
t ,
t+1
||C i || ||C i ||
where ,  denotes the inner product operation, || · || corresponds
to the L2-normalization. When φt→t+1
is more deviated from
i
t
t+1
1, the angle between C i and C i is larger, which implies that
the color transformation is more likely to be motion induced.
In this manner, all the rPPG sensors are sorted based on their
inner products and a fraction β (e.g., β = 18 ) of them ranking
closest to 0 (orthogonal) are pruned as outliers. Fig. 4 shows an
example of spatially pruned results in this space: subject motion
yields a more sparse distribution of rPPG sensors in the spatial
domain as compared to the stationary scenario.
Furthermore, the remaining rPPG sensors are pruned in the
temporally normalized XY space. On the projected chrominance
plane using (3), it can be observed that when the subject is
perfectly stationary, X − αY (pulse direction) is the principal direction while the projections are densely distributed as
an ellipse; when motion appears, the direction orthogonal to
X − αY starts to dominate the space and the projections are
sparsely distributed like a stripe, as shown in Fig. 5. The direction orthogonal to the pulse direction on this chrominance plane
is named as the “motion direction,” which can be expressed as
t→t+1

Mi

t→t+1

= Xi

t→t+1

+ αY i

(8)

where α is identical to the one calculated in (6). The criterion
to prune sensors on the chrominance plane is: selecting the
sensors containing the least motion signals but the most likely
pulse signals. Therefore in the first round, all sensors are sorted

WANG et al.: EXPLOITING SPATIAL REDUNDANCY OF IMAGE SENSOR FOR MOTION ROBUST rPPG

419

Fig. 6. Example of using the adaptive band-pass filtering on frequency spectrums obtained from two subjects (e.g., light skin and dark skin subjects) shown
in Fig. 3.
Fig. 5. Example of spatial pruning in the temporally normalized chrominance
space. This step removes the sensors containing implicit motion-induced distortion residues, but retains the sensors with the most likely pulse signal.

in an ascending order based on the magnitude of their motion
signal |X + αY |. The ones ranking at the high end are more
affected by motion and are thus pruned. In the second round,
the remaining sensors are sorted again in an ascending order
based on their pulse signal X − αY . The ones ranking in the
median position represent the most probable pulse signal and
are thus selected. Similarly, this step uses the same fraction β
to prune the outliers on the chrominance plane.
C. Temporal Filtering
Till this step, there are two alternatives to use the spatially
pruned rPPG sensors: 1) averaging the inliers for subsequent
pulse estimation that is further identical to previous rPPG methods; 2) first extracting independent pulse signals from the inliers
in parallel, and then combining them into a single robust pulse
signal after postprocessing. Due to the residual errors in motion
estimation, the noise in spatial inliers still shows no Gaussian
distribution and is not zero mean. Furthermore, concatenating
the local rPPG sensors separately allows the local optimization
of α in (5) when deriving the pulse signals. Consequently, option (2) is adopted to separate the pulse signal and noise by
generating parallel pulse traces.
Given the fact that the pulse derivatives in local rPPG sensors
are temporally normalized, they can be randomly concatenated
for creating long-term traces. But generating all possible concatenations is an impossible task (e.g., (600!)64 different ways
of concatenation in case of 600 skin pixels over 64 frames), so
a simple solution is proposed to find favorable concatenations:
first, sort all the pulse derivatives (sensors) based on their distance to the mean and concatenate them in the sorted order. The
signal-traces ranking at the top are expected to be fairly reliable
pulse signals, whereas the ones ranking at the bottom are likely
to be suboptimal. Afterward, the adaptive band-pass filtering
and PCA decomposition steps are designed to further enhance
and combine the multiple pulse traces into a single robust rPPG
signal.
1) Adaptive Band-Pass Filtering: Essentially, the pulse rate
of a healthy subject falls within the frequency range [40, 240]
beats per minute (bpm), so the parts of signal that are not in
this frequency band can be safely blocked, i.e., in a temporal
sliding window with 64 frames length, the in-band frequency

range corresponds to [2, 12]. For a given moment, the instant
pulse frequency should be even more concentrated in a smaller
range such as [80, 90] bpm. So using the real-time pulse-rate
statistics, an adaptive band-pass filtering method is developed
to better limit the band-pass filter range.
An example is shown in Fig. 6: the mean frequency-peak position of all pulse traces in the current temporal window is found as
the most probable instantaneous pulse frequency of the subject,
then a fraction β of pulse traces whose frequency-peak position
has a large distance to the most probable instantaneous pulse
frequency are pruned. After that, the original pulse-frequency
band is adapted to the first two harmonics derived from the
mean frequency peak position, i.e., if the most probable peak
position is at 4, the pulse-frequency range is reduced from original [2, 12] to [3, 5] ∪ [6, 10] (first two harmonics). Similarly,
if the most probable peak position is at 5, the pulse-frequency
band is narrowed down to [4, 6] ∪ [8, 12].
Note that the proposed adaptive band-pass filtering method
adjusts the pulse-frequency bandwidth based on instantaneous
statistics in the current sliding window, which does not rely on
any prior assumptions or previous observations (e.g., Kalman
filter) of a specific subject’s pulse rate.
2) PCA Decomposition: To derive a robust rPPG signal from
multiple band-passed pulse traces, the robust pulse signal is defined as a periodic signal with the highest variance. The reasons
are 1) the subject motions are often occasional and unintentional
in a hospital/clinical use case, i.e., nonperiodic motions, 2) the
motion-induced variance has been reduced by motion compensation, so the pulse-induced periodicity is more obvious in a
cleaner signal trace.
Based on this observation, the periodicity of a pulse signal is
defined as a ratio between the maximum power and total power
of the signal spectrum in the pulse-frequency band. When the
signal is more periodic, this ratio is larger. Similarly, the pulse
traces are sorted based on their periodicity, and a fraction β of
traces with low periodicity are pruned.
Finally, PCA is performed on the periodic pulse traces to obtain the eigenvectors, which has two benefits: 1) the decomposed
eigenvectors are orthogonal to each other in the subspace, which
clearly separates the pulse signal and noises, 2) the eigenvectors
are ordered in term of variance, which simplifies the procedure of selecting the most variant trace. In the temporal sliding
window, the eigenvector (among the top five eigenvectors) that

420

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

Fig. 7. Snapshot of the skin types of six subjects in the benchmark dataset.
The subjects’ eyes are covered for protecting their identity only in the printing.

has the best correlation with the mean pulse trace is selected
to be the rPPG signal after correcting the arbitrary sign of the
eigenvector as
t→t+l
=
P̃selected

t→t+l
, P̃mt→t+l
P̃eigen
ean 
t→t+l
|P̃eigen
, P̃mt→t+l
ean |

t→t+l
× P̃eigen

(9)

t→t+l
and P̃mt→t+l
where P̃eigen
ean represent the eigenvector and mean
pulse trace, respectively; ,  corresponds to the inner product
(correlation) between two vectors; and | · | denotes the absolute
value operator.

IV. EXPERIMENT
This section presents the experimental setup for evaluating
the proposed rPPG method. First, it shows the way of creating
the benchmark video dataset. Next, it introduces two metrics
for evaluating the performance of rPPG methods. Finally, it
includes five (r)PPG methods for performance comparison.
A. Benchmark Dataset
To evaluate the proposed rPPG method, six healthy subjects
(students) are recruited from Eindhoven University of Technology. The study is approved by the Internal Committee Biomedical Experiments of Philips Research, and the informed consent
is obtained from each subject. The video sequences are recorded
with a global shutter RGB CCD camera (type USB UI-2230SEC of IDS) in an uncompressed data format, at a frame rate of 20
Hz, 768 × 576 pixels, 8 bit depth, and has a duration of 90 s per
motion category. During the video recording, the subject wears a
finger-based transmissive pulse oximetry (model CMS50E from
Contec Medical) for obtaining the reference pulse signal, which
is synchronized with the recorded video frames using the USB
protocol available on the device. The subjects sit in front of the
camera with their face visible and illuminated by a fluorescent
light source (type: Philips HF3319—EnergyLight White).
Fig. 7 shows a snapshot of the recorded subjects from three
skin-type categories according to the Fitzpatrick skin scale [15]:
Skin-category I with “Skin-type II” male/female; Skin-category
II with “Skin-type III” male/female; and Skin-category III with
“Skin-type V” male/female. All subjects are instructed to perform six different types of head motion: stationary, translation,
scaling, rotation, talking, and mixed motion (mixed motion is the
mixture of all motions). For each recording, the subject remains
stationary in the first 15 s and then performs a specific motion
till the end by repeating it. There is no guidance to restrict the
amount of motion, so it leads to displacements up to the maximum 35 pixels per picture-period in practice. This is intended

Fig. 8. Example of frames in skin-category II male rotation video. In this
video, the subject performs in-plane and out-of-plane rotations.

to better mimic the practical use cases and make the videos
sufficiently challenging for rPPG. Fig. 8 shows some uniformly
sampled frames in the rotation video sequence of skin-category
II male.
The goal of this study is aimed to improve the “motion robustness” of rPPG, “motion” is considered the key variable that
is varied in the dataset. (As mentioned before, the gender and
skin type are also varied.) So when recording each video sequence, the subject is asked to perform a specific type of motion
repeatedly. Each motion is repeated approximately 15 times
in each video sequence. Since motion is the most important
variable affecting the rPPG performance in a single constant
luminance environment, the measurement of the whole video
sequence with repeated subject motion can be considered as
a composition of multiple repeated short-term measurements.
Hence, the video sequences allow studying the measurement
repeatability. The Bland–Altman plots in Fig. 10 shows, for
example, the within-measurement repeatability comparison between rPPG and PPG, in which each scatter point represent the
measurement of one complete pulse. To prevent an explosion
of test data, the subjects selected for recording are representative/typical in each skin category. There are no subjects at all
intermediate skin types, which makes it impossible for us to
draw thorough conclusions on skin-tone invariance of the rPPG
methods.
B. Evaluation Metrics
This study adopts the same SNR metric as used in [3] to measure the signal quality for comparing the strength and weakness
of rPPG methods. In this SNR metric, a temporal sliding window is utilized to segment the whole pulse signal into intervals
for deriving the SNR trace, i.e., the temporal window has a 300
frames stride and a one frame sliding step. In the sliding window, the signal interval is transformed to the frequency domain
using FFT. The SNR is measured as the ratio between the energy around the first two harmonics (pulse in-band frequency)
and the remaining energy (noises out-of-band frequency) of the
spectrum, which is defined as
 220

2
f =40 (Ut (f )S̃(f ))
(10)
SNR = 10 log10 220
2
f =40 (1 − Ut (f )S̃(f ))
where f is the pulse frequency in bpm, S̃(f ) is the spectrum
of the pulse signal, Ut (f ) is a defined binary window to pass

WANG et al.: EXPLOITING SPATIAL REDUNDANCY OF IMAGE SENSOR FOR MOTION ROBUST rPPG

421

Fig. 9. In each category, the color bar is the averaged SNRa while the black bar is the standard deviation. (a) Motion SNRa: it compares the SNRa obtained by
the (r)PPG methods in different motion types (averaged over genders and skin categories). (b) Skin SNRa: it compares the SNRa obtained by the (r)PPG methods
in different skin categories (averaged over genders and motion types).

Fig. 10. Instantaneous pulse-rate plot (first row) and Bland–Altman plot (second row) for six motion types of the male subject in skin-category II. The subject’s
appearance is shown in Fig. 8. The Bland–Altman agreements are calculated between rPPG signals and reference signals (REF), where the reference signals are
the smoothed signals recorded by CBS. To visually compare the agreements between rPPG methods and reference, the Bland–Altman plots of four rPPG methods
are put in one graph and σ of ±1.96σ obtained between PTC and the reference to denote the variance range.

the pulse in-band frequency and block the noisy out-of-band
frequencies. Consequently, the SNRa, an averaged value of the
SNR-trace, is used to summarize the quality of the pulse signal.
Additionally, Bland–Altman plots are included to show the
agreements of the instantaneous pulse rate between the rPPG
and reference PPG sensor. The instantaneous pulse rate, defined
as the inverse of the peak-to-peak interval of the pulse signal,
is derived by a simple peak detector in the time domain. The
reasons of using it for signal comparison are twofold: 1) the
primitive pulse signals obtained by rPPG and PPG have good
alignment with each other, thus their instantaneous rates are
comparable, 2) it captures the instantaneous changes of the pulse
signal and reflects the occasional differences between compared
signals, as an example shown in Fig. 10. In the standard Bland–
Altman plot, the Cartesian coordinate of a pulse rate’s sample
si is calculated as

si (x, y) =

PRi + RRi
, PRi − RRi
2


(11)

where PRi and RRi are ith instantaneous pulse rates obtained by
rPPG and PPG, respectively. RRi is smoothed by a five-point
mean filter for suppressing the noise effect. Furthermore, the
Bland–Altman agreement A between PRi and RRi is calculated

as
n
A=

i=1

ai

n

(12)

with

ai =


1

if | PRi − RRi | < 1.96σ

0

if |PRi − RRi | ≥ 1.96σ

(13)

where n is the total number of samples in a pulse rate; σ denotes
the standard deviation of the difference between PRi and RRi .
Finally, the analysis of variance (ANOVA) is applied on SNRa
values to analyze the significance of difference between (r)PPG
methods under certain categories (e.g., skin or motion), i.e., to
show whether the main variation in SNRa is “between” groups
(rPPG methods) or “within” groups (video sequences). Based
on the results of ANOVA, the post-hoc comparison is used to
further evaluate the posteriori pairwise comparisons between
individual methods to see which one is significantly better than
the other. The ANOVA with post-hoc comparison gives a clear
overview of statistical comparison between investigated (r)PPG
methods.

422

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

TABLE I
SNRA RESULTS GAINED BY (R)PPG METHODS ON BENCHMARK VIDEOS
(AVERAGED OVER GENDERS)

TABLE II
AGREEMENTS GAINED BY RPPG METHODS ON BENCHMARK VIDEOS
(AVERAGED OVER GENDERS)

Videos

FDM

FTM

PTM

PTC

CBS

Videos

FDM

FTM

PTM

PTC

Skin-category I stationary
Skin-category I translation
Skin-category I scaling
Skin-category I rotation
Skin-category I talking
Skin-category I mixed motion
Skin-category II stationary
Skin-category II translation
Skin-category II scaling
Skin-category II rotation
Skin-category II talking
Skin-category II mixed motion
Skin-category III stationary
Skin-category III translation
Skin-category III scaling
Skin-category III rotation
Skin-category III talking
Skin-category III mixed motion
Average

6.54
6.20
3.90
1.53
5.69
1.86
8.26
6.13
7.43
−0.20
2.49
1.18
5.87
2.81
2.16
−1.80
0.30
−0.24
3.34

6.65
6.75
5.48
6.83
5.94
4.24
8.24
6.95
7.39
4.29
2.42
2.97
6.55
3.89
2.29
−0.70
1.24
0.94
4.58

6.73
6.33
5.44
6.78
1.34
4.30
7.93
6.52
7.20
4.30
1.39
1.53
7.24
3.90
2.55
0.83
−0.32
−0.21
4.10

7.18
8.40
8.26
7.91
7.25
7.18
8.80
6.91
8.11
5.90
3.60
3.97
8.93
5.97
7.37
6.09
5.00
4.93
6.76

6.80
7.16
7.14
8.72
5.81
5.92
7.68
4.64
5.48
7.46
3.13
4.09
8.30
6.24
5.52
1.38
6.88
5.44
5.99

Skin-category I stationary
Skin-category I translation
Skin-category I scaling
Skin-category I rotation
Skin-category I talking
Skin-category I mixed motion
Skin-category II stationary
Skin-category II translation
Skin-category II scaling
Skin-category II rotation
Skin-category II talking
Skin-category II mixed motion
Skin-category III stationary
Skin-category III translation
Skin-category III scaling
Skin-category III rotation
Skin-category III talking
Skin-category III mixed motion
Average

96%
80%
63%
41%
66%
36%
98%
65%
83%
27%
57%
48%
74%
45%
31%
19%
40%
24%
55%

95%
77%
79%
75%
70%
62%
98%
62%
84%
57%
58%
67%
79%
52%
53%
25%
49%
32%
65%

96%
86%
80%
71%
45%
57%
97%
58%
80%
54%
47%
57%
84%
52%
50%
33%
34%
28%
62%

96%
97%
97%
95%
93%
88%
99%
76%
83%
84%
65%
78%
85%
75%
68%
43%
65%
49%
80%

Bold entries indicate the best performance of rPPG methods in each category.

Bold entries indicate the best performance of rPPG methods in each category.

V. RESULTS AND DISCUSSION
C. Compared Methods
Based on the benchmark dataset and evaluation metrics, three
comparisons have been performed for the evaluation: 1) comparing the proposed method to the state-of-the-art rPPG method
CHROM [3], 2) comparing the separate steps in the developed
framework to show their independent improvements and contributions to the complete solution, since these separate steps
involve innovations that are not addressed in previous rPPG
studies, and 3) comparing the rPPG methods to the PPG method
to show the disparity between camera-based and contact-based
approaches. The details of the compared (r)PPG methods are
described below:
1) Face-Detect-Mean (FDM) is a reimplementation of the
CHROM method. It uses the Viola–Jones face detector
to locate the face, and applies the OC-SVM method to
select the skin pixels to derive the averaged RGB traces
for pulse-signal estimation.
2) Face-Track-Mean (FTM) is the included substep of the
proposed method. It replaces the Viola–Jones face detector in FDM with the CSK tracker for the better face
localization.
3) Pixel-Track-Mean (PTM) is the included substep of the
proposed method. It extends FTM with spatial redundancy by creating pixel-based rPPG sensors, but takes
the averaged values of the temporally normalized color
differences to derive the pulse signal.
4) Pixel-Track-Complete (PTC) is the complete version of
the proposed method, which adds the spatiotemporal optimization procedure (spatial pruning and temporal filtering) to the PTM.
5) Contact-Based-Sensor (CBS) is a finger-based pulse
oximetry. It is used to record the reference pulse signal
for comparison.

The proposed method is implemented in Java using the
OpenCV 2.4 library [11] and run on a laptop with an Intel Core
i7 2.70 GHZ processor and 8 GB RAM. All five methods are
evaluated on 36 video sequences from the benchmark dataset.
For fair comparison, only the RoI (e.g., subject’s face) needs
to be manually initialized, while the other parameters remained
identical when processing different videos.
The results show that the gender is not the key factor that needs
to be investigated in this dataset, i.e., the differences between
stationary male and female from the same skin-category are
rather small. Thus, the results obtained by the different genders
in the same skin category and motion type are averaged. Tables I
and II summarize the gender-averaged SNRa and Bland–Altman
agreements, respectively. Moreover, the SNRa values in Table I
are further averaged over 1) the three skin categories for comparing the motion robustness, 2) the six motion types for comparing
the skin-tone invariance, as shown in Fig. 9 (the standard deviation of SNRa is also calculated to show the methods’ variability
in each category).
1) Stationary Scenario
Fig. 9(a) shows that all (r)PPG methods gain similar performance on stationary subjects, i.e., the standard deviations of
their SNRa are below 1.0 dB. The reason is that these methods are all using the chrominance-based method [3] for pulse
extraction. Their main difference is in motion estimation and
outlier rejection. No significant improvements can be expected
for static subjects.
2) Motion Scenarios
In videos, where the subjects’ frontal face can be detected by
the Viola–Jones method (e.g., translation, scaling, and talking),

WANG et al.: EXPLOITING SPATIAL REDUNDANCY OF IMAGE SENSOR FOR MOTION ROBUST rPPG

423

Fig. 11. Statistical comparison between five (r)PPG methods in five categories using ANOVA with post-hoc analysis. The ANOVA plots in the first row show
the overview of performance variation between methods in each category, i.e., median (red bar), standard deviation (blue box), minimum, and maximum (black
bar) SNRa values. The post-hoc plots in the second row show the pairwise differences between the methods in each category and highlight the pairs that are
significantly different (in blue and red).

FDM still works properly, whereas FTM that relies on the online
object tracker is approximately 1.0 dB better. The improvement
is due to the object tracker, which leads to a smoother face
localization between consecutive frames compared to the face
detector by exploiting the target’s appearance consistency and
position coherence.
However, the comparison between FTM and PTM implies
that only exploiting the spatial redundancy cannot consistently
improve the signal quality, i.e., in talking videos that contain
local nonrigid mouth/lips motions, PTM increases the noise
sensitivity in local pixel-based rPPG sensors and thus exhibits
more quantization errors (even 2.4 dB less than FTM). This
problem is solved in PTC that incorporates an outliers pruning
procedure to remove the motion-distorted sensors.
In videos with vigorous motions (e.g., rotation and mixed
motion), PTC including its substeps (FTM and PTM) shows
superior performance against FDM in Fig. 9(a). The failure
of FDM in these two types of motion (−0.15 and 0.93 dB,
respectively) is mainly caused by the face detector, which cannot
locate the side-view faces in some frames. Another significant
challenge is from the large motion-induced color distortions
on the skin surface, i.e., both the magnitude and orientation of
skin-reflected light are dramatically changed during the rotation.
In such a case, PTC achieves the largest improvement over
FDM compared to other motion types (6.79 and 4.43 dB more,
respectively), which indicates that the proposed method can
better deal with the subject motions in challenging use cases.
Comparing the subject variability (standard deviation) between
the videos with and without motion, FDM, FTM, and PTM
increase around ±2.0 dB while PTC increases around ±0.7 dB,
which is fairly stable.
Fig. 10 shows the instantaneous pulse rate and Bland–Altman
plots of six motion types in Skin-category II male. In videos with
regular motions (e.g., stationary, translation, scaling, and talking), all rPPG methods are able to precisely capture the instan-

taneous abrupt changes of pulse rate and have good alignments
with corresponding reference signal. In videos with vigorous
motions (e.g., rotation and mixed motion), PTC particularly
outperforms other rPPG methods, i.e., the agreement of PTC
achieves 98% and 89%, respectively.
3) Different Skin Categories
In addition to the motion robustness comparison, the skintone invariance of rPPG methods is analyzed. Fig. 9(b) shows
that FDM, FTM, and PTM have difficulties in dealing with the
darker skin type (Skin-category III) as compared to the brighter
skin types [(Skin-category I and II] (around 3 dB less). The
performance degradation is caused by using the skinchromaticity-based method for pulse extraction: the higher
melanin contents in darker skin absorbs part of the diffuse
light reflections that carry the pulse signal, whereas the specular reflection is not reduced [3]. In contrast, PTC obtains a
relatively consistent performance across the different skin categories, since the skin pixels with specular reflections caused by
either the subject motion or skin absorption are all pruned as outliers. Besides, its temporal filtering suppresses the out-of-band
frequency noise and strengthens the pulse frequency. Fig. 12
shows the instantaneous pulse rate and the Bland–Altman agreement of the stationary male subjects in three skin categories. It
is apparent that only PTC shows consistently high agreements
with the reference signal.
4) ANOVA With Post-Hoc Comparison
To analyze the significance of differences in motion and skintone robustness between methods, the SNRa values in Table I are
grouped into five categories: the skin categories (I, II, and III),
the stationary category, and the overall category. In each of the
skin categories, the significance of differences between methods
on motion robustness is measured (results on moving videos). In

424

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 2, FEBRUARY 2015

in terms of the skin-tone robustness. Fig. 11 shows that on
average PTC does score best.
Also in the overall category, the differences between methods in the complete benchmark dataset are significant 0.0003
(< 0.05). Fig. 11 shows that PTC again yields the largest improvement over the baseline method (FDM) and has a performance that is similar to the contact-based method (CBS), i.e.,
PTC and CBS have significant pairwise differences with FDM
in the posthoc comparison.
It can be concluded that the proposed method, PTC, leads to
significantly improved motion robustness, while for stationary
videos the skin-tone robustness on average is the best though
the differences with other methods are not significant.
Fig. 12. Example of instantaneous pulse-rate plot (first row) and Bland–
Altman plot (second row) for stationary male subjects in three skin categories.
The facial appearance of three male subjects are shown in Fig. 3.

TABLE III
STATISTICS OBTAINED BY ANOVA IN FIVE CATEGORIES
Categories
Skin-category I
Skin-category II
Skin-category III
Stationary
Overall

MS-within

MS-between

F-ratio

p-value

2.42
5.89
3.07
0.86
5.90

12.62
3.71
28.74
0.88
35.23

5.2
0.63
9.36
1.03
5.97

0.0049
0.6466
0.0002
0.4398
0.0003

Bold entries indicate the category with p-value larger than 0.05.

the stationary category, the significance of differences between
methods on skin-tone robustness is investigated. Finally, in the
overall category, the overall significance of difference between
methods is shown using the entire dataset. This paper applied the
balanced one-way ANOVA on these five categories, and posthoc
comparison using Tukeys honestly significant difference criterion. In each category, a common significance threshold (p-value
< 0.05) is used. Fig. 11 shows the results, while Table III lists
the main ANOVA statistics.
In skin-categories I and III, the compared methods have significant differences (both are < 0.05). In skin-category II, the
differences are not significant (p-value = 0.6466). This high
p-value reflects a limited variation between groups (3.71) as
compared to that within groups (5.89). Indeed, the subjects in
this group caused rather large motion variations as compared
to subjects in the other groups. This could happen as limited
instructions for the precise movements to be made were given
to the subjects. In Fig. 11, the ANOVA plots show that PTC
achieves the best performance in all three skin categories with
respect to the subject motion. The post-hoc plots show that
PTC is the only method that is significantly different from the
baseline method (FDM) for skin-categories I and III. CBS, the
contact-based reference method, only has significant difference
with FDM in skin-category I. FTM and PTM have no significant
pairwise differences with FDM in any skin-category, i.e., their
possible motion-robustness improvement is very limited.
In the stationary-category, the p-value is 0.4398 (> 0.05)
and, thus, the differences between methods are not significant

VI. CONCLUSION
This study introduces a motion robust rPPG method that enables the remote detection of a pulse signal from subjects using
an RGB camera. This paper integrates the latest methods in
motion estimation and pulse extraction, and proposes novel algorithms to create and optimize pixel-based rPPG sensors in
the spatial and temporal domain for robust pulse measurement.
Experimental results on 36 challenging benchmark video sequences show that the proposed method significantly improves
the SNR of the state-of-the-art rPPG method from 3.34 dB
(±2.91) to 6.76 dB (±1.56), and improves the Bland–Altman
agreement (±1.96σ) with instantaneous reference pulse rate
from 55% to 80% correct, i.e., a performance that is very close
to the contact-based sensor. ANOVA with post-hoc comparison
shows that the proposed method, PTC, leads to significantly
improved motion robustness, while on stationary videos with
skin-tone variance it is also the best on average though the difference with the baseline method is not significant.
ACKNOWLEDGMENT
The authors would like to thank I. Kirenko, E. Bresch, J.
Westerink, B. den Brinker, W. Verkruijsse, and V. Jeanne at
Philips Research for their support. Also, we are grateful for the
help of volunteers from Eindhoven University of Technology in
creating the benchmark video dataset.
REFERENCES
[1] M.-Z. Poh, D. McDuff, and R. Picard, “Advancements in noncontact, multiparameter physiological measurements using a webcam,” IEEE Trans.
Biomed. Eng., vol. 58, no. 1, pp. 7–11, Jan. 2011.
[2] M. Lewandowska, J. Ruminski, T. Kocejko, and J. Nowak, “Measuring
pulse rate with a webcam—A non-contact method for evaluating cardiac
activity,” in Proc. Federated Conf. Comput. Sci. Inform. Syst.., Sep. 2011,
pp. 405–410.
[3] G. de Haan and V. Jeanne, “Robust pulse rate from chrominance-based
RPPG,” IEEE Trans. Biomed. Eng., vol. 60, no. 10, pp. 2878–2886, Oct.
2013.
[4] G. Balakrishnan, F. Durand, and J. Guttag, “Detecting pulse from head
motions in video,” in Proc. IEEE Conf. Comput. Vision Pattern Recog.,
Jun. 2013, pp. 3430–3437.
[5] W. Verkruysse, L. O. Svaasand, and J. S. Nelson, “Remote plethysmographic imaging using ambient light,” Opt. Exp., vol. 16, no. 26, pp.
21 434–21 445, Dec. 2008.
[6] H.-Y. Wu, M. Rubinstein, E. Shih, J. Guttag, F. Durand, and W. Freeman,
“Eulerian video magnification for revealing subtle changes in the world,”
ACM Trans. Graph., vol. 31, no. 4, pp. 65:1–65:8, Jul. 2012.

WANG et al.: EXPLOITING SPATIAL REDUNDANCY OF IMAGE SENSOR FOR MOTION ROBUST rPPG

[7] P. Viola and M. Jones, “Rapid object detection using a boosted cascade
of simple features,” in Proc. IEEE Comput. Soc. Conf. Comput. Vision
Pattern Recog.,, Dec. 2001, vol. 1, pp. I–511–I–518.
[8] Y. Wu, J. Lim, and M.-H. Yang, “Online object tracking: A benchmark,”
in Proc. IEEE Conf. Comput. Vision Pattern Recog., Jun. 2013, pp. 2411–
2418.
[9] J. Henriques, R. Caseiro, P. Martins, and J. Batista, “Exploiting the circulant structure of tracking-by-detection with kernels,” in Proc. Eur. Conf.
Comput. Vision, , Oct. 2012, vol. 7575, pp. 702–715.
[10] G. Farnebäck, “Two-frame motion estimation based on polynomial expansion,” in Image Analysis, New York, NY, USA: Springer, 2003, vol.
2749, pp. 363–370.
[11] G. Bradski, “The OpenCV library,” Dr. Dobb’s Journal of Software Tools,
vol. 25, no. 11, pp, 120, 122–125, 2000.
[12] Z. Kalal, K. Mikolajczyk, and J. Matas, “Forward-backward error: Automatic detection of tracking failures,” in Proc. 20th Int. Conf. Pattern
Recog.., Aug. 2010, pp. 2756–2759.
[13] Y. Chen, X. S. Zhou, and T. Huang, “One-class SVM for learning in image
retrieval,” in Proc. Image Process.Int. Conf., Oct. 2001, vol. 1, pp. 34–37.
[14] N. A. Ibraheem, R. Z. Khan, and M. M. Hasan, “Comparative study of
skin color based segmentation techniques,” Int. J. Appl. Inform. Syst., vol.
5, no. 10, pp. 24–34, Aug. 2013.
[15] T. Fitzpatrick, “The validity and practicality of sun-reactive skin types I
through VI,” Archives Dermatology, vol. 124, no. 6, pp. 869–871, 1988.

Wenjin Wang received the B.Sc. degree from Northeastern University, Shenyang, China, in 2011 and the
M.Sc. degree from the University of Amsterdam,
Amsterdam, The Netherlands, in 2013. He is currently working toward the Ph.D. degree at Eindhoven
University of Technology, Eindhoven, The Netherlands, and cooperates with the Vital Signs Camera
project, Philips Research Eindhoven.
He works on computer vision and related
problems.

425

Sander Stuijk received the M.Sc. (Hons.) degree in
2002 and the Ph.D. degree from the Eindhoven University of Technology, Eindhoven, The Netherlands,
in 2007.
He is currently an Assistant Professor in the Department of Electrical Engineering, Eindhoven University of Technology. He is also a Visiting Researcher at Philips Research Eindhoven working on
biosignal processing algorithms and their embedded
implementations. His research interests include modeling methods and mapping techniques for the design
and synthesis of predictable systems with a particular interest into biosignals.

Gerard de Haan received the B.Sc., M.Sc., and
Ph.D. degrees from the Delft University of Technology, Delft, The Netherlands, in 1977, 1979, and 1992,
respectively.
He joined Philips Research in 1979 to lead
research projects in the area of video processing/analysis. From 1988 till 2007, he has additionally taught postacademic courses for the Philips Centre for Technical Training at various locations in
Europe, Asia, and the U.S. In 2000, he was appointed
“Fellow” in the Video Processing and Analysis group
of Philips Research Eindhoven, and “Full-Professor” at Eindhoven University
of Technology. He has a particular interest in algorithms for motion estimation,
video format conversion, image sequence analysis, and computer vision. His
work in these areas has resulted in three books, two book chapters, 170 scientific papers, and more than 130 patent applications, and various commercially
available ICs.
Dr. de Haan received five Best Paper Awards, the Gilles Holst Award, the
IEEE Chester Sall Award, bronze, silver, and gold patent medals, while his work
on motion received the EISA European Video Innovation Award, and the Wall
Street Journal Business Innovation Award. He serves in the program committees
of various international conferences on image/video processing and analysis,
and has been a Guest Editor for special issues of Elsevier, IEEE, and Springer.

