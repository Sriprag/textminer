3124

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 11, NOVEMBER 2013

Visual Feature Extraction From Voxel-Weighted
Averaging of Stimulus Images in 2 fMRI Studies
Corey B. Hart∗ and William J. Rose

Abstract—Multiple studies have provided evidence for distributed object representation in the brain, with several recent
experiments leveraging basis function estimates for partial image reconstruction from fMRI data. Using a novel combination
of statistical decomposition, generalized linear models, and stimulus averaging on previously examined image sets and Bayesian
regression of recorded fMRI activity during presentation of these
data sets, we identify a subset of relevant voxels that appear to
code for covarying object features. Using a technique we term
“voxel-weighted averaging,” we isolate image filters that these voxels appear to implement. The results, though very cursory, appear
to have significant implications for hierarchical and deep-learningtype approaches toward the understanding of neural coding and
representation.
Index Terms—Bayesian estimation, component analysis, fMRI,
generalized linear models, imaging, voxel.

I. INTRODUCTION
HE prospect of efficiently decoding neural activity has
been a longstanding goal of experimental systems neuroscience. A variety of techniques have been proposed for decoding activity, with many of them very much dependent on the form
of the neural data encoded. For example, spike triggered averaging and spike triggered covariance have been used to discover
the stimuli that elicit spiking in blowfly H1 retinal neurons [1].
For decoding of human neural activity, recording spike trains is
generally not feasible and more global measures of brain state,
such as EEG, MEG, or fMRI imaging must be employed. There
have been many studies that examine the sensitivity of EEG
potentials with characteristic lag to stimuli [2], [3], and many
attempts at implementing linear classification of EEG for the
decoding of unknown stimuli have been attempted [4], [5].
Functional MRI analysis of blood oxygen level dependent
(BOLD) activations has resulted in the ability to assess stimulus evoked activity on an astonishingly fine (millimeter) spatial
scale. The density of information in this approach has resulted
in a wealth of analytical studies and attempts at neural decoding.
Early studies examining activation amplitudes of implanted
EEG electrodes in human extrastriate cortex found that neurons in this area responded differentially on presentation of

T

Manuscript received February 20, 2013; revised May 19, 2013; accepted June
6, 2013. Date of publication June 13, 2013; date of current version October 16,
2013. Asterisk indicates corresponding author.
∗ C. B. Hart is with the Advanced Technology and Innovations, Lockheed
Martin, King of Prussia, PA 19406 USA (e-mail: corey.hart@lmco.com).
W. J. Rose is with the Lockheed Martin, Information Systems and Global
Solutions, King of Prussia, PA 19406 USA (e-mail: william.j.rose@lmco.com).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2268538

images [6], [7], and this work was later confirmed and extended
by studies done using fMRI techniques [8]. This sort of gross
decoding of activation relying on fMRI amplitudes measured
over a baseline in particular regions has progressed into statistically much more advanced territory. One of the first attempts
at statistically sophisticated fMRI decoding, multivariate pattern analysis (MVPA), relied on the fact that different stimuli
will induce neural responses with spatially distinct patterns of
activity and that classifier and machine learning algorithms can
operate to identify optimal decision boundaries for appropriate
classification of stimulus categories. Kamitan and Tong [9] later
used an MVPA-like algorithm, coupled with a winner-take-all
classifier voting algorithm to detect the orientation of edges in
a subject’s visual attention field.
Later studies [10], [11] have transformed sets of stimulus images and fMRI activity into a common vector space, where fMRI
activity cannot only be used to identify which stimulus images
have been presented, but can reconstruct those images based on
putative features encoded by particular voxels. Most recently,
sophisticated machine learning techniques that decompose images from video clips into a database of edges and shapes and
movement trajectories have been used to combine and average
new video clips to yield an approximate reconstruction of the
stimulus movies [12].
With the increasing complexity of analytics in this field, we
wanted to test the hypothesis that comparatively simple decomposition and data reduction techniques could be used to identify
feature sensitive filters from neural data, filters that could ultimately be used for optimal stimulus reconstruction. Training
and test data were taken from two sources: 2-D geometric shapes
from the data set used in Miyawaki et al. [10] and more complex
images from the seminal 2001 Haxby study [13].
II. METHODS
The overall fitting procedure was as illustrated in Fig. 1. We
used a Bayesian regression procedure to select voxels most relevant for the representation of image features and then used the
activity of those voxels in a quasi-reverse correlation or “voxelweighted averaging” procedure reminiscent of the popular spike
triggered averaging method used in the analysis of neural spike
train activity. All code was implemented in the MATLAB programming environment.
A. Initial Voxel Selection
Both data sets were characterized by a large number of voxels. The first data set we examined was that used by Miyawaki
et al. [10]. We obtained data sets from three subjects, containing

0018-9294 © 2013 IEEE

HART AND ROSE: VISUAL FEATURE EXTRACTION FROM VOXEL-WEIGHTED AVERAGING OF STIMULUS IMAGES IN 2 FMRI STUDIES

3125

feature vectors containing strongly covarying features, while simultaneously minimizing covariance between bases. The principal and independent component matrices were preserved for
the following selective regression procedure. The same procedure was used for the analysis of the Haxby et al. [13] data
set.
C. Bayesian Regression for Selection of Voxels
Fig. 1. Overview of work plan: fMRI data are regressed against projection
of training images onto their principal components for all time steps. Relevant
voxels in regression are chosen using a Bayesian inferencing approach ( [15]).
Time varying activity from relevant voxels used to weight images presented at
each time step, which are then summed to obtain a “most likely” filter describing
that voxels sensitivity to presented stimulus features.

stimulus presentation locked voxel activity from 18,063 1 mm
×1 mm × 1 mm voxels recorded across the entirety of the occipital lobe using a 3.0 Tesla Siemens MAGNETOM Allegra
unit. The mean activation of each voxel was subtracted off that
voxel’s activity, and this shifted activity was then normalized
by the standard deviation of that voxel’s activity over the entire
recording interval. The set contained different time samples,
and in order to simplify the ensuing model fitting, we selected
80 voxels using two criteria: either in terms of the information
entropy in their average activity computed at each trial, or simply by a random sampling of voxels from the data set. In all
reconstruction metrics computed later, randomly sampled voxels yielded much better than entropy ranked voxels and thus we
emphasize those results in the reported data.
The second data set we analyzed was derived from Haxby
et al. [13]. This data set consisted of a set of 40 images broken
into eight distinct image classes (faces, houses, cats, bottles,
scissors, shoes, chairs, and random noise patterns) and the activity from 40 × 64 × 64 voxels, at a voxel size 3.5 × 3.75
× 3.75 mm from all cortical regions. We used this spatially
broad sampling of voxels to validate the hypothesis that our
voxel weighted averaging approach would tend to identify relevant voxels in appropriate object representation areas (ventral
temporal cortex) given a large number of other localization options. Stimuli in this data set were presented in blocks of five
presentations, and all stimuli in a block consisted of the same
image category type. Individual order or presentation within
each block was random and was not preserved by the original
study authors. This created a significant “label noise” problem
for later image reconstruction training. As this set contained
many more voxels than the Miyawaki set, we selected 343 voxels at random from the data set for fitting as well 343 voxels
ranked from largest-to-smallest information entropy content.
B. Component Analysis of Images
We wished to identify voxels sensitive to features of image
presentations that tended to exhibit significant covariance. In
order to isolate these voxels, we performed a Bayesian selective
regression procedure onto images from the Miyawaki data set.
These images were subjected to either a principal components
analysis (PCA) or an independent component analysis (ICA)
via Sejnowski’s ICA implementation [14] to identify descriptive

To identify voxels for the analysis, we performed a Bayesian
regression as outlined by Kuo and Mallick [15]. Briefly put,
this approach uses an iterative Gibbs sampling procedure to fit
principal component loadings for each component onto each
voxel at each time step. For this procedure, we fit the following
equation:


β m j Vj (t)
yim
(1)
Ai (t) =
where Ai represents the projection of the ith component onto the
stimulus image presented at time t, Vj represents the normalized
activity of voxel j at time, γij is a binary selection vector (under
a simple binomial prior, see [15] for details), and βij is a matrix
of regression coefficients. Between 1000 and 10,000 Gibbs samples were performed, but because of the large space of variables
fit, only infrequently identical γ matrices were produced. As we
were merely attempting to obtain a smaller set of voxels, and
not obtain the globally smallest set, we instead chose to rank
the γ generated by the Gibbs sampling algorithm in order of its
associated multivariate normal log likelihood value and chose
the γ with the largest likelihood.
Ten iterations of model fitting were performed, and for each
component, voxels that were selected by the γ vector for at least
seven of the ten voxels were chosen as highly relevant voxels.
The time varying activity was preserved in order to perform the
voxel-weighted averaging that follows. Loadings for each component were fit to voxels independently over all time steps, under
the assumption of no interaction between separate components.
D. Data Shuffling
To assess the significance of correlations between Kuo–
Mallick derived reconstructions and the original matched figures, we compared these correlations with correlations between
reconstructions and non-matched images randomly selected
from the entire data set. For each reconstruction, 100 shuffles
of the data set were performed, and correlations were made at
each draw from this distribution.
E. Voxel-Weighted Averaging
Our ultimate goal was to identify a set of “filters” that could
be applied to time-varying fMRI activity in order to optimally
approximate the images presented to the subject(s) at each time
step and to identify the image features to which given voxels
are sensitive. To do this, we developed an approach inspired by
the common “spike triggered averaging” technique. Assuming
that particular feature-sensitive neurons contained in a given
voxel fire maximally when their best-matched stimulus feature
is presented, a simple encoding of a stimulus image is given by

3126

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 11, NOVEMBER 2013

Fig. 2. (a) Regressions selected as relevant between 60% and 80% of voxels.
(b) Highly significant correlation of test images with regression-based reconstructions (corr. coef.> 0.8, p < 0.001, Kolmogorov–Smirnov test).

the following:
S=



i ϕi

∗ ri (t)

(2)

where S is the stimulus image presented, ϕ the average filter response of the group of neurons in the voxel under consideration,
and ri (t) is the variance-normalized average activity in a voxel
at time step t. It is possible to isolate each filter by multiplying
the stimulus S by rate ri (t) and integrating over time



ϕj = rj (t) Sdt = rj (t)
(3)
i ϕi ∗ ri (t) dt.
III. RESULTS
We chose 80 voxels, either selected at random, or ranked in
terms of information entropy and fit the principal component
(at each time step) to voxel activity (at each time step) using
the Kuo–Mallick procedure referenced in the methods [15]. We
performed ten distinct fits in order to assess the stability of the
voxel selection. In doing this, we found that between 60% and
80% of randomly chosen voxels were selected by the Kuo–
Mallick algorithm as relevant [see Fig. 2(a)]. Using the Kuo–
Mallick procedure and these voxels, we found that we were able
to reconstruct images with correlations to the original image
greater than 0.8 for all trials [see Fig. 2(b)]. Interestingly, fewer
voxels were selected by their information entropy as relevant,
between 40% and 60%, although reconstructions were poorer
and were significantly less than those shown in Fig. 2(b), as

Fig. 3. (a) Difference between correlations of reconstruction-to-matched and
reconstruction-to-shuffled target images significantly >0 (p < 0.0001). (b)
Over ten trials voxels that are frequently (>60% of the time) selected as relevant
in PCA runs show a weak but significant likelihood to be selected as relevant
in ICA runs. (c) Distribution of ten most common voxel locations for one run
(L = left, R = right, x = eccentricity coordinate). Distributions showed a mean
of 60% overlap of locations across subjects.

more than quarter of relevant voxel derived correlations were
less than 0.8.
Comparing the fits against shuffled data (see methods), we
found that the correlation between the test image and the reconstructed image was greater than the shuffled correlations for
95% of shuffles [see Fig. 3(a)].
We then compared voxels selected as relevant in both PCA
and ICA decomposition conditions over ten trials and found that
voxels that were selected as relevant in PCA trials exhibited a
significant tendency for selection in more than half of the ten
trials in ICA trials [see Fig. 3(b)]. Voxels that were selected in
less than half of PCA trials showed no relation to selection in

HART AND ROSE: VISUAL FEATURE EXTRACTION FROM VOXEL-WEIGHTED AVERAGING OF STIMULUS IMAGES IN 2 FMRI STUDIES

Fig. 4. (a) ICA and PCA identify similar filters for regression—identified
highly relevant voxels. Top location: LV1 × 6.1, Bottom location: LV1 × 3.9,
LV3. (b) Mean correlation coefficients between matched stimuli and reconstructions (mean estimated by jackknife procedure) >95% CI computed on mean
correlations between shuffled stimulus and reconstructed images (dotted line).

Finally, we wanted to repeat this analysis for a set of more
complex images, and for this we used the Haxby data set described earlier. However, there is considerable “label noise” in
this data set (one generally only knows that an image of a cat is
being presented from among some set of possible cat images).
The specific image is difficult to match. To address this issue,
we simply chose to use principal components from the first image in each category type as regressors against voxel activity so
that at least once during each presentation of a group there is
an actual match to the correct image. Taking the pseudoinverse
of the estimates of A (1) made from the voxels and multiplying
the first 12 principal components through this matrix, visually
obvious reconstructions of the fitted stimulus images could be
readily made [see data from single subject in Fig. 5(a)].
For regression fits to the five largest PCs (or ICs), we identified those voxels that occurred in seven of the ten trial fits
as the most informative voxels and retained these for stimulus
averaging as above. Generally, about one out of three voxels
were selected. We then performed voxel-weighted averaging on
the images based on the method outlined previously, Due to
the complication posed by label noise, rather than weighting a
particular image by the corresponding voxel activation at that
stimulus presentation time, all images of that class are averaged
and then the corresponding average is weighted by the voxel
activation. As can be seen from the following equation:


ICA trials. Finally, we examined the voxel locations in V1, V2,
and V3 that the algorithm identified as significant [see Fig. 3(c)].
We show an example from one fit and note that largest group
of selected voxels were located in various V1 regions. Ensuing
runs showed an average of 60% overlap with the pictured run in
terms of cortical area composition.
We observed that 22 voxels were identified as contributing
to a principal component’s activation on at least seven of the
ten model fitting runs. We identified these as “highly relevant
voxels” and retained them for a voxel-weighted averaging procedure.
Taking these 22 highly relevant voxels identified from the
PCA analysis, we then multiplied the stimulus image presented
at each time step by the fMRI activation at that same time
step, and averaged this quantity over all steps, according to
(equation 2). Mean activity that was more than two standard errors above zero was retained as significant and a filter consisting
of only “significant’ contributions was retained.
The result was surprising: clear “part representations”
emerged from this summing procedure [see Fig. 4(a)]. These
“parts” represented the sensitivity filters ϕ from (equation 2).
After each filter had been identified, we then multiplied all
filters by the corresponding time-varying voxel activity and
summed all filtered activity to reconstruct an approximation of
the original image set. Reconstructions were almost uniformly
excellent, with correlations all significantly larger (p < 0.0001)
than correlations between randomly scrambled reconstructions
and images [see Fig. 4(b)]. Repeating the analysis for ICA
derived filters, results were entirely analogous (see Fig. 4(a),
third column).

3127

ϕj =

rj (t)

M

Sm
dt
M
m =1

rj (t)

M

(ϕm + errorm )
ri (t) dt.
M
m =1


=

(4)

If the Sm is largely uncorrelated, then it is still possible to
identify a filter ϕm as the errorm /M term will be small enough
to neglect.
This procedure should result in a noisy, but potentially still
discernible feature selective filter. Running averages are shown
for 160 stimulus presentations in Fig. 5(b) and (c). We checked
convergence of images in Fig. 5(b) by comparing correlations
between the average image at each time step and the image after
864 stimulus presentations. At 160 presentations, the change in
the weighted average changed by less than 5% from that point
on, and we defined this as “convergence.” Its clear that after
many averages, the image features begin to converge to basic
facial features [see Fig. 5(b)]. As a check on this approach,
(and due to the fact that facial images may exhibit significant
correlation structure, violating the assumption implicit in (4)
we removed all face images from the voxel-weighted averages,
and then examined both the running sum of the activity [see
Fig. 5(c)]. Basic facial features are still clear in the running
average, even though there are no face images included in this
stimulus set. Note the strong left eye that emerges from this
average set, and the weaker (but still visible) collection of pixels
near the right eye of the image.
After the entire stimulus presentation set was averaged,
we retained only those pixels that were greater than the 50th
percentile in terms of intensity, to better visualize contrasts in

3128

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 11, NOVEMBER 2013

Fig. 5. (a) (Top) Stimulus image presented to subject. (Bottom) reconstruction of stimulus image from FMRI activations. (b) (Left) Running voxel-weighted
average for one characteristic high-relevance voxel from the Haxby data set. (Right) Approximate average face corresponding to voxel-weighted average derived
filter. Note the pronounced activity in the eye region (c) (Left) Running average for this voxel-weighted average with all stimulus faces removed from running
average. (Right) Even without any faces included, as stimuli, basic face-like features align with those in (b).

the images. The resulting filters were very sensitive to particular
image features, such as pixels near the left eye [see Fig. 6(a),
upper left] or right eye (Fig. 6(a) upper right), and sometimes
appeared inhibitory or suppressive of certain features (lower
left). These face-like filters do not seem to be an artifact of
image averaging as averaging 1) weighted images from nonrelevant voxels, 2) all unweighted images, and 3) all variance
normalized and thresholded (by median pixel value) images did
not result in similar features filters [(see Fig. 6(c)]. Furthermore,
face-feature sensitive VWA filters do not appear to be artifacts
of activity in particular subjects [see Fig. 6(d)] as similar facialfeature sensitive filters were identified in the second subject as
well.
Finally, we attempted to localize the point of greatest density
of highly relevant voxels selected in the preceding analysis by
counting relevant voxels enclosed in a sphere with a radius of
20% of the small axis of the workspace. The sphere was allowed to move through the workspace until it enclosed a region
containing the largest density of relevant voxels in the studied
subject. We repeated this optimization for ten steps and noted
that the location of voxels showed low variability. We therefore
chose the median coordinates from this distribution as the most
likely location for relevant voxels. We repeated this procedure
for radii of 5% and 10% of the small axis, but the optimal location did not change significantly. Interestingly, in both subjects,
most (> 60%) of highly relevant voxels were observed to occur
in regions that were anatomically between the occipital lobe
and the ventral temporal lobe (see Fig. 7), although significant
voxels could be found in other regions as well with much less
regularity.
IV. DISCUSSION
We here tested a protocol designed to identify relevant
feature-selective voxels from a large body of potential vox-

els. Using a combination of Bayesian and systems identification
techniques we found that we could indeed identify voxels that
correspond to particular parts of objects.
We first found that we could sort voxels based only on their
information bearing capacity and using only 80 (out of 18,063)
of these voxels, we could accurately reconstruct the images presented to subjects by regressing cortical activity expressed in
the fMRI against principal or independent component factor
loadings. Furthermore, using a binomial prior as a voxel selector, these small number of voxels for reconstruction could
be reduced further, to somewhere approximately between 40
and 60 voxels. These values are comparable with the number
of voxels originally employed in the Miyawaki study, where
for more complete image reconstruction [10] somewhere between 30 and 50 voxels were required by that more complex
algorithm. It is conceivable that providing more sophisticated
priors to the Kuo–Mallick algorithm could be used to identify
feature-relevant voxels with even greater selectivity.
By averaging away differences in images weighted by voxel
activation, it is fairly simple to find the most likely average
filter that best represents the feature sensitivity of the patch of
neurons contained in that voxel. In the Miyawaki data set, the
images presented to the subject (and reconstructed) are not at
all naturalistic. As such, the broad relevance of some of the
feature filters identified [e.g., Fig. 3(a)] is unclear. These filters
may simply correspond to spatial patterns of activity, or may
be sensitive to more complex symmetries and their distribution
across the viewed workspace. The small stimulus space, and
the relatively large number of relevant voxels identified in our
procedure, though, made for a nearly complete set of filters, such
that reconstructions of stimuli using only linear combinations
of the activity of a very small number of voxel-weighted filters
were generally capable of reconstituting more than > 80% of
the variance in each image.

HART AND ROSE: VISUAL FEATURE EXTRACTION FROM VOXEL-WEIGHTED AVERAGING OF STIMULUS IMAGES IN 2 FMRI STUDIES

3129

Fig. 6. (a) Thresholded voxel-weighted average filter from a highly relevant voxel (left, as identified from the Bayesian regression procedure outlined in the
methods) appear sensitive to particular “parts” of stimulus faces (subject 1). (b) Distribution of correlations between VWAs and averaged faces flatter than
distribution of correlations with random images drawn from Haxby set. (c) VWA from nonrelevant voxels, average over all images, and images averaged
postnormalization and thresholded all show little in the way of obvious “face-like” features. (d) Face-sensitive VWAs occur in second examined subject as well.

The Haxby data set posed more challenges, but simultaneously yielded more interesting results. Although the class of
image (“face,” “shoe,” etc.) presented to the subject in the data
was annotated, there was no data regarding which image within
a given class was presented to the subject (no annotation of more
specific image tagging is currently available from the authors—
private communication). However, averaging over all images at
each presentation appeared to be able to sidestep this problem,
as uncorrelated features from the “wrong” images (i.e., those
images of the correct class that were not actually presented during an epoch) could be considered nothing worse than a noise
component in the filter identification process. The result of this
analysis was impressive, to say the least, as for more than half
of the voxels identified we found that recognizable facial features were reconstructed (eyes, facial boundaries). This effect
does not seem to be simply due to the fact that faces in this
data set are well aligned and could bias averages. Using these
same (potentially) feature-sensitive voxels, we removed all trials where faces were presented to the subject and averaged only

other (nonface) voxel-weighted images. Somewhat less sharp,
but still recognizable features were recovered that aligned with
the features extracted from the face-included averages. Given
the fact that the bulk of selected voxels were found in or near
ventral temporal cortex and the putative face-sensitivity of populations of cells in that region, it is perhaps unsurprising that so
many voxels our algorithm selected show sensitivity to object
features that in some way resemble facial features.
These results appear to comport well with the theory of hierarchical representation of objects put forward by advocates
of “deep learning” [16]. Repeated application of simple filters
implementing divisive normalization of input features has been
shown to result in representations of images, speech, and other
experiential domains that preserve various aspects of part-whole
and other hierarchical relationships. Advocates of deep learning
argue that the approach is neuroinspired and that it may actually
represent an implementation of a fundamental organizational
principle of the nervous system. As divisive normalization has
been argued to be a near-universal cortical phenomenon [17]

3130

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 11, NOVEMBER 2013

REFERENCES

Fig. 7. Face-VWA related highly relevant voxels largely represented midway
between ventral temporal cortex and occipital cortex. Greatest density of voxels
from BOLD maps in Haxby data set selected as significant with Kuo–Mallick
fitting are enclosed in sphere represented by 2-D projection into sagittal and
frontal views.

implemented on neurons within cortical columns [18], or between them, it seems plausible to suggest that neurons may exist
that encode stimulus features using a hierarchical model similar
to those advocated in the deep learning literature. Indeed, the
scale of voxels in the Miyawaki data set (1 mm × 1 mm × 1 mm)
is on precisely the correct order for this to be the case. Furthermore, the observation that some neural populations within the
face fusiform area are sensitive to entire faces while neurons
earlier in the processing stream are responsible for encoding
particular pieces of faces [19] would seem to argue strongly
for this hypothesis. Using a technique similar to that outlined
in this paper could help characterize the part representation at
each stage of this putative network and compare those representations against those predicted by a hierarchically implemented
divisive-normalization-based deep learning algorithm.
V. CONCLUSION
Many of the attempts to identify and “decode” neural activity as represented in the fMRI have relied on models that are
either difficult to interpret, including machine learning and simple classifiers coupled with “boosting” procedures [20]. Other
approaches deploy techniques similar to canonical correlation
analysis [21] or the related approach of functional hyperalignment [22], which attempt to bind particular stimulus features
to patterns of fMRI activation by projecting them both into a
common representational space. The filters in our approach are
conceptually much simpler: they merely correspond to the features of an image that tend to cooccur with significant excursions
of the recorded fMRI activity. They are computationally easy
to find, and require only basic component-based decomposition
and regression methods to identify relevant voxels, and then
a simple weighting of stimulus images over the course of the
experiment with those voxel activations.
ACKNOWLEDGMENT
The authors would like to thank J. Haxby as well as
Y. Miyawaki et al. for providing access to their data for
this project. Data from Haxby et al. (2001) is available
at http://dev.pymvpa.org/datadb/haxby2001.html. Data from
Miyawaki et al. (2008) is available at http://www.cns.atr.jp/dni/
en/downloads/fmri-data-set-for-visual-image-reconstruction/.

[1] R. de R. van Steveninck and W. Bialek, “Real-time performance of a
movement-sensitive neuron in the blowfly visual system: Coding and
information transfer in short spike sequences,” Proc. R. Soc. Lond. B Biol.
Sci., vol. 234, pp. 379–414, 1988.
[2] M. Kutas and S. A. Hillyard, “Reading senseless sentences: Brain potentials reflect semantic incongruity,” Science, vol. 207, pp. 203–208, 1980.
[3] M. Kutas and K. D. Federmeier, “Electrophysiology reveals semantic
memory use in language comprehension,” Trends Cognitive Sci., vol. 4,
no. 12, pp. 463–470, 2000.
[4] F. Babiloni, F. Cincotti, L. Lazzarini, J. Millán, J. Mouriño, M. Varsta,
J. Heikkonen, L. Bianchi, and M. G. Marciani, “Linear classification of
low-resolution EEG patterns produced by imagined hand movements,”
IEEE Trans. Rehabil. Eng., vol. 8, no. 2, pp. 186–188, Jun. 2000.
[5] A. Schlogl, “A new linear classification method for an eeg-based braincomputer interface,” Tech. Rep., IST, Austria, Jun. 2001. Available:
http://pub.ist.ac.at/∼schloegl/publications/MDBC_classifier.pdf.
[6] T. Allison, H. Ginter, G. McCarthy, A. Nobre, A. Puce, M. Luby, and
D. D. Spencer, “Face recognition in human extrastriate cortex,” J. Neurophysiol., vol. 71, pp. 821–825, 1994.
[7] T. Allison, G. McCarthy, A. Nobre, A. Puce, and A. Belger, “Human
extrastriate visual cortex and the perception of faces, words, numbers, and
colors,” Cerebral Cortex, vol. 5, pp. 544–554, 1994.
[8] N. Kanwisher, J. McDermott, and M. M. Chun, “The fusiform face area:
A module in human extrastriate cortex specialized for face perception,” J.
Neurosci., vol. 17, no. 11, pp. 4302–4311, Jun. 1, 1997.
[9] Y. Kamitani and F. Tong, “Decoding the visual and subjective contents of
the human brain,” Nature Neurosci., vol. 8, pp. 679–685, 2005.
[10] Y. Miyawaki, H. Uchida, O. Yamashita, M. Sato, Y. Morito, H. C. Tanabe,
N. Sadato, and Y. Kamitani, “Visual image reconstruction from human
brain activity using a com-bination of multiscale local image decoders,”
Neuron, vol. 60, pp. 915–929, 2008.
[11] Y. Fujiwara, Y. Miyawaki, and Y. Kamitani, “Estimating image bases for
visual image reconstruction from human brain activity,” Adv. Neural Inf.
Process. Syst., vol. 22, pp. 576–584, 2009.
[12] S. Nishimoto, T. Vu, T. Naselaris, Y. Benjamini, B. Yu, and J. Gallant,
“Reconstructing visual experiences from brain activity evoked by natural
movies,” Current Biol., vol. 21, no. 19, pp. 1641–1646.
[13] J. V. Haxby, M. I. Gobbini, M. L. Furey, A. Ishai, J. L. Schouten, and
P. Pietrini, “Distributed and overlapping representations of faces and objects in ventral temporal cortex,” Science,, vol. 293, pp. 2425–2430, 2001.
[14] S. Makeig, A. J. Bell, T.-P. Jung, and T. Sejnowski, “Independent component analysis of electroencephalographic data,” in Advances in Neural
Information Processing Systems, vol. 8, D. Touretzky, M. Mozer, and
M. Hasselmo, Eds., Cambridge, MA: MIT, 1996, pp. 145–151.
[15] L. Kuo and B. Mallick, “Variable selection for regression models,”
Sankhya: The Indian J. Statist. (Special Issue on Bayesian Analysis),
vol. 60, pp. 65–81, 1998.
[16] J. Ngiam, P. Koh, Z. Chen, S. Bhaskar, and A. Ng., “Sparse filtering,”
Neural Inf. Process. Syst., vol. 24, pp. 1125–1133, 2011.
[17] O. Schwartz and E. P. Simoncelli, “Natural signal statistics and sensory
gain control,,” Nature Neurosci., vol. 4, no. 8, pp. 819–825, 2001.
[18] H. Heuer and K. Britten, “Contrast dependence of response normalization
in area MT of the rhesus macaque,” J. Neurophysiol., vol. 88, no. 6,
pp. 3398–3408, Dec. 2002.
[19] D. Pitcher, V. Walsh, and B. Duchaine, “The role of the occipital face area
in the cortical face perception network,” Exp. Brain Res., vol. 209, no. 4,
pp. 481–493, 2011.
[20] M. Martı́nez-Ramón, V. Koltchinskii, G. Heileman, and S. Posse, “FMRI
pattern classification using neuroanatomically constrained boosting,” Neuroimage, vol. 31, no. 3, pp. 1129–1141, 2006.
[21] Y. Li, W. Wang, T. Adali, and V. D. Calhoun, “CCA for joint blind source
separation of multiple datasets with application to group fMRI analysis,”
in Proc. IEEE Int. Conf. Acoust., Speech, Signal Process., 2008, pp. 1837–
1840.
[22] J. V. Haxby, J. S. Guntupalli, A. C. Connolly, Y. O. Halchenko,
B. R. Conroy, M. I. Gobbini, M. Hanke, and P. J. Ramadge, “A common,
high-dimensional model of the representational space in human ventral
temporal cortex,” Neuron, vol. 72, pp. 404–416, 2011.

Authors’ photographs and biographies not available at the time of publication.

