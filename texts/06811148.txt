74

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 1, JANUARY 2015

Aerial Obstacle Detection With 3-D Mobile Devices
Juan Manuel Sáez, Francisco Escolano, and Miguel Angel Lozano

Abstract—In this paper, we present a novel approach for aerial
obstacle detection (e.g., branches or awnings) using a 3-D smartphone in the context of the visually impaired (VI) people assistance.
This kind of obstacles are especially challenging because they cannot be detected by the walking stick or the guide dog. The algorithm
captures the 3-D data of the scene through stereo vision. To our
knowledge, this is the first work that presents a technology able to
obtain real 3-D measures with smartphones in real time. The orientation sensors of the device (magnetometer and accelerometer)
are used to approximate the walking direction of the user, in order
to look for the obstacles only in such a direction. The obtained 3-D
data are compressed and then linearized for detecting the potential
obstacles. Potential obstacles are tracked in order to accumulate
enough evidence to alert the user only when a real obstacle is found.
In the experimental section, we show the results of the algorithm
in several situations using real data and helped by VI users.
Index Terms—Computer vision, mobile vision, visually impaired
(VI).

I. INTRODUCTION
A. Contextualization
LINDNESS is considered the major sensory disability (it
is estimated that 80% of the human sensorial information
is provided by sight), which determines to a large extent the
life of a person, the interaction with the environment and with
the society, and so on. A report of the WHO [1] indicates that
there were 285 million visually impaired (VI) people in the
world in 2010. These amount includes different scales of visual
impairment, where the severe is blindness (visual acuity below
5%). This group represents a 13.6% of the VI (39 million people
in the world).
One of the daily challenges faced by a blind person is the
autonomous movement. Regarding global orientation, there are
different GPS-based systems available in the market with specific cartographies and a voice interface that solve this problem
(e.g., the Kapten system [2]). As for the detection and obstacle
avoidance, classic systems such as the walking stick and the
guide dog are the most used.
Despite there exist technological advances in this field [3],
[4], they have not became daily use tools for this community.
This is due to the fact that the classic systems achieve their goals

B

Manuscript received December 29, 2013; revised April 12, 2014; accepted
April 30, 2014. Date of publication May 7, 2014; date of current version December 30, 2014. The work of J. M. Sáez and M. A. Lozano was supported
by the University of Alicante under research Grant GRE10-21. The work of
F. Escolano was supported by the Spanish Government through the project
TIN2012-32839.
The authors are with the Mobile Vision Research Laboratory, Department
of Computer Science and Artificial Intelligence, University of Alicante, Alicante 03690, Spain (e-mail: jmsaez@dccia.ua.es; sco@dccia.ua.es; malozano@
ua.es).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2014.2322392

Fig. 1.

Examples of aerial obstacles.

successfully and the new developments are bulky and uncomfortable, hindering the social integration of the user. In addition, these devices often send acoustic signals via earphones,
which deprives the blind user of his main information source:
the sound.
B. Mobility in Open Spaces
Large open spaces are a challenging context for the VI. They
are low-structured environments, such as parks, where VIs have
a limited number of structured references. In these environments, the traditional cane is also of limited help, and most of
the sensorial references are auditive (traffic on the left/right,
child playing, and people chatting).
In the literature, we found some notable examples of mobility
developments for the VI. Some of them refer to text reading in
the street (identifying street names and/or bus lines). There are
two main approaches to identify patches of the image containing text: learning based [5] and grouping based [6]. The latter
method has been recently extended for dealing with severe blur
[7]. Factor graphs are also applied to another important topic
in mobility: crosswalks protocols. In [8], for finding the best
alignment between the user and the crosswalks, audio feedback
is exploited to align the VI properly. In [9], 360◦ panoramas have
been incorporated and converted to an aerial view of the nearby
intersection for a later integration with google maps satellite
imaginary. Since, in general, GPS has a limited reliability because of the potential proximity of buildings, images become
the most reliable source of information. For instance, in [10],
vision is used for guiding VI to a target.
Another application for VIs in the context of mobility deals
with aerial obstacle avoidance. These obstacles have no projection on the floor (typically tree branches, awnings, or similar
elements). Some examples of this kind of obstacles are shown
in Fig. 1. In [11], our experience in stereo-based SLAM has provided a method for finding stereo maps with a stereo camera carried by a human user [12]. Having a short-term map computed

2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

SÁEZ et al.: AERIAL OBSTACLE DETECTION WITH 3-D MOBILE DEVICES

75

Fig. 2. Smartphones endowed with a 3-D camera. HTC EVO 3-D (left) and
LG Optimus 3-D (right).

on-the-fly, we are able to classify obstacles in front of the user as
aerial or not-aerial. In this paper, we propose to adapt this kind
of application to mobile devices (smartphones). In this regard,
the main limitation to overcome is that SLAM-based short-term
maps are too computationally demanding for a practical use,
especially when real-time constraints arise. The structure of the
environment could be also estimated through a monocular-based
approach (see, for example, [13] where a monocular SLAM system is integrated into a smartphone). These approaches are suitable because all the smartphones integrate a camera. However,
the range information extracted with this kind of algorithms is
up to scale. In other words, the relative scale of the data depends
on the nature of the environment. Then, the scale of the data
changes as the environment changes. In practice, this kind of
algorithms only works in limited space environments.
C. Goals
The main goal of our proposal is to develop a mobile application that acts as a walking stick or a guide dog complement.
It does not replace these elements, but it solves their main problem, that is, their inability to detect aerial obstacles. In the case
of walking sticks, this limitation is obvious. Dogs cannot be
trained to detect these obstacles, because they are not aware of
the height difference between them and their owners.
One of its main advantages is that the application is embedded
into a smartphone, obtaining a comfortable and discreet system
that favors the user social integration. Furthermore, the smartphone is also able to notify the presence of an obstacle by means
of acoustic signals (through the phone speaker, not earphones)
or vibrations. The latter option makes the system less noticeable
and does not deprive the user of the sense of hearing.
Our approach is based on distance measures taken from the
environment within a range of several meters. These measures
are obtained from a stereo pair of images. Hence, this software
requires a hardware capable of obtaining the scene in stereo.
Within these devices, we find the 3-D phones that are endowed
with a parallax-barrier glasses-free 3-D screen and a double back
camera (see Fig. 2). The purpose of these cameras is merely multimedia, but this equipment brings the opportunity of applying
stereo vision on mobile devices (see Fig. 3). From the observation of the pair of images provided by the double camera, the
scene can be partially reconstructed in 3-D. This reconstruction
includes the obstacles in front of the user and their distances.
In addition to the observation of the stereo pair of images,
the application uses data from different sensors such as magne-

Fig. 3. Reference and depth images (top left), and some views of the resulting
3-D scene (bottom right).

tometers and accelerometers. These sensors provide the global
orientation of the device, which is a key to solve the direction
in which the user is walking. With this information at hand, we
estimate the volume in which the obstacles should be detected.
This system has been developed for the Android platform,
because other platforms (like iOS) do not have currently available 3-D devices. Nevertheless, it could be ported to any other
platform whenever the required hardware is available.
II. AERIAL OBSTACLE DETECTION
The pipeline of our obstacle detection approach consists of
four phases: 1) capture a stereo pair of images; 2) obtain a set of
3-D points using a dense stereo algorithm; 3) build a histogram
of 3-D points in the direction in which the user is walking; and
4) check for obstacles in the histogram.
A. Scene Reconstruction
Let (ItL , ItR ) be the stereo pair of images provided by the
camera at instant t. Our goal is to obtain a set of 3-D points Pt =
{p1 , p2 , ..., pN }, where pi = (xi , yi , zi ) in metric coordinates
with respect to the optical center of ItL .
Mobile devices equipped with a 3-D camera provide a pair
of rectified and prealigned images, so that the epipolar line of
every pixel in the left image corresponds to the same row in the
right one. This fact allows us to apply a dense stereo algorithm
[14] to obtain a disparity map Dt from the pair of images.
The device also provides the extrinsic data from its stereo
camera: focal distance f (in pixels) and baseline B (in meters).
The 3-D scene can be reconstructed combining this information
with the disparity map Dt . For each pixel i in the disparity image
whose value is not unknown, a 3-D point pi = (xi , yi , zi ) can
be obtained as follows:
zi =

fB
,
Dt (ui , vi )

xi =

u i zi
,
f

yi =

vi z i
f

(1)

with ui and vi being the coordinates of the pixel in the 2-D
disparity image (with the origin of coordinates in the image
center).

76

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 1, JANUARY 2015

Fig. 4. We need to know the direction in which the user is walking to detect
the obstacles that may not match with the pointing direction of the camera.

B. Distance Histogram From 3-D Data
Let Vt be the direction in which the user is walking at instant t.
Only the obstacles found in this direction should be considered,
and therefore, 3-D data obtained in the previous step should be
filtered to remove side obstacles. Unfortunately, Vt is not always
the direction the smartphone is pointing to. On the one hand, we
have to consider that the device lays on the user’s chest, so that
it has a pitch angle αt that differs between users. On the other
hand, a swing movement is produced as the user walks. This
produces a yaw angle βt that is always changing. Therefore, the
direction Vt is not constant with respect to the device and should
be estimated.
Vector Vt can be estimated from (αt , βt ). The global orientation of the device (αtg , βtg , γtg ) (pitch, yaw, roll) is obtained
from the coordinate system defined by the accelerometer/gravity
 t , and the cross prod t , the magnetometer reading M
reading G




uct Gt × Mt (Gt and Mt are approximately orthogonal). The
value of αt can be determined directly by the device sensors.
To make Vt parallel to the floor, global pitch should be set to
αt ← αtg .
If the movement of the user were straight (e.g., the movement in a vehicle), αt would be enough to obtain Vt . However, the swinging movement of the user causes βtg to change
constantly. Thus, βt has to be estimated from a set of N
g
g
g
, βt−2
, ..., βt−N
. The estimation of
previous readings of βt−1
g
βt is the difference between βt and the expected value of
the set of previous readings. Considering that this distribution is bimodal, a k-mean algorithm [15] with k = 2 is applied to separate them into the subsets βA and βB , having
g
g
g
, βt−2
, ..., βt−N
. The estimation of βt is then
βA ∪ βB = βt−1
g
obtained as βt = βt − {E(β A ) + E(β B )}/2.
Given the estimations of (αt , βt ), the walking direction vector
Vt can be built. Around the axis determined by the center of our
reference system and Vt , we place a parallelepiped of size 1 m ×
1 m × 4 m corresponding to the extrusion of the user’s torso in
the walking direction (see Fig. 4). This parallelepiped is used

to register the subset of 3-D points Pt ∈ Pt that will intersect
with the user’s torso if the movement continues in the estimated
direction. These points represent the possible obstacles for the
user.
To interpret the obstacles, the parallelepiped is quantized in
different bins, representing a discrete set of distances from the
user position. We divide the parallelepiped in sections of s metres in depth (s = 0.05 m in our setup) and count how many
3-D points belong to each block. This is represented by a
histogram Ht . Each bin Ht [i] represents the fraction of 3-D
points contained between the planes s(i)Vt and s(i + 1)Vt of
the parallelepiped. Ht represents a 1-D distribution of obstacles
in the walking direction.
It is worth to remark that Pt has a projective nature, given
that it is provided by a stereoscopic system. The higher the
distance of observation, the higher the point sparseness. The
trend of the degree of sparseness follows an exponential increase with respect to distance. This implies that cells Ht [i] will
present a decreasing density as i increases, which is due to the
anisotropic error distribution but not to the obstacles. To deal
with this problem, a unitary square Ci is created for each bin
Ht [i] at distance s(i)Vt . The square is projected on the reference
image, and we take the size Si of the projection. These sizes
have the same projective nature than Ht [i], but in inverse order.
Thus, we can obtain a linearized version of the histogram as follows: Ht [i] ← Ht [i]/Si . The values of the histogram are also
affected by the 3-D occlusions of the points (each point of Ht [i]
projects a 3-D shadow over the following bins that decreases
their densities). However, in our problem, the key obstacles are
the closest ones, which are the least affected by this fact.

C. Obstacle Detection From Distance Histogram
Each cell in Ht represents a possible obstacle. A single observation may present obstacles at different distances. Hence,
it follows that Ht is multimodal. Mean-shift [16] is then used
to separate it into different distributions, by using a uniform
K-unit kernel. From the set of obtained centers, we keep the
most significant ones at instant t, that is Ot = o1 , o2 , ..., oN .
The initial set of potential obstacles Ot may contain some
phantom data due to the noise in the 3-D reconstruction
step. A robust set of obstacles Ot is obtained by considering only the obstacles detected in the last M observations
Ot−M +1 , Ot−M +2 , ..., Ot . An obstacle oi ∈ Ou matches an obstacle oj ∈ Ov if the distance between them in the histogram is
less than K units, in consonance with the size of the mean-shift
kernels. This guarantees that pairs of centers close enough will
be discarded.
Given the set of obstacles Ot , the one on with the lowest
index n (the nearest one to the user) is selected, whose distance
is d(on ) = n · s. If this distance is below a given threshold (in
our case 2 m) then it is considered a potential threat and an
alert signal (sound or vibration) is generated with a frequency
inversely proportional to the distance d(on ). Closer obstacles
cause a higher alert frequency.

SÁEZ et al.: AERIAL OBSTACLE DETECTION WITH 3-D MOBILE DEVICES

Fig. 5.

77

Look of the user interface.

III. APPLICATION INTERFACE
The usability of this application is directly related to its portability, because the device must hang from the user’s neck with
the camera facing forward and the screen on the chest to activate
the obstacle detection mode [see Fig. 11(left)]. Once the proximity sensor detects the device is in this position, the screen is
locked and the obstacle detection begins. The detection finishes
by flipping the device, or simply by separating it from the chest.
The obstacle detection is performed up to 4 m forward,
within the space corresponding to the user’s torso (a volume of
1.0 × 1.0 × 4.0 m3 is estimated), correcting the swing movement produced when the user walks. The user receives obstacle
alerts when they are closer than 2 m in the walking direction.
The application presents an accessible interface, designed for
blind users and it is based on three gestures: vertical swipe to
change the menu item, horizontal swipe to explore the different
values for the current item, and touch to select the current value.
After each gesture, the device pronounces the current selection
by voice synthesis, to let the user know the actions that have
been executed. For example, in Fig. 5(left), the setting mode
is currently selected. An horizontal swipe changes this setting
from pause to obstacles or telemeter. A vertical swipe moves to
the warnings setting.
The interface allows us to configure different features: Mode,
which could be obstacles (for walking assistance), telemeter
(for free environment exploration) or pause; Alerts, which may
be beep (acoustic signal) or vibration; Volume, which sets the
volume of the system; Voice, which sets the speech velocity;
Language, which sets the language of the application (English ,
German, French, or Spanish); About and Exit.
IV. EXPERIMENTS
In this section, we present some tests about the most critical
aspects of our proposal.
A. Implementation Details
Besides the drastic changes that we have performed in the approach, the implementation has also suffered big changes with
respect to [11], according with the new platform. Both 3-D
smartphones (see Fig. 2) are based on Android, whose principal
language is Java. Nevertheless, we have used Qt1 for Android
1 qt-project.org

Fig. 6. Accuracy of the scene reconstruction. (Top) First, second, and third
quartile of the 1-D depth distribution of each image. (Bottom) Some examples
of wall images at different distances.

(also known as Necessitas), a C++-based SDK that generates
the code directly on Android native, which is more suitable for
real-time applications. Also, we have used OpenCV4Android,2
the well-known computer vision library [17], for image manipulation. In order to speed up some parts of the algorithm,
we have used parallelization strategies (through threading) that
exploit the device dual-core processor, as well as vectorization
strategies with neon intrinsics (a set of instructions similar to
Intel SSE integrated with the ARM architectures). These tools
are justified by the computational requirements of the problem
and the limitations of the platform.
B. Measure Accuracy
In this first experiment, we evaluate the accuracy of the distance estimation in our proposal. We have taken 19 3-D images
of a wall perpendicular to the focal axis of the camera. These
images have been taken at distances from 0.35 to 3.95 m, every
20cm. For each image, a set of 3-D points is obtained. Given
that the only element of the image is a wall, all the 3-D points
should be placed at the same z coordinate, corresponding to the
distance from the camera to the wall.
The results are shown in Fig. 6. Columns represent the images
taken at each distance. For each image, a 1-D depth distribution has been obtained, and its first, second (median), and third
quartiles are displayed in the figure.
It can be seen that as depth increases, the distribution becomes
sparser and more noise is introduced. Our system discards measures larger than 4 m due to the exponential growth of noise
with the distance. In the case of short distances, below 2 m, it
provides accurate measures (with a low error). We can also see
that the standard deviation of the set of points increases as we
increase the distance to the obstacles.
C. Histogram Linearization
In this experiment, we explore the projective nature of the
histogram. We have taken three observations of a single object
2 opencv.org

78

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 1, JANUARY 2015

Fig. 9. Obstacle tracking in a corridor environment. The horizontal axis represents the time. Each column displays the distance histogram for each frame.

Fig. 7. (Top) Raw P t and (bottom) linearalized P t∗ version of distance histograms obtained from three observations of a single object at different distances.

Fig. 8. Obstacle tracking in a park environment. The horizontal axis represents
time. Each column displays the distance histogram for each frame.

Fig. 10. Mean-shift results. First and fourth rows: Only one obstacle is detected (either shelving or stuffed elephant). Second and third rows: Two obstacles
are detected (shelving and stuffed elephant).

(a fire extinguisher) at different distances. For each observation,
we obtained its distance histogram Pt . Fig. 7(top) shows the raw
histogram of each observation represented in different colors,
and Fig. 7(bottom) shows the linearized histograms Pt∗ . The
horizontal axis represents the histogram bins (we consider a
total distance of 4 m, and each bin is taken every 5 cm, so
that we have 80 bins). The vertical axis represents the number
of points in each bin (or the result of the linearization, in the
linearized version).
We can see that the raw histogram presents a variable density
depending on the distance to the object, due to the effect of
the projective geometry (the closer is the object, the wider is
its area). Therefore, the value of the histogram bins cannot be
directly compared, which makes mean-shift not applicable. In
the linearalized version, the densities of different observations
of a single object achieve (approximately) a balance.

of each sequence frame (processed at 9 frames/s approximately),
so that we can observe the evolution of the histogram over time.
The blue line represents the threshold we use to notify the
user about the presence of an obstacle (2 m in our setting).
The histogram represents 4 m in total. The red points represent
the obstacles that have been detected as real, that is, means
obtained by mean-shift at a lower distance that the specified
threshold and with tracking information enough to be considered
a real obstacle and not a phantom.
In the first environment (see Fig. 8), a tree is avoided. Note that
once the tree has been avoided, it stops detecting this obstacle.
In the second environment, we first get close to a wall, and then,
we move away from it. We can see this reflected in the shape of
the plot.

D. Obstacle Tracking
In this experiment, we evaluate the robustness of the obstacle
detection over time in two different environments: a park (see
Fig. 8) and a corridor (see Fig. 9). In these figures, the horizontal
axis represents the time, and the vertical axis is the distance histogram. That is, each column represents the distance histogram

E. Multimodal Histograms
Our obstacle detection approach uses mean-shift because the
distance histogram is multimodal and may contain different
distributions. This experiment aims to test the robustness of
our approach and it consists of analyzing a sequence where a
shelving is always observed at the back of the scene (see Fig. 10).
In the first frame, only the shelving is observed, so that it is
represented by a single distribution in the histogram (the result of

SÁEZ et al.: AERIAL OBSTACLE DETECTION WITH 3-D MOBILE DEVICES

Fig. 11.

Fig. 12.

79

Test users: Maria Dolores (left) and Yolanda (right).

Fig. 13.

Second test with Maria Dolores. See the text for details.

Fig. 14.

Test with Yolanda. See the text for details.

First test with Maria Dolores. See the text for details.

mean-shift is displayed in red). In the second and third frames,
a stuffed elephant appears at the front of the scene. It shares
the scene with the shelving in the back, which yields a second
distribution. We can see that mean-shift correctly detects both
distributions. In the last frame, the stuffed elephant covers all the
projection (we only consider the parallelepiped corresponding to
the user’s torso extrusion). Therefore, only a single distribution
is obtained.
F. Tests With VI Users
The last experiment consists of several tests with blind users.
In Fig. 11, we can see the people who have collaborated in this
experiment: Maria Dolores (left) and Yolanda (right). Maria
Dolores works in ONCE foundation as a psychologist. She is
blind since she was 20 years old. She has an almost null residual
vision (between 2% and 3%). She is only able to perceive light or
darkness. Yolanda works as a counselor in a secondary school.
She is a psychologist too. She was born blind and does not have
any residual vision.
Fig. 12 shows a test with Maria Dolores. There is a palm
tree leaf within the path. She walks slowly because she is not
following a margin (she is walking in an open space). Some
pictures of the scene taken from outside are shown in the first
row of the figure, and the application visual log is shown in the
second row. In the visual log, we can see the distance histogram
over the image. In the left row, the obstacle has been detected.
In the central row, a notification is sent, because the obstacle
is closer than 2 m. In the right image, the obstacle has been
avoided.
A second test with Maria Dolores is shown in Fig. 13. In this
case, she is following the curb with the cane; hence, she walks

faster. In the path, there is a fuzzy object: a bush. This kind of
obstacles could not be detected by other sensors like sonar-based
ones. The figure has the same format as the previous experiment:
scene from outside (top) and visual log (bottom), before (left)
and after (right) avoiding the obstacle.
Fig. 14 shows a test with Yolanda similar to the previous
one. She is walking following the curb with her cane, and the
application detects a the branch of palm tree. The displayed data
follow the format described above.
All the experiments have been executed in an LG Optimus
3-D Max smartphone (the results obtained with HTC EVO
3-D are similar), which is endowed with a 1.2-GHz dual-core
processor. The resolution of the captured images is 360 × 240
(we need a pair of images). The 1520-mAh battery provides an
operation time of about 132 min with the application running.
Therefore, the application should be used only sporadically, in
unknown environments.
The application is able to process an average of 9.17 frames/s.
Thus, the average lag of an obstacle alert is about 109 ms. The
walking velocity of a person is usually in the range between
4 (slow) and 6 km/h (very fast). A blind person is usually
slower than the lower limit. If we suppose a velocity of 4 km/h
(1.11 m/s), with a processing time of 109 ms per frame, and
taking into account that the average reaction time of a person
is 750 ms, then the elapsed time since the obstacle appears until the user reacts is 859 ms. In the worst case, the user walks
0.95 m from the instant in which the obstacle comes into the
field of vision. Therefore, there is a margin of 1.05 m to avoid

80

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 1, JANUARY 2015

the obstacle. For this reason, the alert threshold is set to 2 m, but
the obstacle tracking is performed from 4m.
Dolores and Yolanda are our usual collaborators, but we have
tested the approach with many other volunteers of the blind
community. Here, we summarize the feedback that we recovered
from nine users that have tested the prototype. All of them
consider that the problem we are facing represents a handicap in
their lives, and a solution like this proposal could improve their
quality of live. Seven of them agree with using a smartphone,
that could be reused for other useful tasks, while two of them
prefer an ad-hoc cheaper platform. With respect to the interface
and the accessibility of the application, most of them agree that
it is easy to use (8 of 9). We have observed that all users get a full
control of the application in a guided session of around 10 min.
Finally, the best result that we have observed (that we cannot
show with data) is the great sensation that they experience in
the first use, when they can sense the distances to the objects
without touching them.
V. CONCLUSION AND FUTURE WORK
It is worth to highlight that the technology presented in this
paper is new for this kind of devices. Until now, smartphones
were not able to extract real measures from the environment.
This application extracts about 30 000 real environment measures per frame at 9 frames/s in commercial devices.
The major limitation of this technology is the dependence
on a hardware that must incorporate a 3-D camera. Our future
work includes adapting this application to monocular devices.
A way to do this is to incorporate a catadioptric device that splits
a single-camera observation into two separated ones. Another
alternative consists on rethinking the algorithm with a structure
from motion (SFM) approach instead of the stereo one. This
change could affect many parts of the approach, because the
3-D results of the SFM algorithms are up to scale, that is, we
only know the relative scale (depth) of a point with respect to
the other points in the image, but the absolute scale is unknown
and continuously changing.
ACKNOWLEDGMENT
The authors would like to thank ONCE, the Spanish Association for Blind People, for kindly testing the approach presented
in this paper. This work was awarded with the VII Vodafone
Foundation Prizes in the “Mobile for Good” category in 2013.
REFERENCES
[1] S. P. Mariotti, Global Data on Visual Impairments 2010. World Health
Organization, Jan. 2012.
[2] J. Denham, “Oh Kapten! My Kapten! Where am I? A review of
the Kapten plus personal navigation device,” AFB Access World
Mag., vol. 12, no. 7, Jul. 2011. Available: http://www.afb.org/afbpress/
pub.asp?DocID=aw120707
[3] J. L. Finkel and J. He, “Ultrasonic path guidance for visually impaired,”
U.S. Patent 6 671 226B1, Dec. 30, 2003.
[4] S. Shoval, L. Ulrich, and J. Borenstein, “Computerized obstacle avoidance
systems for the blind and visually impaired,” in Intelligent Systems and
Technologies in Rehabilitation Engineering. Boca Raton, FL, USA: CRC
Press, Dec. 2000.
[5] X. Chen and A. L. Yuille, “Detecting and reading text in natural scenes,”
in Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2004, pp. II-366–II-373.

[6] H. Shen, J. Coughlan, and V. Ivanchenko, “Figure-ground segmentation
using factor graphs,” Image Vis. Comput.. vol. 27, no. 7, pp. 854–863,
2009.
[7] P. Sanketi, H. Shen, and J. M. Coughlan, “Localizing blurry and lowresolution text in natural images,” in Proc. IEEE Workshop Appl. Comput.
Vis., Jan. 2011, pp. 503–510.
[8] H. Shen, K. Y. Chan, J. Coughlan, and J. Brabyn, “A mobile phone system
to find crosswalks for visually impaired pedestrians,” Technol. Disability,
vol. 20, no. 3, pp. 217–224. 2008.
[9] J. Coughlan and H. Shen, “Crosswatch: A system for providing guidance
to visually impaired travelers at traffic intersections,” J. Assist. Technol.,
vol. 7, no. 2, pp. 131–142, 2013.
[10] R. Manduchi and J. M. Coughlan, “The last meter: Blind visual guidance
to a target,” in Proc. SIGCHI Conf. Human Factors Comput. Syst., 2014,
pp. 3113–3122.
[11] J. M. Sáez and F. Escolano, “Stereo-based aerial obstacle detection
for the visually impaired,” in Proc. Workshop Comput. Vis. Appl. Visually Impaired, 2008. Available: http://www.ski.org/Rehab/Coughlan_lab/
General/CVAVI08pubs/AerialObstacles.pdf
[12] J. M. Sáez and F. Escolano, “Entropy minimization SLAM for autonomous vehicles and wearable devices,” Comput. Vis. Image Understanding, vol. 115, no. 2, pp. 270–285, 2011.
[13] G. Klein and D. Murray, “Parallel tracking and mapping on a camera
phone,” in Proc. 8th IEEE Int. Symp. Mixed Augmented Reality, 2009, pp.
83–86.
[14] K. Konolige, “Small vision systems: Hardware and implementation,” in
Proc. Int. Symp. Robot. Res., 1997, pp. 111–116.
[15] J. A. Hartiga and M. A. Wong, “Algorithm AS 136: A K-means clustering
algorithm,” J. Roy. Statist. Soc., vol. 28, no. 1, pp. 100–108, 1979.
[16] Y. Cheng, “Mean shift, mode seeking, and clustering,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 17, no. 8, pp. 790–799, Aug. 1995.
[17] G. Bradski, “The OpenCV library,” Dr. Dobb’s J. Softw. Tools, vol. 25,
no. 11, pp. 120–126, 2000.
Juan Manuel Sáez received the Bachelor’s and
Ph.D. degrees in computer science from the University of Alicante, Alicante, Spain, in 1999 and 2005,
respectively.
He is currently an Associate Professor at the University of Alicante, where he is also a Member of the
Mobile Vision Research Lab. His research interests
include the development of vision-based algorithms
combined with 3-D range data for robot applications,
smartphones, wearable devices, and 3-D modeling.

Francisco Escolano received the Bachelor’s degree
in computer science from the Polytechnical University of Valencia, Valencia, Spain, in 1992, and the
Ph.D. degree in computer science at the University of
Alicante, Alicante, Spain, in 1997.
Since 1998, he has been an Associate Professor at
the University of Alicante. He has been Postdoctoral
Fellow at the Biomedical Engineering Department at
the University of South California. His research interests include the development of efficient and reliable
pattern recognition and computer vision algorithms.
In 2001, he cofounded the Robot Vision Group from which emerged the Mobile
Vision Research Lab at the University of Alicante.

Miguel Angel Lozano received the Bachelor’s and
Ph.D. degrees in computer science from the University of Alicante, Alicante, Spain, in 2001 and 2008,
respectively.
Since 2007, he has been a Lecturer in the Department of Computer Science and Artificial Intelligence, University of Alicante, where he is also the
Head of the Mobile Vision Research Lab. He has visited Computer Vision & Pattern Recognition Lab at
the University of York, and Bioinformatics Lab at the
University of Helsinki. His research interests include
pattern recognition and computer vision.

