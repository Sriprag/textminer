IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

525

Models and Methods for Quantitative Analysis
of Surface-Enhanced Raman Spectra
Shuo Li, James O. Nyagilo, Digant P. Dave, Member, IEEE, and Jean Gao, Member, IEEE

Abstract—The quantitative analysis of surface-enhanced Raman
spectra using scattering nanoparticles has shown the potential and
promising applications in in vivo molecular imaging. The diverse
approaches have been used for quantitative analysis of Raman
spectra information, which can be categorized as direct classical
least squares models, full spectrum multivariate calibration models, selected multivariate calibration models, and latent variable
regression (LVR) models. However, the working principle of these
methods in the Raman spectra application remains poorly understood and a clear picture of the overall performance of each
model is missing. Based on the characteristics of the Raman spectra, in this paper, we first provide the theoretical foundation of
the aforementioned commonly used models and show why the
LVR models are more suitable for quantitative analysis of the Raman spectra. Then, we demonstrate the fundamental connections
and differences between different LVR methods, such as principal component regression, reduced-rank regression, partial least
square regression (PLSR), canonical correlation regression, and
robust canonical analysis, by comparing their objective functions
and constraints. We further prove that PLSR is literally a blend of
multivariate calibration and feature extraction model that relates
concentrations of nanotags to spectrum intensity. These features
(a.k.a. latent variables) satisfy two purposes: the best representation of the predictor matrix and correlation with the response
matrix. These illustrations give a new understanding of the traditional PLSR and explain why PLSR exceeds other methods in
quantitative analysis of the Raman spectra problem. In the end,
all the methods are tested on the Raman spectra datasets with
different evaluation criteria to evaluate their performance.
Index Terms—Direct classical least squares (DCLS), latent variable regression (LVR), multivariate calibration, partial least square
regression (PLSR), quantitative analysis, surface-enhanced Raman
spectrum (SERS).

I. INTRODUCTION
AMAN scattering or the Raman effect is a physical phenomenon when a monochromatic laser light interacts with
molecular vibrations or other excitations, resulting in the energy
of the laser photons being shifted upward or downward. The
shifts in energy are referred as the Raman frequencies or the

R

Manuscript received February 17, 2013; revised September 2, 2013; accepted
September 5, 2013. Date of publication September 16, 2013; date of current version March 3, 2014.
S. Li and J. Gao are with the Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX 76010 USA (e-mail:
shuo.li@mavs.uta.edu; gao@uta.edu).
J. O. Nyagilo and D. P. Dave are with the Department of Bioengineering, University of Texas at Arlington, Arlington, TX 76010 USA (e-mail:
james.nyagilo@uta.edu; ddave@uta.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2281947

Raman shifts. A characteristic range of the Raman shifts, which
give the unique spectral information of a particular molecule, is
collectively referred to as the Raman spectrum [1], [2].
The inherent weak magnitude of the Raman scattering limits
the sensitivity and, as a result, the biomedical applications of
the Raman spectroscopy. The development of surface-enhanced
Raman spectroscopy or scattering (SERS) offers an exciting opportunity to overcome this limitation. The SERS-nanoparticles,
normally silver or gold colloids or substrate containing silver or
gold, are designed to enhance the intensities of the Raman spectra. When surface plasmons of silver or gold are excited by the
laser, they result in an increase in the electric fields surrounding
the metal. Given that the Raman intensities are proportional to
the electric field, there is a large increase in the measured signal
intensity [2].
With such an enhancement, the SERS has been regarded as
one of the most sensitive techniques that can provide a spectral fingerprint of an individual chemical compound and has
been a routine analytical method used in food and pharmaceutical industry, chemical, and biological communities [3]. It
has been utilized to quantify the banned food dye by Cheung
et al. [4], to analyze sulfa drugs by Lai et al. [5], by Strickland
and Batt [6] to detect carbendazim, to determine the amount
of creatinine in human serum by Stosch et al. [7], and to detect DNA sequence [8]–[10]. The SERS also has been investigated in biomedical diagnostics especially in the cancer detection. Antibody conjugated nanoparticles, which can be attached to specific proteins in the cancer cells, are injected into
a human body. The cancer cells can be detected by imaging
large amount of nanoparticles gathered in certain place inside
body by the Raman imaging techniques. Kim et al. [11] used
the antibody-conjugated SERS dots to target the surface receptors HER2 and CD10 of the breast cancer cells (MCF-7) and
the floating leukemia cells (SP2/O) in the living cells. Keren
et al. [12] demonstrated the ability of the modified Raman microscope to detect single-walled carbon nanotubes conjugated
with arginine–glycine–aspartate (RGD) peptide fractions in an
integrin positive U87MG tumor model in a living mice. These
RGD peptide fractions bind to αv β3 integrin, which is overexpressed in the angiogenic vessels and various tumor cells.
Zavaleta et al. [13] demonstrated the picomolar sensitivity and
multiplexing capabilities of SERS nanoparticles and showed
SERS to be a potential noninvasive preclinical imaging technique. Kennedy et al. [14] also developed nanoparticle probes
for SERS imaging of the cell surface receptor proteins.
In order to estimate the amount of the receptor proteins which
will eventually leads to the amount of cancer cells, a key step in
quantitative analysis of the SERS spectrum is to determine the

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

526

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

concentration and mixing ratio of individual Nano-Tags from
the mixing signal. In this paper, we analyze the mathematical definitions and essential meanings of different models and
methods, explain the suitable situation for each method, and illustrate which one is more reasonable for quantitative analysis of
SERS. As a straightforward model, direct classical least squares
(DCLS) models have been used [12], [13], [15]. Though the
estimation process is direct, it brings bias from the assumption
of availability of theoretical source signals. Other quantitative
models include full spectrum multivariate calibration (FSMC)
models, selected (or weighting) calibration models, and latent
variable regression (LVR) models will be discussed in Section II.
Based on the properties of the Raman spectra, we illustrate why
LVR is better than other models for the Raman spectrum information process.
In Section III, we shed light on LVR model-based methods. The principal component regression (PCR), reduced-rank
regression (RRR), orthonormalized PLS regression (OPLSR),
canonical correlation regression (CCR), partial least squares
regression (PLSR), PLS Wold 2-block mode A (PLS-W2A),
and robust canonical analysis (RCA) are compared. We will
show that PLSR methods can extract features both for signal
representation and prediction purpose, which is more robust
than RRR, more efficient than PCR, and more reasonable than
PLS-W2A when mixing concentrations are not related. Therefore, the PLSR methods are the more appropriate choice for
the quantitative Raman spectrum analysis. Also, since there are
several variants of PLSR methods, we analyze the differences
between variants and the details of algorithms to help readers
easily to choose and implement. In Section IV, different models
and methods are evaluated on three real SERS datasets.

Fig. 1. Unstable Raman signals of two samples using mixed Nano-Tags. Signals with the same color are five duplicate Raman signals of one sample obtained
at different times.

II. MODEL COMPARISON
A. Preprocessing and Notations
A real-life Raman signal is composed of the desired Raman
spectrum together with unstable background noise and noise
from other resources, which makes the Raman signal irreproducible (shown in Fig. 1). This inherent unstable background is
mainly because of the emission of fluorescence [16]. Besides,
some instrumental factors, like variations in the laser power
or wavelength, optical train variations or irreproducible sample
placement, and the change of position and angle of Ag or Au sol
attached on analyte molecules during time [17], will also give
the unstable signals. In order to reduce the noise effect on quantitative SERS analysis, the baseline correction methods [18] are
usually used as the preprocessing to the Raman signals. The top
panel in Fig. 2 shows the original spectrum and the bottom one
reflects the results after baseline correction.
In this paper, the theoretical Raman spectrum and the preprocessed Raman signal of a pure Nano-Tag are called the
source spectrum and the source signal, noted as Dx × 1 vectors, s̃ and s, respectively. The Raman spectrum and the Raman
signal of a mixture measurement of Nano-Tags are called the
mixture spectrum and the mixture signal, noted as the Dx × 1
vectors, x̃ and x, respectively. The rows of Dy × Dx matrices, S̃ = [s̃1 , . . . , s̃D y ]T and S = [s1 , . . . , sD y ]T , contain all

Fig. 2. Baselines of the Raman signals (top figure) and results of baseline
correction (bottom).

Dy source spectra and preprocessed source signals, and each
of them has Dx Raman shifts. The source signals are collected
from the solutions of pure materials with concentration α. The
rows of the N × Dx matrix X = [x1 , . . . , xN ]T are N preprocessed mixture signals obtained from samples of mixed NanoTags, and each sample is mixed by those Dy pure Nanotags.
The rows of the N × Dy matrix Y = [y1 , . . . , yN ]T are the
corresponding ground truth ratios of mixing volumes of those
pure Nano-Tags.
B. Direct Classical Least Squares
Zhang et al. [19], Keren et al. [12], and Zavaleta et al. [13]
used DCLS, also called classic least squares (CLS) in [20], to
estimate concentrations of the pure Nano-Tags. Keren et al. [12]
and Zavaleta et al. [13] reported three properties of Raman spectrum: 1) source spectra profiles do not change when pure NanoTags are mixed; 2) a mixture spectrum equals to the summation
of source spectra; and 3) within a certain range of concentrations, the intensities of source spectra are approximately linearly
related to the concentrations of pure Nano-Tags. Based on these

LI et al.: MODELS AND METHODS FOR QUANTITATIVE ANALYSIS OF SURFACE-ENHANCED RAMAN SPECTRA

properties, theoretically, the mixture spectrum can be modeled
as a linear combination of the source spectra with the mixing
ratios as the weights
x̃ = S̃T y + ẽ

(1)

where elements of the Dx × 1 vector ẽ are the random noises at
all the Raman shifts. For example, two solutions of pure NanoTags, whose source spectra are s̃1 and s̃2 , and both with the
concentration of α, are mixed into one sample with the ratio
of mixing volume as 30% : 70% (y is [0.3,0.7]T ). Then, the
concentration of each Nano-Tag in the mixture sample is 0.3α
and 0.7α. Based on the property (c), the two source spectra are
approximately 0.3s̃1 and 0.7s̃2 . And based on the property (a)
and (b), the mixture spectrum should approximately be 0.3s̃1 +
0.7s̃2 = S̃T y. By minimizing the summed square errors ẽT ẽ,
the mixing concentrations are estimated as
αŷ = α(S̃S̃T )−1 S̃x̃.

(2)

The DCLS model is straightforward since it only requires
source spectra as the training data. But in reality, it is difficult
to get the theoretical source spectra S̃ and mixture spectra x̃ in
(1), instead, we can only get the preprocessed mixture signal x
and source signals S, which can be expressed as
x = x̃ + ex and S = S̃ + Es

(3)

where ex and Es are noises and preprocessing errors, respectively. Therefore, in the DCLS model (1), all the estimations are
based on certain unreliable source signals, which may cause the
biased results.
One way to reduce the bias is to treat S̃ as unknown parameters, and use a batch of mixture signals together with their
ground truth mixing concentrations to directly find the relationship between mixture signals and concentrations. In the following sections, the models we will introduce are based on this
idea.
C. Multivariate Calibration
By combining (3) and (1), we can get x = S̃T y + ẽ + ex .
If we observe N mixture signals, together with ground truth
mixing concentrations, we can get the multivariate calibration
model [21] as
X = YS̃ + Ẽ + Ex

(4)

where X and Y are given, unknown parameters S̃ needed to be
estimated, rows of Ẽ and Ex are random noises and preprocessing errors. Since the purpose of the calibration is to estimate
concentrations from spectra, it is convenient to rewrite (4) into
a multiple multivariate linear regression model
Y = XB + Er + Ẽr

(5)

where the matrix of regression coefficients B is in fact the
general inverse matrix of S̃, Er = −Ex B can be treated as the
bias items and Ẽr = −ẼB is the matrix of regression errors.
Instead of solving (5), normally it is equivalent to solve
Y = XB + Ẽr

(6)

where X and Y are centered (zero-mean matrices). This model
uses all the Raman shifts information, so it is called the FSMC.

527

Also, since (5) is a least squares model, model (4) is also called
the inverse least squares model in [20]. If the rank of the covariance matrix XT X equals to Dx , by minimizing the trace of
Er ETr , B is solved as
B̂ = (XT X)−1 XT Y.

(7)

For a given new preprocessed testing mixture signal x, the
mixing concentrations of each Nano-Tag can be predicted as
ŷ = B̂T (x − μx ) + μy , where μx and μy are the mean vectors
of rows of X and Y.
Zavaleta et al. [13] claimed that the source Raman spectra will
not change when the pure Nano-Tags are injected into a living
organism, nor did they change as a function of tissue depth. So
in real applications, it is legitimate to do the calibration in vitro,
and do the predictions in vivo.
For a typical Raman signal, usually there exist hundreds or
thousands of Raman shifts, but for a certain biomedical study,
only a small amount of samples are available. Therefore, one
problem of the FSMC is the covariance matrix XT X in (7) is
not invertible. To solve this singularity problem, a singular value
decomposition (SVD)-based method [22] can be used to calculate the generalized inverse of XT X (described in Appendix A).
But this general inverse method cannot deal with the overfitting
problem [23] caused by the limited number of the training data.
Normally, the overfitting will cause large absolute values of
elements of B̂. So to solve the overfitting problem, ridge regression (RR) [24] adds a constraint BF < τ , where .F is the
Frobenius norm defined as BF = tr(BT B), and tr(.) is the
trace of the matrix. The solution of RR is
B̂ = (XT X + κI)−1 XT Y

(8)

where I is Dx × Dx identity matrix and κ is a small parameter.
Though RR solves the overfitting problem in the FSMC model,
the limitation is that it treats all the Raman shifts equally in the
regression and ignores that not all the Raman peaks carry the
same weight in estimating the regression coefficients.
A simple solution is to first select K Raman shifts out
of all by some feature selection methods, where K < N .
Then, the training sample matrix is changed to an N × K
matrix Xs , and the estimated matrix of coefficients becomes
B̂ = (XTs Xs )−1 XTs Y. This is called a selected multivariate
calibration (SMC) [21] Model. The problem of the SMC is that
some weak Raman peaks and background Raman shifts that
contain quantitative information will easily be discarded, which
affects the accuracy.
D. Latent Variable Regression
To deal with the information loss problem in the SMC model,
instead of choosing only a few Raman shifts, a LVR model [25],
[26] linearly combines all the Raman shifts (variables) with K
groups of weights W = [w1 , . . . , wK ] ∈ RD x ×K , and extracts
features
T = XW

(9)

where T = [t1 , . . . , tK ] is an N × K matrix containing K unrelated latent variables (LVs) of the zero-mean matrix X. Then,
the multivariate regression is carried out between T and the

528

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

zero-mean matrix Y
Y = TC + Ẽr

(10)

where C is the matrix of regression coefficients, and Ẽr is
the same as the one in (6). If the Raman signals are thought
as data points in a high-dimensional space, the small numbers
of data points stay in a low-dimensional subspace. The LVR
model basically finds a K-dimensional subspace to project those
data points, then the regression analysis is done between the
projections T and concentrations Y. Similar to (7), C is solved
as
C = (TT T)−1 TT Y.

(11)

In the LVR model, the regression coefficients matrix B in (5) is
decomposed as
B = WC = W(WT XT XW)−1 WT XT Y.

(12)

Given a testing mixture signal x, the concentrations ŷ can be
predicted as
ŷ = BT (x − μx ) + μy .

(13)

With a reasonable choice of numbers of extracted features
K that is normally selected by a cross-validation method, LVR
can reduce the overfitting problem, and so is more robust than
FSMC. The linear combination or the projection in (9) is in
essence the feature extraction process in which the columns of
T are the new extracted features. Therefore, comparing with
RR, LVR also provides a way for biological feature extraction.
Since the extracted features comes from the information of all
the Raman shifts, it avoids the information loss problem in SMC.
III. COMPARISON STUDY OF LVR METHODS
The weight vectors W in (9) are usually found by solving
a constraint optimization problem. For different purposes of
feature extraction, different LVR methods have different objective functions and constraints. In this section, we compare
those purposes and formulations of several LVR methods, including PCR, RRR, OPLSR and PLSR, CCR, PLS-W2A, and
RCA. We then explain why PLSR methods (including PLS2 and
SIMPLS) are better than other methods for quantitative analysis of the SERS. In the following sections, both X and Y are
preprocessed zero-mean matrices. All the indexes i and j in
the following formulations are defined as i = 1, 2, . . . , K and
j = 1, 2, . . . , (i − 1).
A. Principal Component Regression (PCR)
PCR uses principal component analysis (PCA) to find W.
PCA is a technique widely used for dimensionality reduction
and feature extraction [23]. The goal of PCA is to make T
retain the variation or information presented in X as much as
possible. The successive formulation of PCA is to seek wi one
by one, and at each time obtains an unrelated component (LV)
ti that has the largest variance
obj. max Jp cr1 = var(ti ) s.t. wi  = 1; tTi tj = 0
wi

(14)

where var(ti ) = tTi ti is the sample variance of ti . wi  =
wiT wi is the Euclidean norm of wi . The first constraint nor-

malizes the lengths of the weight vectors. The second constraint
makes sure the new LV is unrelated (independent) with the previous ones. The number of components (LVs) we can have is
limited by the rank of X. From (14), the weight vectors {wi }K
i=1
are solved as the first K eigenvectors of XT X (in this paper, the
first K eigenvectors means the first K eigenvectors corresponding to the first K largest eigenvalues). Since the covariance
matrix XT X is a symmetric matrix and all eigenvectors are orthonormal, the K-dimensional subspace is spanned by columns
of W.
Another formulation called the simultaneous formulation of
PCA is to find all wi at once
obj. max Jp cr2 =
W

K


(tTi ti ) s.t. wi  = 1; tTi tj = 0.

(15)

i

Jolliffe [27] proved the equivalence between the simultaneous
and successive formulation of PCA, and gave the solution of W
also as the first K eigenvectors of XT X.
Besides maximizing the variances of LVs, another objective
function of PCA is to minimize the representing error [23] or
to minimize the information in the residual matrix. The corresponding formulation is
min Jp cr3 = X − TPTx F
W

(16)

where .F is the Frobenius norm, and columns of Px are
the loading vectors. The equivalence between (16) and (15) is
proved in Appendix B.
Both the objective functions of PCA, maximizing total variances (15) or minimizing representation errors (16), show that
the score vectors of PCA T are the best K-dimensional representation of X. In other words, W of PCR gives more weights
to the Raman shifts that have larger variances of the intensities,
which may not be the locations of the Raman peaks, but are
outlier or noise peaks. Some weak Raman peaks that are highly
correlated with Y may not get large weights. So, for prediction
purpose, LVs of PCR may not be efficient.
B. Reduced-Rank Regression and Orthonormalized Partial
Least Squares Regression
The goal of RRR [28] is to make T the best rank K approximation to Y. The objective function is to minimize the
approximation (or regression) error. Considering the constraints
of independence of LVs, the formulation of RRR is
obj. min Jrrr = Y − TPTy F s.t. tTi tj = 0.
W

(17)

Columns of Py are the regression coefficients between ti and Y.
The calculation of Py is provided in Appendix C. The solution
of W can be calculated [29] as the first K eigenvectors of
(XT X)−1 (XT YYT X).
The purpose of OPLSR [30] is to extract features that are
mostly correlated with columns of Y without considering the
variances of LVs. The objective function is described as [31],
[32]
obj. max Jopls = tr(TT YYT T) s.t. TT T = I
W

(18)

LI et al.: MODELS AND METHODS FOR QUANTITATIVE ANALYSIS OF SURFACE-ENHANCED RAMAN SPECTRA

which is actually derived from the formulation of
obj. max tr{(TT T)−1 (TT YYT T)} s.t. tTi tj = 0.
W

(19)

The above equivalence is proved in Appendix C. Equation (19)
can be rewritten as
obj. max
W

K


(var(ti )−1 cov(ti , Y)2 ) s.t. tTi tj = 0

(20)

i=1

which is also proved in Appendix C. var(.) is defined in (14) and
cov(ti , Y) = tTi Y is the sample covariance vector between ti
D y
and each column of Y. cov(ti , Y)2 = j =1
cov(ti , yj )2 =
T
T
ti YY ti . Equation (19) is equivalent to the formulation of the
RRR in (17), which is proved in Appendix C. That is to say
OPLSR and RRR are essentially the same. Their W gives more
weights to the Raman shifts, where the intensities of the Raman
spectra are more correlated with the concentrations Y, without
considering representing the Raman spectra. The problem is
that it may give more weights to the weak Raman peaks or even
background that are more correlated with the concentrations,
instead of the main Raman peaks with higher intensities. So
they are sensitive to the small changes of intensities, which
make the predictions not robust enough.

529

1) PLS2: PLS2 [34], [36] iteratively deflates on X to get
the residual matrix Xi and obtains the corresponding projection
direction ri by solving the following formulation:
obj. max rTi XTi YYT Xi ri , s.t. ri  = 1
ri

(22)

where Xi is obtained from a deflation process in Algorithm 1,
in which, eig(A) means getting the first eigenvector of matrix
A corresponding to the biggest eigenvalue. Hoskuldsson [36]
proved that the independence constraint tTi tj = 0 in (21) is
satisfied after the deflation process. R = [r1 , . . . , rK ] in (22) are
the weight vectors of the residual matrix Xi , which are different
with W. The relation between ri and wi is ti = Xi ri = Xwi
[40]. After deflation, W can be calculated as W = R(PT R)−1
[34] or W = P(PT P)−1 [41].

C. PLS Regression
Partial least squares (PLS) technique is originally designed
to model the relationships between several data blocks or sets
of variables [33]. Developed from PLS and nonlinear iterative
partial least squares (NIPALS) algorithm, PLSR algorithms,
mainly including PLS2 and SIMPLS, are especially used for
the purpose of regression and prediction [34]. PLS2 is based
on the NIPALS algorithm [34]–[37]. For the special case of
1-D response variable Y, it is called PLS1 [38]. Hoskuldsson
[36] analyzed several statistical properties of PLS2 and Wold
et al. [34] gave a good picture of PLS2. The SIMPLS algorithm
designed by de Jong [39] improves NIPALS by avoiding the
deflation on original data matrices.
The purpose of the PLSR is to maximize the covariance between T and concentrations Y [29]
obj. max cov(ti , Y)2 s.t. wi  = 1; tTi tj = 0
wi

(21)

where cov(ti , Y) is the vector of the sample covariances between ti and columns of Y. Comparing the objective function
in (21) with (14) and (20), we can see PLSR is the compromise
between RRR and PCR. It makes T both represent X and approximate Y simultaneously. W of PLSR gives higher weights
to the Raman shifts that have both big variances of intensities
and high correlations with concentrations, which are more likely
to be the positions of the main Raman peaks.
The objective function in (21) equals to maximize
wiT XT YYT Xwi , with the normalization constraint of wi ,
for i = 1, w1 is the first eigenvector of XT YYT X. But for
i = 2, . . . , K, because of the independence constraint of the
LVs, there is no closed-form solution to W in (21). PLS2 [38]
and SIMPLS [39], are designed to solve this problem. In the
following two sections, we will compare the details of the two
algorithms.

ri is actually the left singular vector of XTi Y. Since the size of
matrix XTi YYT Xi is large, step 2 in Algorithm 1 is time consuming. The faster way is to first calculate the right singular vector di by eig(YT Xi XTi Y), then use the relationship between
the left and right singular vector, ri = XTi Ydi /XTi Ydi , to
get ri .
2) SIMPLS: Instead of based on the residual matrices Xi ,
SIMPLS directly looks for the directions of projections in
the original X space by projecting the cross covariance ma+
trix XT Y onto the orthogonal subspace P⊥
i = I − Pi−1 Pi−1
+
iteratively to satisfy the unrelated constraint, with Pi−1 =
(PTi−1 Pi−1 )−1 PTi−1 is the Moore–Penrose inverse of Pi−1 and
Pi−1 = [p1 , . . . , pi−1 ] is the loading matrix. The objective
function of SIMPLS in each iteration is
T
T
⊥
obj. max wiT P⊥
i X YY XPi wi , s.t. wi  = 1.
wi

(23)

T
T
⊥
wi can be solved as the first eigenvector of P⊥
i X YY XPi
corresponding to the largest eigenvalue. de Jong [39] proved that
these wi satisfy the constraint tTi tj = 0 in (21). The SIMPLS
algorithm is summarized in [39].

D. Symmetric and Asymmetric Relation
PCR, RRR or OPLSR, PLSR have the asymmetrical relationship between X and Y [38], because these methods are from X
to predict Y, and only get LVs of X. These methods are only
suitable for the low dimensional and independent response variables Y. If Dy > N or there is collinearity between columns
of Y, LVR methods with the symmetrical relationship between
X and Y, which get the LVs for both X and Y, have to be
used. It is called symmetrical relationship because both X and
Y can be the predictor or the response matrix. In the following

530

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

sections, we will introduce three such methods, including CCA,
PLS-W2A, and RCA.
For the symmetric relation LVR methods, the components
number K is limited by the rank of both X and Y: K ≤
min(rank(X), rank(Y)). But for the asymmetric relation LVR
methods, K is limited by the rank of X: K ≤ rank(X). In
our application of quantitative analysis of the Raman spectra,
normally, we only want to predict concentrations Y from the
Raman spectra X, and the number of pure Nano-Tags (rank of
Y) is usually low, which will limit the number of LVs used in
the model and so the effectiveness of the model if we use the
symmetric relation LVR methods.
E. Canonical Correlation Regression
CCR is based on the technique of canonical correlation analysis (CCA) [42]. The purpose of CCA is to find K pairs of
LVs of X and Y, {(ti = Xwi , ui = Yvi )}K
i=1 , such that each
LV within each set is only correlated with a single LV in the
other set. The objective function is to maximize the correlation
coefficients of two LVs. The first way to describe CCA is the
successive formulation
obj. max Jccr1 = corr(ti , ui )

where corr(ti , ui ) = √

t Ti

t Ti u i
ti

√

u Ti

ui

(24)

is the sample correlation co-

obj. max Jccr2 = tTi ui ; s.t. tTi ti = 1; uTi ui = 1
(w i ,v i )

= 0; uTi uj = 0; tTi uj = 0; tTj ui = 0.

(25)

Appendix D shows W and V are calculated as the first K
eigenvectors of matrix (XT X)−1 (XT Y)(YT Y)−1 (YT X) and
(YT Y)−1 (YT X)(XT X)−1 (XT Y). Besides successive formulation, CCA has another equivalent simultaneous formulation [43], [44]
obj. max Jccr3 =
(w i ,v i )

K


Another variant of PLS is PLS-W2A by Wegelin (Wold’s
Two-block mode A PLS [37]), which is especially used for
modeling and predicting between two data blocks [35]. Nowadays, it is also commonly used for feature extraction applications [45]–[47]. PLSR methods only find LVs of dependent variables to relate to each independent variable, and they assume the
independent variables are unrelated. Similar to CCR, PLS-W2A
can handle the collinearity between columns of Y and even the
ill-conditional problem in matrix Y (numbers of variables of
Y is bigger than the numbers of samples). The method calculates K pairs of unrelated LVs {(ti = Xwi , ui = Yvi )}K
i=1
for both data blocks X and Y that have the maximum sample
covariances
w i ,v i

efficient between the two LVs. The constraints make sure that
the components are unrelated. Because the scales of wi and vi
do not affect the correlation coefficient value, it is always proper
to fix the projection variance as a constant, usually as 1. So (24)
is usually rewritten as (e.g., [42], [43])

tTi tj

F. PLS Wold 2-Block mode A

obj. max cov(ti , ui )

(w i ,v i )

s.t. tTi tj = 0; uTi uj = 0; tTi uj = 0; tTj ui = 0

of Y. The combinational weights ci of columns of Y make
CCR can also deal with the collinearity in matrix Y. When
the numbers of pure Nano-Tags in the mixtures are more than
sample numbers and the existence of certain pure Nano-Tags
are highly correlated with the mixtures, CCR is a substitution
of RRR and OPLS.

s.t. wiT wi = ai ; tTi tj = 0; tTi uj = 0
viT vi = ai ; uTi uj = 0; tTj ui = 0

(28)

where ai is a constant to fix the length of wi and vi . cov(ti , ui )
is the sample covariance between ti and ui .
Similar to PLSR, there is no closed-form solution to W and
V in (28). Based on the NIPALS algorithm, the PLS-W2A uses
the deflation process on both X and Y, which is summarized
in Algorithm 2, to satisfy the independence constraints. Here,
ri and di are the projection directions corresponding to the
deflated data Xi and Yi instead of the original data X and
Y. For i = 1, X1 = X, and Y1 = Y, so r1 is the same as
w1 and d1 is the same as v1 . But for i = 2, . . . , K, they are
different. The relation is ti = Xi ri = Xwi and ui = Yi di =
Yvi . After getting all the loading vectors P = [p1 , . . . , pK ]
and Q = [q1 , . . . , qK ], all the combinational weights vectors
W and V can be calculated as W = P(PT P)−1 and V =
Q(QT Q)−1 .

(tTi ui ) s.t. tTi ti = 1; uTi ui = 1

i=1

tTi tj = 0; uTi uj = 0; tTi uj = 0; tTj ui = 0.

(26)

As discussed in Section II-C, the singularity problem of the
covariance matrix XT X in the solution of CCR and RRR or
OPLSR can be solved by the generalized inverse of XT X based
on SVD (described in Appendix A). Another commonly used
approach (e.g., [43]), similar to RR, named regularized CCA, is
to add a regularized term κ to the covariance matrix
(XT X + κI)−1 (XT Y)(YT Y + κI)−1 (YT X)W = WΛ.
(27)
Similar to RRR and OPLS, W of CCR also gives higher
weights to the Raman shifts that are more correlated with LVs

G. Robust Canonical Analysis
Tishler et al. [48] presented an “Intercorrelations Analysis”
method or “Canonical Covariance,” which is similar to the

LI et al.: MODELS AND METHODS FOR QUANTITATIVE ANALYSIS OF SURFACE-ENHANCED RAMAN SPECTRA

531

Fig. 3. Pure and mixture Raman signals of different Nano-tags. D:C-90:10, for example, means the ratio of volume of DTTC and CV is 90% : 10%. (a) Pure
DTTC-CV. (b) Pure HITC-IR140. (c) Pure DOTC-DTTC-HITC-IR140. (d) Mixture DTTC-CV(D:C). (e) Mixture HITC-IR140(H:I). (f) Mixture DOTC-DTTCHITC-IR140.

PLS-W2A. In the reported work, RCA is used as a modeling
method. Later, Tishler and Lipovetsky [49] presented an RCA
regression model for prediction, which solves all eigenvectors of
XT YYT X at once. Rosipal and Kramer [38] named it as PLSSB, and Wegelin [37] called it PLS-SVD, since it is actually
PLS-W2A without a deflation process. The projection directions W of RCA are the first K eigenvectors of the generalized
eigenfunctions
XT YYT XW = WΛ

TABLE I
OPTIMIZED COMPONENTS NUMBER K ∗ USING TWO CRITERIA

(29)

where the diagonal elements of the diagonal matrix Λ are descending ordered eigenvalues. RCA is a kind of compromise
between CCA’s nondeflation and PLS-W2A’s covariance as objective. The mathematical description can be illustrated as
obj. max tTi ui s.t. wi  = vi  = 1; wiT wj = viT vj = 0. (30)
w i ,v i

Comparing with (28), RCA releases the unrelated components
constraints. So LVs in RCA are not necessarily mutually unrelated, and there is overlapped information between the LVs,;
therefore, RCA may not be efficient for the Raman signal.
IV. EXPERIMENTS
A. Experiment Design
1) Datasets Description: The datasets were collected
from the Raman spectroscopy system with 20×, 0.4NA
lens, and 785-nm laser wavelength. The Raman shifts range

from −79.65 to 2071.80 cm−1 with 1044 Raman shifts
values. All the Nano-Tags are made from 54.67 nm Au
Nano-particles, coated with the dyes: DTTC and Cresyl
violet (CV) (in dataset 1); HITC and IR140 (in dataset
2); DOTC, DTTC, HITC, and IR140 (in dataset 3). The
pure Nano-Tag solutions are prepared with a concentration
of 1.1e10 Nano-Tags/ml. Then, with 11 ratios of volume
{(0:100%), (10% : 90%), . . . , (90% : 10%), (100% : 0)}, we
mix two pure Nano-Tags solutions in the first two groups, and
with 21 ratios of volume {(25% : 25% : 25% : 25%), (20% :
25% : 25% : 25%), (15% : 25% : 25% : 25%), . . . , (0:25% :
25% : 25%), (25% : 20% : 25% : 25%), (25% : 15% : 25% :
25%), . . . , (25% : 25% : 25% : 5%), (25% : 25% : 25% : 0)},
we mix four pure Nano-Tags solutions in the third group
(the missing percentages are made up with water), and get
three groups of mixture Nano-Tag solution samples. For each

532

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

TABLE II
ESTIMATION ERRORS FOR EACH METHODS ON THREE DATASETS

pure and mixture Nano-Tag solution sample, we collect five
duplicated Raman signals, with 20s integration. To avoid the
effect of the strong signal intensity from the Rayleigh scattering
and some noise frequencies on two sides, from 1044 Raman
shifts, we extract the middle range (90–900th) frequencies
(from 160.13 to 1845.00 cm−1 ). So for datasets 1 and 2
(DTTC-CV and HITC-IR140), the ground truth ratio matrix is
Y ∈ 55×2 , and the mixture spectra matrix is X ∈ 55×811 ,
and for dataset 3 (DOTC-DTTC-HITC-IR140), the ground truth
ratio matrix is Y ∈ 105×4 , and the mixture signals matrix is
X ∈ 105×811 . Also, we average each five duplicates for both
the pure and mixture signals, and get the pure average signals
S̄ ∈ 2×811 (datasets 1, 2) and S̄ ∈ 4×811 (dataset 3); and
the mixture average signals X̄ ∈ 11×811 and X̄ ∈ 21×811 ,
and the average ground truth ratio matrices Ȳ ∈ 11×2 and
Ȳ ∈ 21×4 . The duplicate and average source signals are
shown in Fig. 3(a)–(c), and the average mixture signals are
shown in Fig. 3(d)–(f).
2) Evaluation Methods and Criteria: In order to take full
use of all three datasets, we use three cross-validation methods
to evaluate the predicting ability of methods. Cross validation
on duplicate testing spectra (CVD): all five duplicate spectra of
the same mixing ratio are treated as the testing samples, and
all other mixture signals are treated as the training samples.
Iteratively, until every duplicate is treated as the testing sample
once.
Cross validation on average testing spectra (CVA): the average spectrum of the five duplicates with the same ratio is treated
as the testing sample and all the other duplicate spectra are
treated as the training samples. Iteratively, until every average
spectrum is treated as the testing sample once.
Cross validation on average testing average training spectra
(CVAA): the average spectrum of the five duplicates with the
same ratio is treated as the testing sample and all the other
average spectra are treated as the training samples. Iteratively,
until every average spectrum is treated as the testing sample
once.
Two criteria are used to evaluate the prediction accuracy: L1norm mean error (L1ME) and L2-norm mean error (L2ME).
N D y
L1ME is defined as L1ME = N 1D y
j =1 |ŷi,j − yi,j |,
i=1
which is actually
the
mean
absolute
error.
L2ME
is defined

N D y
1
2
as L2ME = N D y
j =1 (ŷi,j − yi,j ) , which is actui=1
ally the root mean square error (RMSE). ŷi,j and yi,j are the
elements of the matrix of estimated ratios Ŷ and ground truth

ratios Y, respectively, of the ith sample and the jth Nano-Tag.
Dy is number of pure Nano-Tags and N is number of testing
signals. Two criteria are different when describing the effectiveness of algorithms. L1ME just shows the average difference
between the estimated values and the ground truth values, while
L2ME punish more on big errors. For example, method A gets
two estimated errors: 1 and 5, and method B gets 2 and 4. Their
√
L1ME are the same, which
√ is 3, but method A’s L2ME is 13
and method B’s is only 10. In this case, we think B is better
than A, since A gets the biggest error.
In this paper, we used the iteratively curve-fitting baseline
correction method [18] to remove the backgrounds, and the
polynomial curve-fitting order pOrder needs to be decided. Different orders (from 3 to 10) are tested, the one giving the lowest
RMSE returns as the optimized pOrder∗ .
3) Methods Specification: For DCLS, in order to reduce the
influence of the instability of the pure Raman spectra, we use
average source signals S̄ as the standard sources to estimate:
Ỹ = XS̄T (S̄S̄T )−1 . For RR and rCCR, the parameter κ in
(8) and (27) are set as 0.1. To maximize the performance of
all the LVR methods, the component number K needs to be
optimized for each dataset and each cross-validation method.
Every possible component number is tested, the one giving the
lowest RMSE returns as the optimized one K ∗ . The results are
in Table I.
B. Experimental Results
In Table II, we show the L1ME and L2ME of different regression methods, using three datasets and three cross-validation
methods. The results of CVA and CVAA tend to be better than
CVD, since the average testing signals will reduce the influence
of unstable backgrounds. And CVA is better than CVAA, since
CVA has more training samples than CVAA.
For the DCLS method, CVA tests each duplicate mixture signal and CVAA tests each average mixture signal. We leave CVD
as empty. The results of DCLS are worse than other calibration
methods, because the source signals are not reliable enough,
especially in dataset 3. (We find that the heights of the Raman
peaks in the mixture are higher than those in source signals.
Theoretically, they should be lower, since the concentrations of
each pure Nano-Tag is lower in the mixture solution.) Most of
the results of LVR methods (PLS2, SIMPLS, PCR, OPLS) are
better than those of RR, which means the latent space calibration model is better than the full spectrum calibration model,

LI et al.: MODELS AND METHODS FOR QUANTITATIVE ANALYSIS OF SURFACE-ENHANCED RAMAN SPECTRA

533

Fig. 4. Percentages of X and Y represented by T of LVR methods. (a) R x , dataset 1. (b) R x , dataset 2. (c) R x , dataset 3. (d) R y , dataset 1. (e) R y , dataset 2.
(f) R y , dataset 3.

Fig. 5. Relation between LVs and concentrations, for dataset 1. Elements Y 1 are the mixing ratios of CV, and elements of Y 2 are the mixing ratios of DTTC. K
is the number of components used in LVR methods. (a) Y 1 and TC 1 , K = 4. (b) Y 1 and TC 1 , K = 8. (c) Y 1 and TC 1 , K = 20. (d) Y 2 and TC 2 , K = 4.
(e) Y 2 and TC 2 , K = 8. (f) Y 2 and TC 2 , K = 20.

534

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

because there may be noise in certain latent dimensions, and by
choosing the optimized component number K ∗ , most noise are
removed. CCR, PLS-W2A, and RCA are bad for datasets 1 and
2, and relatively better in dataset 3. That is because of the limited number of components problem. In datasets 1 and 2, rank
of Y is 1, so only one component can be used, and in dataset 3,
only four components can be used, so the accuracy is relatively
increased. In the end, the similar results of PLSR methods prove
that PLS2 and SIMPLS are almost the same, and they are better
than the other LVR methods, which shows the robustness of
PLSR methods (the reason is explained in Section III-C).

matrixes. For the Raman signal analysis, asymmetric relation
methods are better than symmetric relation because the latter
has the limited component number. By comparing the objective
functions and constraint forms of LVR methods, we illustrate
that PLS2 and SIMPLS are almost the same, both belonging
to PLSR methods, and both are better than PCR and OPLS (or
RRR), since they are the combination of best representation of
the input signal X and best prediction of the response Y. The
conclusion and discovery reported in this paper will have a great
impact on the quantitative Raman signal analysis.

C. Discussions

APPENDIX A

1) Representation and Prediction Effectiveness: In order to
show the effectiveness of LVs T to represent the Raman signals X and to predict the concentration Y, Fig. 4 shows how
much information of X and Y are presented in T, which can
be defined as Rx = (1 − (X − TPT )/X) × 100% and
Ry = (1 − (Y − TC)/Y) × 100%.
The curves of PLS2 and SIMPLS are overlapped from
Fig. 4(a)– (e), which means their effectiveness are almost the
same. The effectiveness of PLS2, SIMPLS, and PCR for representing X are similar [see Fig. 4(a)–(c)], but for predicting
Y, PLS2, and PCR is better than PCR [see Fig. 4(d)– (f)]. The
effectiveness of the OPLS for both purposes is the worst, and
until the number of components is big enough (around 20), it
gets close to the other three methods. Fig. 4 explains why the
prediction accuracy of PLSR is better than PCR (see Table II),
and the components used are less (see Table I).
2) Calibration Effectiveness: To observe the calibration effectiveness of LVR methods, we run OPLS, PLS2, and PCR on
dataset 1, using three component numbers (K = 4, K = 8, K =
20), and with 50 mixture signals and corresponding mixing ratios as the training samples, and get the score T and coefficients
C in (10). Fig. 5 shows the relation between the ratios of the
ith Nano-Tag Yi and TCi , where Ci is the ith column of C. It
illustrates that the low-dimensional representation (or weighted
mean) of spectra intensities (T) and the relative concentrations
of pure components (Y) are linearly related, which explains
why calibration methods work well on the Raman spectra data.
It also demonstrates the calibration effectiveness of these methods increases with more components used, and PLS2 is more
effective than PCR and OPLS because of its fastest convergence.
V. CONCLUSION
This paper presents a comprehensive overview of the current
existing computational models and methods in the Raman spectrum information analysis. The pros and cons of each methodology is discussed in depth with an emphasis on the specific
biological data information. We group the current schemes into
four models, DCLS model, FSMC, selected calibration model,
and LVR model. Based on the properties of the Raman spectra,
and the fact that data points usually stay in a low-dimensional
subspace, we analyze why the LV regression model outperforms
the other three. Among the LVR methods, we divide them into
two groups based on the symmetrical relation between input

CALCULATION OF GENERALIZED INVERSE OF MATRIX A
The generalized inverse (or pseudoinverse) can be calculated using SVD decomposition [22]. The (m × n) matrix
A as A = UΣVT , where columns of the (m × m) matrix
U are the left-singular vectors, columns of the (n × n) matrix V are the right-singular vectors, and diagonal entries of
the m × n diagonal matrix Σ are the decreasing singular values σi , with i = 1, . . . , min(m, n). If A is the rank k matrix, where k < min(m, n), then for i = k + 1, . . . , min(m, n),
σi = 0. Then, the generalized inverse of A is calculated as
T
A− = Uk Σ−1
k Vk , with columns of (m × k) matrix Uk and
(n × k) matrix Vk are the first k columns of U and V, and
diagonal entries of the k × k diagonal matrix are the first k σi .

APPENDIX B
PROOF FOR PCR
Equation (16) can be written as
min Jp cr3 = tr{(X − XWPTx )T (X − XWPTx )}
W

= tr(XT X − 2XT XWPTx + Px WT XT XWPTx ).
(31)
Take the derivative of Jp cr3 to Px and set to 0, PTx can be calculated as the generalized inverse of W (noted as W− ). So the
objective function in (16) becomes J = X − XW(W)− 2 ,
and so the lengths of the projection directions do not affect
the objective function. Without loss of generality, we can have
the constraint WT W = I. And since PTx W = I, Px = W. So
(31) can be rewritten as
min Jp cr3 = tr(−2PTx XT XW + WT XT XWPTx Px )
W

= tr(−2WT XT XW + WT XT XW)
= −tr(WT XT XW) = −

K


(wiT XT Xwi )

i

= max Jp cr2 .
W

(32)

LI et al.: MODELS AND METHODS FOR QUANTITATIVE ANALYSIS OF SURFACE-ENHANCED RAMAN SPECTRA

Take derivative of L to w2 and v2 , and set to 0, we get

APPENDIX C
PROOF FOR RRR
Equation (17) can be written as
min Jrrr = tr(YT Y − 2YT XWPTy + Py WT XT XWPTy ).
W

(33)
Take the derivative of Jrrr to Py and set to 0, we can get
(34)

Substitute (34) back into (33), the objective function of RRR
becomes
min J = tr(YT Y − YT XW(WT XT XW)−1 WT XT Y).
W

Since the YT Y is constant, the objective function becomes
max tr[(WT XT XW)−1 (WT XT YYT XW)]
W

(35)

which equals to (19). Because of the constraints wiT XT Xwj =
0, WT XT XW is a diagonal matrix, so (35) can be rewritten as
max
W

K

(wiT XT Xwi )−1 (wiT XT YYT Xwi ).

(36)

i=1

In (36), the scales of all {wi }K
i=1 do not affect the objective
function, so they can always be adjusted to make wiT XT Xwi =
1. So (36) can be rewritten as
max
W

K


(wiT XT YYT Xwi ) = tr(WT XT YYT XW) (37)

i=1

with the constraint WT XT XW = I, which is equal to (18).
APPENDIX D
CALCULATION OF (24) FOR CCR
When i = 1, j = 0, from (24) we get the Lagrange function
as
L = w1T XT Yv1 −λ1 (w1T XT Xw1 −1)−σ1 (v1T YT Yv1 −1).
Take the derivative of L to w1 and v1 and set to 0, we get
XT Xw1 = λ1 XT Yv1
T

XT Yv2 − λ2 XT Xw2 − γ21 XT Xw1 = 0

(41)

Y Xw2 = σ2 Y Yv1 − δ21 Y Yv1 = 0.

(42)

T

Py = YT XW(WT XT XW)−1 .

T

Y Yv1 = σ1 Y Xw1 .

(38)
(39)

By multiplying w1T and v1T on both sides of (38) and
(39), respectively, we get λ1 = σ1 . From (39), we get v1 =
σ1 (YT Y)−1 YT Xw1 , and substitute into (38), we get w1 as the
first eigenvector of matrix (XT X)−1 (XT Y)(YT Y)−1 (YT X).
Similarly, we get v1 as the first eigenvector of matrix
(YT Y)−1 (YT X)(XT X)−1 (XT Y). The corresponding eigenvalues are both λ21 .
When i = 2, j = 1, by multiplying w2T and v2T on
both sides of (38) and (39), constraints w2T XT Yv1 =
0 and w1T XT Yv2 = 0 can be deduced from constraints
w2T XT Xw1 = 0 and v2T YT Yv1 = 0. So the Lagrange multipliers is
L = w2T XT Yv2 −λ2 (w2T XT Xw2 −1)−σ2 (v2T YT Yv2 −1)
− γ21 (w2T XT Xw1 ) − δ21 (v2T YT Yv1 ).

535

(40)

T

T

By multiplying w1T and v1T on both sides of (41) and
(42), respectively, we get γ21 = δ21 = 0. So constraints
w2T XT Xw1 = 0 and v2T YT Yv1 = 0 are also removed.
And we can get w2 and v2 as the second eigenvector
of matrix (XT X)−1 (XT Y)(YT Y)−1 (YT X) and (YT Y)−1
(YT X)(XT X)−1 (XT Y). Iteratively, until i = K, all 0 constraints are reduced, and all W and V are calculated as the
K eigenvectors of matrix (XT X)−1 (XT Y)(YT Y)−1 (YT X)
and (YT Y)−1 (YT X)(XT X)−1 (XT Y).
REFERENCES
[1] Y. Xie, “Surface-enhanced hyper Raman and surface-enhanced Raman
scattering: Novel substrates, surface probing molecules, and chemical
applications,” Ph.D. dissertation, Dept. Chem., Hong Kong Univ. Sci.
Technol., Hong Kong, 2007.
[2] J. R. Lombardi and R. L. Birke, “A unified approach to surface-enhanced
Raman spectroscopy,” J. Phys. Chem. C, vol. 112, pp. 5605–5617, 2008.
[3] S. E. J. Bell and N. M. S. Sirimuthu, “Quantitative surface-enhanced Raman spectroscopy,” Chem. Soc. Rev., vol. 37, pp. 1012–1024, 2008.
[4] W. Cheung, I. T. Shadi, Y. Xu, and R. Goodacre, “Quantitative analysis of
the banned food dye Sudan-1 using surface enhanced Raman scattering
with multivariate chemometrics,” J. Phys. Chem. C, vol. 114, pp. 7285–
7290, 2010.
[5] K. Lai, F. Zhai, Y. Zhang, X. Wang, B. A. Rasco, and Y. Huang, “Application of surface enhanced Raman spectroscopy for analyses of restricted
sulfa drugs,” Sens. Instrum. Food Quality Safety, vol. 5, pp. 91–96, 2011.
[6] A. D. Strickland and C. A. Batt, “Detection of carbendazim by surfaceenhanced Raman scattering using cyclodextrin inclusion complexes on
gold nanorods,” Anal. Chem., vol. 81, pp. 2895–2903, 2009.
[7] R. Stosch, A. Henrion, D. Schiel, and B. Guttler, “Surface-enhanced Raman scattering based approach for quantitative determination of creatinine
in human serum,” Anal. Chem., vol. 77, no. 22, pp. 7386–7392, 2005.
[8] R. J. Stokes, A. Macaskill, P. J. Lundahl, W. E. Smith, K. Faulds, and
D. Graham, “Quantitative enhanced Raman scattering of labeled DNA
from gold and silver nanoparticles,” Small, vol. 3, pp. 1593–1601, 2007.
[9] D. Graham and K. Faulds, “Quantitative SERRS for DNA sequence analysis,” Chem. Soc. Rev., vol. 37, pp. 1042–1051, 2008.
[10] H. Zhang, M. H. Harpster, H. J. Park, P. A. Johnson, and W. C. Wilson,
“Surface-enhanced Raman scattering detection of DNA derived from the
West Nile virus genome using magnetic capture of Raman-active gold
nanoparticles,” Anal. Chem., vol. 83, pp. 254–260, 2011.
[11] J.-H. Kim, J.-S. Kim, H. Choi, S.-M. Lee, B.-H. Jun, K.-N. Yu, E. Kuk,
Y.-K. Kim, D. H. Jeong, M.-H. Cho, and Y.-S. Lee, “Nanoparticle probes
with surface enhanced Raman spectroscopic tags for cellular cancer targeting,” Anal. Chem., vol. 78, pp. 6967–6973, 2006.
[12] S. Keren, C. Zavaleta, Z. Cheng, A. de la Zerda, O. Gheysens, and
S. S. Gambhir, “Noninvasive molecular Imaging of small living subjects
using Raman spectroscopy,” Proc. Nat. Acad. Sci. United States of America, vol. 105, pp. 5844–5849, 2008.
[13] C. L. Zavaleta, B. R. Smith, I. Walton, W. Doering, G. Davis, B. Shojaei,
M. J. Natan, and S. S. Gambhir, “Multiplexed imaging of surface enhanced
Raman scattering nanotags in living mice using noninvasive Raman spectroscopy,” Proc. Nat. Acad. Sci. United States of America, vol. 106, pp.
13 511–13 516, 2009.
[14] D. C. Kennedy, K. A. Hoop, L.-L. Tay, and J. P. Pezacki, “Development of
nanoparticle probes for multiplex SERS imaging of cell surface proteins,”
Nanoscale, vol. 2, pp. 1413–1416, 2010.
[15] J. L. Abell, J. M. Garren, J. D. Driskell, R. A. Tripp, and Y. Zhao, “Labelfree detection of micro-RNA hybridization using surface-enhanced Raman
spectroscopy and least-squares analysis,” J. Amer. Chem. Soc., vol. 134,
no. 31, pp. 12 889–12 892, 2012.
[16] C. Gobinet, V. Vrabie, M. Manfait, and O. Piot, “Preprocessing methods
of Raman spectra for source extraction on biomedical samples: application
on paraffin-embedded skin biopsies,” IEEE Trans. Biomed. Eng., vol. 56,
no. 5, pp. 1371–1382, May 2009.

536

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

[17] L. Zhang, Q. Li, W. Tao, B. Yu, and Y. Du, “Quantitative analysis of
thymine with surface-enhanced Raman spectroscopy and partial least
squares (PLS) regression,” Anal. Bioanal. Chem., vol. 398, pp. 1827–
1832, 2010.
[18] F. Gan, G. Ruan, and J. Mo, “Baseline correction by improved iterative polynomial fitting with automatic threshold,” Chemometr. Intell. Lab.
Syst., vol. 82, pp. 59–65, 2006.
[19] L. Zhang, M. J. Henson, and S. S. Sekulic, “Multivariate data analysis for
Raman imaging of a model pharmaceutical tablet,” Anal. Chim. Acta.,
vol. 545, pp. 262–278, 2005.
[20] M. J. Pelletier, “Quantitative analysis using Raman spectroscopy,” Appl.
Spec., vol. 57, pp. 20A–42A, 2003.
[21] R. G. Brereton, “Introduction to multivariate calibration in analytical
chemistry,” Analyst, vol. 125, pp. 2125–2154, 2000.
[22] G. H. Golub and C. F. V. Loan, Matrix Computations, 3rd ed. Baltimore,
MD, USA: The Johns Hopkins Univ. Press, 1996.
[23] C. M. Bishop, Pattern Recognition and Machine Learning. M. Jordan, J.
Kleinberg, and B. Scholkopf, Eds. New York, NY, USA: Springer-Verlag,
2006.
[24] A. E. Hoerl and R. W. Kennard, “Ridge regression: applications to
nonorthogonal problems,” Technometrics, vol. 12, pp. 69–82, 1970.
[25] R. Sundberg, “Small sample and selection bias effects in calibration under
latent factor regression models,” J. Chemometr., vol. 21, pp. 227–238,
2007.
[26] R. Bro, “Multivariate calibration: What is in chemometrics for the analytical chemist?” Anal. Chim. Acta, vol. 500, pp. 185–194, 2003.
[27] I. T. Jolliffe, Principal Component Analysis, 2nd ed. New York, NY,
USA: Springer-Verlag, 2002.
[28] M. K.-S. Tso, “Reduced-Rank regression and canonical analysis,” J. R.
Stat. Soc. Ser. B-Stat. Methodol., vol. 43, pp. 183–189, 1981.
[29] A. J. Burnham, R. Viveros, and J. F. MacGregor, “Frameworks for latent
variable multivariate regression,” J. Chemometr., vol. 10, pp. 31–45, 1996.
[30] K. Worsley, J. B. Poline, K. J. Friston, and A. C. Evans, “Characterizing
the response of PET and fMRI data using multivariate linear models,”
Neuroimage, vol. 6, pp. 305–319, 1997.
[31] L. Sun, S. Ji, S. Yu, and J. Ye, “On the equivalence between canonical
correlation analysis and orthonormalized partial least squares,” in Proc.
Int. Joint Conf. Artif. Intell., 2009, pp. 1230–1235.
[32] J. Arenas-Garcı́a, K. B. Petersen and L. K. Hansen, “Sparse kernel orthonormalized PLS for feature extraction in large data sets,” in In Advances
in Neural Information Processing Systems. Cambridge, MA, USA: MIT
Press, 2007.
[33] H. Wold et al., in Path Models With Latent Variables: The NIPALS
Approach, H. M. B. , Ed. New York, NY, USA: Academic, 1975.
[34] S. Wold and M. Sjöströma, L. Eriksson, “PLS-regression: a basic tool of
chemometrics,” Chemometr. Intell. Lab. Syst., vol. 58, pp. 109–130, 2001.
[35] H. Wold, “Soft modelling, the basic design and some extensions,” Syst.
Under Indirect Observ., vol. 2, pp. 589–591, 1982.
[36] A. Hoskuldsson, “PLS regression methods,” J. Chemometr., vol. 2,
pp. 211–228, 1988.
[37] J. A. Wegelin, “A survey of partial least squares (PLS) methods, with
emphasis on the two-block case,” Univ. Washington, Dept. Statist., Seattle,
WA, USA, Tech. Rep. N.371, 2000.
[38] R. Rosipal and N. Kramer, “Overview and recent advances in partial least
squares,” Lecture Notes Comput. Sci., vol. 3940, pp. 34–51, 2006.
[39] S. de Jong, “SIMPLS: An alternative approach to partial least squares
regression,” Chemometr. Intell. Lab. Syst., vol. 18, pp. 251–263, 1993.
[40] C. J. F. ter Braak and S. de Jong, “The objective function of partial least
squares regression,” J. Chemometr., vol. 12, pp. 41–54, 1998.
[41] S. Li, J. Gao, J. O. Nyagilo, and D. P. Dave, “Probabilistic partial least
square regression: A robust model for quantitative analysis of Raman
spectroscopy data,” in Proc. IEEE Int. Conf. Bioinformat. Biomed., 2011,
pp. 526–531.
[42] H. Hotelling, “Analysis of a complex of statistical variables into principal
components,” J. Educat. Psychol., vol. 24, pp. 417–441, 1933.
[43] D. R. Hardoon, S. R. Szedmak, and J. R. Shawe-Taylor, “Canonical correlation analysis: An overview with application to learning methods,”
Neural Comput., vol. 16, pp. 2639–2664, 2004.
[44] J. R. Kettenring, “Canonical analysis of several sets of variables,”
Biometrika Trust, vol. 58, pp. 433–451, 1971.
[45] C. Dhanjal, S. R. Gunn, and J. Shawe-Taylor, “Efficient sparse kernel
feature extraction based on partial least squares,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 31, no. 8, pp. 1347–1361, Aug. 2009.
[46] A. Kembhavi, D. Harwood, and L. S. Davis, “Vehicle detection using
partial least squares,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33,
no. 6, pp. 1250–1265, Jun. 2011.

[47] A. Sharma and D. W. Jacobs, “Bypassing synthesis: PLS for face recognition with pose, low-resolution and sketch,” in Proc. IEEE Conf. Comput.
Vis. Pattern Recognit., 2011, pp. 593–600.
[48] A. Tishler, D. Dvir, A. Shenhar, and S. Lipovetsky, “Identifying critical
success factors in defense development projects: A multivariate analysis,”
Technol. Forecast. Soc. Change, vol. 51, pp. 151–171, 1996.
[49] A. Tishlera and S. Lipovetsky, “Modelling and forecasting with robust
canonical analysis: method and application,” Comput. Oper. Res., vol. 27,
pp. 217–232, 2000.

Shuo Li received the B.S. degree in software engineering and the M.S. degree in computer science,
both from Sichuan University, Sichuan, China, in
2006 and 2009, respectively. He is currently working
toward the Ph.D. degree in the Department of Computer Science and Engineering, University of Texas
at Arlington, Arlington, TX, USA.
His research interests include machine learning,
image and signal processing, with a particular emphasis on their joint applications in biomedical and
bioinformatics problem.

James O. Nyagilo received the B.S. degree in physics
from the Jomo Kenyatta University of Agriculture
and Technology, Nairobi, Kenya, in 2005. After a
14-month stint at the National Radiation Protection
Laboratories in Kenya, he joined the University of
Texas at Arlington, Arlington, TX, USA, where he
received the M.S. and Ph.D. degrees in biomedical
engineering, in 2008 and 2012, respectively.
In his graduate studies, he was involved in
developing surface enhanced raman spectroscopy
nanoprobes that can be used in detection of cancer
cells in vitro and in vivo. He is currently a Faculty Research Associate under
Dr. Digant Dave at the University of Texas at Arlington.

Digant P. Dave (M’95) received the B.S. degree from
the Delhi College of Engineering, Delhi, India, in
1988, and the M.S. and Ph.D. degrees in physics
from the Texas A&M University, College Station,
TX, USA, in 1990 and 1994, respectively.
He is currently an Associate Professor with
the Bioengineering Department at the University of
Texas at Arlington, Arlington, TX. His research interests include biomedical optical imaging system and
medical application of lasers.

Jean Gao (M’03) received the B.S. degree in biomedical engineering from the Shanghai Medical University, Shanghai, China, the M.S. degree in biomedical
engineering from the Rose–Hulman Institute of Technology, Terre Haute, IN, USA, and the Ph.D. degree
in electrical and computer engineering from the Purdue University, West Lafayette, IN.
She is currently an Associate Professor in the Department of Computer Science and Engineering, University of Texas at Arlington, Arlington, TX, USA.
Her research interests include machine learning, pattern recognition, and data mining with specific applications to biomedical informatics and bioimage processing.

