2290

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Spatiotemporal Representations of Rapid Visual
Target Detection: A Single-Trial EEG
Classification Algorithm
Galit Fuhrmann Alpert∗,# , Ran Manor# , Assaf B. Spanier, Leon Y. Deouell, and Amir B. Geva

Abstract—Brain computer interface applications, developed for
both healthy and clinical populations, critically depend on decoding brain activity in single trials. The goal of the present study was
to detect distinctive spatiotemporal brain patterns within a set of
event related responses. We introduce a novel classification algorithm, the spatially weighted FLD-PCA (SWFP), which is based
on a two-step linear classification of event-related responses, using
fisher linear discriminant (FLD) classifier and principal component analysis (PCA) for dimensionality reduction. As a benchmark
algorithm, we consider the hierarchical discriminant component
Analysis (HDCA), introduced by Parra, et al. 2007. We also consider a modified version of the HDCA, namely the hierarchical
discriminant principal component analysis algorithm (HDPCA).
We compare single-trial classification accuracies of all the three
algorithms, each applied to detect target images within a rapid
serial visual presentation (RSVP, 10 Hz) of images from five different object categories, based on single-trial brain responses. We
find a systematic superiority of our classification algorithm in the
tested paradigm. Additionally, HDPCA significantly increases classification accuracies compared to the HDCA. Finally, we show that
presenting several repetitions of the same image exemplars improve accuracy, and thus may be important in cases where high
accuracy is crucial.
Index Terms—Brain computer interface (BCI), classification,
electroencephalography (EEG), rapid serial visual presentation
(RSVP).

I. INTRODUCTION

R

ECENT advances in neuroscience have led to an emerging interest in brain computer interface (BCI) applications

Manuscript received November 24, 2012; revised October 12, 2013; accepted
October 23, 2013. Date of publication November 7, 2013; date of current version
July 15, 2014. This work was supported by a grant from the Israeli Defense Ministry. Asterisk indicates corresponding author. # Authors contributed equally.
∗ G. Fuhrmann Alpert is with the Department of Psychology, The Hebrew
University of Jerusalem, Jerusalem 91905, Israel. (e-mail: Galit.Fuhrmann@
mail.huji.ac.il.).
R. Manor and A. B. Geva are with the Department of Electrical and Computer
Engineering, Ben-Gurion University of the Negev, Beer-Sheva 84105, Israel.
(e-mail: ran.manor@gmail.com; geva@ee.bgu.ac.il.).
L. Y. Deouell is with the Edmond and Lily Safra Center for Brain Sciences,
The Hebrew University, Jerusalem 91905, Israel, and also with the Department
of Psychology, The Hebrew University of Jerusalem, Jerusalem 91905, Israel.
(e-mail: msleon@mscc.huji.ac.il.).
A. B. Spanier is with The Selim and Rachel Benin School of Engineering, The Hebrew University, Jerusalem 91905, Israel. (e-mail: assaf.spanier@
mail.huji.ac.il.).
This paper has supplementary downloadable material available at
http://ieeexplore.ieee.org. (File size: 1 MB).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2013.2289898

for both disabled and healthy populations. These applications
critically depend on online decoding of brain activity, in response to single events (trials), as opposed to delineation of
the average response frequently studied in basic research. Electroencephalography (EEG), a noninvasive recording technique,
is one of the commonly used systems for monitoring brain activity. EEG data is simultaneously collected from a multitude
of channels at a high temporal resolution, yielding high dimensional data matrices for the representation of single-trial brain
activity. In addition to its unsurpassed temporal resolution, EEG
is noninvasive, wearable, and more affordable than other neuroimaging techniques, and is thus a prime choice for any type of
practical BCI. The two other noninvasive technologies used for
decoding brain activity, namely functional MRI and MEG, require cumbersome, expensive, and nonmobile instrumentation,
and although they maintain their position as highly valuable research tools, they are unlikely to be useful for routine use of
BCIs. Invasive imaging methods, such as electrocorticography
(ECoG; intracranial EEG), also exist. These methods provide
greater signal to noise ratio and increased spatial resolution
compared to EEG and may prove to be of great importance for
specific patient populations. Obviously, these methods are not
applicable for healthy populations, as is EEG.
Traditionally, EEG data has been averaged over trials to characterize task-related brain responses despite the on-going, task
independent noise present in single-trial data. However, in order to allow flexible real-time feedback or interaction, taskrelated brain responses need to be identified in single trials, and
categorized into the associated brain states. Most classification
methods use machine-learning algorithms to classify single-trial
spatiotemporal activity matrices based on statistical properties
of those matrices [1]–[5]. These methods are based on two main
components: a feature extraction mechanism for effective dimensionality reduction, and a classification algorithm.
Typical classifiers use a sample data to learn a mapping rule
by which other test data can be classified into one of two or more
categories. Classifiers can be roughly divided to linear and nonlinear methods. Nonlinear classifiers, such as neural networks,
hidden Markov model, and k-nearest neighbor [1], [5], can approximate a wide range of functions, allowing discrimination of
complex data structures. While nonlinear classifiers have the potential to capture complex discriminative functions, their complexity can also cause overfitting and carry heavy computational
demands, making them less suitable for real-time applications.
Linear classifiers, on the other hand, are less complex and are
thus more robust to data overfitting. Naturally, linear classifiers

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

perform particularly well on data that can be linearly separated.
Fisher linear discriminant (FLD), linear support vector machine
(SVM), and logistic regression (LR) are popular examples [3],
[6]–[9]. FLD finds a linear combination of features that maps
the data of two classes onto a separable projection axis. The
criterion for separation is defined as the ratio of the squared
distance between the classes mean to the variance within the
classes. SVM finds a separating hyper-plane that maximizes
the margin between the two classes. LR, as its name suggests,
projects the data onto a logistic function. All linear classifiers
offer fast solution for data discrimination, and are thus most
commonly applied in classification algorithms used for realtime BCI applications.
Whether linear or nonlinear, most classifiers require a prior
stage of feature extraction. Selecting these features has become
a crucial issue, as one of the main challenges in deciphering
brain activity from single-trial data matrices is the high dimensional space in which they are embedded, and the relatively
small sample sizes the classifiers can rely on in their learning stage. Feature extraction is in essence a dimensionality reduction procedure mapping the original data onto a lower dimensional space. A successful feature extraction procedure will
pull out task-relevant information and attenuate irrelevant information. Some feature extraction approaches use prior knowledge, such as specific frequency-bands relevant to the experiment (e.g., 5–15 Hz for motor imagery experiment [2]) or brain
locations most likely to be involved in the specific classification problem. For instance, the literature has robustly pointed
out parietal scalp regions to display high amplitude signals in
target detection paradigms; this target-related response, maximal at parietal regions and known as the P300 component, has
been repeatedly observed approximately 300–500 ms poststimulus [10]. Such prior-knowledge based algorithms, in particular
P300 based systems, are commonly used for a variety of BCI
applications [11]–[14]. In contrast, other methods construct an
automatic process to pull out relevant features based on supervised or unsupervised learning from training data sets.
Some approaches for automatic feature extraction include
common spatial patterns (CSP), autoregressive models (AR),
and principal component analysis (PCA). CSP extracts spatial
weights to discriminate between two classes [3], by maximizing
the variance of one class while minimizing the variance of the
second class. AR is mostly used to model temporal correlations
in a signal, although spatial autoregressive models (SAR) also
exist. Discriminative AR coefficients can be selected using a
linear classifier [2]. Other methods search for spectral features
to be used for classification [4]. PCA is used for unsupervised
feature extraction, by mapping the data onto a new, uncorrelated
space where the axes are ordered by the variance of the projected
data samples along the axes, and only those axes reflecting most
of the variance are maintained. The result is a new representation of the data that retains maximal information about the
original data yet provides effective dimensionality reduction.
PCA is used in the current study and is further elaborated in the
following sections.
Such methodologies of single-trial EEG classification algorithms have been implemented for a variety of BCI applica-

2291

tions, using different experimental paradigms. Most commonly,
single-trial EEG classification has been used for movementbased and P300 based- applications [11], [14]. Movement tasks,
both imaginary and real, have been studied for their potential
use with disabled subjects [15]. P300 applications, based on
visual or auditory oddball experiments [11], originally aimed
at providing BCI-based communication devices for locked-in
patients [9], [12], [16] and can also be used for a variety of applications for healthy individuals [17]. Emotion assessment, for
example, attempts to classify emotions to categories (negative,
positive, and neutral) using a combination of EEG and other
physiological signals [18], offering a potential tool for behavior
prediction and monitoring.
Here, we aim at implementing a BCI framework in order to
sort large image databases into one of two categories (target
images; nontargets). We use EEG patterns as markers for targetimage appearance during rapid visual presentation [6]–[8], [19].
Subjects are instructed to search for target images (a given category out of five) within a rapid serial visual presentation (RSVP;
10 Hz). In this case, the methodological goal of the classification
algorithm is to automatically identify, within a set of event related responses, single-trial spatiotemporal brain responses that
are associated with the target-image detection. In addition to
the common challenges faced by single-trial classification algorithms for noisy EEG data, specific challenges are introduced
by the RSVP task, due to the fast presentation of stimuli and the
ensuing overlap between consecutive event-related responses.
Some methods have thus been constructed specifically for the
RSVP task.
One such method, developed by Bigdley et al. [6] specifically for single-trial classification of RSVP data, used spatial
independent component analysis (ICA) to extract a set of spatial weights and obtain maximally independent spatialtemporal
sources. A parallel ICA step was performed in the frequency domain to learn spectral weights for independent time-frequency
components. PCA was used separately on the spatial and spectral sources to reduce the dimensionality of the data. Each feature set was classified separately using FLDs and then combined using naive Bayes fusion (i.e., multiplication of posterior
probabilities).
A more general framework was proposed by Parra et al. [19]
for single-trial classification, and was also implemented specifically for the RSVP task. The suggested framework uses a bilinear spatialtemporal projection of event-related data on both
the temporal and spatial axes. These projections can be implemented in many ways. The spatial projection can be implemented, for example, as a linear transformation of EEG scalp
recordings into underlying source space [20] or as ICA. The
temporal projection can be thought of as a filter. The dual projections are implemented on nonoverlapping time windows of
the single-trial data matrix, resulting in a scalar representing
a score per window. The windows scores are summed [19] or
classified [7] to provide a classification score for the entire single trial. In addition to the choice of projections, this framework
can support additional constraints on the structure of the projections matrix. One option is, for example, to learn the optimal
time window for each channel separately and then train the

2292

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

spatial terms [21]. Another alternative is to learn the spatial and
temporal weights simultaneously under different smoothness
constraints [22]. Other variations of projections and classifiers
may also be considered under this framework [17].
Within a similar framework, we present here a novel two-step
classification algorithm. We compare classification performance
of our algorithm to that of the basic algorithm suggested by
Parra et al. [19], as well as to a modified version of it, based on
Gerson et al. [7]. We also present the spatiotemporal distribution
of the discriminative activity, which may indicate the underlying
neural networks involved.
II. MATERIALS AND METHODS
A. Subject
Fifteen subjects participated in the main experiment (Exp1;
eight females and seven males, mean age 26 years, standard
deviation five years). Three subjects were excluded from the
analysis due to excessive recording noise. The first excluded
subject had excessive eye blink artifacts resulting in a loss of
nearly 60% of the data. The second subject had technical problems with the electrodes causing repeated recording failures.
The third excluded subject felt very uncomfortable, which lead
to many motion artifacts, thus the subject was released half way
through the experiment.
Four subjects participated in a second experiment (Exp2; two
females; two males, mean age 23, standard deviation two years).
All subjects were students of the Hebrew University of
Jerusalem, without any previous training in the task. All subjects
had normal or corrected to normal vision, with no neurological
problems, and were free of psychoactive medications at the time
of the experiment. Subjects were paid for their participation. The
experiment was approved by the local ethics committee at the
Hebrew university of Jerusalem.
B. Stimuli
Stimuli were 360 × 360 pixels (6.5 × 6.5 of visual angle at
a distance of 100 cm) grayscale images of five different categories including 145 exemplars each of faces, cars, painted
eggs, watches, and planes, presented at the center of a CRT
monitor (Viewsonic model g57f, refresh rate 100 Hz, resolution
1024 × 768) on a gray background (see Fig. 1). The images were
preprocessed to have the same mean luminance and contrast.
C. Experimental Procedure
Subjects were seated in a dimly lit, sound attenuated chamber,
supported by a chin and forehead rest. Subjects were instructed
to count the occurrence of images of a predefined target category,
presented within a RSVP. Each image exemplar was presented
several times during the experiment. Eye position was monitored
using an Eyelink 2k/1000 eye tracker (SR research, Kanata, ON,
Canada) at 1000-Hz resolution. Presentation was briefly paused
every 80–120 trials and the subject was asked to report how
many targets appeared in the last run, and thereafter restart the
count. This was done to avoid the working memory load of
accumulating large numbers.

Fig. 1. Experimental Paradigm: Images of five different categories (Cars;
Faces; Planes; Clock faces; Eggs) are presented every 90–110 ms. In each
presentation block, a different object category is defined as the Target category,
and subjects are instructed to count the number of image occurrences of the
target category (e.g., planes, marked here by arrows).

Two experiments were conducted. The main experiment consisted of five categories of images (cars, painted eggs, faces,
planes, or clock faces). Images were presented in four blocks,
with a different target category in each block (clock faces were
not used as targets). The order of blocks was counterbalanced
across subjects. Each block consisted of an RSVP of 6525 images, presented without interstimulus intervals every 90–110 ms
rates (i.e.,˜10 Hz). In each block, 20% of the images were targets, randomly distributed within the block. The experimental
paradigm is depicted in Fig. 1.
The second experiment (Experiment 2) consisted of only two
image categories. In this task, the targets were always images
of cars, and the nontarget images were noise images produced
as scrambled images of the same car images. We ran this significantly easier, pop-out detection task, in order to compare performance to similar studies reported in the literature (e.g., [7],
[19]).
D. Data Collection and Preprocessing
EEG recordings were acquired by an Active 2 system
(BioSemi, the Netherlands) using 64 sintered Ag/AgCl electrodes, at a sampling rate of 256 Hz with an online low-pass
filter of 51 Hz to prevent aliasing of high frequencies. Seven
additional electrodes were placed as follows: two on the mastoid processes, two horizontal EOG channels positioned at the
outer canthi of the left and right eyes (HEOGL and HEOGR,
respectively), two vertical EOG channels, one below (infraorbital, VEOGI), and one above (supraorbital, VEOGS) the right
eye, and a channel on the tip of the nose. All electrodes were
referenced to the average of the entire electrode set, excluding the EOG channels. Offline, a bipolar vertical EOG (VEOG)
channel was calculated as the difference between VEOGS and
VEOGI. Similarly, a bipolar horizontal EOG channel (HEOG)
was calculated as the difference between HEOGL and HEOGR.
A high-pass filter of 0.1 Hz was used offline to remove slow
drifts. The data was segmented to one-second event-related segments starting 100 ms prior to and ending 900 ms after the onset
of each image presentation, yielding, for each subject, a large
spatiotemporal data matrices for the representation of singletrial brain activity. Each single-trial matrix consisted of 64 rows
of channels and 256 columns of time samples. Baseline correction was performed by subtracting the mean activity averaged

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

over 100 ms prior to stimulus onset for each trial and channel
independently. Blinks were removed by rejecting trials in which
the VEOG bipolar channel exceeded ±100 μV. The same criterion was also applied to all other channels to reject occasional
recording artifacts and gross eye movements.
E. Classification Algorithms
We consider three different algorithms for single-trial EEG
classification. Each of the methods will be explained below in
detail.
In all cases, we represent the nth single-trial data by the
spatiotemporal activity matrix Xn of size D × T , containing
raw event-related signals recorded from all EEG channels at all
time points, locked to the onset of an image presentation. D is the
number of EEG channels, and T is the number of time points per
trial. yn = f (Xn ) is the binary decision (target/nontarget trial)
of the classification algorithm for the single-trial spatiotemporal
data matrix Xn .
We use several measures of performance. First, random subsampling cross-validation is used: a random 80% of the trials
are used for training and the remaining 20% of trials are used for
testing. This procedure is repeated 30 times. The classification
performance is measured by the percent correct classification of
single trials in the test group, as well as by the hit rate (percent
target trials which were correctly classified as targets), false
alarms (percent nontarget trials which were falsely classified
as targets), and the corresponding d measure of discrimination
of target images (d = Z(hit rate)−Z(false alarm rate) where
Z is the inverse of the cumulative Gaussian distribution [23]).
To systematically compare between the different classification
methods considered in this study, we run the three algorithms
on the exact same train/test permutations.
We also consider another performance measure implementing a variation of the leave-one-out procedure: the classifier
is trained using the complete data set excluding all instances
of one image exemplar (e.g., image of a specific face; recall
that each image is presented several times per experimental
block). We then test our classifier on each of the repetitions of
the excluded image exemplar. The final label of the exemplar
(Target/Nontarget) is decided by majority vote of the repetitions’ label. For example, in the case of Experiment 1–each of
the four experimental blocks consisted of up to 11 presentations/repetitions of the same image exemplar. Some trials were
removed due to artifact rejection preprocessing, leaving N trials available for analysis. For each available image exemplar
presentation trial, we compute the classifier‘s target/nontarget
decision and the accuracy is defined as the number of trials in
which the classifier provided the correct response, divided by
the total number of available trials (N ). Total performance for
N trials is computed per subject as the mean correct labeling
over all available trials for a single image exemplar, averaged
over all image exemplars.
Additionally, to test performance as a function of the number
of repetitions used for the leave-one-out voting, we compute
performance following the same procedure, using only subsets
of N trial repetitions for voting per image. For each considered

2293

N , performance per image is computed as the average accuracy
for voting between 30 possible subsets of N , randomly chosen
out of all combinations.
For example, given an image exemplar that was presented
a total of nine repetitions throughout the experiment, there are
126 possible combinations to choose subsets of N = 5 trial
repetitions. Only 30 of those combinations, randomly chosen,
are used to compute performance. Total performance for N
repetitions is computed per subject as the mean performance
per image, averaged over all individual image exemplars.
We turn now to describe the details of each of the three tested
algorithms. We start with our new algorithm, and then describe
the comparison algorithms.
Note that all three algorithms start with a first step of computing spatial (channel) weights by applying linear classifiers to
the data. In algorithm I, this is performed over the whole time
series, while in algorithms 2 and 3, spatial weights are computed
after splitting the time series into independent time windows.
The main difference between the algorithms is then in the way
this information is being used in the next steps.
1) Algorithm I [Spatially Weighted FLD–PCA (SWFP)]:
Our algorithm is based on a two-step linear classification. In
both classification steps, we use FLD analysis [24].
1) Step I:
a) Classify time points independently to compute a spatiotemporal matrix of discriminating weights (U).
To implement this, take each column vector xn ,t of
the input matrix Xn ; Each column represents the
spatial distribution of EEG signals at time t, and at
this step of analysis all time points are treated independently. Train a separate FLD classifier for each
time point t = 1. . .T , based on all n = 1. . .N trials
in the training set, to obtain a spatial weight vector
wt for each time point t. Set these weights vectors
as the columns of the spatiotemporal weighting matrix U. The dimensions of U are the same as the
dimensions of X.
b) Use this weighting matrix U to amplify the original
spatiotemporal data matrix (Xn ) by the discriminating weights at each spatiotemporal point, creating the spatially-weighted matrix Xw
n . To implement
this amplification, compute the Hadamard product
of the trial input matrix Xn and the weighting matrix U, by element-wise multiplication of the two
matrices (MATLAB notation.*).
Xw
n = U. ∗ Xn
(Xw
n )t,d = (U)t,d · (Xn )t,d
t = 1. . .T, d = 1. . .D.

(1)

c) For dimensionality reduction of Xw
n , use PCA on
the temporal domain—for each spatial channel d
independently—to represent the time series data
as a linear combination of only K components.
PCA is applied independently on each row vector
xd , d = 1. . .D of the spatially weighted matrix Xw
n,

2294

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

following mean subtraction. For each row d, this provides a projection matrix Ad of size T × K, which
is used to project the time series data of channel
d on the first K principal components; thus reducing dimensions from T to K per channel. K = 6
was empirically chosen to explain >70% variance
in Experiment 1. The resulting matrix X̂n is of size
D × K, where each row d holds the PCA coefficients for the K principal temporal projections.
x̂dn = xd Ad .

(2)

2) Step II:
a) Concatenate the rows of the matrix X̂n to create
a feature representation vector zn , representing the
temporally approximated, spatially weighted activity of the single trial n.
zn = [x̂1n . . . x̂D
n ].

(3)

b) Train and run an FLD classifier on the feature vectors {zn }N
n =1 to classify the single-trial matrices Xn
into one of two classes (using zero as the decision
boundary).
yn = f (zn ).

(4)

We define the most discriminative response latency as the latency t for which the highest percent correct classification was
achieved in step 1.a. The associated discriminative spatial activation pattern is given by U(t).
2) Algorithm II [Hierarchical Discriminant Component
Analysis (HDCA)]: To evaluate the performance of our algorithms compared to existing methods, we implemented the
HDCA classification algorithm as introduced by Parra et al [7],
[19]. The algorithm is implemented here without any changes,
and used as a benchmark algorithm for comparing performance.
The algorithm uses the following model:
yn =

K


uTk Xn vk

(5)

k =1

where uk and vk are, respectively, the spatial and temporal projections vectors for time window k out of the total K nonoverlapping time windows of the single trial. These projections can
be resolved in several ways, such as ICA, temporal filtering,
and source space transformation. Specifically, the projections
vectors could be learned using classification, separately for the
spatial and temporal domains, to find the most discriminative
projection vectors [19]. The algorithm is described in the following steps (for details, see original paper [7], [19]):
1) Choose a set of K nonoverlapping time windows. We
define Xk ,n as the time samples of Xn which belong to
window k, i.e., the cropped time series that includes only
time points within the window k.
In the current study, we used a window size of 100 ms as
suggested in [19], thus K was set to 9.

For each time, window k independently:
a) Train a FLD classifier to compute a spatial projection
vector uk , based on the data from all training trials,
taken within the window (Xk ,n ).
b) Use the spatial projection vector to map the window
matrix from all channels onto a single global signal.
sk ,n = uTk Xk ,n .

(6)

c) Sum the elements of each temporal vector sk ,n to
a single scalar value representing a global signal of
time window k

rk ,n =
(sk ,n )i
(7)
i

2) Concatenate the K scalar representations of the global
signals from all time windows {rk ,n } into a single combined global signal rn , representing the single-trial signals
from all channels at all time points. The vector rn can be
thought of as a feature representation of the single-trial
data matrix Xn , where each time-windowed global signal
rk ,n is a feature.
rn = [r1,n · · · rK ,n ].

(8)

3) Use a linear classifier to classify the set of {rn } feature
vectors:
zn = cT rn =

K


ck rk ,n

(9)

k =1

{ck } are the weights per window k, (cT as the concatenated form) assigned by the classifier, to compute the
scalar score zn of the single trial. yn is the binary decision
using a zero threshold over zn .
3) Algorithm III [Hierarchical Discriminant Principal Component Analysis (HDPCA)]: We also consider a version of
HDCA (Algorithm II above; [7], [19]) with two in-house modifications, which we predicted will improve performance. After
using the spatial projection vector (in (6)), we apply PCA on the
temporal axis for dimensionality reduction. The data is projected
onto the first principal components that together explain 99%
of the variance. This first modification leaves out components
with near zero variance not contributing significant information, making sure that discriminative information is not lost.
The second modification is that instead of summing temporal
representations (PCA coefficients in this case), we classify them
using another FLD classifier, which is trained separately on the
data. This step selects the best PCA components for discrimination, as has been suggested in [19]. Overall, the modified
algorithm is the same as Algorithm II described above, for step
1-2a, with the following steps replacing steps 2b–2c:
2) b) Project the single-trial matrix Xk ,n onto a single representative global temporal signal, using the spatial projection vector uk , as described in Algorithm II.
c) Apply PCA on the global temporal signals to extract the
first Ccom p principal components, which together explain

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

2295

represents a single channel. Note that on an average, despite
the rapid sequence of events and the overlapping responses,
the main divergence between the target and nontarget ERPs
occurs between 200–600 ms postimage presentation, in line
with the N2—P3 target detection literature [10]. The same can
be observed with single-trial responses (see Fig. 2 Bottom),
though evidently the responses are more variable, setting the
challenge of detecting target related activity in single trials.
B. Single-Trial Classification Analysis
and Summary of Performance

Fig. 2. Top: Mean ERPs to targets (red) and nontarget (blue) images, at each
of the 64 recorded channels (one line per channel; taken from a single sample
subject 504). Green: Channel CPz (solid: target; dashed: nontarget). Bottom:
Single-trial ERPs to target (Left) and nontarget (Right) images, recorded at CPz.
(traces shown after baseline correction). The vertical black line marks the time
of image onset.

99% of the variance. The result of the PCA is a projection
matrix Ak of size T × Ccom p .
T
k
xpn ca
,k = uk Xn A .

(10)

(xpn ca
,k )

d) Use the PCA coefficients
as features vectors for
a linear classifier. The classifier training gives a weight
vector vk which discriminates the PCA coefficients to
target/nontarget samples.
rn ,k = xpn ca
,k vk .

(11)

Perform steps 3, 4 as in Algorithm II.
To summarize, all three algorithms start with a first step of linear classification, which is used to determine the discriminationbased spatial projection of the data. Yet, while HDCA and its
modified version (HDPCA) use the spatial projection vector
(uk ) to collapse data from all channel locations onto a single
representative global channel (sk ) prior to temporal manipulation of the data, SWFP uses the concatenated information from
all channels. Moreover, SWFP amplifies the original data matrix by the spatiotemporal discriminating weights prior to PCA
dimensionality reduction; channel and times that are most important for classification are given even greater impact further
in the process.
III. RESULTS
A. Event Related Responses (Targets versus Non-Targets)
Due to low signal to noise ratios (SNRs) of EEG data, the
standard approach to analyzing event-related responses is to
study the mean event related potential (ERP), averaged over
repeated trials of the same stimulus condition. Fig. 2 (Top)
depicts a ’butterfly plot’ of ERPs elicited by the target (Red)
and nontarget (blue) ERPs, collapsed over blocks of the main
paradigm (Experiment 1) for a single sample subject. Each line

Fig. 3(a) shows the receiver operating curves (ROC) and area
under curve (AUC) values for all subjects using the SWFP algorithm. While the classification accuracy varies across subjects,
even the worst case is far from the chance performance denoted
by the diagonal. Fig. 3(b) summarizes the comparison of performance between the three single-trial classification algorithms
for Experiment 1 (trials from all blocks collapsed for analysis,
regardless of the different targets). Across subjects, the proposed
SWFP algorithm correctly classified between 66–82% (mean:
72.89; std: 4.74) of trials. For comparison, the performance
of the HDCA (Algorithm II) ranged between 57–70% correct
(mean: 62.37%; std: 3.41%) and the performance of HDPCA
(Algorithm III) ranged between 66%–81% (mean: 71.58; std:
4.68%). In comparison with the other algorithms considered
in this study, we consistently find an increased performance of
our proposed SWFP algorithm, compared to both the benchmark HDCA algorithm and the modified version of it (HDPCA;
Algorithm III). Indeed, nonparametric Wilcoxon signed rank
tests (p < 0.05) show that the SWFP does significantly better
than the HDCA benchmark algorithm in terms of highercorrect
classificationrate, increased hit rate, and decreased false alarms.
We also find that our small modification to the HDCA algorithm
(HDPCA; Algorithm III) boosts performance almost to the level
of the SWFP. Specifically, hit rates are not found to be significantly different between the two methods. Yet, HDPCA has a
higher false alarms rate compared to the SWFP. Congruently,
we find higher d’ values (combining hit rates and false alarms)
using SWFP compared to both HDCA and HDPCA algorithms,
and significantly smaller d’ values for the benchmark HDCA
method, compared to its modified version HDPCA.
While the SWFP out-performed the two other algorithms,
we note that the classification performance on our experimental paradigm was not as high as that previously reported using
the benchmark HDCA algorithm of Parra et al. (Algorithm II;
[19], e.g., [7]). This might be due to the harder perceptual
task we used, with widely varying stimuli and changing targets across blocks, compared to simpler target detection of targets versus backgrounds, which is kind of a pop-out perceptual
phenomenon. Thus, we also tested classification performance
of all three algorithms on an easier task in which targets were
defined as images of a single category (Cars), and nontargets
were scrambled noise images (Experiment 2). In this case, target images were significantly easier to perceptually detect than
in our main paradigm, as targets visually pop-out. Classification
performance (N = 4) for this task is summarized in Table I.

2296

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Fig. 3. Top left: ROC curves for the SWFP algorithm. One line per subject. Continuous line: Experiment 1 subjects. Dashed line: Experiment 2. Green dots
indicate False alarm and Hit rates, as calculated for the actual threshold in use. Calculated AUC per subject are indicated on the right. The remaining figures show
performance per subject for each of the considered algorithms—SWPF, HDCA, and HDPCA in Experiment 1. Top right: Percent correct classification. Bottom
left: Hit Rate; Bottom right: False Alarm Rate. Data is based on all recording blocks per subject. Error bars indicate standard deviations across 30 permutations.
TABLE I
PERFORMANCE COMPARISON ON EXPERIMENT 2

Results indicate that in this experiment too, as in Experiment 1,
performance of both SWFP and HDPCA, was increased compared to the benchmark HDCA algorithm by an average of near
15% correct classification.
The following analyses used the SWFP algorithm.
C. Spatiotemporal Maps of Discriminating Brain Responses
Using the SWFP classifier allowed us not only to label each
trial, but also to investigate networks most involved in the target
detection task, by observing the spatiotemporal distribution of
the most discriminative activity.
To this end, we study the performance as a function of time
poststimulus presentation as computed at Step 1a of SWFP (see
Methods). The temporal dependence is shown for a sample subject in Fig. 4. For each train/test permutation, performance is presented as a function of time postimage presentation. Clearly, the
highest classification accuracy is achieved around 250–450 ms,
suggesting that brain-recorded activity at these latencies is the
most informative about the original label of the image (nontar-

Fig. 4. Temporal dependence of SWFP classification accuracy for a sample
subject. Top: Percent correct (Left), Hit Rate (middle), False Alarms (Right).
Performance as a function of time from image presentation (time 0), for all
30 cross-validation train/test permutations. Each row represents a single permutation; temporal dependence is computed at Step 1a of SWFP (see Methods). Bottom: Mean temporal performance, computed as the average of all 30
permutations–Red: % correct; Blue: Hit Rate, Black: FA (sample subject 504).

get/target). This is observed both as increased hit rate for target
images and reduced false alarms for nontarget images. Discriminating activity decays back to baseline values around 750 ms
postimage presentation.

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

2297

Fig. 5. Temporal dependence of SWFP classification accuracy. Each color-coded image represents a different subject. Each row in an image represents a single
cross-validation permutation for that subject; color-coded is percent correct. For each permutation, highly discriminative latencies are those at which percent
correct classification is high (red).

Fig. 5 shows the temporal dependence of correct classification for all subjects at each cross-validation permutation. Note
that at different permutations, temporal dependence may vary to
some extent, but there is a high degree of consistency within subject, across cross validation permutations. The specific pattern
of temporal dependence of performance varies across subjects
however, highlighting the somewhat idiosyncratic yet stable pattern of brain responses that usually escapes notice when grand
averages are used.
For each cross-validation permutation, we defined the most
discriminative response latency (Tb est ) as the poststimulus latency at which the highest percent correct classification is
achieved. Fig. 6 shows the probability distributions for best
latencies, calculated for all cross-validation permutations. Evidently, different subjects have different preferred latencies for
target/nontarget discrimination, but almost all are around 300–
500 ms postimage presentation (see also Fig. 5), roughly overlapping the latencies of the classic N2–P3 complex, observed in
mean ERP analyses.
The nature of the SWFP algorithm allows us also to investigate the spatial topography of targets/nontarget discriminating patterns, as depicted by the classifier’s weights at each
scalp location. As described in step 1a of the SWFP algorithm,
the spatial distribution of theclassification weights at time t
are given by U (t). Note that these spatial maps represent the
classifier’s optimal spatial filters, rather than directly depicting
the underlying discriminative brain patterns [25], [26]. Therefore, nonzero weights are not only the result of discriminative
brain activity but could also be affected by nondiscriminative
noise reduction effects on the spatial filter/weights of the trained
classifier.
Fig. 7 depicts, for a sample subject, the mean spatial topography of target/nontarget classification weights, averaged over all

Fig. 6. Probability distributions of best latencies of target/nontarget discrimination, evaluated for each subject by different permutation of train/test cross
validation.

cross-validation permutations, at different time points poststimulus presentation. It is clear that dominant classification weights
build up towards 300-ms poststimulus presentation, and that for
this subject classifications are maximal around CPz, over the
central–parietal area.
Spatial weights were quite stable across permutations.
Fig. 8 shows the spatial mapping of the classification weights
(U (Tb est )), at the best latencies (Tb est ), as determined at individual cross-validation permutations. For each single permutation,
a circle is drawn at each electrode location, with the diameter

2298

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Fig. 7. Topography of target/nontarget classification weights (U (t)) at different time points postimage presentation for sample subject 504. Color indicates the
value of a weight, as determined in step I of SWFP. Dark colors (Red/Blue) correspond to high classification weights. The maps were averaged across permutations.
Weights are normalized by the norm of the mean of the weights across all time samples. The green rectangle highlights times surrounding the subject’s best latency
for target/nontarget discrimination (285ms).

of the circle reflecting the discrimination weight at that location. Thus, if the weights are similar across permutations, the
circles around an electrode should overlap and the line should
be thin. In contrast, variance across permutations would show
up as concentric rings or as a thicker line. It is clear that while
there are some differences of spatial weights at different permutations, they are relatively small. That is, spatial locations with
large classification weights are consistently large at the different
permutations.
For further analysis, we therefore investigate the corresponding mean spatial distribution of classification weights, averaged
at the subject‘s best latency median across validation permutaed
).
tions (Tbmest
Fig. 9 summarizes the spatial distribution of dominant classificationweights for all subjects, each at their best latency for
ed
)). While peak classifitarget/nontarget classification(U (Tbmest
cationweights at best latencies tend to be at central, parietal or
occipital electrodes, the exact location of the classificationsignal

is subject specific. In few subjects, dominant weights were found
to be less localized and to involve also frontal regions.
D. Image Repetitions Improve Classification Performance
In an attempt to increase single image classification accuracy,
we tested whether multiple presentations of the same image exemplar improves performance, using a leave-one-out procedure
(see Methods). For each image exemplar in its turn, the classifier
is trained based on trials from all experimental blocks, excluding trials of the considered image exemplar. The classifier is
then tested on each of the repetitions of the excluded image
exemplar and each repetition is thus independently classified
as target or nonTarget. The final label of the image exemplar
(target/nontarget) is decided by majority vote of the repetitions’
labels.
In the specific experiment considered here, the same image exemplar could appear in different blocks as either a target/nontarget stimulus (e.g., face images were targets in a

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

2299

Fig. 8. Spatial distribution maps of classification weights at best latency of target/nontarget discrimination (sample subject 504) at the best latency of different
cross validation permutations. Circle sizes represent the weight for target/nontarget discrimination at each spatial location (in absolute values). Large circles
indicate spatial locations that are important for classification. At each location, there are 30 circles, one for each train/test permutation (each at a different shade of
yellow-orange-red), hence the circle thickness represents the variance across permutations.

face-target block, and nontargets in all other blocks). Testing
was thus performed separately for the image-target blocks and
image-nontarget blocks. Performance was computed for each
image exemplar, and averaged over all image exemplars in the
experiment.
We found that this leave-one-out voting procedure for repetitions of each image exemplar, dramatically improves image classification performance by an average of 16.5% correct
classification (12.5%–20% for different subjects), to near perfect classification in some subjects (83%–95.5%; mean 89.4%).
Specifically, it increases target-hit rates by an average of 20%
(17%–27% for different subjects), and reduces false-alarms by
an average of 22% (16%–25% for the different subjects), resulting in hit rates of 75%–91% (mean 83% hits) and false alarm
rates approaching zero (0%–9%; mean 4%).
Finally, we also investigated the dependence of leave-oneout voting performance upon the number of repetitions used for
voting (see Methods for details). Results are presented in Fig. 10.
We find that, compared to single-trial analysis, the leave-oneout classification accuracy converges to a 20%–50% increase in
accuracy (reaching nearly 100% correct in some subjects) by
using only 6–8 repetitions for voting. Using more than eight
trials for voting hardly affects accuracy.

IV. DISCUSSION
Despite considerable advances in computer vision, the capabilities of the human visuo-perceptual system still surpasses
even the best artificial intelligence systems, especially as far
as its flexibility, learning capacity, and robustness to variable
viewing conditions. Yet, when it comes to sorting through large
volumes of images, such as micro and macroscopic medical
images, or satellite aerials, humans are generally accurate, but
too slow. The bottleneck does not stem mainly from perceptual
processes, which are pretty quick, but from the time it takes to
register the decision, be it orally, in writing, or by a button press.
To overcome this impediment, observers can be freed from the
need to overtly report their decision, while a computerized algorithm sorts the pattern of their single-trial brain responses,
as images are presented at a very high rate [6], [7], [19]. In
this study, we re-examined the feasibility of this approach, and
provided improvements over previously suggested algorithms.
EEG is characterized by a low SNR, and therefore it is traditionally analyzed by averaging out the noise over many repetitions of the same stimulus presentation. However, for the purpose of using EEG to label single images in real time (or nearly
so), the algorithm must be able to deal with single-trials. This
was complicated in the present implementation by the need to

2300

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Fig. 9. Topography of classification weights at best latency of target/nontarget discrimination. The maps were averaged across permutations. Each subject’s map
is normalized by its Euclidean norm. Color represents the classification weight for target/nontarget discrimination at each spatial location. Dark colors (red/blue)
indicate spatial locations that are important for classification.

present the images rapidly (at 10 Hz), such that brain responses
to subsequent image presentations overlapped. That is, the response to an image presentation has not yet decayed before the
next stimulus was presented. This requires special consideration
when selecting a classification algorithm.
Classification performance is clearly sensitive to the algorithm in use, and is based on nontrivial feature extraction, statistical characterization and classification of the data. Some
algorithms have been constructed specifically for the RSVP
task. In particular, a general framework was proposed by Parra
et al. [19] for single-trial classification, and implemented specifically for the RSVP task. We have thus chosen to use it here as
a benchmark algorithm to test classification performance on the
RSVP data collected using our object-category target detection
paradigm.
Performance depends not only on the classification algorithm
and features in use, but also on other factors, including the
particular experimental paradigm of study. P300 speller tasks
for example, in which the goal is to decipher which letter the
observer has in mind, are reported to achieve near 76%–80%

single-trial classification accuracy [27], [28]. Performance in
those cases is commonly increased up to 100% by averaging
across trials [29]. Correct single-trial left/right hand movement
classification for motor tasks may be as high as 80%–90% in
single trials [2], [3].
Particularly, classification performance in several RSVP tasks
has also been reported. For example, expert imagery analysts
participated in a study by Poolman et al. [20] detecting targets (missile sites, artillery sites, and helipads) from satellite
images that were presented at 10 Hz. In another RSVP study
of Bigdley et al. [6], satellite images were presented to naive
observers at 12 Hz. Each presented image consisted of a focal
center within blurred surrounding and target images contained
a clear image of a plane pasted over the focal center. In Parra’s
et al.’s study [19], where the benchmark HDCA algorithm was
presented, the focus was on triaging images. Trained image analysts were instructed to identify helipads in monochromatic
aerial images. The task was performed twice: once in which
images were presented at random order and the other after images have been triaged by the EEG classifier. The RSVP task

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

2301

Fig. 10. Performance accuracy for classifying single image exemplars depends on the number of trials used for the leave-one-out voting procedure. Percent
correct leave-one-out classification of single image exemplars, as a function of the number (N ) of repetition trials used for voting. Blue: one line per subject. Red:
Grand average over subjects. The sawtooth shape of the graph is caused by the noise associated with a majority vote over an even number of trials. This type of
noise diminishes as the number of trials increases.

presented the images at either 5 or 10 Hz and images were
reordered by the classifier scores for target detection. Gerson
et al. [7] use a similar algorithm to Parra [19] to triage a continuous 10-Hz sequence of natural scenes, with target images defined
as natural scenes containing at least one person. The subjects’
task was to detect the appearance of a person over a background
scene. Overall, these paradigms seem to be easier than the one
considered in our main Experiment 1 because they only require
detection of targets versus backgrounds. All these cases likely
create a pop-out perceptual phenomenon, facilitating target
detection.
In order to compare performance to the studies reported in the
literature, we also introduced Experiment 2. In this paradigm,
where subjects were asked to detect target images of Cars, out of
meaningless noise images, we find similar or better performance
than previously reported.
Specifically, the paper by Poolman et al. [20] reports an average AUC of 0.78 with 79% hit rate and 46% false alarms, compared to AUC of 0.98, 91% hit rate and 1.5% false alarms using
SWFP in our study. Correct classification at Bigdley et al. [6]
was measured only by the measure of AUC and reported as
0.84 on average across all subjects, compared to AUC of 0.98
using SWFP in our study (Experiment 2). Unfortunately, clas-

sification numbers in the benchmark study of Parra et al. [19]
were not provided, but target detection was shown to improve
significantly after the EEG triage. Gerson et al. [7] reported hit
rates only slightly lower than our SWFP hit rate, but they did
not report the false alarm rate.
Since each experiment included different simulation, it may
be difficult to compare these performance measures. Using
our own data of Experiment II, the new SWFP outperformed
the HDCA even in this simpler paradigm, achieving near perfect (97%–99% correct) performance. The HDPCA algorithm
achieved better classification than the original HDCA, although
not as high as the SWFP.
In our main paradigm (Experiment 1), subjects were asked
to identify images from a predefined object category out of
five. This is a more challenging task, as it requires discrimination between object categories and not merely detection of
the presence or absence of a certain object. Performance accuracies are thus expected to be lower. Nevertheless, we find
that our proposed SWFP algorithm reaches 66%–82% correct
performance (mean: 72.89; std: 4.74). Moreover, we find that
like in the simpler case of Experiment 2, it outperforms the
benchmark HDCA algorithm [19] by 10% correct classification,
a difference that is statistically significant. Statistical analysis

2302

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

also demonstrates that SWFP performs more accurately than
HDCA both in terms of increased hit rates as well as decreased false alarms. Additionally, our HDPCA modification
to the benchmark original HDCA algorithm [19], boosts performance almost to the level of SWFP although we find that HDPCA is somewhat less conservative, in that it produces significantly more false alarms than SWFP, resulting in reduced total
accuracy.
We note that the performance of the benchmark HDCA is
sensitive to the window size used for analysis (see Methods:
Step 1 of Algorithm 2). Decreasing the window size from 100 ms
as reported in the original benchmark study [19], to 50-ms time
window as implemented in Gerson et al. [7], raises performance
by an average of 7% correct in Experiment 1, and of 6% in
Experiment 2. Even with the smaller window, SWFP somewhat
outperforms HDCA in term of d’ (Wilcoxon signed rank test,
p < 0.05) mainly because of a lower false alarm rate. This may
be considered as another advantage of using SWFP over both
HDCA and HDPCA, as the SWFP does not depend on a choice
of a specific window size for analysis, but rather takes all time
points into account.
HDPCA differs from HDCA by the addition of a
discriminating-based weighting vector for time windows, such
that time windows that are more important for target/nontarget
discrimination have a greater effect on classification (see also
[7]) as well as PCA for dimensionality reduction, resulting in
noise reduction prior to final classification. In fact, we find that
PCA, implemented in both SWFP and HDPCA on time series
from individual channels, performs a kind of spectral decomposition, where the first few principle components turn out to
represent the lowest frequency components of the original signal
[Fig. 11 (supplementary figure)]. Noise reduction is thus an outcome of choosing the first few principal components, or rather
a combination of the lowest frequency components, explaining
the original variability in the signal.
The main difference of SWFP from the other two algorithms
(HDCA and HDPCA) is that while HDCA and its modified
version (HDPCA) use the spatial projection vector (uk ) generated in the first step, in order to collapse data from all channel
locations onto a single representative global channel (sk ) prior
to temporal manipulation of the data, SWFP uses the concatenated information from all channels. Therefore, both HDCA and
HDPCA discard potentially relevant information, which indeed
turns out to be useful for single-trial classification. In addition,
SWFP amplifies the original data matrix by the spatiotemporal
classification weights prior to PCA dimensionality reduction;
channel and times that are most important for classification are
given even greater impact further in the process. As a result, dimensionality reduction is more effective in extracting relevant
features for classification.
We designed our main experimental paradigm to consist of
several repetitions of each image exemplar. We tested whether
voting across the classification decision for multiple presentations of the same image exemplar improves performance.
We used a leave-one-out procedure, by which the classifier is
trained on a large database of EEG recorded brain responses,
and tested separately for all presentations of a single image ex-

emplar, which was excluded from the training set. We found
that a majority-based decision rule per image exemplar dramatically improves image classification performance by an average
of 16.5% correct classification, reaching over 95% correct performance in some subjects. Specifically, it increased Target-hit
rates by an average of 20% to exceed 90% hits in some subjects,
and reduces false alarms by an average of 22% to near zero false
alarms for some subjects. The important implication for BCI application is that performance can be improved dramatically for
single stimulus exemplars by presenting six to eight repetitions
only of each stimulus, resulting in near perfect classification.
Thus, in cases where high accuracy is crucial, the extra cost
of time required to present every image several times might be
justified.
Finally, we were also interested in the brain activations supporting single-trial classification. We therefore analyzed spatiotemporal distributions of discriminating activity, without any
prior assumptions about the times and spatial locations most important for target detection. Specifically, in this study, we found
that the best time and topography for target/nontarget discrimination was reminiscent to the reported literature of P300 target
detection ERPs [10], namely central–posterior regions, most efficiently starting around 300–400-ms postimage presentation.
This provides a proof of concept for the SWFP algorithm, suggesting it is a feasible algorithm to be used in the future for
automatically exploring spatiotemporal activations at the basis
of other discriminating tasks. Interestingly, we found variance
across subjects in the exact time and location of the dominant classification weights, indicating individual differences.
For example, in a few of the subjects, the dominant classification weights were found to be less localized and to involve also
frontal scalp regions contributing to target/standard discrimination. While the pattern of classification weights in some subjects
differs from the classical spatiotemporal distribution of P300
ERPs, we find the patterns to be reliable and informative across
trials; in fact, some subjects with nonclassical spatiotemporal
distribution of weights, show the highest single-trial classification performance. This suggests that the spatiotemporal brain
representation of target detection is subject-specific, yet for each
subject, it is consistent across trials.
ACKNOWLEDGMENT
We thank S. Shalgi, and E. Wigderson for their help in running
the experiments and analyzing the data.
REFERENCES
[1] F. Lotte, M. Congedo, A. Lécuyer, F. Lamarche, and B. Arnaldi, “A review
of classification algorithms for eeg-based brain–computer interfaces,” J.
Neural Eng., vol. 4, p. R1, 2007.
[2] G. Pfurtscheller, C. Neuper, A. Schlogl, and K. Lugger, “Separability of
eeg signals recorded during right and left motor imagery using adaptive
autoregressive parameters,” IEEE Trans. Rehabil. Eng., vol. 6, no. 3,
pp. 316–325, Sep. 1998.
[3] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Muller,
“Optimizing spatial filters for robust eeg single-trial analysis,” IEEE Signal
Proc. Mag., vol. 25, no. 1, pp. 41–56, 2008.
[4] T. Felzer and B. Freisieben, “Analyzing eeg signals using the probability
estimating guarded neural classifier,” IEEE Trans. Neural Syst. Rehabil.
Eng., vol. 11, no. 4, pp. 361–371, Dec. 2003.

FUHRMANN ALPERT et al.: SPATIOTEMPORAL REPRESENTATIONS OF RAPID VISUAL TARGET DETECTION

[5] K.-R. Muller, C. W. Anderson, and G. E. Birch, “Linear and nonlinear
methods for brain-computer interfaces,” IEEE Trans. Neural Syst. Rehabil.
Eng., vol. 11, no. 2, pp. 165–169, Jun. 2003.
[6] N. Bigdely-Shamlo, A. Vankov, R. Ramirez, and S. Makeig, “Brain
activity-based image classification from rapid serial visual presentation,”
IEEE Trans. Neural Syst. Rehabil. Eng., vol. 16, no. 5, pp. 432–441, Oct.
2008.
[7] A. Gerson, L. Parra, and P. Sajda, “Cortically coupled computer vision
for rapid image search,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 14,
no. 2, pp. 174–179, Jun. 2006.
[8] Y. Huang, D. Erdogmus, S. Mathan, and M. Pavel, “Large-scale image
database triage via eeg evoked responses,” in Proc. IEEE Int. Conf. Acoust.
Speech Signal Process., 2008, pp. 429–432.
[9] M. Kaper, P. Meinicke, U. Grossekathoefer, T. Lingner, and H. Ritter,
“BCI competition 2003-data set iib: Support vector machines for the p300
speller paradigm,” IEEE Trans. Biomed. Eng., vol. 51, no. 6, pp. 1073–
1076, Jun. 2004.
[10] E. Donchin, W. Ritter, and W. McCallum, “Cognitive psychophysiology:
The endogenous components of the ERP,” in Event-related Brain Potentials Man, E. Callaway, P. Teuting, and S. H. Koslow, Eds. New York,
NY, USA: Academic Press, 1978, pp. 349–411.
[11] E. Donchin, K. Spencer, and R. Wijesinghe, “The mental prosthesis: assessing the speed of a p300-based brain-computer interface,” IEEE Trans.
Rehabil. Eng., vol. 8, no. 2, pp. 174–179, Jun. 2000.
[12] E. Sellers and E. Donchin, “A p300-based brain-computer interface: Initial
tests by ALS patients,” Clin. Neurophysiol., vol. 117, pp. 538–548, 2006.
[13] J. Wolpaw, N. Birbaumer, D. McFarland, G. Pfurtscheller, and T. Vaughan,
“Brain-computer interfaces for communication and control,” Clin. Neurophysiol., vol. 113, pp. 767–791, 2002.
[14] J. Wolpaw and D. McFarland, “Control of a two-dimensional movement
signal by a noninvasive brain-computer interface in humans,” in Proc.
Nat. Academy Sci. United States Am., vol. 101, p. 17849, 2004.
[15] J. Muller-Gerking, G. Pfurtscheller, and H. Flyvbjerg, “Designing optimal
spatial filters for single-trial eeg classification in a movement task,” Clin.
Neurophysiol., vol. 110, pp. 787–798, 1999.
[16] K.-R. Müller, M. Tangermann, G. Dornhege, M. Krauledat, G. Curio, and
B. Blankertz, “Machine learning for real-time single-trial eeg-analysis:
From brain-computer interfacing to mental state monitoring,” J. Neurosci.
Methods, vol. 167, pp. 82–90, 2008.

2303

[17] P. Sajda, E. Pohlmeyer, J. Wang, L. Parra, C. Christoforou, J. Dmochowski,
B. Hanna, C. Bahlmann, M. Singh, and S. Chang, “In a blink of an eye
and a switch of a transistor: Cortically coupled computer vision,” in Proc.
IEEE, Mar. 2010, vol. 98, no. 3, pp. 462–478.
[18] G. Chanel, J. Kierkels, M. Soleymani, and T. Pun, “Short-term emotion
assessment in a recall paradigm,” Int. J. Human-Comput. Studies, vol. 67,
pp. 607–627, 2009.
[19] L. Parra, C. Christoforou, A. Gerson, M. Dyrholm, A. Luo, M. Wagner,
M. Philiastides, and P. Sajda, “Spatiotemporal linear decoding of brain
state,” IEEE Signal Proc. Mag., vol. 25, no. 1, pp. 107–115, 2008.
[20] P. Poolman, R. Frank, P. Luu, S. Pederson, and D. Tucker, “A single-trial
analytic framework for eeg analysis and its application to target detection
and classification,” NeuroImage, vol. 42, pp. 787–798, 2008.
[21] A. Luo and P. Sajda, “Learning discrimination trajectories in eeg sensor,”
J. Neural Eng., vol. 3, p. L1, 2006.
[22] M. Dyrholm, C. Christoforou, and L. Parra, “Bilinear discriminant component analysis,” J. Mach. Learn. Res., vol. 8, pp. 1097–1111, 2007.
[23] T. Wickens, Elementary Signal Detection Theory. London, U.K.: Oxford
University Press, 2001.
[24] R. A. Fisher, “The use of multiple measures in taxonomic problems,” Ann.
Eugenica, vol. 7, pp. 179–188, 1936.
[25] B. Blankertz, S. Lemm, M. Treder, S. Haufe, and K.-R. Müller, “Singletrial analysis and classification of ERP componentsa tutorial,” Neuroimage, vol. 56, no. 2, pp. 814–825, 2011.
[26] L. C. Parra, C. D. Spence, A. D. Gerson, and P. Sajda, “Recipes for the
linear analysis of eeg,” Neuroimage, vol. 28, no. 2, pp. 326–341, 2005.
[27] A. Lenhardt, M. Kaper, and H. Ritter, “An adaptive p300-based online brain–computer interface,” IEEE Trans. Neural Syst. Rehabil. Eng.,
vol. 16, no. 2, pp. 121–130, Apr. 2008.
[28] K. Li, R. Sankar, Y. Arbel, and E. Donchin, “Single trial independent
component analysis for p300 bci system,” Proc. IEEE Annu. Int. Conf.
Eng. Med. Biol. Soc., pp. 4035–4038, 2009.
[29] A. Rakotomamonjy and V. Guigue, “Bci competition iii: Dataset iiensemble of svms for bci p300 speller,” IEEE Trans. Biomed. Eng., vol. 55,
no. 3, pp. 1147–1154, Mar. 2008.

Authors’ photographs and biographies not available at the time of publication.

