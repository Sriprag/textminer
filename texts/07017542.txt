IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

1553

Direct Gaze Estimation Based on Nonlinearity
of EOG
Hiroyuki Manabe∗ , Masaaki Fukumoto, and Tohru Yagi

Abstract—Electrooculography (EOG) is one of the measures
used to estimate the direction of a person’s gaze; however, conventional EOG techniques suffer from a drift issue which makes it
difficult to extract an accurate absolute eye angle. The technique
proposed here is based on the nonlinearity of the EOG and offers a
practical solution to this problem. It estimates the absolute eye angles before and after a saccade, which cancels the offset due to the
drift. Additionally, it does not require any effort from the user or
any target, but instead uses only the difference of the EOGs. Experiments with five subjects confirm that the proposed technique can
estimate the absolute eye angle with an error of less than 4◦ . They
also show improvements are achieved with several options such as
weighting and multiple saccades. The technique will contribute to
practical EOG-based interaction systems.
Index Terms—Electrooculography (EOG), Gaze estimation,
nonlinearity, saccade.

I. INTRODUCTION
EVERELY mobility handicapped persons, such as those
with amyotrophic lateral sclerosis, are benefiting from new
technology that lets their eyes, in particular their gaze, be a
means of input to a computer and other devices. Such interfaces
are also becoming of interest to the general public. Since they
provide valuable information about the user, eye tracking techniques have been researched in various fields. For example, they
reflect the user’s psychological state and cognitive mechanism,
and this makes them effective measures in psychology [1]–[3].
They are also effective at detecting driver fatigue [4]. In addition, they can be used to identify the user’s focus of attention
or gaze path, which is useful information for designing web
pages [5] and advertising [6]. Moreover, human–computer interactions can be enhanced by eye tracking technology since it
can provide quick and direct access to surrounding objects and
take immediate gestural inputs. For example, it is reported that
gaze interaction is faster than using a mouse [7]. Considering
the growing popularity of ubiquitous and wearable computers,
gaze input interfaces will no doubt play an important role in the
future.

S

Manuscript received September 12, 2014; revised December 15, 2014 and
January 12, 2015; accepted January 13, 2015. Date of publication January 21,
2015; date of current version May 18, 2015. Asterisk indicates corresponding
author.
∗ H. Manabe is with the Research Labs, NTT DOCOMO, Yokosuka 239-8536,
Japan, and the Tokyo Institute of Technology, Tokyo 152-8550, Japan (e-mail:
manabehiroyuki@acm.org).
M. Fukumoto was with the Research Labs, NTT DOCOMO. He is now with
Microsoft Research.
T. Yagi is with the Tokyo Institute of Technology.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2015.2394409

The current state of development has raised the importance of
having an effective eye tracking technique. The most popular approaches to eye tracking use an infrared camera(s), for example,
corneal reflection tracking [8] and pupil tracking [9]. Regular
RGB cameras also have an eye tracking capability [4], [10].
In these camera-based-techniques, the eye tracker is mounted
on the desktop [8] or is portable [10] or even wearable [9].
Among these form factors, wearable trackers are most suitable
for interfaces used in everyday situations because they can be
used anytime, anywhere.
Another approach to eye tracking is based on electrooculography (EOG); the resting potential of the retina is measured by
electrodes attached to the skin around the eyes and used to detect
eye movements. It is a relatively low cost, low power method
that has a quick response and does not obstruct the user’s visual
field. Moreover, an electrooculogram has a wide ranging eye
tracking capability; for example, it can detect extremely large
or quick eye movements, and it works even if the eyes are closed.
Because of these features, EOGs captured by a wearable device
are well suited to everyday interactions, and many studies have
already used EOGs to detect the sequences of eye movements
called eye gestures, or activity recognition systems [11]–[14].
So far, however, EOGs have only been used for specific purposes such as in interfaces for people with disabilities [15] and
clinical examinations [16].
Implementing an EOG-based interface raises several issues,
among which drift is a major concern since it decreases the tracking accuracy. Drift is the baseline shift of EOG values. The amplifier’s output reflects not only eye-movement components but
many other factors, such as the electrode potential and several
kinds of noise, and the baseline gradually shifts over time. Even
if the subject gazes at the same target, the dc level of the EOG
does not remain the same. This means that the estimated eye
angle changes even if the eye does not move. Since the drift
changes the baseline slowly and it can be ignored over short
periods, EOGs are an accurate means of tracking relative eye
movement, e.g., the saccade amplitude. Drift, however, can not
be ignored over longer periods, and it often causes the EOG to
give an inaccurate absolute eye angle.
Several strategies have been proposed to overcome the drift
issue. The first strategy tries to lower the impact of the drift;
examples include applying a wavelet transform [13] and using
multiple EOGs [14]. This strategy is effective in extending the
period over which the drift can be ignored; however, it does
not guarantee an accurate estimation over a longer period. Accoupled EOGs are an easy solution to the drift problem [15] and
it can be used to estimate the absolute eye angle [17]. Unfortunately, it does not capture slow eye movements, and it continues

0018-9294 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

1554

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

-40°

-30° -20° -10° 0° 10° 20°

30°

40°

0.3cm
Targets
53cm

θ
+
Electrode

+

-

-

Electrode

V

Fig. 1. Illustration of EOG-based eye angle estimation and arrangement of
targets in the experiments. Eyeball rotation changes the electrical potential in
the periocular region. The change is captured as the potential difference between
the electrodes attached around the eyes, and the eye angle, θ, is estimated in this
way. Note that the face is not to scale.

to provide inaccurate angle readings once an estimation error
occurs.
Two calibrations are generally needed for EOG-based gaze
estimation: initial and offset calibrations. The former is performed once prior to a sequence of gaze estimations and it is
done to adjust the measurements to differences in the electrode
positions and users. The latter is often required during a sequence of estimations to offset the drift. Offset calibration is
quite simple; for example, the user is asked to gaze at a specific
target. Frequent offset calibration, this is the second strategy,
can ensure accurate eye angle estimation [18]; however, the current methods require explicit effort on the user’s part and thus
may interrupt the main function of the interface. Here, although
the offset calibration can be conducted using a visual evoked
potential [19], it is difficult to put into practical use because
flashing light sources have to be used.
The third approach is tuning the interaction. The drift can be
ignored when the interaction is based not on the absolute eye angle but rather on short-time relative eye movements (saccades)
and their sequence (eye gestures). Many saccade-based interaction schemes have been developed [11]–[13]. These are effective
in some applications; however, they impose severe restrictions
on the interaction design. Using the eye gesture immediately
prior to one for the offset calibration can avoid this limitation;
however, this means that an additional eye gesture is necessary
and the available period is short.
As indicated previously, none of the individual conventional
strategies is able to solve the drift problem. Moreover, although
they are often combined in practical systems, the drift issue nevertheless remains a major concern for EOG-based interaction.
While EOGs are useful for estimating someone’s gaze, they
also have a negative impact; eye movement adds artifacts to
electroencephalogram (EEG). Many attempts have been made
to remove EOG components from EEGs [20]. For example,
blind source separation including independent component analysis has been tried [21], [22]. Regression methods can be
also used to reduce artifacts [23], [24]. A comparison of these
techniques has been reported [25]. These techniques focus on
the differences between the measured EEG and EOG signals.

What complicates the issue is that EEGs and EOGs bidirectionally contaminate each other [26], [27].
On the other hand, several studies have tried to combine physical, kinetic or electric models of the eyeball with measured signals. For example, a simple monopolar model can explain most
data [28]. Moreover, several dipole models have been studied
[29], and one of them, the battery model, is able to explain the
measured signal [30]. In fact, such a model-based technique has
been used for EEG correction [31]. The aforementioned models fit the signals to some extent, but so far none is thought
to be good enough. They do, however, offer the important insight that there is a nonlinear relationship between the EOG and
eye angle. Since this nonlinearity is small, many EOG-based
interaction schemes assume a simple linear model. Although
this assumption initially seems to work well in practice, drift
eventually degrades the performance of the system. Note that
there are methods that take the nonlinearity into consideration
in order to improve the accuracy [32], but they are effective only
when the drift is negligible.
Our objective in this paper is to provide a solution to the drift
issue affecting EOG measurements in order to create a truly
effective EOG-based interface. We use the nonlinearity of the
EOG and estimate the absolute eye angle directly, while conventional techniques do so cumulatively, to establish an automatic
drift calibration technique. A preliminary version of this study
has already been reported [33]. This paper explores improvements to the technique and provides deeper analyses.
II. GAZE ANGLE ESTIMATION BASED ON EOG
Before introducing our technique, we will describe how EOG
measurements are made and how the conventional linear estimation technique works. Fig. 1 illustrates the principle of EOGbased eye tracking as given by the battery model [30]. The
eyeball works like a battery, positive on the front side (cornea)
and negative on the back side (retina). When the eye rotates right
with respect to the eye angle, θ (the angle between the front of
the face and the gaze direction), the potential of the right side
electrode increases while that of the left side electrode decreases.
Since the EOG is measured as the potential difference between
the two electrodes. The dc level of the EOG directly reflects the
eye angle θ, and a dc-coupled amplifier is used to gather the data.
Thus, the EOG differs significantly from other bio-potential signals such as EEGs, electrocardiograms and electromyograms,
since they are captured with ac-coupled configurations. Though
the relationship between θ and the measured EOG is complicated and not linear, as is reported in [30], [32], the conventional
estimation schemes regard the EOG to be linear with respect to
eye angle. The range wherein the EOG is almost linear with
respect to the horizontal eye angle is reported to be ±35◦ in
[34], and ±45◦ in [35].
A. Conventional Estimation Technique
In order to simplify the description, only horizontal eye movement is considered. EOG is determined from E0 and E1 which
are the potentials obtained at electrodes 0 and 1. The measured
EOG includes the ideal retina driven potential, p, which is a

MANABE et al.: DIRECT GAZE ESTIMATION BASED ON NONLINEARITY OF EOG

function of θ, and not an eye related component e(t) including noise and the dc offset, whose dominant component is the
difference in the electrode potentials. The details are described
in [36]. Among the various components of e(t), the lowest frequency component is called drift. Thus, the measured EOG,
EOG, is modeled as
EOG(t) = E1 (t) − E0 (t)

(1)

= p(θ(t)) + e(t).

(2)

Since the dc level of e(t) is much larger than p(θ), the difference
between two EOGs is used for the eye angle estimation.
ΔEOG(t1 , t0 ) = EOG(t1 ) − EOG(t0 )

(3)

= p(θ(t1 )) − p(θ(t0 )) + Δe(t1 , t0 ). (4)
Conventional techniques assume p(θ) to be a linear function of
θ with a coefficient a and generally represent θ(t) as follows:
p(θ(t1 )) − p(θ(t0 )) = a(θ(t1 ) − θ(t0 )).

(5)

a is obtained in the initial calibration process, where the eyes
are moved from a predefined angle θ0 to θ1 (the difference is
Δθ) within a short period of time such that Δe can be ignored.
a=

ΔEOG
.
Δθ

because of the small time difference. Accordingly Δθ̂ can always be accurately estimated assuming Δe(t1 , t0 ) is zero. Note
that the sum of Δθ̂ for saccades does not match the true absolute eye angle because Δθ̂ has some error, small saccades are
ignored, and the eyes sometimes move without saccades, in a
manner called smooth pursuit.
III. PROPOSED TECHNIQUE
Our proposal takes the second strategy mentioned previously,
and it automatically performs the offset calibration on every saccade. It differs from the conventional methods in the following
points; the EOG is taken to be a nonlinear function of eye angle,
the eye angle is estimated using multiple EOGs, the absolute
eye angle is, in addition to relative movement, directly output
without using targets after each estimation and calculated only
from the differences of EOGs during a short time period, and
no intentional operations by the user are required.
When the user is gazing at surrounding objects, a saccade
and a fixation are repeated in turn, and the fixations are often
in the range of 200–400 ms [37]. Since our technique can be
performed every saccade, its calibration is quite frequent, which
means Δe in (7) does not change much.

(6)

Finally, the eye angle is estimated as θ̂(t) assuming that
Δe(t, t0 ) is zero in the following equation:
1
{ΔEOG(t, t0 ) − Δe(t, t0 )} + θ(t0 )
(7)
a
1
≈ ΔEOG(t, t0 ) + θ(t0 ).
(8)
a
The absolute eye angle is cumulatively calculated. The main factor degrading the estimation quality over time is that Δe(t, t0 )
can not be taken to be zero over a long period of time. For example, Fig. 3 shows measured EOGs. Since the subject gazed
at the same target, the center one, both at the beginning and the
end of the sequence, the measured values would be same if Δe
was zero throughout the sequence. Actually, however, Δe was
not zero, and the measured value shifted due to drift.
The first drift solution attempts to make Δe smaller. It extends
the duration of the measurement in order to make an accurate eye
angle estimation. An accurate eye angle can be attained by using
the immediately prior measured EOG(t0 ) and θ(t0 ), because
it is expected that Δe(t, t0 ) remains small enough. Thus, by
frequently updating t0 , the offset calibration step, the estimation
error can be suppressed. This is the second strategy mentioned in
Section I. A third strategy is writing or modifying applications to
suit saccade-based techniques; this means dropping θ̂ and using
Δθ̂ instead for short time periods. When assigning t0 and t1 to
the time just before and after the saccade, Δθ̂ can be calculated
from (7).
1
Δθ̂ = θ̂(t1 ) − θ̂(t0 ) = {ΔEOG(t1 , t0 ) − Δe(t1 , t0 )}. (9)
a
This corresponds to updating t0 at every saccade; however, there
is no need to update θ(t0 ). Δe(t1 , t0 ) becomes negligible because p(θ(t1 )) − p(θ(t0 )) is large and the degree of drift is small
θ̂(t) =

1555

A. Direct Estimation Based on Nonlinear Model
The ith EOG in a series of EOGs is given by
ΔEOGi (t1 , t0 ) = pi (θ(t1 )) − pi (θ(t0 )) + Δei (t1 , t0 ). (10)
Consider the time just before a saccade, t0 , and the time just
after it, t1 . Δei (t1 , t0 ) becomes negligible, as in the case of
estimating Δθ̂ mentioned before. Thus, ΔEOGi for the eye
movement from θ0 to θ1 can be represented in the presence of a
saccade as follows:
ΔEOGi (t1 , t0 ) ≈ pi (θ(t1 )) − pi (θ(t0 )).

(11)

Note that the aforementioned equation seems to be a timedependent function, but it can instead be viewed as θ-dependent
by regarding θ0 and θ1 as θ(t0 ) and θ(t1 ). In order to make this
difference clear, ΔEOG will be used to represent a θ-dependent
function.
ΔEOGi (θ1 , θ0 ) ≈ pi (θ1 ) − pi (θ0 ).

(12)

Given that pi (θ) is a nonlinear function and multiple EOGs (the
number of EOGs is M ) are captured, we can get the angles
{θ̂1 , θ̂0 } not cumulatively, but directly by solving the following
equation:
M


{ΔEOGi − (pi (θ1 ) − pi (θ0 ))}2 → min.

(13)

i=1

This equation shows that the absolute eye angle can be simply
calculated from the ΔEOGs collected over short periods of
time, which means the offset due to drift will be automatically
compensated every saccade without any effort by the user.
Our technique works when the EOG can be represented by
known nonlinear functions of θ, pi (θ). The coefficients of pi (θ)

1556

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

can be estimated from the signals measured in the initial calibration process. When the user is asked to shift her/his gaze
from θk 0 to θk 1 , the ith EOG: ΔEOGi,k (θk 1 , θk 0 ) is measured.
By repeating this procedure, K sets of {ΔEOGi,k , θk 0 , θk 1 }
are obtained. Minimizing σi2 , defined as follows, gives the coefficients of pi (θ).
σi2 =

K
1 
{ΔEOGi,k (θk 1 , θk 0 )
K
k =1

− (pi (θk 1 ) − pi (θk 0 ))}2 .

(14)

B. Options for Performance Improvement
The estimation ability of the equations can be improved in a
number of ways: The form of p(θ) can be selected, weights can
be introduced, and multiple saccades can be treated instead of a
single saccade.
1) Applied p(θ): There are several electrical models for representing EOGs, and they may be suitable for p(θ). These models, however, often use complicated functions involving many
unknown parameters such as mono/dipole(s) and electrode positions, and electrical parameters of the tissues around the eye.
All these parameters must be measured or estimated in the initial calibration, which requires special equipment or sampling
of a lot of data. Since the objective is to get accurate eye angles
rather than to use accurate models, simple polynomial equations
of θ can be used instead. This corresponds to focusing on the
characteristics of the measured signal rather than the electrical
and kinetic models. The use of simple polynomial equations
ensures that almost all cases will be well approximated, even
though the electrode arrangement and electrical properties of
the facial tissue will differ from one user to the next. When
using the N th order polynomial function of θ as p(θ), the ith
EOGs can be represented as follows and {aij } can be estimated
from (14).
pi (θ) ≈

N


N


M


wi2 {ΔEOGi,n

i=1

− (pi (θn +1 ) − pi (θn ))}2
0


Fn (θn +1 , θn ) → min.

(18)
(19)

n =1−N

The calculated θ̂1 corresponds to the current eye angle among
the estimated N + 1 angles, {θ̂1 , θ̂0 , ..., θ̂1−N }.
IV. EVALUATION

(15)

Experiments were conducted with five subjects in order to
confirm that the proposed technique worked and to analyze the
effectiveness of the options.

aij (θ1j − θ0j ).

(16)

A. Experiment Setup

j =1

2) Weight: A weight, wi for ΔEOGi , can be used to adjust
the impact of each EOG on the estimation result. When weights
are introduced, the estimation function becomes
M


Fn (θn +1 , θn ) =

aij θj

j =1

ΔEOGi (θ1 , θ0 ) ≈

smooth pursuit. Considering such sequential saccades, a sequence of N consecutive saccades will have N changes in the
eye angle, consisting N + 1 absolute eye angles.
For example, if the initial eye angle is θ−1 , the first saccade
moves it to θ0 and the second (current) one moves from θ0 to
θ1 . Three absolute angles {θ̂1 , θ̂0 , θ̂−1 } can be simultaneously
estimated from the two saccade data. It is expected that the
estimation error would decrease when more than two angles
are estimated from multiple saccades. Because the volume of the
EOG dataset for estimating per angle becomes relatively large,
the impact of distinct noise on the EOG becomes small. Once
the first saccade is estimated, it seems reasonable to use the
estimated value for calculating the second saccade. However,
the estimated value of the first saccade includes some error,
and unfortunately, this error can not be measured. Using the
corrupted estimated values will introduce new errors in not only
the current but also future estimations. To alleviate this problem,
this option conducts estimations with multiple unknown saccade
angles, even if they were previously estimated. The N + 1-angle
estimation from N saccades is performed as follows, defining
ΔEOGi,n (1 − N ≤ n ≤ 0) as the signal of the nth saccade
before the current one (n = 0)

wi2 {ΔEOGi − (pi (θ1 ) − pi (θ0 ))}2 → min.

(17)

i=1

When the weights are simple binary ones, wi = 1 or 0, they
represent whether the estimation uses the corresponding EOG
or not. By assigning σi in (14) to be the weights, wi = σi−1 , we
can normalize the impact of the EOGs on the estimation.
3) Multiple Saccades: Function (13) and (17) estimate the
eye angles before and after the saccade by using the difference in
EOGs before and after the saccade. It is known that eye movements often consist of sequential saccades and fixations without

The use of many electrodes yields many EOGs, which would
improve the estimation accuracy. Seven sensing electrodes and
one ground electrode were used in this experiment. Seven electrodes (8 mm in diameter) arranged as shown in Fig. 2. Here,
three electrodes, two horizontally far from the eye (#1 and 7),
and one between the eyebrows (#5), were examined in addition
to the usual arrangement (#2, 3, 4, 6). The ground electrode was
attached to the left cheek.
Nine targets were placed horizontally in a line at 10◦ intervals
from −40◦ to + 40◦ (see Fig. 1). Note that the face shown in
the figure is distorted. The subject placed his head on a chin
rest and was asked to gaze at the targets. The potentials of the
seven electrodes were recorded using dc-coupled amplifiers with
200-Hz sampling, a 50-Hz notch filter for aggressively removing
power-line noise, and a 30-Hz low-pass filter. Twenty-one EOGs
were calculated from seven channels (7 C2 = 21). The EOGs

MANABE et al.: DIRECT GAZE ESTIMATION BASED ON NONLINEARITY OF EOG

1557

Fig. 2. Seven electrode arrangement tested in the experiments. #7 is not shown,
but is symmetrical to #1. The positions usually used for tracking horizontal eye
movements are #2 and 6 (#3 and 4 for vertical movements).

Fig. 4. Example of results for sequence (ii). The subject gazed at the center
target, another target (depending on the sequence), and the center target again.
ΔEOG i was manually clipped like in Fig. 3.

Fig. 3. Example of results for sequence (i). The subject gazed sequentially
at nine targets spaced at 10 ◦ . The difference between the EOGs before and
after a saccade was manually clipped for ΔEOG i . Note that only six of the
twenty-one captured EOGs are listed.

were then median filtered with a window size of 250 ms to
reduce high-frequency noise. Five subjects, all males and none
of whom wore glasses in the experiment, participated. They
were informed of the objectives of the research and that they
could stop the measurement at anytime.
B. Measured EOGs
Two EOG sequences, (i) and (ii), were obtained. Figs. 3 and 4
show part of the measured EOGs when the subject was asked
to gaze at the targets sequentially. The order of the targets in
sequence (i) was 0, 10, 20, 30, 40, 30,...−30, −40, −30,..., 0◦ ,
while it was 0, 10, 0, 20, 0, 30,..., 0, −30, 0, −40, 0◦ in sequence
(ii). “1–2” in the figure indicates that the value was calculated
from electrodes #1 and #2, and all EOGs shown in the figure are
calculated as the differences with respect to the potential of electrode #1. The EOGs have a stair-like form reflecting the subject’s
eye movements. The flat and rising parts of the steps, respectively, correspond to fixations and saccades. ΔEOG(40, 30) in
the figure represents the difference of the EOGs, “1–6,” meaning ΔEOG“1−6  (40, 30), indicating the saccade when the eye
rotated from 30◦ to 40◦ . All saccades in Fig. 3 correspond to eye
movements of +10◦ or −10◦ , whereas the step size varied in
Fig. 4. Most EOGs increase with eye angle except “1−2,” and
the amplitude of each step differs from one EOG to the next.
Note that the subject gazed at the same target (the center one)

Fig. 5. Measured and calculated ΔEOG i shown as plots and solid lines
from dataset (i) for subject 1. The solid line is calculated from the estimated p i
(p i is a fourth order polynomial function). Note that ΔEOG i (θ + 10, θ) and
−ΔEOG i (θ, θ + 10) are plotted at θ. It is clear that ΔEOG i depends on
the eye angle, which means p i and the eye angle have a nonlinear relationship.
The dotted line is calculated from dataset (ii).

for the plots at the right and left edges of both figures, but the
measured EOGs did not have the same values because of drift
as mentioned before.
As shown in the figures, sequences (i) and (ii) each reflected
sixteen saccades. The subjects were asked to perform more than
twenty sequences for (i) and then more than ten for (ii). The
subject had a rest between sequences (i) and (ii). After the data
were captured, the measured EOGs were manually examined,
and the sequences that clearly included large noise components
such as blinking or gazing at the wrong target position were
eliminated from the analysis. The sequences, [(i)/(ii)] that ended
up being analyzed for the five subjects numbered 22/11, 23/12,
23/12, 20/11, and 19/12. The datasets for the analysis were the
whole sequences; for example, dataset (i) for subject 1 included
22 sequence (i)s. The EOGs were manually clipped to obtain
ΔEOGi .
C. Nonlinearity of EOG
Fig. 5 plots some of the gathered ΔEOGi values for dataset
(i) and subject 1. Here, ΔEOGi (θ + 10, θ) is directly plotted at θ while ΔEOGi (θ − 10, θ) is inverted and plotted at

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

1-2
Sub.1

300

2-5
Sub.3

Sub.2

40
30

2-6
Sub.5

Sub.4

1-2
2-6

1-4
4-6

2-5
Average

20
200

15

2-6

100

2-5

0

1-2

σi

Calculated ΔEOG(θ +10,θ) [μV]

1558

10
Dataset (i)

5
Dataset (i)

-40

-30

-20

-10

0

Eye angle θ

10

20

30

0

1

2

3

4

5

6

7

8

Order of pi(θ)

D. Estimation of Eye Angle
There are several options and many combinations of them for
performing the estimation, including varying the order of the
p(θ), applying weights, varying the number of saccades used,
and using two datasets. In the following sections, the effect of the
order of p(θ) is shown first. After that the estimation results with
no options and the best combination for one subject are shown
in order to confirm that the technique succeeds in making an
absolute eye angle estimation. Finally, the improvements had
by the various options are indicated for all subjects.
1) Order of p(θ): Fig. 7 shows the σi obtained when estimating the polynomial functions using (14) as pi (θ) for subject 1

^

1.5
1.0

^

RMSE in eye movement

RMS of (θ1 - θ0) - (θ1 - θ0)

2.0

0.5
0.0
10
Order 2nd 3rd

Dataset (i)

4th

5th

6th

8

^

RMS of θ1 - θ1

θ − 10, because ΔEOGi (θ1 , θ0 ) = −ΔEOGi (θ0 , θ1 ) [see
(12)]. For example, both ΔEOGi (10, 0) and −ΔEOGi (0, 10)
are plotted at 0◦ . Note that each plot has been horizontally shifted
to avoid overlap (the other graphs do the same).
If ΔEOGi is assumed to be linear, the plot should be horizontal in the figure because all ΔEOGi correspond to 10◦
eye movements. Although the horizontal EOGs, ΔEOG“2−6  ,
seem to be constant when the angle is small, it is clear that
ΔEOGi depends on the eye angle θ. This means that the EOG
and eye angle have a nonlinear relationship. pi (θ) was calculated
as a fourth order polynomial function using (14), and the solid
lines represent ΔEOGi (θ + 10, θ) calculated from pi (θ). The
ΔEOGi (θ + 10, θ) values calculated using the pi (θ) estimated
from dataset (ii) are also shown as dotted lines. It is clear that
the ΔEOGi are well approximated by the simple polynomial
functions derived from this different dataset.
Fig. 6 shows the calculated ΔEOGi (θ + 10, θ) using fourth
order polynomial functions as pi (θ) for all subjects. The nonlinearity of the EOG is present for all subjects. While there are
slight differences between subjects, the rough tendency seems to
be the same; for example, ΔEOG“1−2  decreases as the angle
increases and ΔEOG“2−5  increases.
The results for dataset (ii) also show that ΔEOGi is nonlinear
and that a fourth order polynomial function approximates the
plot well; thus, the different sequences exhibited the same trend.

Fig. 7. σ i obtained when varying the order of p i (θ) in (14) with dataset (i)
for subject 1. Though σ i becomes smaller as the order increases, it seems to
saturate at fourth or fifth order.

RMSE in absolute angle

Fig. 6. ΔEOG i calculated from dataset (i) for all subjects. Although there
are slight differences among the subjects, all plots exhibit the same nonlinear tendency; i.e., ΔEOG “ 1 −2  decreases as the angle increases and ΔEOG “ 2 −5 
increases.

6
4
2
0

Sub. 1

Sub. 2

Sub. 3

Sub. 4

Sub. 5

Average

Fig. 8. Estimation errors in eye movement (upper) and absolute angle (lower)
for different orders of p(θ) for dataset (i) and all subjects. The estimation was
performed on dataset (i) with the weighted two-angle (one-saccade) option. The
order of p(θ) has little impact on the estimation accuracy.

and dataset (i). The results indicates how well pi (θ) represents
the measured data. The average is that of all twenty-one EOGs.
Since the dataset essentially includes only eight variations of
{θk 1 , θk 0 }, the maximum order is limited to eight and the value
of σi at eighth order reflects the distribution of the measured
signal. σi becomes smaller as the order increases. It, however,
seems to saturate at fourth or fifth order, and this tendency is
the same for the other subjects and datasets.
The estimation accuracy was then measured with various orders of p(θ). The dataset (i) was first divided to two, i.e., N
sequences and N sequences in case that the dataset included 2N
sequences. One half was used for the initial calibration and the
estimation of pi (θ), and the other half was used for the eye angle estimation. Then, the roles of the groups of sequences were
exchanged, the estimation was conducted again, and the results
were merged. This procedure was applied in the following experiments in the case of using the same dataset. Fig. 8 shows
the estimation errors in eye movement (upper) and absolute angle (lower), calculated as the root mean square error (RMSE).

MANABE et al.: DIRECT GAZE ESTIMATION BASED ON NONLINEARITY OF EOG

1559

Fig. 9. Example of estimated angle for subject 1. Values were extracted
from dataset (i): (a) with the nonweighted two-angle (one-saccade) option and
(b) with the weighted six-angle (five-saccade) option. Both eye movements
(upper) and absolute angle (lower) were successfully estimated, and the options
improved the accuracy of the absolute angle estimation.

The weighting was the inverse of σi in (14). Two angles were
estimated from one saccade, i.e., the two-angle option. The order had little influence on the error, so the choice of order was
not critical.
Figs. 7 and 8 imply that p(θ) does not have to be based on
a strict model and that simple low-order polynomial functions
are effective enough. Fourth order polynomial functions were
used in this study since they visually fit the measured signals,
in addition to the implication.
2) Estimation Result With a Subject: Fig. 9 shows an example estimation for dataset (i) and subject 1. There are two
cases: 1) nonweighted two-angle (one-saccade) estimation, i.e.,
the simplest option, and 2) standard-deviation-weighted sixangle (five-saccade) estimation, the best combination among those tested. The upper graph shows the estimated eye movement
θ̂1 − θ̂0 and the lower one the absolute angle θ̂1 . The estimated
absolute angle θ̂1 matches the true value, while the estimated
eye movement θ̂1 − θ̂0 , whose true value is +10 or −10◦ , is
very accurate.
Fig. 10 shows the estimation errors in the eye movement and
absolute angle from Fig. 9. In both cases over 95% of the eye
movements were successfully estimated with errors of less than
±1.5◦ and the options offered no improvement. In contrast,
whereas the errors in absolute angle are larger than those in
eye movement, the options yielded a significant improvement.
These figures confirm that the technique successfully estimates
the absolute eye angle simply from the difference between the
measured EOGs during a saccade(s) and that it also estimates
eye movement accurately.
Fig. 11 shows sequentially arranged θ̂1 and θ̂0 with various options. Since the technique accurately estimates eye
movement, the bar lengths fit the true value. However, the accuracy for absolute angle is not so high, and there are gaps between
the previous θ̂1 and current θ̂0 . The high-accuracy option, i.e.,
the weighted six-angle option, reduces these gaps.
The results for dataset (ii), not shown, also indicated that the
technique succeeded in estimating the absolute angle, and that
the options improved accuracy.

Fig. 10. Estimation errors in eye movement (upper) and absolute angle
(lower) corresponding to Fig. 9. While the errors in absolute angle decreased
with the options, no improvement was found for the eye movement, which was
already accurately estimated without options.

Fig. 11. Example of estimated saccades for dataset (i) of subject 1 with
various options, shown time sequentially. While the weighted six-angle estimation gave accurate saccades and seamless transitions between previous θ̂1 and
current θ̂0 , gaps are found between them in the estimated saccades with the
nonweighted two-angle option.

3) Results for all Subjects: Fig. 12 shows the estimation
errors in eye movement and absolute angle for dataset (i) for
various options and all subjects. High accuracy was obtained
for eye movement and the overall averaged error was 1.0◦ , but
no improvement was offered by the various options. On the
other hand, the absolute angle was successfully estimated for all
subjects, and the use of weighting and multiple saccades were
effective options regardless of the subject. While the overall
averaged error was 7.0◦ with the nonweighted two-angle option,
it fell to 3.0◦ with the weighted and six-angle option.
The results from dataset (ii) are shown in Fig. 13. The error
in eye movement was 1.2◦ –1.3◦ , while those in absolute angle
were 5.1◦ with the nonweighted two-angle option and 2.2◦ with
the weighted six-angle option. These values might not seem
to compare favorably with the error of 0.8◦ reported in [38].

1560

Fig. 12. Estimation errors in eye movement (upper) and absolute angle (lower)
with various options for dataset (i) and all subjects. Weighting and multiplesaccade options improved accuracy for all subjects. The nonweighted two-angle
option gave errors of 7.0 ◦ and weighted six-angle option gave errors of 3.0◦ as
overall averages in absolute angle.

Fig. 13. Estimation errors in eye movement (upper) and absolute angle (lower)
with various options from dataset (ii) for all subjects. The options improved
the accuracy for each subject. The nonweighted two-angle option and weighted
six-angle option, respectively, gave errors of 5.1◦ and 2.2 ◦ in overall average
absolute angle. The errors in eye movement were 1.2◦ −1.3 ◦ .

However, the reader should note that the error of the conventional techniques has been often reported in terms of the absolute
angle, but were calculated under the condition that the previous
eye angle was known and usually centered; that is, it almost
corresponds to eye movement in this study, θ̂1 − θ̂0 . Though
there is such a difference between the errors in absolute angle, the error of 2.2◦ approaches the accuracy reported in the
studies on conventional techniques. The estimation error with
the conventional linear model-based technique, in terms of the
absolute angle in this study, can be calculated using one of the
measured signals, “2–6.” It was assumed that the offset calibration requiring the user’s explicit effort was always conducted
just before each sequence and the signal value at the end of the
sequence was converted into an eye angle. This corresponds to
the “Drift” in Fig. 3 and its conversion into an angle. The results
included 5.2◦ for sequence (i)s and 3.5◦ for (ii)s as averages

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

Fig. 14. Estimation errors in eye movement (upper) and absolute angle (lower)
with various options for all subjects. Dataset (i) was used for the initial calibration, and dataset (ii) was used for the gaze estimation. The technique
succeeded in estimating the absolute angle accurately even when different sequences were used. The nonweighted two-angle option and weighted six-angle
option, respectively, gave overall average errors of 5.9◦ and 3.0 ◦ in absolute
angle. The errors in eye movement were 1.9◦ −2.1 ◦ .

of all subjects. Since the drift generally becomes smaller over
time, the sequence (ii)s which were measured after (i)s yielded
smaller errors. This assumption requires quite frequent offset
calibrations, almost every 20 s, and it is very impractical. When
the calibration was reduced to every two sequences, the error
deteriorated to 12.7◦ for (i)s and 6.7◦ for (ii)s. This result indicates that the proposed technique removes the burden of having
to make frequent calibrations to keep high accuracy.
The results shown previously were obtained using the same
dataset for the initial calibration and gaze estimation. In actual systems, the sequence used for the initial calibration would
differ from that for the gaze estimation, which means the estimation would often be performed on saccades that are not used
in the initial calibration. Thus, it is important to measure the
performances when different datasets were used. Fig. 14 shows
an example of this. Here, dataset (i) was used for the initial
calibration and dataset (ii) for the gaze estimation. 75% of the
saccades (12/16) in sequence (ii) were not included in sequence
(i). The results confirmed that the technique successfully estimated the absolute angle and that the nonweighted two-angle
option had an error of 5.9◦ and the weighted six-angle option
had an error of 3.0◦ . Though the errors in eye movement were
high for subject 3, the overall average error was 1.9◦ –2.1◦ . Reversing the roles of the datasets yielded an absolute angle error
of 3.7◦ with the weighted six-angle option and an eye movement
error of 0.8◦ –1.0◦ . Though the errors were higher compared with
those obtained when the same dataset was used, the degradation
was limited and the achieved accuracy is of practical use. These
results show that the technique works well even if different sequences are used. It also indicates that the initial calibration
could be simplified, which would improve the practicality of
interaction systems.
Only two pairs showed a negative effect of applying options; both can be seen in Fig. 14 by comparing the weighted
two- and three-angle options of subject 3, and the nonweighted

MANABE et al.: DIRECT GAZE ESTIMATION BASED ON NONLINEARITY OF EOG

and weighted six-angle options with subject 5. Positive effects
were found in the other 178 pairs, and Wilcoxon signed rank
tests confirmed that applying options significantly improves the
accuracy.
V. DISCUSSION
The results confirmed that our technique successfully estimates both absolute angle and eye movement. Several limitations must, however, be overcome before it can be implemented
in EOG-based interaction systems. The first one is how to perform automatic extraction of saccades in real time. Existing
techniques for identifying saccades such as [37] suggest an approach to automatic extraction. The second one is to cover various eye movements (only horizontal movements were examined
in the experiments). The third one is to make the transition between the previous θ̂1 and current θ̂0 seamless. One possible
solution is assigning a confidence value to each estimation. For
example, the minimized value of (17) represents some kind of
confidence. When the value of the current estimation is much
smaller than the previous one, the previous θ̂1 can be ignored.
On the other hand, when the current value is much higher, the
current process uses just the previous θ̂1 as the known current
θ0 to estimate the current θ̂1 . The fourth limitation is that it is
unclear which saccades and how many should be processed. If
the noise level can be assumed to be constant, large values of
the difference of EOGs have higher signal-to-noise ratios. Accordingly, the technique should be selectively applied to large
saccades. While sequential saccades with short durations would
yield higher accuracy when the multiple-saccade option is applied, those with long durations would decrease the accuracy.
This is because long durations are more likely to contain very
small eye movements that are difficult to identify exactly and
may shift the eye angle, which would invalidate the assumption
that the current θ0 can be taken as the previous θ1 . The criteria
determining whether the technique should be applied or not and
which options should be selected have to be established. Finally,
the following consideration is not a mandatory improvement,
but is desirable for actual interactive systems. The electrode
arrangement used in the experiments was not optimized, and
hence, a suitable arrangement should be identified. This paper
focused on saccades and ignored other eye movements such as
smooth pursuit. If the interactive system also uses smooth pursuit, we could take this into account by combining our method
with a conventional technique.
Though more study is needed, including improving the accuracy to a finer value than what conventional camera-based
techniques have already achieved [8], it is clear from this study
that our technique will work effectively in interaction systems.
For example, interaction based on the absolute angle of gaze
is practical, and more detailed information can be used even
if it is saccade-based. In the case that quite high accuracy is
required such as for selecting a small icon on a display, it is
better to use other more precise techniques rather than our proposal. EOG’s low power consumption feature, however, makes
the device always-on and the users will be able to use it in
anytime, anywhere in everyday life. Though dc-coupled EOGs

1561

were tested in the experiment, our method will also work with
ac-coupled signals. The use of ac-coupling will yield inexpensive user-friendly equipment. Moreover, the technique itself or
the finding that the measured EOG can be represented by a
simple polynomial function may be of help in removing EOG
components from EEGs.
VI. CONCLUSION
We proposed a gaze estimation technique that is based on the
nonlinearity between the EOG and eye angle. It estimates the
absolute eye angles before and after a saccade by using only
the difference between EOGs before and after a saccade. An
experiment conducted on five subjects confirmed that the EOG
has a nonlinear relationship with eye angle and that the proposed technique successfully estimates both the absolute eye
angle and eye movement. Using weighting and multiple saccades improves the estimation accuracy. The estimation error in
absolute angle was less than 4◦ , and high accuracy was attained
in assessing the eye movement. This technique provides a practical solution to the drift problem which has, up to now, made it
difficult to realize EOG-based eye tracking schemes.
REFERENCES
[1] S. Hutton, “Cognitive control of saccadic eye movements,” Brain Cognition, vol. 68, no. 3, pp. 327–340, 2008.
[2] J. M. Henderson, “Human gaze control during real-world scene perception,” Trends Cognitive Sci., vol. 7, no. 11, pp. 498–504, 2003.
[3] S. P. Liversedge and J. M. Findlay, “Saccadic eye movements and cognition,” Trends Cognitive Sci., vol. 4, no. 1, pp. 6–14, 2000.
[4] W. B. Horng et al., “Driver fatigue detection based on eye tracking and
dynamk, template matching,” in Proc. IEEE Conf. Netw., Sens. Control,
Mar. 2004, pp. 7–12.
[5] J. H. Goldberg et al., “Eye tracking in web search tasks: Design implications,” in Proc. Symp. Eye Tracking Res. Appl., 2002, pp. 51–58.
[6] S. Berger et al., “Assessing advertising effectiveness: The potential
of goal-directed behavior,” Psychology Marketing, vol. 29, no. 6,
pp. 411–421, 2012.
[7] L. E. Sibert and R. J. K. Jacob, “Evaluation of eye gaze interaction,” in
Proc. Conf. Human Factors Comput. Syst., 2000, pp. 281–288.
[8] T. Ohno et al., “FreeGaze: A gaze tracking system for everyday gaze
interaction,” in Proc. Symp. Eye Tracking Res. Appl., 2002, pp. 125–132.
[9] Y. Ishiguro and J. Rekimoto, “GazeCloud: A thumbnail extraction method
using gaze log data for video life-log,” in Proc. IEEE Symp. Wearable
Comput., 2012, pp. 72–75.
[10] C. Holland and O. Komogortsev, “Eye tracking on unmodified common
tablets: Challenges and solutions,” in Proc. Symp. Eye Tracking Res. Appl.,
2012, pp. 277–280.
[11] A. Bulling et al., “Wearable EOG goggles: Eye-based interaction in everyday environments,” in Proc. Extended Abstracts Conf. Human Factors
Comput. Syst., 2009, pp. 3259–3264.
[12] B. Estrany et al., “Human computer interface by EOG tracking,” in Proc.
Conf. Pervasive Technol. Related Assistive Environ., 2008, pp. 96:1–96:9.
[13] A. Bulling et al., “Eye movement analysis for activity recognition using
electrooculography,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 33,
no. 4, pp. 741–753, Apr. 2011.
[14] H. Manabe and M. Fukumoto, “Full-time wearable headphone-type gaze
detector,” in Proc. Extended Abstracts Conf. Human Factors Comput.
Syst., 2006, pp. 1073–1078.
[15] K. Yamagishi et al.,, “Development of EOG-based communication system
controlled by eight-directional eye movements,” in Proc. IEEE Conf. Eng.
Med. Biol. Soc., Aug. 2006, pp. 2574–2577.
[16] M. Marmor et al., “ ISCEV standard for clinical electro-oculography (2010
update),” Documenta Ophthalmologica, vol. 122, pp. 1–7, 2011.
[17] N. Itakura and K. Sakamoto, “A new method for calculating eye movement displacement from AC coupled electro-oculographic signals in head
mounted eye-gaze input interfaces,” Biomed. Signal Process. Control,
vol. 5, no. 2, pp. 142–146, 2010.

1562

[18] T. Yagi et al., “Drifting and blinking compensation in electro-oculography
(EOG) eye-gaze interface,” in Proc. IEEE Conf. Syst., Man, Cybern., 2006,
pp. 3222–3226.
[19] D. W. Patmore and R. B. Knapp, “Towards an EOG-based eye tracker for
computer control,” in Proc. Conf. Assistive Technol., 1998, pp. 197–203.
[20] R. Croft and R. Barry, “Removal of ocular artifact from the EEG:
A review,” Neurophysiologie Clinique/Clinical Neurophysiol., vol. 30,
no. 1, pp. 5–19, 2000.
[21] R. N. Vigário, “Extraction of ocular artefacts from EEG using independent component analysis,” Electroencephalography Clinical Neurophysiol., vol. 103, no. 3, pp. 395–404, 1997.
[22] C. A. Joyce et al., “Automatic removal of eye movement and blink artifacts
from EEG data using blind component separation,” Psychophysiology,
vol. 41, no. 2, pp. 313–325, 2004.
[23] A. Schlögl et al., “A fully automated correction method of EOG artifacts
in EEG recordings,” Clinical Neurophysiol., vol. 118, no. 1, pp. 98–104,
2007.
[24] D. Moretti et al., “Computerized processing of EEG-EOG-EMG artifacts
for multi-centric studies in EEG oscillations and event-related potentials,”
Int. J. Psychophysiol., vol. 47, no. 3, pp. 199–216, 2003.
[25] G. L. Wallstrom et al., “Automatic correction of ocular artifacts in the
EEG: A comparison of regression-based and component-based methods,”
Int. J. Psychophysiol., vol. 53, no. 2, pp. 105–119, 2004.
[26] T.-P. Jung et al., “Removing electroencephalographic artifacts by blind
source separation,” Psychophysiology, vol. 37, pp. 163–178, 2000.
[27] J. Kelly et al., “Fully automated reduction of ocular artifacts in highdimensional neural data,” IEEE Trans. Biomed. Eng., vol. 58, no. 3,
pp. 598–606, Mar. 2011.
[28] V. Häkkinen et al., “The effect of small differences in electrode position on
EOG signals: Application to vigilance studies,” Electroencephalography
Clinical Neurophysiol., vol. 86, pp. 294–300, 1993.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

[29] P. Berg and M. Scherg, “Dipole models of eye movements and blinks,”
Electroencephalography Clinical Neurophysiol., vol. 79, pp. 36–44, 1991.
[30] N. Itsuki et al., “A battery model of the eyeball to calculate standing potential of the eye,” J. Jpn. Ophthalmological Soc., vol. 99,
pp. 1012–1016, 1995.
[31] J. Kierkels et al., “A model-based objective evaluation of eye movement
correction in EEG recordings,” IEEE Trans. Biomed. Eng., vol. 53, no. 2,
pp. 246–253, Feb. 2006.
[32] F. Simini et al., “Gaze tracker by electrooculography (EOG) on a headband,” in Proc. 10th Int. Workshop Biomed. Eng., 2011, pp. 1–4.
[33] H. Manabe et al., “Automatic drift calibration for EOG-based gaze input
interface,” in Proc. IEEE Conf. Eng. Med. Biol. Soc., 2013, pp. 53–56.
[34] N. Itsuki et al., “Improved method for measuring electrooculogram and
its evaluation,” in Proc. IEEE Conf. Control, Autom. Robot. Vision, 2004,
pp. 947–952.
[35] D. Kumar and E. Poole, “Classification of EOG for human computer
interface,” in Proc. IEEE Conf. Eng. Med. Biol. Soc., 2002, pp. 64–67.
[36] E. McAdams, “ Biomedical electrodes for biopotential monitoring and
electrostimulation,” in Bio-Medical CMOS ICs. New York, NY, USA:
Springer, 2011, pp. 31–124.
[37] D. D. Salvucci and J. H. Goldberg, “Identifying fixations and saccades
in eye-tracking protocols,” in Proc. Symp. Eye Tracking Res. Appl., 2000,
pp. 71–78.
[38] J. Rapela et al., “Assisting autistic children with wireless EOG technology,” in Proc. IEEE Conf. Eng. Med. Biol. Soc., 2012, pp. 3504–3506.

Authors’ photographs and biographies not available at the time of publication.

