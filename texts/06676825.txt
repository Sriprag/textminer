1678

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Semiautomatic Segmentation of Brain Subcortical
Structures From High-Field MRI
Jinyoung Kim, Christophe Lenglet, Yuval Duchin, Guillermo Sapiro, Fellow, IEEE, and Noam Harel

Abstract—Volumetric segmentation of subcortical structures,
such as the basal ganglia and thalamus, is necessary for noninvasive diagnosis and neurosurgery planning. This is a challenging
problem due in part to limited boundary information between
structures, similar intensity profiles across the different structures, and low contrast data. This paper presents a semiautomatic segmentation system exploiting the superior image quality
of ultrahigh field (7 T) MRI. The proposed approach utilizes the
complementary edge information in the multiple structural MRI
modalities. It combines optimally selected two modalities from
susceptibility-weighted, T2 -weighted, and diffusion MRI, and introduces a tailored new edge indicator function. In addition to
this, we employ prior shape and configuration knowledge of the
subcortical structures in order to guide the evolution of geometric
active surfaces. Neighboring structures are segmented iteratively,
constraining oversegmentation at their borders with a nonoverlapping penalty. Several experiments with data acquired on a 7 T MRI
scanner demonstrate the feasibility and power of the approach for
the segmentation of basal ganglia components critical for neurosurgery applications such as deep brain stimulation surgery.
Index Terms—Basal ganglia and thalamus, deep brain stimulation (DBS), geodesic active surface (GAS), segmentation, ultrahigh
field MRI.

I. INTRODUCTION
IFFERENTIATION and localization of brain structures
are crucial components for any neuroscience research or
clinical applications. Volumetric segmentation is a prerequisite
for many neuroimaging studies such as voxel-based morphometry (VBM), statistical shape analysis, white matter fiber trac-

D

Manuscript received September 4, 2013; revised September 14, 2013; accepted November 17, 2013. Date of publication November 27, 2013; date of
current version September 2, 2014. This work was supported in part by the
NIH under Grant R01NS085188, Grant R01 EB008645, Grant R01 EB008432,
Grant S10RR026783, Grant P30 NS057091, Grant P30 NS076408, Grant
P41 RR008079, and Grant P41 EB015894, in part by the Human Connectome
Project (U54 MH091657) from the 16 NIH Institutes and Centers that support
the NIH Blueprint for Neuroscience Research and the W.M. Keck Foundation.
The work of G. Sapiro was supported by Office of Naval Research, National
Geospatial-Intelligence Agency, Army Research Office, and Air Force Office
of Scientific Research.
J. Kim is with the Department of Electrical and Computer Engineering, Duke
University, Durham, NC 27708 USA (e-mail: aeneas.kim@gmail.com).
C. Lenglet and Y. Duchin are with the Center for Magnetic Resonance
Research, University of Minnesota, Minneapolis, MN 55455 USA (e-mail:
clenglet@umn.edu; yuval.duchin@gmail.com).
G. Sapiro is with the Departments of Electrical and Computer Engineering and Biomedical Engineering, Duke University, Durham, NC 27708 USA
(e-mail: guillermo.sapiro@duke.edu).
N. Harel is with the Department of Neurosurgery, Center for Magnetic
Resonance Research, University of Minnesota, Minneapolis, MN 55455 USA
(e-mail: noam@cmrr.umn.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2292858

tography from diffusion-weighted magnetic resonance imaging
(MRI), or seed-based analysis of resting-state functional MRI
(fMRI). It is also critical for surgical interventions such as deep
brain stimulation (DBS) [1] or tumor resection. However, manual segmentation is prone to inherent confounds such as operator
subjectivity and inter- or intraobserver variability of border definitions, which are all driven by the quality and richness of
the input data. Most importantly, manual segmentation of fine
brain structures is a tedious, time consuming, and significantly
limiting factor for any clinical or translational workflow that requires anatomical definition. The problem is further aggravated
when multiple modalities are available, each modality providing enhanced information for the segmentation of specific structures, forcing the user to discover that and to constantly switch
between them. These challenges will become more and more
relevant with the proliferation and advances of high-field MR
machines that provide higher resolutions images with superior
contrast that allows the delineations of smaller structures with
greater shape complexity.
Various segmentation frameworks have been reported to automate the manual segmentation during the last two decades.
However, most segmentation methods still require user intervention, and some artifacts such as oversegmentation around
boundaries of neighboring objects are unavoidable. In particular, when an image has low-contrast or objects to be segmented
are occluded, segmentation techniques have shown limited performance [2], [3]. Therefore, segmentation of complex and adjacent objects such as subcortical structures in brain MR images
still remains a challenging task. In general, segmentation approaches are based on local edge information (edge based) or
the intensity of a given image (region based). Accuracy of edge
detection and the image quality such as its contrast-to-noise ratio (CNR) and signal-to-noise ratio (SNR) are critical factors in
the segmentation performance. On the other hand, region-based
approaches utilize the distribution of intensities over the entire
region of interest, and they are more robust to noise or missing
information than edge based approaches [4]. However, neighboring regions can have similar intensity distributions that often
overlap.
Recently, it has been reported that Susceptibility Weighted
Image (SWI) at higher magnetic fields provides superior image
contrast, thereby allowing improved delineation of subcortical
structures [5]. Moreover, detailed anatomical information obtained by combining SWI with T2 -weighted (T2 W) or fractional
anisotropy (FA) images enables localization and visualization
of subcortical structures [6].
In this paper, we focus on the segmentation of subcortical
structures such as the basal ganglia and thalamus from MRI

2168-2194 © 2013 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution
requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

data obtained at high magnetic field (7 T), critical for any neurosurgery planning and particularly for DBS procedures [1].
We start with an edge based segmentation approach to exploit
sufficient edge information on the MRI (with high CNR and
SNR), embedded in an active contour/surface model [7]. The
geodesic active contour/surface (GAC/GAS) model [8] originally translated the energy based active contours’ minimization
problem into a geometric curve evolution approach computing a geodesic curve in a Riemannian space via the level-set
method [9], thereby handling topological changes of evolving
curves as well as increasing attraction of the active contour toward the boundary, even with high variation of gradient values.
Its three-dimensional (3-D) extension led to the geodesic active
surface (GAS) model [10], which is the basis of our proposed
framework. However, this approach fails to achieve accurate
segmentation results for occluded objects or regions with weak
or missing boundaries that commonly exist in MRI data.
Various approaches have been proposed in order to address
this problem by incorporating shape prior information [2], [3],
[11], [12]. In [11] in particular, training shapes are represented
by the level-set method as a Gaussian distribution in the subspace obtained by principal component analysis (PCA), and
level-set curves evolve toward a best-fit shape estimated using
maximum a posteriori (MAP) within the GAC framework. More
recently, [12] jointly incorporates shapes of multiobjects and
their relative pose relationships into a region-based approach for
the segmentation of multiple structures. Our proposed method
considers the volumetric shape model incorporated into the GAS
framework [11].
Additionally, we extract the edge information integrating
edge maps generated from multimodal MR images (referring
to different image contrasts in this context) such as SWI, T2 W
image, and FA image, using a new edge indicator function.
Boundary information from the shape prior, initially located
on a region overlapping with an object after registration onto
the data to be segmented, is applied as a weighting factor for
the edge maps of the multimodal images. Several segmentation
approaches using multimodal MR data have been introduced
and applied for localization of deep brain structures or prostate
cancer [13], [14]. These works present segmentations improved
by combining additional information from the diffusion data
(e.g., FA or apparent diffusion coefficient (ADC) image). However, we fully exploit superior contrast and SNR properties of
SWI, T2 W image and FA image at the high field, providing a
new edgemap. Moreover, we present the optimal combination
of multimodal data for each brain subcortical region.
Oversegmentation around boundaries between neighboring
structures is inevitable during the semiautomatic segmentation
process. Overlapping regions are often found also on manual
segmentations because of inaccurate definition of the boundary
information. However, accurate delineation of adjacent structures, such as the basal ganglia and thalamic structures in
the brain, provides crucial information in neurosurgery procedures such as DBS [5]. Some approaches have been proposed
in the literature in order to overcome such overlapping problem [15]–[17]. A multistage level set segmentation within the
geodesic active region framework [15], and coupled parametric

1679

active contours [16], respectively, employing an additional force
to constrain the overlapping during the propagation of multiple
regions are introduced. More recently, an efficient multiobject
level set method using the projection method within the GAC
framework was proposed in [17].
However, simultaneous segmentation of adjacent structures
might lead to inaccurate delineation (i.e., dividing line) even
though it disjoints those structures in an efficient way, in practical cases (e.g., SN and STN) where there exist unclear boundaries (i.e., similar intensity profiles within adjacent structures)
between those regions or the initialization is not favorable, especially, in [16].
In this work, we have added a penalty term into our framework, considering adjacency between basal ganglia and thalamic structures, thereby incorporating another layer of prior
structural information. The segmentation process for each
structure follows the subject-specific manual analysis pipeline
presented in [6]. Structures are initially segmented with two
different contrast MR images (simultaneous combination of additional contrasts does not necessarily improves the results since
the regions are not all visible in all contrasts), as well as prior
knowledge about those regions and represented by the level-set
method. Then, each predefined level-set surface is utilized as a
nonoverlapping constraint, limiting the possible deformation of
the other evolving surface toward its adjacent structures within
our framework. Moreover, a set of segmented adjacent structures can be further refined in order to produce more accurate
delineation during the iterative process.
The remaining parts of the paper are organized as follows.
Section II presents each extension within the GAS framework
in detail. In Section III, we present the overall schematic for
the segmentation of basal ganglia and thalamic structures. We
then present experimental results on real 3-D MRI in Section IV.
Finally, we conclude with possible future research directions in
Section V.

II. METHOD
We extend the geodesic active surface (GAS) model (or minimal surfaces) by incorporating additional global information,
including shape priors and nonoverlapping constraints. The target application in this work is the segmentation of subcortical
structures, as a key ingredient in DBS protocols, and additional
known nonoverlapping constraints are exploited, in the form
of negative distance forces between the corresponding evolving surfaces. This encodes the basic anatomical relationships
between the different components in these regions. The perstructure shape prior model is built via a probabilistic approach
and incorporated into the GAS model by estimating its best-fit
shape and pose while guiding the evolving surfaces toward it.
A newly introduced edge indicator function is obtained by integrating edge maps generated from the Laplacian of the smoothed
multimodal datasets (SWI, T2 W, and FA image from DWI), together with boundary information about the given shape prior
(initially, it is registered onto the region maximally matching
with the structure to be segmented). Overview of the proposed

1680

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

embedding function given by the signed distance map, u : R+ ×
R3 → R, whose zero level-set function is the surface S (i.e.,
u(t, S) = 0)




∇u
∇u
∂u
= |∇u| div g (I)
= g (I) |∇u| div
∂t
|∇u|
|∇u|
+ ∇g (I) · ∇u.

(4)

As it is standard practice, the following minimal surfaces
model is obtained by adding a constant motion force c, weighted
by g(I), in order to increase the speed of convergence [10]
 


∇u
∂u
= div g (I)
+ cg (I) |∇u|
∂t
|∇u|
Fig. 1.

= g (I) (c + H) |∇u| + ∇g (I) · ∇u.

Schematic overview of the proposed segmentation.

segmentation framework is presented in Fig. 1. The next sections
describe these contributions in detail.
A. GAS Model
Given a 3-D image I : R3 → R+ . (this will later be
extended to multimodal data, meaning vectorial 4-D images) and an evolving two-dimensional (2-D) surface S :
R+ × [0, 1]2 → R3 in parametric form, S(t, r, v) = {x(t, r, v),
y(t, r, v), z(t, r, v)}, (t, r, v) ∈ R+ × [0, 1]2 , the goal of active
surfaces is to propagate this 2-D surface in the 3-D image space
such that it evolves toward the region of interest and stops at
its boundary. For this purpose, a deformable surface model was
proposed by Terzopoulos et al. [7]. The GAS model was developed to extend this classical model [10], and is based on solving
the functional

g(I(S))da
(1)
EGAS (S) =
A (S )

where da is the Euclidean element of area,and the Euclidean
area of the surface S is given byA(S) :=
da. Also, g is a
decreasing function such that g(χ) → 0 as χ → ∞ (to be more
precise, as the gradient magnitude of the variable goes to zero).
This edge indicator function should attract the surface toward
the objects of interest, and in [8] and [10], it was selected as
(considering it is now defined in the whole image space and not
just on the evolving surface)
g(I) =

1
ˆγ
1 + |∇I|

(2)

where Iˆ is a smoothed version obtained by regularizing I using
anisotropic diffusion, and γ is 1 or 2. For the simplification, we
write g(I) instead of g(I(S)).
The following surface evolution (Euler–Lagrange) equation,
minimizing EGAS , is obtained by calculus of variations:
∂S
 )N

= (g(I)H − ∇g(I) · N
(3)
∂t
 is the inner unit normal
where H is the mean curvature and N
to the evolving surface S. The steepest descent flow described
earlier is implemented using the level-set method [9] via an

(5)

In this level-set representation, the surface u evolves at all
points normal to the level set as a function of the image gradient
and the surface curvature at that point. The term ∇g (I) · ∇u
provides stable detection of boundaries even if variations in
their gradient are large, and makes the model more robust to
parameters choice [10].
In this paper, the segmentation of subcortical structures from
MRI data is performed within this GAS framework, with extension to be presented in the next three sections.
B. Guidance From Statistical Shape Models
The GAS model utilizes edge information to detect objects as
discussed in the previous section. This approach has shown reliable and fast in many applications [3], [11], [18], [19]. However,
low contrast or occlusion around objects’ boundaries might lead
to inaccurate segmentations. In this case, guiding the surface
evolution via shape (and pose) prior information can considerably improve the quality of the segmentation. In [11], the shape
model is built based on a probabilistic approach and is then incorporated into the GAS framework. The modeling of the shape
prior and the estimation of the shape and pose parameters is
briefly summarized later.
1) Shape Representation: Each surface in the provided training dataset T = {S1 , . . . , Sl . . . , Sn }, represented as a binary
segmentation, is embedded as the zero level-set of a higher
dimensional surface Sl ∈ R3 → R, using the signed distance
map, where each point (N 3 points if it is assumed that a 3-D
shape template is cropped into a size N in each dimension) encodes the distance to the nearest points on the surface. A mean
surface (shape) μ is obtained as the arithmetic mean of the training shape T . The variance of the shape is computed using PCA.
3
A matrix M ∈ RN ×n is constructed consisting of column vectors Ŝl , obtained from subtracting the mean μ from Sl . Then,
the covariance matrix (1/(n − 1))M M  is decomposed using
singular value decomposition (SVD)
U ΣU  =

1
M M .
n−1

(6)

Here, U ∈ RN ×n is a unitary matrix whose columns represent
n orthogonal modes of shape variation, and Σ ∈ Rn ×n is a
diagonal matrix of the corresponding eigenvalues as scaling
3

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

factors along these variations. An estimate of a new shape u
is represented by combining the first k principal components,
and is given by the coefficients ψ ∈ Rk . The dimension of the
training set is therefore reduced to k by projecting u − μ onto
the k principal components,
ψ = Uk (u − μ)

(7)

where Uk is a matrix with the first (largest corresponding eigenvalues) k columns of U . Given ψ, the estimate ũ of u is reconstructed as
ũ = Uk ψ + μ.

(8)

2) Prior Shape and Pose Estimation: The probability of a
particular surface is computed by assuming, following the PCA
model, a Gaussian distribution in the reduced shape subspace


1
1  −1
P (ψ) = 
(9)
exp − ψ Σk ψ
2
(2π k |Σk |)
where Σk is a matrix with the first (largest eigenvalues) k rows
and columns of Σ.
Note that the shape model cannot guide the surface evolution without its global pose information, as the structures must
be registered for the shape information to be relevant. Let the
shape surface u∗ be determined from the shape parameter ψ
and the pose p. The surface u evolves toward the target shape by
estimating u∗ using the MAP at a given discrete time t following
u (t + 1) = u (t) + λ1 (u∗M AP (t) − u (t))

(10)

where λ1 ∈ [0, 1] is a coefficient that controls the effect of the estimated surface model. More specifically, u∗ is estimated using
MAP at each update of the surface evolution
u∗M AP = argmax P (u∗ |u, ∇I).
u∗

(11)

Accordingly, parameters ψ and p are also estimated (12), and
u∗M AP is then computed using Bayes Theorem [11]
	ψM AP , pM AP 
 = argmax P (ψ, p|u, ∇I).

(12)

ψ ,p

3) Level-Set Evolution With Shape Information: The evolution equation of the surface in (10) is incorporated into the
(discrete time) level-set equation (5):
u (t + 1) = u (t) + λ1 (u∗M AP (t) − u (t))
+ λ2 (g (I) (c + H) |∇u (t)| + ∇g (I) · ∇u (t)) (13)
where λ2 controls the tradeoff between the shape prior and the
image forces. In this framework, the surface evolves globally,
toward the MAP estimate of a given shape model (prior), and
locally based on image gradient and surface curvature.
C. New Multimodal Edge Indicator Function
The inverse function g (I) of image gradient in (2), commonly selected as an edge indicator in active surface models,
often fails to generate clear edge information when the objects
to be segmented have low contrast boundaries. In this section,
a new edge indicator function is introduced by combining edge
maps generated from the Laplacian of multimodal images (i.e.,

1681

multiple modalities all derived from MRI), together with boundary information from the shape prior presented in the previous
section.
Recently, it has been shown that SWI at higher magnetic fields
shows superior contrast, especially within the basal ganglia and
thalamus structures, by comparison with T1 W and T2 W images [5], [20]. To take advantage of this, the edge information
of the SWI is integrated with that of T2 W images, and with FA
images obtained from diffusion MRI, defining a new edge indicator function. This automates the procedure typically followed
by experts performing the manual segmentation and switching
between various modalities to exploit information from multiple
contrasts [6]. The main steps to compute this new edge indicator
function with fusion of multimodal images are described next.
1) Edge Maps From Two Modality MR Images: The stopping function g in (2) is substituted by g  using a sigmoid function, whose center and width are controlled by the user [2].
Additionally, the zero crossing of the Laplacian image, instead
of the image gradient, is applied in order to detect more detailed
boundaries
g  (I) =

1
−((Δ Iˆ−β )/α )

(14)

1+e

where β is the center of the intensity range and α is in inverse proportion to the slope of the function at β (i.e., the slope
is 1/4α). Also, ΔIˆ is the Laplacian of a regularized image.
Note that regularization such as anisotropic diffusion before the
Laplacian operation is important for the noisy SWI.


and gLow
, are computed with
Two edge map terms, gHigh
βHigh and βLow , and a fixed value of α, tuned by the user
with respect to each modality image, such as the T2 W (or FA)
and SWI images, using (14). These edge map terms have comparable values (0 or 1) within regions with strong boundaries
or homogeneous intensities, but have different values in intermediate regions (see Fig. 2). More specifically, a positive (or
negative) α is selected to transform higher intensity values of
the Laplacian magnitude into homogeneous regions (or boundaries) by the sigmoid function. As in Fig. 2(a), if α > 0, βHigh is
manually chosen to be the value of the Laplacian magnitude of

, where regions with intenthe smoothed SWI to produce gHigh
sity values over βHigh are considered as strongly homogeneous,
thereby capturing more edge information on the SWI. Also,
βLow is manually chosen to be the value of the Laplacian mag
,
nitude of the smoothed T2 W (or FA) image to produce gLow
where regions with intensity values under βLow are considered
as strong boundaries, thereby capturing wider homogeneous regions on the T2 W (or FA) image. On the other hand, as in
Fig. 2(b), if α < 0, βHigh is manually chosen to be the value of
the Laplacian magnitude of the smoothed T2 W (or FA) image

, where regions with intensities over βHigh are
to produce gHigh
considered as strong boundaries, thereby capturing wider homogeneous region on the T2 W (or FA) image. Additionally, βLow is
manually chosen to be the value of the Laplacian magnitude of

, where regions with intensities under
the SWI to produce gLow
βLow are considered as homogeneous regions, thereby capturing
more edge information on the SWI.

1682

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Fig. 3. Interpretation of gn e w on the clear edge and unclear edge in a 1-D

(α < 0). (a) g  with β H ig h on T2 W in (14), gH
. (b) g  with β L ow on
ig h (T 2 W )
SWI in (14), gL ow (S W I) . (c) Dirac measure for the level set representation of a

 

mean shape, δ0 u ∗0 . (d) New edge map gn e w .

Fig. 2. Sigmoid function with βL ow = −10, β H ig h = 10, and (a) α = 1 and
(b) α = –1.

2) Generation of a New Edge Map gnew : A new edge map



and gHigh
. Let gLow
gnew , is obtained from gLow
(T 2 W (or FA)) or

gHigh(T 2 W (or FA)) be the edge map terms computed using (14)
with βLow and a fixed positive α, or βHigh and a fixed negative


α, on the T2 W (or FA) image; and let gLow
(SW I) or ghigh(SW I)
be the edge map terms computed with βLow and a fixed negative
α, or βHigh and a fixed positive α, on the SWI. An edge map


and gLow
with
gnew is computed by weighted averaging of gHigh
∗
the (smoothed) Dirac measure δε of u0 , the level set of a given
shape prior on the initial position (registered onto the test data),

to weight the values of glow
(T 2 W (or FA)) on the homogeneous

region and gHigh(SW I) on the boundary surface (zero level set)

within the shape prior if α > 0, or values of gHigh(T
2 W (or FA))

on the homogeneous region and g Low (SW I) on the boundary
surface within the shape prior if α < 0:
⎧
(1 − δε (u∗0 )) g  Low (T 2 W (or FA))
⎪
⎨
+δε (u∗0 ) g  High(SW I) ,
α>0
gnew =
(15)
(1
−
δε (u∗0 )) g  High(T 2 W (or FA))
⎪
⎩
+δε (u∗0 ) g  Low (SW I) ,
α<0
where the Dirac measure δε is defined as the regularized version
of the derivative δ0 (z) of the Heaviside function H(z) [2], [4]:
δε (z) =

ε2


where

H(z) =

ε
,
+ z2

1,
0,

δ0 (z) =

z≥0
z<0

d
H(z)
dz
(16)

Here, ε is the width of the function. Specifically, δε (u∗0 ) with
ε = 1 is 1 on the boundary of a given shape prior on the initial position. In this case, gnew captures sufficient boundary
information on the SWI if it is assumed that the shape prior
on the initial position optimally matches the object to be seg-

mented (gnew is fixed during the shape/surface evolution for
stability). In particular, the initial shape surface u∗0 is obtained
by averaging shape priors registered onto the object to be segmented (or the boundaries of the regions on the edge map


(gHigh(SW
I) if α > 0, gLow (SW I) if α < 0) generated from the
SWI), using FSL FLIRT (FMRIB’s Linear Image Registration
Tool) [21] or the Euler transformation in 3-D provided by the
user (i.e., u∗0 = μ). Proper initial placement of a given shape
model contributes not only to accurately integrate boundary information of the initial shape surface into the new edge map, but
also reduces the MAP estimation time of the shape prior step.
Note that corresponding training shapes registered initially onto
the test data are overlapped with structures to be segmented
since training shapes are manually segmented versions from
other datasets on the same ROI as the test dataset.

,
Fig. 3 shows a simple interpretation of gHigh(T
2 W)

∗
gLow (SW I) , δ0 (u0 ), and gnew for a clear and an unclear edge in

case of α < 0 in one-dimensional (1-D). gLow
(SW I) has wider
boundary regions due to small βLow and superior contrast on

. Also, δ0 (u∗0 ) is 1 on the
the SWI, compared with gHigh(T
2 W)
boundary of the initial shape prior. gnew is computed as the


∗
weighted average of gLow
(SW I) and gHigh(T 2 W ) with δ0 (u0 ),


capturing gLow
(SW I) on the boundary and gHigh(T 2 W ) on the
homogeneous region within the initial shape prior. Therefore,
gnew has more detailed boundary information, when compared


with gLow
(SW I) or gHigh(T 2 W ) .
Fig. 4 shows the SWI and T2 W images in three orthogonal

directions, corresponding to the Laplacian outputs, gLow
(T 2 W )

with α = 0.5 and βLow = 8, gHigh(SW I) with α = 0.5 and βHigh
= 13, δε of a shape prior for the left external Globus Pallidus

(GPe), and gnew on the ROI of the 2-D axial slice. gLow
(T 2 W )

contains wider homogeneous regions, while gHigh(SW I) has

more detailed edge information. In particular, gHigh(SW
I) provides clearer separation of the left GPe and internal Globus
Pallidus (GPi) [see red circle in Fig. 4(h)]. This is attributed

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

1683

Fig. 5. Image gradient magnitude of the SWI and T2 W images and their
corresponding g in (2). (a) Gradient magnitude of smoothed T2 W. (b) Gradient
magnitude of smoothed SWI. (c) gT 2 W [inverse of (a)]. (d) gS W I [inverse of
(b)].

Fig. 6. Iterative segmentation flow for SN and STN within modified GAS
framework.

Fig. 4. New edge map generated by combining axial T2 W image with SWI.
(a) Axial T2 W image. (b) Axial SWI. (c) ROI of T2 W image. (d) ROI of SWI.
(e) Laplacian of smoothed T2 W image. (f) Laplacian of smoothed SWI. (g)

with α = 0.5, βH ig h =
gL ow (T W ) with α = 0.5, βL ow = 8. (h) gH
ig h (S W I)
2
13. (i) δε (u ∗0 ) with ε = 1. (j) gn e w . Note that regions around left GPe and GPi
(the red circle) in (j) are improved.

(3) is presented in Fig. 5. The edge map produced by g does not
have sufficient information, by comparison with the edge map
generated by gnew . In particular, boundaries between the left
GPe and GPi are not well identified. The new edge map introduced in this section is more accurate, integrating information
from the T2 W and SWI images together with the shape prior.

to the superior contrast of the SWI, enabling the identification of thin boundaries (lamina pallid medialis) separating GPe
and GPi [5]. Stronger boundaries are exhibited by the intensity
transformation using g  with βHigh . Finally, we observed that
gnew [see Fig. 4(j)] shows clearer boundaries by weighted av

∗
eraging of gLow
(T 2 W ) and gHigh(SW I) with δε (u0 ) of ε = 1.
More specifically, edge information around the left GPe in gnew

comes from boundaries of gHigh(SW
I), and homogeneous region

within the left GPe comes from gLow (T 2 W ) if it is assumed that


the intensity of gHigh(SW
I) on boundaries and gLow (T 2 W ) on an
homogeneous region within a given shape prior for left GPe on
the initial position is ideally 0 and 1, respectively. On the other


hand, gHigh(SW
I) and gLow (T 2 W ) are averaged with the weight∗

ing value δε (u0 ) on the region where gHigh(SW
I) is not 0 on the

boundary or gLow (T 2 W ) is smaller than 1 on the homogeneous
region within a given shape prior for GPe [see gray scale region
in Fig. 4(j)]. For comparison, the image gradient of each singlemodality image, SWI or T2 W image, and the corresponding g in

D. Anatomical Constraints Between Adjacent Structures
Overlap between adjacent segmented objects is often inevitable even if the global shape information and the new edge
indicator function discussed in the previous sections are employed. Anatomical constraints for nonoverlap and adjacency
should be considered for more accurate segmentation of the
basal ganglia [15]–[17] (such constraints can be imposed for
other anatomical segmentation tasks as well). In this section, a
global penalty term constraining the propagation of each adjacent surface to avoid overlaps is considered and incorporated
into our extended active surface model.
1) Level Set Evolution With Anatomical Constraints: A presegmented version obtained from our model (e.g., segmented
SN in Fig. 6) is utilized as a repulsion constraint for its adjacent structures. This means the presegmented objects act as a
global force in opposite direction to the shape priors (and edge
and curvature) of their neighboring objects during the segmentation process. Furthermore, adjacent structures are iteratively

1684

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

segmented, avoiding overlapping regions between them by constraining the oversegmentation of their adjacent structures. During the iterative process, the structures are corrected in order
to obtain clear boundaries between them and to maximize the
Dice’s coefficient (DC) values defined as
DC =

2 (VA ∩ VB )
VA + VB

(17)

where VA and VB are the respective volumes of structures A
and B to be compared for their similarity measurement.
We consider the resulting surface in (13), whose adjacent
structure to be segmented exists, as uadj . It is represented by
level sets using a signed distance function. uadj is fixed at this
step of the iteration, and the negative distance between uadj and
the current evolving surface u adjacent to it acts as the repulsion
force during the segmentation process. The surface evolution
equation in the negative direction of the distance between uadj
and u is given by (at a given discrete time t and with the constant
weight λ3 )
u (t + 1) = u (t) − λ3 (uadj (t) − u (t)) .

(18)

The surface evolution equation with nonoverlapping constraints in (18) is incorporated into the update expression (13)
with shape priors and gnew as introduced in the previous sections,

Fig. 7. Segmentation results of SN and STN at each iteration. Top shows contours in both axial and coronal slices, and bottom represents the corresponding
3-D structures. (a) First iteration: The green and red represent the first segmented SN without the constraint and the first segmented STN with the first
SN, respectively. (b) Second iteration: The green and red represent the second
segmented SN with the first STN and the first segmented STN with the first SN,
respectively. (c) Third iteration: The green and red represent the second segmented SN with the first STN and the second segmented STN with the second
SN, respectively. The blue represents manually segmented SN and STN.
TABLE I
CORRESPONDING NONOVERLAPPING CONSTRAINTS FOR EACH
STRUCTURE TO BE SEGMENTED

u (t + 1) = u (t) + λ1 (u∗ (t) − u (t))
+ λ2 (gnew (c + H) |∇u (t)| + ∇gnew · ∇u (t))
− λ3 (uadj (t) − u (t)) .

(19)

2) Iterative Process for Nonoverlapping Structures: This
process is iterated, so that different structures can be segmented
using a specific set of nonoverlapping constraints. Fig. 6 shows
such an iterative segmentation workflow for the substantia nigra
(SN) and subthalamic nucleus (STN). First, SN is segmented
without applying penalty for overlapping. If the structure does
not have neighboring structures to be segmented, λ3 is set to
0, disabling the nonoverlap constraints. Initially segmented SN
is utilized as the nonoverlapping constraint to segment STN
at the next iteration. Then, the segmented STN is also utilized
to constrain and correct oversegmentation of SN. This process
can be repeated until convergence (defined as the state where
no significant changes in the segmentation of the desired structures occur, considering overlapping region and DC values at
the same time). Variations of the SN and STN at each iteration
are presented in Fig. 7. Each structure is shown as a 2-D contour
on axial and coronal slices. Red, green, and blue contours represent segmented STN, segmented SN, and the “ground truth”
(manual segmentation), respectively, for each structure. Initial
segmentation results in Fig. 7(a) show large overlapping regions,
attributed to oversegmentation of each structure around unclear
boundaries between SN and STN. The overlapping regions are
considerably reduced, and segmentation results are corrected
toward the ground truth, as the segmentation progresses.
Note that nonoverlapping penalties can be additionally incorporated as the number of adjacent structures increases. For
example, GPe has two adjacent structures—Pu and GPi (see

Table I)—and thus each structure is segmented with an initially
segmented GPe. Segmentation of GPe at the next iteration is
constrained by both presegmented Pu and GPi.
The nonoverlapping constraint introduced in this section improves the segmentation mostly for unclear boundaries between
neighboring structures by providing each structure with global
shape information about its neighboring structure, and the iteration process enables to further refine boundaries between those
structures by the user intervention. Moreover, this could significantly aid in the segmentation when the input image has lower
contrast or SNR such as in clinical 1.5 or 3 T. In particular,
this process is a critical feature for the accurate segmentation of
basal ganglia structures to be presented in the next section.
III. APPLICATION TO THE SEGMENTATION
OF BASAL GANGLIA STRUCTURES
A. Segmentation Workflow
The workflow for the proposed semiautomatic volumetric
segmentation process of the basal ganglia component and thalamus is shown in Fig. 8. GPe and GPi are segmented on the axial
images since the lamina pallid medialis [5], which represents
boundaries between GPe and GPi, is shown well in the axial
SWI. SN and STN are segmented on the coronal images since

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

1685

TABLE II
TRAINING SHAPE SET AND GROUND TRUTH FOR EACH TEST AND THE
CORRESPONDING SUBJECT

Fig. 8. Schematic workflow for the semiautomatic 3-D segmentation of basal
ganglia components and thalamus.

single-modality tools for segmentation of subcortical regions.
We quantitatively measure the performance of each approach
using the DC (17) and visually analyze segmented volumes on
the Amira software package [26], facilitating the simultaneous
visualization of multiple structures.
A. Experimental Environment

this direction shows high contrast, allowing the delineation between SN and STN. Also, the FA image is utilized to segment
caudate nucleus (CN), putamen (Pu), and thalamus (Tha). We
fully utilize multimodal images, combining SWI, T2 W, and FA
from DWI to segment all the structures according to the subjectspecific analysis pipeline presented in [6]. More specifically,
axial T2 W image registered onto the SWI, and axial SWI itself
are utilized to generate an edge map for the segmentation of
GPe and GPi. Coronal T2 W image registered onto the SWI, and
coronal SWI itself, are utilized to segment SN and STN. CN,
Pu, and Tha are segmented from the axial SWI registered onto
the FA image, and the FA image itself.
B. Anatomical Constraints Between the Basal
Ganglia Structures
The training set for the shape priors consists of manual segmentations obtained from other subjects, or the same subject
on other scan dates. Corresponding nonoverlapping constraints
for each structure are summarized in Table I, considering the
high probability of connection between neighboring structures
presented in [6]. All segmented structures are finally overlapped
on the desired modality after registration.
IV. EXPERIMENTAL RESULTS
In this section, we present segmentation results of the basal
ganglia component and thalamus on real 3-D 7 T MRI using
our proposed method. Quantitative evaluations on various combinations of two single-modal images are performed to identify
the optimal multimodality combination approach. Segmentation
results are compared with those obtained with GAS [8], GAS
with shape priors [11], GAS based on g  in (14) with shape priors, GAS based on the optimal multimodal image, gnew in (15),
and our approach without nonoverlapping constraints, validating the effects of the different GAS extensions in our proposed
method. Additionally, we compare our proposed method with
FSL FIRST [22], [23] and FreeSurfer [24], [25], widely used

1) Implementation Details: Our proposed method was implemented in the ITK/VTK framework [27], [28], which provides open source C++ libraries for image segmentation and
registration. The implementation was also integrated into the
3-D Slicer program [29], a free software package for image
visualization and analysis. In particular, modularization of the
implementation within the 3-D Slicer program allows developers to test algorithms by tuning parameters easily and rapidly
under the provided graphical user interface (GUI) environment.
GAS [10] and GAS with shape priors [11] are currently available
on ITK libraries and tested for the comparison. Our proposed
method was built from ITK classes related to these approaches.
2) Data Acquisition and Preprocessing: We utilized six
MRI datasets, including T1 W (available on only one dataset),
T2 W, SWI and FA (from DWI) on each one, scanned (under
approved IRB) from five subjects using a 7 T magnet at the
Center for Magnetic Resonance Research of the University of
Minnesota. Table II shows the used training shape sets and manual segmentations as ground truth for each dataset and the corresponding subject. For each dataset (T2 W, SWI, and FA image)
from 1 to 5, 14 structures—left and right sides of GPe, GPi, SN,
STN, CN, Pu, and Tha within the basal ganglia region—were
manually segmented by an anatomical expert.
Dataset 6 is utilized to segment structures using training
shapes (manually segmented structures) from all different subjects 1–5. Its segmentation results are evaluated only visually
since manually segmented versions were not available. Also,
sagittal T1 W data in addition to T2 W, SWI, and FA is included
on dataset 6 to segment CN, Pu, and Tha, comparing our proposed model with FSL FIRST and FreeSurfer (note that they
work only on T1 W data) [22]–[25].
Detailed acquisition information for all datasets, the manual segmentation pipeline, and the registration process for each
structure are presented in [6]. Training shape sets for the structures on each dataset are built by using the leave-one-out
method [30], e.g., training shape sets for test set 1 consist of
manual segmentations for each structure from datasets 2, 3, 4,

1686

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

and 5, leaving the ones from dataset 1 out. Moreover, training
shape sets for test sets 1, 2, 5, and 6 are manually segmented
versions from all the other subjects. The training shapes for each
structure and the data are initially registered onto datasets within
the same ROI using FSL FLIRT (FMRIB’s Linear Image Registration Tool) [21], overlapping with structures to be segmented
on the data. Alignment of training shapes by registration onto
the test dataset is critical to capture variations between training
shapes, dealing with the correspondence problem [11]. The variations in training shapes are measured by using PCA modeling
(described in Section II-B) as the principal components. A shape
prior is represented by shape (and pose) parameters (being estimated using a MAP approach) and the principal components,
and guides the evolution of the surfaces toward the best fit.
Additionally, initial surfaces (i.e., coordinate points in three
dimensional spaces) should be properly defined as a major part
of the user intervention in our active surface based segmentation
framework, and thus such surface points (e.g., three or four
points within each target structure in this work) are chosen
based on new edgemaps generated by exploiting multimodal
MRIs on each dataset in three dimensional space using the
Amira visualization tool [26].
Finally, for the selection of the parameters (i.e., αlow , αhigh ,
βlow , and βhigh ) generating the new edgemap and the weights
of each constraint (e.g., propagation, shape prior, and nonoverlapping penalty) for the surface evolution, one of the datasets
is firstly used to specify such parameters. Initially set values
are utilized for the segmentation of each structure from other
datasets and then are tuned by quantitatively and visually evaluating the results.
B. Experimental Results on the Real MRI
1) Optimal Combination of Multimodal MR Images: Recently, direct visualization of deep brain subcortical structures
by exploiting superior image contrast on high resolution SWI
and T2 W at 7 T is presented in [5] and [6]. Furthermore, [6]
provided the subject-specific analysis pipeline which enables
localization of subcortical structures by combining a pair of
SWI and T2 W or FA images (process done manually there).
More specifically, GPe and GPi were manually segmented by
combining axial SWI and T2 W images. Also, SN and STN
were identified by combining coronal SWI and T2 W images.
The FA image and axial SWI were simultaneously utilized to
segment CN, Pu, and Tha. In this section, we investigate the optimal combination of multimodal MR images, segmenting each
structure on various combinations (e.g., T2 W+SWI, T2 W+FA,
and FA+SWI), and quantitatively evaluating segmentation results from the one of datasets (note that intensity properties of
single-modal images are consistent over datasets).
For dataset 3, GPe and GPi are segmented using the corresponding training shapes and by three combinations, i.e., 1) axial
SWI and axial T2 W image registered onto axial SWI; 2) axial
T2 W image registered onto axial SWI and FA image registered
onto axial SWI; and 3) axial SWI and FA image registered onto
axial SWI. Also, SN and STN are segmented using the corresponding training shape set and by three combination, i.e.,

Fig. 9. DC values of segmentations from GAS and our proposed model (without and with nonoverlapping constraints), based on three combinations of two
single-modal images for each structure. (a) GPe. (b) GPi. (c) SN. (d) STN.
(e) CN. (f) Tha. (g) Pu. The left and right columns represent left and right
structures, respectively. The blue, green, and red represent the combination of
T2 W and SWI, T2 W and FA, and FA and SWI, respectively.

1) coronal SWI and coronal T2 W image registered onto coronal
SWI; 2) coronal T2 W image registered onto coronal SWI and
FA image registered onto coronal SWI; and 3) coronal SWI and
FA image registered onto coronal SWI. Finally, CN, Pu, and Tha
are segmented using the corresponding training shapes and by
three combinations, i.e., 1) axial SWI registered onto FA image
and axial T2 W image registered onto FA image; 2) axial T2 W
image registered onto FA image and FA image; and 3) FA image
and axial SWI registered onto FA image.
Note that GPe, GPi, SN, and STN are manually segmented
on the high resolution SWI and T2 W, while CN, Tha, and Pu are
manually segmented on the low resolution FA image. Therefore,
we utilized data registered onto higher resolution image (SWI) to
segment GPe, GPi, SN, and STN and onto lower resolution data
(FA) to segment CN, Tha, and Pu for more accurate comparison
with the corresponding manual versions.

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

1687

Fig. 10. Comparison of segmentation results for GPe and GPi on dataset 3. The light green and brown represent GPe and GPi, respectively. The blue contours
represent manual segmentations. Top and bottom in each figure represent contours and volumetric segmentations, respectively. (a), (b), and (c) show segmentation
results of GAS, GAS with shape prior using g, GAS with shape prior using g  on axial T2 W image (left column) and axial SWI (right column), respectively. (d),
(e), and (f) show segmentation results of GAS, the proposed approach without nonoverlapping constraints, and the proposed approach, respectively, with surface
distance maps (right column, top: GPe, bottom: GPi) on axial T2 W image combined with axial SWI.

Fig. 11. Comparison of segmentation results for SN and STN on dataset 3. The red and yellow represent SN and STN, respectively. The blue contours represent
manual segmentations. Top and bottom in each figure represent contours and volumetric segmentations, respectively. (a), (b), and (c) show segmentation results
of GAS, GAS with shape prior using g, GAS with shape prior using g  on coronal T2 W image (left column) and coronal SWI (right column), respectively. (d),
(e), and (f) show segmentation results of GAS, the proposed approach without nonoverlapping constraints, and the proposed approach, respectively, with surface
distance maps (right column, top: SN, bottom: STN) on coronal T2 W image combined with coronal SWI.

1688

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Fig. 12. Comparison of segmentation results from the single-modality based approaches for CN, Tha, and Pu on dataset 3. The violet, dark green, and cyan
represent CN, Tha, and Pu, respectively. The blue contours represent manual segmentations. Top and bottom in each figure represent contours and volumetric
segmentations, respectively. (a), (b), and (c) show segmentation results in the one view of CN and Tha from GAS, GAS with shape prior using g, and GAS with
shape prior using g  on FA image (left column) and SWI (right column), respectively. (d), (e), and (f) show segmentation results in another view of CN, Tha, and
Pu from GAS, GAS with shape prior using g, GAS with shape prior using g  on FA image (left column) and SWI (right column), respectively.

Fig. 13. Comparison of segmentation results from the multimodality-based approaches for CN, Tha, and Pu on dataset 3. (a), (b), and (c) show segmentation
results in the one view (left) of CN and Tha and another view (right) of CN, Tha, and Pu from GAS, the proposed approach without nonoverlapping constraints,
and the proposed approach, respectively, on the FA image combined with axial SWI. The violet, dark green, and cyan represent CN, Tha, and Pu, respectively. The
blue contours represent manual segmentations. Top and bottom in each figure represent contours and volumetric segmentations, respectively. GPe (light green)
segmented on T2 W combined with SWI [i.e., segmented GPe in Fig. 10(f)] is incorporated as contours in (b) and (c), respectively, to see overlaps between Pu and
GPe. Note that overlaps between Pu and GPe in (c) are considerably reduced [see top right of (b) and (c)]. (d), (e), and (f) show corresponding surface distance
maps (top: CN, bottom: Pu and Tha, left: top view, right: bottom view).

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

1689

Fig. 14. Manual segmentations for each structure on dataset 3. Top left shows
GPe (light green) and GPi (brown) on the axial SWI. Top right represents SN
(red) and STN (yellow) on the coronal SWI. Bottom left shows Pu (cyan) and
GPe (light green) on the FA image. Bottom right represents CN (violet) and Tha
(dark green) on the FA image.

Moreover, each structure is iteratively segmented while applying the nonoverlapping constraint, thereby reducing overlap
with adjacent structures and maximizing the DC value within
our proposed framework.
For the comparison with our proposed segmentation method
based on multimodal images, we apply the multichannel image
based active contour model [31], [32] to classical edge based
active contour, thereby using two different contrast MR images.
For that purpose, classical GAS is extended to utilize multimodal
images by employing a new edge indicator function introduced
in our approach. All the structures are segmented using GAS,
and using our proposed model without nonoverlapping constraints, based on three combinations again. Finally, DC values
between segmented structures and their corresponding manually
segmented versions for each approach are calculated.
Fig. 9 shows DC values between segmentations obtained from
each approach based on three multimodal MR combinations and
their corresponding manual versions for each structure. We observe that our proposed approach based on combination of T2 W
and SWI for GPe, GPi, SN, and STN yields better segmentation
results than other combinations. In particular, overall segmentation results on FA image combined with SWI were inaccurate
(see DC values for GPi and STN in our proposed model in
Fig. 9). On the other hand, CN, Tha, and Pu were more accurately segmented with the combination of FA and SWI than
other combinations. Based on these experiments, we can see that
SWI provides additional anatomical details around GPe, GPi,
SN, and STN regions; while T2 W identifies more clearly structural information within those ROIs than the FA image. For CN,
Tha, and Pu, we can see that FA image provides more detailed
anatomical information than T2 W image within the structures,
and SWI still shows obvious boundaries around the regions.
In addition, we can observe that shape priors and nonoverlapping constraints within our proposed framework contributed to
improve the overall segmentation accuracy (see DC values between tested approaches on the combination of T2 W and SWI
for GPe, GPi, SN, and STN, and on the combination of FA and
SWI for CN, Tha, and Pu in Fig. 9).

Fig. 15. Average DC values and standard errors of segmented results for each
approach on data set from 1 to 5. (a) and (b) represent DC values for left and
right structures, respectively.

2) Quantitative and Visual Evaluation: Segmentation results from our proposed model based on the optimal combination
of multimodal images (as described in the previous section) for
each of the structures on the datasets from 1 to 5 in Table II
are presented in this section. More specifically, GPe and GPi
are segmented using the corresponding training shapes and by
combining axial SWI and axial T2 W images registered onto axial SWI. SN and STN are segmented using the corresponding
training shape set, and by combining coronal SWI and coronal
T2 W images registered onto coronal SWI. CN, Pu, and Tha are
segmented using the FA image and axial SWI registered onto the
FA image with corresponding training shape set. For the comparison, all the structures are also segmented using GAS, GAS
with shape priors, GAS based on g  with shape priors, based on
the single-modality image, and GAS based on multimodal images. In particular, for approaches using the single-modal image,
GPe and GPi are segmented on the axial T2 W image registered
onto axial SWI or the axial SWI, respectively. Similarly, SN
and STN are segmented on coronal T2 W images registered onto
coronal SWI or coronal SWI. Also, the FA image or axial SWI
registered onto the FA image are utilized to segment CN, Pu,
and Tha.
Segmentation results for dataset 3 (similar results are obtained for the other data), represented as 2-D contours with
superimposed ground truth (blue contour) and 3-D volumes, for
each structure, are shown using the Amira environment in Figs.
10–13. In particular, for segmentation results based on multimodal images (Figs. 10, 11(d)–(f), and 13), we present color
maps (on the manual segmentation) representing surface distances between each segmentation result and its manual one for

1690

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

TABLE III
DC VALUES OF THE PROPOSED APPROACH DURING ITERATIVE PROCESS FOR EACH DATA SET

Fig. 16. Comparison of segmentation results from the multimodality-based
approaches for GPe and GPi on dataset 6. The light green and brown represent
GPe and GPi, respectively. Top and bottom in each figure represent contours and
volumetric segmentations, respectively. Figures (a), (b), and (c) show segmentation results of GAS, the proposed approach without nonoverlapping constraints,
and the proposed approach, respectively, on the axial T2 W image combined
with axial SWI.

Fig. 17. Comparison of segmentation results from the multimodality-based
approaches for SN and STN on dataset 6. The red and yellow represent SN
and STN, respectively. Top and bottom in each figure represent contours and
volumetric segmentations, respectively. (a), (b), and (c) show segmentation
results of GAS, the proposed approach without nonoverlapping constraints, and
the proposed approach, respectively, on the coronal T2 W image combined with
coronal SWI.

more clear visual comparison. Fig. 14 shows the 3-D manual
segmentation on the same dataset 3.
DC values of each segmented result for datasets 1–5 during the nonoverlapping iterative process are summarized in
Table III. In addition, Fig. 15 presents average DC values and
standard deviation errors for all the structures and tested segmentation algorithms on datasets 1–5.
In Figs. 10, 11(a)–(c), and 12, we observe that the segmentation results are visually improved by incorporating the
shape prior term. Moreover, more accurate segmentation results are obtained when using the new edge detection function
(see Figs. 10, 11(d)–(f), and 13). Note that segmentation results
from GAS based on multimodal images show better performance (yielding higher DC values) than those from GAS and
even GAS with a shape prior term on the single-modal image
(see GPe and SN in Fig. 15). However, its segmented surfaces

are unrefined since it does not exploit prior shape information
[see Figs. 10, 11(d), and 13(a)]. Overall results still includes
over- and undersegmented areas and overlapping regions between neighboring structures, whereas our complete approach
shows significantly improved segmentation results with reduced
overlapping regions (see Figs. 10, 11(f), and 13(c) and (f), and
especially their surface distance maps on the overlapping regions). Additionally, our approach yields overall higher DC
values (see Fig. 15).
We also observe that while the DC values for left STN, right
SN, CN, and Pu with a single-modality image (using GAS based
on g or g  with shape priors) are similar to those of our approach,
their visual segmentation results were inaccurate due to underand oversegmentation and overlapping regions between neighboring structures [see Figs. 11(b) and (c) and 12(b), (c), (e), and
(f)]. Furthermore, the proposed approach with nonoverlapping

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

Fig. 18. Comparison of segmentation results from the multimodality-based
approaches for CN, Tha and Pu on dataset 6. The violet, dark green, and cyan
represent CN, Tha, and Pu, respectively. (a), (b), and (c) show segmentation
results in the one view of CN, Tha, and Pu from GAS, the proposed approach
without nonoverlapping constraints, and the proposed approach, respectively, on
the FA image combined with axial SWI. (d), (e), and (f) show another view for
(a), (b), and (c). Top and bottom in each figure represent contours and volumetric
segmentations, respectively. GPe (the light green) segmented on T2 W combined
with SWI [i.e., segmented GPe in Fig. 16(c)] is incorporated as contours in (e)
and (f), respectively, to see overlaps between Pu and GPe. Note that overlaps
between Pu and GPe in (f) are considerably reduced [see top right of (e) and
(f)].

constraints yields significantly reduced overlapping regions between neighboring structures even though their DC values are
similar to those without nonoverlapping penalty. Specifically,
overlapping regions between SN and STN segmented using our
proposed approach were significantly reduced, comparing to
those without nonoverlapping penalty (see Fig. 11(e) and (f)),
whereas their DC values are similar to or even slightly lower than
those without nonoverlapping penalty (e.g., 0.67 (0.7 for w/o
nonoverlapping) for left STN and 0.76 (0.76 for w/o nonoverlapping) for right STN on dataset 3).
In addition, overall DC values are increased or maintained
during the iterative segmentation process within our approach
(see Table III). In the few cases where the DC values are reduced after iteration, we still note that our approach shows clear
delineation between adjacent structures, whereas manual segmentations have overlapping regions (also see Fig. 11(f) and
DC values of left SN and STN on dataset 3 in Table III). This
means that those manual segmentations were not completely
well defined around boundaries, even if they were produced by
an anatomy specialist.
Next, we work with dataset 6, where the ground truth is
not available and thus it requires only qualitative evaluation.
Particularly, this dataset includes T1 W image. We further compare commonly used segmentation tools (e.g., FSL FIRST and

1691

Fig. 19. Comparison of segmentation results from the single-modality-based
approaches on T1 W image for CN, Tha, and Pu (dataset 6). The violet, dark
green, and cyan represent CN, Tha, and Pu, respectively. (a), (b), and (c) show
segmentation results in the one view of CN, Tha, and Pu from GAS, GAS with
shape prior using g, and GAS with shape prior using g  on T1 W image, respectively. (d), (e), and (f) show another view for (a), (b), and (c). Top and bottom
in each figure represent contours and volumetric segmentations, respectively.

Fig. 20. Segmentation results from FSL FIRST and FreeSurfer on T1 W image
for CN, Tha, and Pu (dataset 6). The violet, dark green, and cyan represent CN,
Tha, and Pu, respectively. (a) and (b) show segmentation results of FSL FIRST
and FreeSurfer, respectively. Top and bottom in each figure represent contours
and volumetric segmentations, respectively. Two views of for CN, Tha, and Pu
are shown in left and right columns.

1692

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

Fig. 21. Comparison of segmentation results from the multimodality-based approaches on T1 W data combined with FA image, SWI, or T2 W image for CN,
Tha and Pu (dataset 6). The violet, dark green, and cyan represent CN, Tha, and Pu, respectively. Top and bottom in each figure represent contours and volumetric
segmentations, respectively. First row [i.e., (a), (b), and (c)] shows segmentation results of GAS. Second row [i.e., (d), (e), and (f)] shows segmentation results of
the proposed approach without nonoverlapping constraints. Third row [i.e., (g), (h), and (i)] shows segmentation results of the proposed approach. First [i.e., (a),
(d), and (g)], second [i.e., (b), (e), and (h)], and third [i.e., (c), (f), and (i)] columns represent segmentation results of each approach on T1 W combined FA, SWI,
and T2 W, respectively. The left and right sides in each figure are two views of CN, Tha, and Pu. GPe (The light green) segmented on T2 W combined with SWI
[i.e., segmented GPe in Fig. 16(c)] is incorporated as contours in (d)–(i) to see overlaps between Pu and GPe. Note that overlaps between Pu and GPe in (g)–(i)
are considerably reduced [see top right of (d)–(i)].

FreeSurfer), which work on only this modal MR image, with our
proposed framework based on the combination of T1 W image
and another single-modal MR image, especially for CN, Pu, and
Tha structures.
Similarly in previous experiments, we performed experiments
using our proposed model by combining the T2 W image and
SWI to segment GPe, GPi, SN, and STN in axial and coronal
directions, respectively. The FA image, registered onto higher
resolution SWI, is combined with SWI to segment CN, Pu, and
Tha. In particular, segmented CN, Pu, and Tha are visually compared with those obtained by using the low resolution FA image
on dataset 3 to see whether the low resolution FA image affects
the segmentation accuracy. Additionally, GAS based on multimodal images is tested for the comparison. Figs. 16–18 show
the results, demonstrating the improvements obtained with our

proposed approach. We observed that the segmentation results
on dataset 6 are comparable with the results on dataset 3 even
if training shapes from all the different subjects were simultaneously employed. Also, we can see that CN, Tha, and Pu
segmented on the higher resolution FA image combined with
SWI provides more accurate and detailed structural information than those segmented on the lower resolution FA image
combined with SWI.
In addition, we tested our proposed model and GAS (extended to utilize multimodal images) on sagittal T1 W data (registered onto axial SWI) combined with T2 W, SWI, or FA image, comparing with FSL FIRST and FreeSurfer, widely used
single-modality segmentation tools. We should note that FSL
FIRST and FreeSurfer do not work on T2 W, SWI, or FA image,
which provides more anatomical information than T1 W within

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

subcortical structures [5] and do not segment GPe, GPi, SN, and
STN. Therefore, T1 W image was applied and tested to segment
only CN, Pu, and Tha. Also, we visually compared with GAS,
GAS with shape priors, and GAS based on g  with shape priors
on single T1 W data, respectively (see Fig. 19). Fig. 20 shows
segmentation results of FSL FIRST and FreeSurfer on T1 W
data.
We observe that the obtained FSL FIRST segmentation results are qualitatively better when compared with other previous techniques, including our proposed model tested here [see
segmentation results of FSL FIRST in Fig. 20(a)], while segmentation results obtained using FreeSurfer include incorrectly
segmented regions [see left Pu regions in Fig. 20(b)]. Note that
segmentation outputs (in the FreeSurfer space) should be registered onto the test data. We should note that both approaches are
based on probabilistic learning models, which require a highly
computational cost for segmentation [22]–[25], while our model
extends classical and simple edge-based segmentation model.
For example, FSL FIRST takes over 15 min to segment CN,
Tha, and Pu (including two-stage registration of test data onto
the MNI152 template [23]). Moreover, FreeSurfer works only
for the whole brain segmentation, taking over 10 h.
On the other hand, our approach requires few minutes per
each structure. Note that the preprocessing (e.g., registration of
training sets onto test data and between two single-modal MR
images) can be also initially performed within a few minutes
prior to the segmentation.
Our approach shows qualitatively comparable results, which
demonstrates that the segmentation performance on only the
low contrast T1 W data is improved by the new edge indicator
function, exploiting detailed edge information of SWI or FA
image (see Fig. 21(g), (h), and (i)) with a prior shape model
and nonoverlapping constraints. We also observe that the segmentation results on the combination of FA image and SWI
(considered as the optimal combination in the previous section)
exhibit better performance than those of T1 W combined with
FA image, SWI, and T2 W image [see Figs. 18(c) and (f) and
21(g)–(i)].
To conclude, in this experiment, we have demonstrated that
the combination of edge map from multimodal images, shape
priors, and nonoverlapping constraints within our proposed approach contributes to clear improvements on the quality of subcortical structures segmentation.
V. DISCUSSION AND CONCLUSION
This paper presented a novel active surface model for the segmentation of subcortical structures such as the basal ganglia and
thalamus using ultrahigh field MRI. A statistical shape model
is employed to guide the evolving surface toward structures to
be segmented on edge maps with limited information. We introduce a novel edge indicator function, exploiting the superior
SNR and CNR of SWI at high-field MRI. This new edge indicator function generates features combining two edge maps
obtained from the Laplacian of single MR modal images such as
SWI, T2 W, or FA image with boundary information on the initial
position of the given shape priors. Moreover, a nonoverlapping

1693

repulsion force is added, iteratively delineating boundaries between neighboring objects and improving the overall quality of
the segmentation.
From the quantitative evaluation of segmentation on different
combinations of two MRI modalities, we observe that the combination of T2 W and SWI within GPe, GPi, SN, and STN region,
and FA and SWI within CN, Pu, and Tha region, exhibits better
segmentation results than other combinations. Note that these
combinations are consistent with those utilized for each structure in manual segmentation [6]. Furthermore, we demonstrated
that global shape constraints (i.e., shape priors and nonoverlapping penalty) within our proposed approach, lead to significant
improvement on the segmentation quality of neighboring subcortical structures.
Although the proposed approach performs volumetric segmentation of complex and adjacent structures such as the basal
ganglia and thalamus, showing overall accurate results, several
factors can be further considered to prove its effectiveness in
practical cases.
The proposed method exploits edge information from only
two single-modal MR images, where each structure is fairly or
even partially visible. The segmentation performance will be
improved by incorporating more sufficient edge information if
other single-modal images acquired at different 7 T MRI protocols are additionally available (e.g., FLASH 2-D T∗2 weighted
imaging [33]).
However, combination of all the different image modalities
does not necessarily ensure better results, since some structures
might not be well identified on the specific modal MR image.
As seen in Fig. 21(g)–(i), overall segmentation results of CN,
Pu, and Tha are not as accurate as those obtained from FSL
FIRST, even if the performance on only the low contrast T1 W
data is improved by exploiting detailed edge information of
SWI or FA image which provides more anatomical information
than T1 W image within subcortical structures [5] with shape
constraints. Note that thalamus regions on the T1 W image are
homogenous due to its low contrast [5], and thus utilization of
the edge information is limited within our proposed approach.
In particular, those structures are more accurately segmented
on the combination of FA image and SWI within our proposed
model [see Fig. 18(c) and (f)].
Therefore, the optimal combination for the complementary
use of edge information from more single-modal MR images
from different MR protocols, particularly, where target structures are fairly visible, should be considered to improve the
segmentation quality in our proposed framework.
Our proposed model performed the segmentation by exploiting only 5 training shapes (from all the clinical data), whereas
FSL FIRST was trained using 336 manually labeled T1 W images [23]. This low number of training data might be critical
when aiming at having segmentation models adapted to a population with specific age or disease. Particularly, we consider exploiting shape prior information of subcortical structures within
normal controls or a specific patient group (e.g., elderly patients
with advanced PD for applications to the DBS) to remove biases
from each subgroup. This work aims to demonstrate an effective way of exploiting superior contrast and SNR properties of

1694

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 5, SEPTEMBER 2014

the high field MR data from an individual subject with additional constraints for shapes (within a specific subgroup) and
nonoverlapping within a simple segmentation framework, but
not to analyze shape changes of the target structures among or
even within the specific population. We assume that the shape
variation of structures is not significant within each subgroup,
and this allows the small size of training sets in our proposed
framework.
Nevertheless, anatomical analysis of structures within a subgroup of specific patients or even among subgroups needs to be
further investigated [34]–[38]. If there exists a significant variability in shape and pose among target structures even within the
specific population, the large size of training sets might provide
more reliable information about the structures [34].
Our future work also includes segmentation of structures on
the clinical MRI (1.5 or 3 T) based on statistical shape and
pose relationships between multiple adjacent structures (at the
ultrahigh field (7 T) MR imaging), where might have relatively
small variations within a specific subgroup [35].
Additionally, we need to analyze the effect of using or not using shape priors obtained from the same subject at the different
time (e.g., the datasets 3 and 4) to validate segmentation results
without biases for a specific shape. This might be important in a
practical case where we should utilize shape priors from all the
different subjects. As presented in Table III, in some structures,
segmentations on the other datasets yield higher DC values than
those on the datasets 3 and 4. Furthermore, we did not observe
significant shape variations among our datasets [6]. From these
points, we cannot conclude that there are biases when segmentation on the dataset 3 or 4 utilizes shape information from the
same subject at the different date. We aim to more accurately
segment subcortical structures from specific subjects but not to
compare segmentation results of each dataset and thus exploit
shape priors available at the different date from the same subject.
This might provide more reliable information for the segmentation, even if we do not observe it from our experimental results.
However, we should also note that for practical applications of a
segmentation framework, shape priors, particularly, in small size
of training sets, need to be built from all the different subjects.
Finally, our proposed approach requires several empirically
set parameters. Moreover, some of such parameters (e.g., initial
surface coordinate points) mainly affect the segmentation quality. Although parameters are considerably reduced in a semiautomatic segmentation way, we should further investigate the
dependency between such parameters, thereby significantly reducing the number of them.
ACKNOWLEDGMENT
The authors would like to thank the Editor-in-Chief and
Associate Editor for their handling of the paper.
REFERENCES
[1] D. Dormont, D. Seidenwurm, D. Galanaud, P. Cornu, J. Yelnik, and
E. Bardinet, “Neuroimaging and deep brain stimulation,” Amer. J. Neuroradiol., vol. 31, pp. 15–23, Jan. 2010.

[2] M. J. Madden, “Segmentation of images with low-contrast edges,” M.S.
thesis, Dept. Electron. Eng., West Virginia University, Morgantown, WV,
USA, 2007.
[3] W. Fang and K. L. Chan, “Incorporating shape prior into geodesic active contours for detecting partially occluded object,” Pattern Recognit.,
vol. 40, pp. 2163–2172, 2007.
[4] T. F. Chan and L. A. Vese, “Active contours without edges,” IEEE Trans.
Image Process., vol. 10, no. 2, pp. 266–277, Feb. 2001.
[5] A. Abosch, E. Yacoub, K. Ugurbil, and N. Harel, “An assessment of
current brain targets for deep brain stimulation surgery with susceptibility
weighted imaging at 7 tesla,” Neurosurgery, vol. 67, pp. 1745–1756, Dec.
2010.
[6] C. Lenglet, A. Abosch, E. Yacoub, F. De Martino, G. Sapiro, and N. Harel,
“Comprehensive in vivo mapping of the human basal ganglia and thalamic
connectome in individuals using 7 T MRI,” PLoS ONE, vol. 7, pp. 1–14,
Jan. 2012.
[7] D. Terzopoulos, A. Witkin, and M. Kass, “Constraints on deformable
models: Recovering 3D shape and nonrigid motions,” Artif. Intell., vol. 36,
pp. 91–123, 1988.
[8] V. Caselles, R. Kimmel, and G. Sapiro, “Geodesic active contours,” Int’l
J. Comput. Vision, vol. 20, pp. 61–79, 1997.
[9] S. Osher and J. A. Sethian, “Fronts propagating with curvature dependent
speed: Algorithms based on Hamilton–Jacobi formulations,” J. Comput.
Phys., vol. 79, pp. 12–49, 1988.
[10] V. Caselles, R. Kimmel, G. Sapiro, and C. Sbert, “Minimal surfaces based
object segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 19,
no. 4, pp. 394–398, Apr. 1997.
[11] M. Leventon, E. Grimson, and O. Faugeras, “Statistical shape influence
in geodesic active contours,” in Proc. 2000 IEEE Conf. CVPR, vol. 1,
pp. 316–323.
[12] M. G. Uzunbas, O. Soldea, D. Unay, M. Cetin, G. Unal, A. Ercil, and
A. Ekin, “Coupled nonparametric shape and moment-based intershape
pose priors for multiple basal ganglia structure segmentation,” IEEE Trans.
Med. Imag., vol. 29, no. 12, pp. 1959–1978, Dec. 2010.
[13] L. M. Kacem, C. Poupon, J. F. Mangin, and F. Poupon, “Multi-contrast
deep nuclei segmentation using a probabilistic atlas,” in Proc. 2010 IEEE
Int. Symp. Biomed. Imag.: From Nano Macro, pp. 61–64.
[14] Y. Artan, M. A. Haider, D. L. Langer, and I. S. Yetik, “Semi-supervised
prostate cancer segmentation with multispectral MRI,” in Proc. 2010 IEEE
Int. Symp. Biomed. Imag.: From Nano Macro, pp. 648–651.
[15] N. Paragios and R. Deriche, “Coupled geodesic active regions for image
segmentation: A level-set approach,” in Proc. 2000 Proc. 6th Eur. Conf.
Comput. Vision Part II, Dublin, Ireland, pp. 224–240.
[16] C. Zimmer and J.-C. Olivo-Marin, “Coupled parametric active contours,”
IEEE Trans. Pattern Anal. Mach. Intell., vol. 27, no. 11, pp. 1838–1842,
Nov. 2005.
[17] B. C. Lucas, M. Kazhdan, and R. H. Taylor, “Multi-object geodesic active contours (MOGAC),” in Medical Image Computing and ComputerAssisted Intervention (Lecture Notes in Computer Science 7511). New
York, NY, USA: Springer, 2012, pp. 404–412.
[18] N. Paragios and R. Deriche, “Geodesic active contours and level-sets for
the detection and tracking of moving objects,” IEEE Trans. Pattern Anal.
Mach. Intell., vol. 22, no. 3, pp. 266–280, Mar. 2000.
[19] L. M. Lorigo, O. Faugeras, W. E. L. Grimson, R. Keriven, R. Kikinis,
A. Nabavi, and C.-F. Westin, “Codimension-two geodesic active contours
for the segmentation of tubular structures,” in Proc. 2000 IEEE Conf.
CVPR, vol. 1, pp. 444–451.
[20] Z. Cho, H. Min, S. Oh, J. Han, C. Park, J. Chi, Y. Kim, S. Paek,
A. M. Lozano, and K. H. Lee, “Direct visualization of deep brain stimulation targets in Parkinson disease with the use of 7-Tesla magnetic
resonance imaging,” Neurosurgery, vol. 113, pp. 639–647, Sep. 2010.
[21] M. Jekinson, P. Bannister, and S. M. Smith, “Improved optimization for
the robust and accurate linear registration and motion correction of brain
images,” NeuroImage, vol. 17, pp. 825–841, 2002.
[22] M. W. Woolrich, S. Jbabdi, B. Patenaudea, M. Chappell, S. Makni,
T. Behrens, C. Beckmann, M. Jenkinson, and S. M. Smith, “Bayesian analysis of neuroimaging data in FSL,” NeuroImage, vol. 45, no. 1, pp. 173–
186, 2009.
[23] B. Patenaude, S. M. Smith, D. N. Kennedy, and M. Jenkinson, “A
Bayesian model of shape and appearance for subcortical brain segmentation,” NeuroImage, vol. 56, no. 3, pp. 907–922, 2011.
[24] B. Fischl, D. H. Salat, E. Busa, M. Albert, M. Dieterich, C. Haselgrove,
A. J. van der Kouwe, R. Killiany, D. Kennedy, S. Klaveness, A. Montillo,
N. Makris, B. Rosen, and A. M. Dale, “Whole brain segmentation: Automated labeling of neuroanatomical structures in the human brain,” Neuron,
vol. 33, pp. 341–355, Jan. 2002.

KIM et al.: SEMIAUTOMATIC SEGMENTATION OF BRAIN SUBCORTICAL STRUCTURES FROM HIGH-FIELD MRI

[25] B. Fischl, D. H. Salat, A. J. van der Kouwe, N. Makris, F. Segonne,
B. T. Quinn, and A. M. Dale, “Sequence-independent segmentation of
magnetic resonance images,” NeuroImage Math. Brain Imag., vol. 23,
no. 1, pp. S69–S84, 2004.
[26] Amira, Visage imaging. [Online]. Available: http://www.amira.com/
[27] Insight segmentation and registration Toolkit Kitware. [Online]. Available:
http://www.itk.org/
[28] The visualization Toolkit Kitware. [Online]. Available: http://www.vtk.
org/
[29] S. Pieper, R. Kikinis, J. Miller, M. Halle, B. Lorensen, and W. Schroeder.
3D slicer. [Online]. Available: http://www.slicer.org/
[30] B. W. Siverman, Density Estimation for Statistics and Data Analysis.
London, U.K.: Chapman Hall/CRC, 1986.
[31] G. Sapiro, “Vector-valued active contours,” in Proc. 1996 IEEE Conf.
CVPR, pp. 680–685.
[32] B. Sandberg and T. F. Chan, “A logic framework for active contours on
multi-channel images,” J. Vision Commun. Image Representation, vol. 16,
pp. 333–358, 2005.
[33] H. U. Kerl, L. Gerigk, I. Pechlivanis, M. Al-Zghloul, C. Groden, and
I. S. Nölte, “The subthalamic nucleus at 7.0 Tesla: Evaluation of sequence and orientation for deep-brain stimulation,” Acta Neurochirurgica,
vol. 154, no. 11, pp. 2051–2062, 2012.
[34] S. Daniluk, K. G. Davies, S. A. Ellias, P. Novak, and J. M. Nazzaro, “Assessment of the variability in the anatomical position and size of the
subthalamic nucleus among patients with advanced Parkinson’s disease
using magnetic resonance imaging,” Acta Neurochirurgica, vol. 152, no. 2,
pp. 201–210, 2010.
[35] M. Bossa, E. Zacur, S. Olmos, and Alzheimer’s Disease Neuroimaging
Initiative, “Statistical analysis of relative pose information of subcortical
nuclei: Application on ADNI data,” NeuroImage, vol. 55, no. 3, pp. 999–
1008, 2011.
[36] M. McKeown, A. Uthama, R. Abugharbieh, S. Palmer, M. Lewis, and
X. Huang, “Shape (but not volume) changes in the thalami in Parkinson
disease,” BMC Neurology, vol. 8, no. 8, 2008.
[37] K.-K. Shen, J. Fripp, F. Mériaudeau, G. Chételat, O. Salvado, P. Bourgeat,
and The Alzheimer’s Disease Neuroimaging Initiative, “Detecting global
and local hippocampal shape changes in Alzheimer’s disease using statistical shape models,” NeuroImage, vol. 59, no. 3, pp. 2155–2166, 2012.
[38] W. F. den Dunnen and M. J. Staal, “Anatomical alteration of the subthalamic nucleus in relation to age: A postmortem study,” Mov. Disord., vol. 20,
no. 7, pp. 893–898, 2005.
Jinyoung Kim received the B.Sc. and M.Sc. degrees in electrical and computer engineering from
the Hanyang University, Korea, in 2004 and 2008,
respectively, and the M.Sc. degree in electrical and
computer engineering from the University of Minnesota, USA, in 2012. From 2008 to 2010, he was
with the Visual Display Division, Samsung Electronics, Korea. He is currently working toward the Ph.D.
degree in the Department of Electrical and Computer
Engineering, Duke University, Durham, NC, USA.
His research interests include computer vision and
machine learning with applications to brain MRI segmentation.
Christophe Lenglet received the M.Sc. degree
in computer science and engineering from the
Compiègne University of Technology, France and in
applied mathematics from École Normale Supérieure
de Cachan, France, in 2003, and the Ph.D. degree in
biomedical imaging and neuroscience from INRIA
Sophia Antipolis—Méditerranée, France, in 2006.
He then joined the Imaging and Visualization Department at Siemens Corporate Research in Princeton, New Jersey as a Research Scientist. In 2008, he
moved to the University of Minnesota as a Research
Associate in the Department of Electrical and Computer Engineering. He is currently an Assistant Professor at the Center for Magnetic Resonance Research
(Department of Radiology), University of Minnesota, and a Scholar of the Institute for Translational Neuroscience (ITN). His research interests include the
development of computational tools to harness the power of high-field magnetic
resonance imaging (MRI) for neuroscience and clinical applications. His work
aims at better understanding alterations of brain connections in neurodegenerative disorders.

1695

Yuval Duchin received the B.Sc. and M.Sc. degrees
in physics and computer science from the Tel Aviv
University and the M.B.A. degree from the TechnionMachon Technologi Le’ Israel, in 2001, 2003, and
2008, respectively.
From 2006 to 2008, he was a senior physicist
at TopSpin Medical Ltd., Israel. He is currently a
Research Fellow at the Center for Magnetic Resonance Research, University of Minnesota, Minneapolis, USA. His current research interests include developing algorithms for the analysis and processing
of ultrahigh-field MR imaging for deep brain stimulation (DBS) procedures.

Guillermo Sapiro (M’94–SM’03–F’14) was born in
Montevideo, Uruguay, on April 3, 1966. He received
the B.Sc. (summa cum laude), M.Sc., and Ph.D. degrees from the Department of Electrical Engineering
at the Technion, Israel Institute of Technology, in
1989, 1991, and 1993 respectively.
He was a Postdoctoral Researcher at MIT. He became a Member of the Technical Staff at the research
facilities of HP Labs, Palo Alto, CA, USA. He was
with the Department of Electrical and Computer Engineering, University of Minnesota, where he was
a Distinguished McKnight University Professor and Vincentine Hermes-Luh
Chair in Electrical and Computer Engineering. He is currently the Edmund
T. Pratt, Jr., School Professor with Duke University, Durham, NC, USA. He
is involved in theory and applications in computer vision, computer graphics,
medical imaging, image analysis, and machine learning. He has authored or
coauthored over 400 papers in these areas and has written a book published by
Cambridge University Press, January 2001. He is the founding Editor-in-Chief
of the SIAM Journal on Imaging Sciences.
Dr. Sapiro was awarded the Gutwirth Scholarship for Special Excellence
in Graduate Studies in 1991, the Ollendorff Fellowship for Excellence in Vision and Image Understanding Work in 1992, the Rothschild Fellowship for
Postdoctoral Studies in 1993, the Office of Naval Research Young Investigator
Award in 1998, the Presidential Early Career Awards for Scientist and Engineers
(PECASE) in 1998, the National Science Foundation Career Award in 1999,
and the National Security Science and Engineering Faculty Fellowship in 2010.
He received the test of time award at ICCV 2011.

Noam Harel received the B.Sc. degree in biology
from Tel Aviv University, Israel and the M.Sc. and
Ph.D. degrees in physiology and neuroscience for
mapping auditory areas using optical imaging technique from the University of Toronto, Canada, in
1996 and 2000, respectively.
For his post doctoral training, he moved to the
Center for Magnetic Resonance Research (CMRR),
University of Minnesota where his research focused
on the development of methods for high-resolution
MRI and functional MRI (fMRI) applications using
high magnetic fields (7T & 9.4T). In particular, he developed fMRI capabilities
for mapping columnar and laminar organization in cerebral cortex both in human and animal models. In 2002, he moved to the University of Pittsburgh as a
research associate and soon after returned to Minnesota and joined CMRR as a
faculty member. He is currently an Associate Professor in the departments of Radiology and Neurosurgery, at the University of Minnesota. His current research
focuses on the development and integration of 7T and high field neuroimaging
data into deep brain stimulation (DBS) surgical navigation in particular and
brain surgery in general.

