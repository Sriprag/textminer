328

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Facial Image Analysis for Fully Automatic Prediction
of Difficult Endotracheal Intubation
Gabriel Louis Cuendet∗ , Student Member, IEEE, Patrick Schoettker, Anıl Yüce, Student Member, IEEE,
Matteo Sorci, Hua Gao, Christophe Perruchoud, and Jean-Philippe Thiran, Senior Member, IEEE

Abstract— Goal: Difficult tracheal intubation is a major cause
of anesthesia-related injuries with potential life threatening complications. Detection and anticipation of difficult airway in the
preoperative period is, thus, crucial for the patients’ safety. We
propose an automatic face-analysis approach to detect morphological traits related to difficult intubation and improve its prediction.
Methods: For this purpose, we have collected a database of 970
patients including photos, videos, and ground truth data. Specific
statistical face models have been learned using the faces in our
database providing an automated parametrization of the facial
morphology. The most discriminative morphological features are
selected through the importance ranking provided by the random
forest algorithm. The random forest approach has also been used to
train a classifier on these selected features. We compare a threshold
tuning method based on class prior with two methods, which learn
an optimal threshold on a training set for tackling the inherent
imbalanced nature of the database. Results: Our fully automated
method achieves an AUC of 81.0% in a simplified experimental
setup, where only easy and difficult patients are considered. A further validation on the entire database has proven that our method is
applicable for real-world difficult intubation prediction, with AUC
= 77.9%. Conclusion: The system performance is in line with the
state-of-the-art medical diagnosis, based on ratings provided by
trained anesthesiologists, whose assessment is guided by an extensive set of criteria. Significance: We present the first completely
automatic and noninvasive difficult intubation detection system
that is suitable for use in clinical settings.
Index Terms—Anesthesia, difficult intubation prediction, facial
image analysis, pattern recognition.

I. INTRODUCTION
HE priority of the anesthesiologist, after having induced
general anesthesia, is to ventilate the patient and secure
his airways. As the patient is under the influence of drugs,
whose main effects are the loss of consciousness, analgesia,

T

Manuscript received October 9, 2014; revised July 2, 2015; accepted July 9,
2015. Date of publication July 15, 2015; date of current version January 16,
2016. This work was supported by the Swiss Commission for Technology and
Innovation (CTI), under the Project 12636.1 and the title Prediction of Difficult
Tracheal Intubation With Automatic Face Analysis and Artificial Intelligence.
Asterisk indicates corresponding author.
∗ G. L. Cuendet is with the Signal Processing Laboratory, Ecole Polytechnique Fédérale de Lausanne, Lausanne 1015, Switzerland (e-mail: gabriel.
cuendet@epfl.ch).
P. Schoettker and M. Sorci are with the University Hospital Center (CHUV).
A. Yüce and H. Gao are with Ecole Polytechnique Fédérale de Lausanne.
C. Perruchoud is with the University Hospital Center (CHUV), and also with
the Ensemble Hospitalier de la Côte.
J.-P. Thiran is with the Ecole Polytechnique Fédérale de Lausanne, with the
University Hospital Center (CHUV), and also with the University of Lausanne.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2015.2457032

and muscular paralysis, mechanical ventilation is mandatory.
Despite all the advancements in anesthesiology, difficult airway
management still represents a major cause of anesthesia-related
injuries with potential life threatening complications [1]. Recent analysis of airway management related claims in the UK
[2] and in the USA [3] show that respiratory events, most of
them being difficult intubation or inadequate ventilation, come
first in the proportion of cases with poor clinical outcomes (severe harm, brain damage, or death). The worst-case scenario in
airway management is the “Can’t intubate, can’t ventilate” situation, in which the patient is impossible to be ventilated by face
mask and intubated with an endotracheal tube. The estimated incidence of such a situation is estimated between 0.01 and 3 in 10
000 cases [4]. Nowadays, up to one third of all deaths attributed
to anesthesia are consecutive to the inability to either ventilate or intubate [5]. Numerous technical advances have allowed
the facilitation of intubation by improving the view at laryngoscopy [6]–[8] or monitoring the placement of the endotracheal
tube [9], [10] but difficult intubation still remains an area of
concern [2].
Detection and anticipation of difficult airway in the preoperative period is crucial for patients’ safety. In cases of suspected
difficulty, specific equipment and personnel will be called upon
to increase safety and the chances of successful intubation. In
daily practice, anesthesiologists predict the difficulty of tracheal
intubation with bedside tests, which correlate poorly with the
ground truth. Experienced anesthesiologists associate, in addition to the available bedside tests, a global clinical judgment,
probably based on a larger number of morphological parameters than those contained in the available bedside tests (see
Section I-A). Nevertheless some patients with a difficult airway
remain undetected despite the most careful preoperative airway
evaluation.
The usage of computer vision methods, and more specifically
face-analysis methods, is on the rise in areas, such as marketing
and emotion analysis [11], [12], face-tracking systems to increase safety in cars [13], [14] as well as in medicine [15]–[17]
to name just a few. Improvements in facial landmarks detection
and tracking [18], [19] allow for fast and robust face trackers
[20]. Those can detect and interpret specific features of the face,
based on landmark positions, making them suitable for facial
morphology analysis.
In this study, we describe a clinical application of face analysis to detect morphological traits related to difficult intubation,
hypothesizing that advanced face-analysis methods could improve the prediction of difficult intubation and identify relevant
characteristics helping the prediction.

0018-9294 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

A. Related Work
For the last 30 years, numerous definitions have been proposed and used by anesthesiologists, but no unique definition of
difficult intubation exists. The vast majority of endotracheal intubations are performed using a laryngoscope, which allows the
visualization of the larynx and the placing of the endotracheal
tube between the vocal cords into the trachea. Cormack and
Lehane proposed a classification of the laryngoscopic view using four grades based on the visibility of laryngeal structures or
glottic exposure [21]. This classification was later modified by
Yentis and Lee who proposed to divide the original grade 2 into
grade 2a and grade 2b [22]. The later classification is used to define the difficult laryngoscopy as a view corresponding to grade
3 or grade 4. Nevertheless, it has recently been pointed out by
Krage et al. that the reproducibility of this classification is limited [23]. Moreover, a poor view of the vocal cords can increase
the difficulty of the intubation but other factors, such as the
position of the head of the patient or the experience of the anesthesiologist also have influence on the success of the intubation.
Despite the need for a standard classification of the difficult intubation in the medical community, no such uniform definition has
been widely adopted. Thus, the incidence and the factors associated with difficult intubation vary from one institution to another
and are virtually impossible to compare directly. The incidence
of difficult laryngoscopy in the operating room has been reported
to range from 0.3% to 13% [24]. In an attempt to provide a definition of the difficult intubation, Adnet et al. proposed the intubation difficulty scale (IDS) [25], taking into account the number
of attempts, the number of operators directly attempting the intubation, the use of alternative devices or techniques, the glottic
exposure, or the lifting force applied during laryngoscopy.
Prediction of difficult endotracheal intubation has been
largely explored in the past 25 years by anesthesiologists. Several physical and morphological characteristics have been identified as predictors of difficult laryngoscopy or difficult intubation. Those include obesity, poor mobility of the head and neck,
poor mobility of the jaw, receding mandible, long upper incisors, decreased mouth opening (or small interincisor gap with
the mouth fully open), shortened thyromental distance (TMD),
short neck, and small neck circumference. Several difficult intubation bedside screening tests exist.
The TMD, or Patil–Aldreti test, is the distance from the upper edge of the thyroid cartilage to the chin, measured with the
head fully extended. A short TMD equates to an anterior lying
larynx that is at a more acute angle and also results in less space
for the tongue to be compressed by the laryngoscope blade. A
TMD greater than 7 cm is usually associated with easy intubation, whereas a TMD smaller than 6 cm may predict a difficult
intubation. However, with a sensitivity of 48% and a specificity
of 79% in predicting difficult intubation [26], this distance is
not a good predictor by itself and is often used in combination
with other predictors. The ratio of height to thyromental distance
improves the accuracy of predicting difficult laryngoscopy compared to TMD alone (sensitivity and specificity of 77% and 54%,
respectively) [27].

329

Originally described by Mallampati et al. [28] and modified
by Samsoon and Young [29], the Mallampati score assesses
the airway according to the visibility of oropharyngeal structures observed on a sitting patient with the mouth wide open
and the tongue out. The hypothesis of the author is that the
larger the base of the tongue, the more it overshadows the larynx, resulting in a poor laryngoscopic view and a potentially
difficult laryngoscopy. The volume of the tongue is, thus, an
important, yet difficult to assess, parameter when assessing the
difficulty of an endotracheal intubation. Since it is not possible
to determine the volume of the tongue relative to the capacity
of the oropharyngeal cavity, it is logical to infer that the base
of tongue is disproportionately large when it is able to mask
the visibility of the faucial pillars and uvula. The score ranges
from class 1 to class 4, class 1 indicating full visibility of the
oropharyngeal structure and class 4 none. Various metaanalysis
reported different sensitivity and specificity for the Mallampati
and modified Mallampati tests. In [30], Cattano et al. reported
a sensitivity and a specificity of 35% and 91%, respectively. In
[31], Lundstrm et al. included 55 studies and 177 088 patients
and reported a sensitivity of 0% to 100% and a specificity of
44% to 100%. They computed a ROC curve, and the area under
the curve (AUC) was 0.753, which categorize the diagnostic
test as good. In [32], the reported AUC for the Mallampati and
modified Mallampati tests are, respectively, 0.58 and 0.83. In
those studies, the authors agree that the clinical value of the Mallampati test is limited as it has poor to moderate discriminative
power when used alone.
The upper lip bite test, proposed by Khan et al. [33], evaluates
the ability of the patient to cover his upper lip with the lower
incisors by moving forward the lower jaw (in a movement of
prognathism). The results range from grade I to grade III, where
grade I and garde II predict easy laryngoscopy, whereas grade III
predicts difficult laryngoscopy. The authors initially observed a
sensitivity of 76.5% and a specificity of 88.7%. Those results
were confirmed in a recent study (78.95% and 91.96%, respectively) [34].
Eberhart et al. conducted a comparison between Mallampati score and upper lip bite test on 1107 patients [35] and
concluded that both tests are poor predictors for difficult laryngoscopy when used as single preoperative bedside screening
tests. None of those simple tests have been shown to be accurate
in predicting airway management problems. Their sensitivity
and predictive positive values are generally low, precluding an
accurate prediction of difficult endotracheal intubation. Thus,
several studies have been proposed to derive a score from multivariate analysis.
The Wilson risk sum score [36] scores five of the aforementioned factors from 0 to 2: the weight, the vertical head and
neck movement, the jaw movement (prognathism), the receding
mandible, and buck teeth. By varying the threshold values on
the sum of those scores, the true positive rate (TPR) and false
positive rate (FPR) of difficult laryngoscopy assessment are varied. The authors initially proposed a threshold value of 4, i.e.,
a score greater or equal to 4 predicts a difficult endotracheal
intubation.

330

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

TABLE I
COMPARISON OF THREE MULTIVARIATE TESTS [38]
Model

Sens.

Spec.

PPV

NPV

AUC

Acc.

Wilson model [36]
Arné model [37]
Naguib model [24]

40.2
54.6
81.4

92.8
94.9
72.2

25.6
39.7
15.3

96.2
97.1
98.4

79.0
87.0
82.0

66.5
74.7
76.8

Arné et al. proposed a simplified score model [37]. In addition
to the morphological criteria such as interincisor gap, ability to
prognate, TMD, and range of head and neck movement, it also
considers the medical history of the patient and the Mallampati
score.
Naguib et al. performed a clinical, radiologic, and 3-D computer imaging study [24] on 57 patients among which 25 had
an unanticipated difficult intubation. A multivariate discriminant analysis was performed on the clinical measurements and
identified four risk factors that correlated with the difficult laryngoscopy and intubation: thyrosternal distance, TMD, neck circumference, and Mallampati classification.
Table I shows the predictive performances of those three multivariate models as reported in [38]. The authors recruited 194
patients (97 with a difficult airway and 97 controls) over a period of five years. For the purpose of their study, unanticipated
difficult intubation was defined as difficult laryngoscopy (corresponding to a grade 3 or grade 4 Cormack and Lehane laryngoscopic view) and difficult tracheal intubation (two or more
attempts at placing the endotracheal tube) or the use of an alternative device (laryngeal mask airway or bougie) when using
optimal head and neck positioning (the sniffing position). Positive predictive value (PPV) and negative predictive value (NPV)
were calculated based on a prevalence of difficult intubation of
5.8% as reported in a recent metaanalysis [39]. Note that the
sensitivity, specificity, and AUC are the most appropriate measures to compare performances between datasets, mainly due to
the class imbalance problem.
Recently, Fritscherova et al. [40] conducted a case-control
study on 148 patients, and concluded that the three statistically
higher predictors were the interincisors distance, the TMD, and
a decreased temporomandibular joint movement.
As none of those tests fulfill the high sensitivity and high
PPV criteria, anesthesiologists themselves do not agree on the
usefulness of such a prediction [41], [42].
New technological approaches aimed at craniofacial phenotyping, using still photographs, X-ray technologies, or
laser scanning with an automated 3-D rendering, have been
recently applied to the detection of difficult airways. Suzuki
et al. calculated five ratios and angles from measurements
derived from the placement of anatomic markers on patients
photographs [43] demonstrating that the submandibular angle
seemed to be associated with difficult tracheal intubation.
They also used morphing software to construct average easy
and difficult to intubate faces. The improved availability of
cone-beam computed tomography, 3-D imaging, and computer
simulation has been used by Schendel and Hatcher for the
evaluation of the airway [44]. In recent years, some studies
took the advantage of machine learning [45] or statistical face

models [46] in order to provide better prediction and defend
the usefulness of preoperative difficult tracheal intubation
prediction. However, these newer methods require either X-ray
or computed tomographic imaging methods with issues, such
as availability, cost, and radiation dose to the patient.
The method proposed in [46], even though conceptually similar to our proposed approach, presents several important differences. Their method is not fully automatic but semiautomatic as
it requires the manual placement of fiducial markers and manual measurement of the TMD by an anesthesiologist. To limit
any potential confounding effects of gender and racial group,
they recruited only male Caucasians. Moreover, the definition
of the difficult intubation used does not include all patients but
only very easy and difficult patients such that those who were
neither easy nor difficult to intubate according to their criteria
were not included. This significantly reduces the variability in
the data due to other factors than the difficulty of intubation and
renders the resulting model inapplicable in a real-world clinical
environment. This paper presents biased results as the authors
do not clearly separate the data into training and test sets, and
use the test set to select the model.
Finally, the number of patients considered to validate those
newer approaches is often low. For instance, in [46] Connor and
Segal reported results on a validation set of only 20 difficult and
20 easy patients, thus not demonstrating the generalizability of
the proposed method.
More recently, Cattano et al. proposed a new assessment form
on airway prediction but showed that it did not improve resident
ability to predict a difficult airway [47].
Our proposed method has been developed and validated using
more than 900 patients. It does not require any medical history
or measurement on the patient other than frontal and profile
photographs, making it practical even for untrained personnel.
The processing of the photographs is completely automatic and
does not require any manual initialization. The processing time
is of the order of the second, making the proposed method directly applicable in a clinical setting. Specifically, for one out of
the four images, the face detection requires approximately 0.9 s,
the image alignement, the features extraction, and classification
run in real time, i.e., in approximately 30–40 ms. In order to
assess its performance in a real-world scenario, we present results including all levels of difficulty and not only very easy and
difficult patients. We demonstrate that the proposed method performs as well as state-of-the-art multifactorial tests performed
manually by experienced anesthesiologists.
An outline of this paper is as follows. The data collection
process and setup are described in Section II. In Section III, we
describe the face models training and fitting processes, as well
as the learning process. The results obtained are presented in
Section IV and compared to diagnosis-based prediction results.
Finally, conclusions and discussion of future research topics are
given in Section V.
II. DATA COLLECTION
Since March 2012, adult patients at the University Hospital
in Lausanne (CHUV) undergoing general anesthesia requiring
tracheal intubation and related to any type of elective surgical

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

331

TABLE II
PATIENTS POPULATION METADATA
Mean [min, max]
Age
Height [cm]
Weight [kg]
Gender [M/F]
Total

Fig. 1.

Photo booth at CHUV.

procedures except obstetric and cardiac surgery have been
preoperativelly recruited. The study has been approved by
the Human Research Ethics Committee (Ethical approval
number 183/09, Chairperson Prof. R. Darioli) from the
Ethical Committee of the Canton of Vaud, Switzerland. Each
patient gets appropriate information about the research by the
anesthesiologist during the preoperative consultation and gives
his or her written consent to participate in the study.

53 [17, 92]
169.5 [142, 205]
76.8 [40, 160]
488/482
970

III. METHODS
Given a set of images for each patient, we make use of faceanalysis methods in order to extract meaningful features from
the face and neck. These features include simple distances between selected landmarks, as well as information on the global
shape or texture variation of the head. In a second step, the
statistical relevance of those features is computed in order to
discover which of them are relevant in the scope of prediction
of difficult intubation. The most relevant features are then fed to
a classifier. The classifier learns how to discriminate between
easy, intermediate, and difficult to intubate patients.

A. Setup
We developed and set up a photo booth-like equipment (see
Fig. 1) in the surgical prehospitalization center to collect multimodal data on recruited patients. These data include frontal and
profile photos and videos taken with two HD webcams, one in
front and one on the left side of the patient at approximately 40
cm. We also record the voice of the patient and capture depth
maps with a Microsoft Kinect for future analysis.
While sitting in the photo booth, the patient is asked to perform different facial motions as well as head motions. Those include neutral expression, opening the mouth, sticking the tongue
out, lateral rotation, and vertical extension of the head. A graphical user interface, developed on MATLAB, allows an operator
to guide the patient through the different poses he has to take
and to capture the data at the appropriate moment.
We also collect patient demographics, such as age, sex,
weight, height, and presence of denture during the preoperative anesthesia consultation. Details of peroperative airway
management by the in-charge anesthetist are introduced in a
dedicated database containing information on the ease of facemask ventilation, laryngoscopic grade [22] with an appropriate
size MacIntosh blade, years of training of intubator (minimum
of two years training in anesthesia is mandatory), lifting force
necessary for intubation (normal or increased), usage of accessory means, such as external laryngeal manipulation, intubation
bougie, stylet, or video-laryngoscopic equipment, and injuries
related to airway management. Number of airway providers and
number of intubation trials are also recorded. The IDS [25] is
routinely calculated. This information allows obtaining a ground
truth for the intubation difficulty.
In the two years period from March 2012 to March 2014, we
have recorded 2725 patients. The ground truth is available for
970 of those (see Section III-C1). Table II shows the metadata
of the patients’ population used in this study.

A. Detecting the Face and Tracking the Landmarks
Facial image analysis methods often include two main parts:
first, we need to determine automatically the rough location of a
face in the image using a face detector, then precise locations of
each landmark are found by accurately fitting a model of the face
on the image. Finally, features are computed using individual
landmark positions as well as their global configuration.
1) Face Detector: In order to initialize the fitting of the face
model, both the rough location of the face in the image, as well
as its scale, needs to be determined.
We use Yang’s parts-based detector [48] in order to detect the
face in the images. This method is a general, flexible mixture of
parts model able to capture contextual cooccurrence relations
between parts, augmenting standard spring models that encode
spatial relations. It has been shown to perform very well on face
detection [18] and to be particularly reliable for extreme head
poses. The good flexibility of the method allows us to train a
single detector for all frontal images, even though the patients
are performing very different facial motions, such as opening
the mouth widely or sticking out the tongue. An additional
detector is trained for profile images as many parts of the frontal
images are not visible in the profile images. We use a manually
annotated subset of our data to train both detectors. For the
frontal detector, the training set consists of 406 annotated images
including neutral face, mouth open, and tongue out images. Both
the original image and the horizontal flip of the image are used.
For the profile detector, the training set consists of 134 annotated
images.
The frontal face detector performs very well and detects 100%
of the frontal faces in the 2910 images of the 970 patients performing all facial motions. This set includes 2553 unseen images, i.e., not used for traininig the face detector. The profile face
detector, on the other hand, fails to detect the face of only four

332

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

Fig. 2. Details of the four templates, each corresponding to a facial motion: (a) Frontal, neutral, 99 points. (b) Frontal, mouth open, 99 points. (c) Frontal, tongue
out, 99 points. (d) Profile, neutral, 52 points. In green, the anatomical and morphological features described in Section III-B.

patients, which are removed from the final analysis, reaching a
detection rate of 99.56% on unseen images.
The detection of the face provided by the face detector is then
used to initialize the fitting process of the face model.
2) Face Model for the Image Alignment Problem: Finding
the precise location of each predefined landmark in a new, unseen image is considered as an image alignment problem. Image alignment is the process consisting of rigidly moving and
nonrigidly deforming a template to minimize its distance to
a query image. Image alignment process is characterized by
three elements: template representation, distance metric, and
optimization scheme.
In this study, we follow the image alignment method described in [20]. The template is nonparametric and consists of
scale-invariant feature transform (SIFT) features [49] extracted
from patches around each landmark. This nonparametric shape
model is able to better generalize than other parameterized appearance models in unseen situations, and this representation is
robust against changes in illumination. The squared difference
between the SIFT features values computed in the aligned image
and in the template is used as the distance metric. This results
in the following minimization problem over Δx:
f (x0 + Δx) = h (d(x0 + Δx)) − φ∗ 22

(1)

where x0 is the mean shape, Δx is the update of the shape,
d is the image, h is a nonlinear feature extraction function (in
our case, the SIFT features), and φ∗ = h(d(x∗ )) represents the
SIFT values in the manually labeled landmarks.
The supervised descent method optimization scheme, thoroughly described in [20], learns a series of descent directions
and rescaling factors (done by the Hessian, in the case of Newton’s method) such that it produces a sequence of updates
(xk +1 = xk + Δxk ) starting from x0 that converges to x∗ in
the training data. x0 is the initial configuration of the landmarks
provided by the face detector, which corresponds to an average
shape, scaled, and translated, and x∗ is the correct configuration
of the landmarks, generally obtained by manual annotations of
the images.
a) Definition of the Templates: In the scope of this study, we
define one template per facial motion, necessary to get accurate

landmark positions on photos with different facial motions. In
order to train those models, we have defined one neutral and
frontal template with 99 points, two different frontal 99 points
templates with large facial motions (one with the mouth open
and the second with the mouth open and the tongue out),and
one profile template consisting of 52 points. We then manually
annotated images for each of those templates to train the face
model described above.
The template corresponding to a neutral position and neutral
expression contains landmarks for each eyebrow, eye, the nose,
the mouth, and the chin; it has 99 points in total [see Fig. 2(a)].
It also includes points on the neck in order to assess neck characteristics, such as the width. The two templates, corresponding
to images with extreme facial motions (mouth open and tongue
out), have the same points as the neutral 99 points template [see
Fig. 2(b) and (c)]. The landmarks defining the internal perimeter of mouth opening follow teeth or lips, depending on what
is present in the image. The same set of landmarks was used
for assessing the tongue out movement with a segmentation of
the oral cavity, allowing grading of an automated modified Mallampati classification [50]. The segmentation of the oral cavity
is shown in yellow on Fig. 2(c). For profile images, a template
of 52 points was defined [see Fig. 2(d)]. The points on the jaw
and the neck allow assessing jaw movement, while performing
mandibular movement.
b) Validation of the face model: In order to validate the face
model, we use K-fold cross validation. For each model, the
images from onefold are kept for testing the model, while the
images from all other folds are used to train the model. The
greater the number of folds, the more training images are used
at each run. The obtained model is then fitted on the annotated
images in the excluded fold, and the obtained landmark positions are compared to the manual annotations. This procedure
is repeated for each fold. This way, the model is tested on each
available annotated image. Note that the face detector is first
run on the images in order to initialize the face model. We,
thus, test the whole pipeline at once. In order to quantify the
evolution of the error with respect to the number of training images, we run this K-fold cross-validation scheme for each model
with two-, three-, four-, five-, and tenfolds. These correspond to
50%, 66.6%, 75%, 80%, and 90% of the annotations used for

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

333

Fig. 3. Distribution of the errors (differences between the landmark positions obtained automatically and the manual annotations) on each landmark for the four
templates: (a) Frontal, neutral, 99 points. (b) Frontal, mouth open, 99 points. (c) Frontal, tongue out, 99 points. (d) Profile, neutral, 52 points.

those, the two models with the mouth open and the tongue out
exhibits a larger normalized point-to-point error than the neutral one. Again, the points on the chin and the neck are the less
accurate (see Fig 3). It should be noted that the points around
the mouth are reasonably accurate and those are also the most
interesting for our application. The points around the eyes are
the most accurate, thus making them good candidates for normalization. It can be seen that removing the landmarks from
the chin and the neck from the mean computation improves
the mean point-to-point error by 15% to 25% depending on the
model. Indeed, those landmarks are significantly less accurate
than the rest of the model, as discussed earlier. In the final application, all available annotated images will be used for training.
Thus, the actual performances of the models will be better as
they will have been trained with more annotated images.

Fig. 4. Mean point-to-point error (distance between the landmark positions
obtained automatically and the manual annotations) normalized by the distance
between the eyes.

training. The total number of annotated images is 150 for each
of the frontal models and 92 images for the profile model.
Fig. 3 shows the distributions of the errors for each landmark and each model when trained and tested using tenfolds
cross validation (90% of the annotations for training). During
the testing step, the error between each landmark and the corresponding annotation is computed for each test image. We then
report these errors on the mean shape of each model and fit a
Gaussian function for better visualization.
The quality of the model varies from one model to the other.
The profile model is the least accurate [see Fig. 3(d) but is also
trained on fewer images. Moreover, the annotations might be
less consistent from one training image to the other, due to the
increased difficulty of annotating the profile face. The points on
the chin and the neck from the profile model do not correspond
to any salient landmarks on the images; therefore, increasing
the annotation difficulty as well as decreasing the face tracker
ability to precisely locate those landmarks.
Fig. 4 shows the mean point-to-point error normalized by the
distance between the eyes for the three frontal models. Amongst

B. Computing the Features
Most of the anatomical and morphological features of interest
consist of distances between landmarks on the face and the neck.
The aligned template gives the positions of those landmarks after
fitting the face model on the subject image. Specifically, those
distances are the vertical distance between the upper lip and the
nose, the vertical distance between the lower lip and the tip of
the chin, the width of the neck, the width of the face, and the
height of the face, all five computed on the frontal neutral image
[see lines 1–5, respectively, on Fig. 2(a)]. They are the TMD in
neutral position, the distance between the angle of the mandible
and the tip of the chin, the distance between the hyoid bone
and the chin, and the distance between the hyoid bone and the
thyroid cartilage, all four computed on the profile neutral image
[see lines 1–4, respectively, on Fig. 2(d)]. Finally, they are the
height of the mouth opening, the width of the mouth opening,
and the area of the mouth opening, all three computed from the
frontal image with the mouth open [see lines 1–2 and surface 3,
respectively, on Fig. 2(b)]. In addition, we compute the distance
between the eyes on all frontal images. This distance is used to
normalize the features listed above allowing us to be more robust
against moderate head pose variations, and to be able to compare
them between patients. Indeed, the fact that all patients do not
sit at the exact same distance to the camera and do not have

334

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

the same head pose introduces an important bias in the features.
After normalization, all distances are divided by the distance
between the eyes. This one exhibits small variations between
subjects is most likely not correlated with difficult intubation,
and can be computed reliably from the landmarks around the
eyes as they are very accurate.
In addition to the distances between landmarks, we also consider coefficients from a PCA on the shape and coefficients from
a PCA on the texture for the inside of the mouth on the frontal
tongue out model as features. Specifically, we compute those
coefficients in the following manner.
To compute the PCA coefficients on the shape, we consider
the set of face images used for training, each image having a
set of v 2-D landmarks returned by the face tracker [xi , yi ], i =
1, 2, . . . , v. The collection of landmarks of one image is treated
as one observation from the random process defined by the shape
model s = [x1 , y1 , x2 , y2 , ..., xv , yv ]T . Eigenanalysis is applied
to the observation set, keeping 98% of energy, and the resultant
model represents a shape as
s(p) = s0 +

n


pi si

(2)

i=0

where s0 is the mean shape, si is the ith shape basis, and p =
[p1 , p2 , . . . , pn ] are the shape parameters.
Those parameters p provide information on the global variation of the shape. They are ranked by the value of their corresponding eigenvalue, in a decreasing order, or, similarly by the
amount of total variance of the training data that they explain.
The first modes of variation explain the bigger amount of total
variance, and are, thus, likely to explain the variance of the data
due to head pose, gender, or other factors that are not significant
in the prediction of the difficult intubation. On the other hand,
the last ones only explain a small amount of the total variance
and merely model the effect of the noise in the annotations. Even
though not all coefficients are relevant for classification, each of
them has the advantage of encoding a variation mode affecting
the relative configuration of several landmarks by itself. Thus,
by selecting a few, relevant coefficients, we can potentially get
information about global configurations of landmarks (or global
morphology of the face) correlated with difficult intubation.
To compute the PCA coefficients on the texture, we first compute a piecewise affine transform between the landmarks segmenting the oral cavity on each image [see the yellow contour
on Fig. 2(c)] and the same landmarks on the mean shape. The
texture inside those landmarks is then warped onto the mean
shape and normalized to zero mean and unit standard deviation.
At training time, the warped and normalized texture from the
images in the training set are used to compute a PCA basis. Similarly to the PCA on the shape, the eigenvectors corresponding
to the biggest ordered eigenvalues and explaining 75% of the
texture variance are kept, while the others are discarded. At testing time, the warped and normalized texture from the images in
the test set is then projected on that basis, resulting in a vector
of coefficients used as features. For more details, the reader is
referred to [50], where the same method is used for automatic
Mallampati classification.

Section III-C3 provides more details about the feature selection techniques that have been used to find those relevant
coefficients.
C. Classification
1) Class Definitions: In order to train and test the system,
each patient is assigned one of the following labels, considered as ground truth and related to their difficulty of intubation
easy, intermediate, or difficult. As no precise definition of the
difficult intubation has been unanimously accepted, this classification is obtained by combining two complementary definitions, namely the widely accepted definition of the difficult
laryngoscopy, which considers a laryngoscopy as difficult if the
Cormack–Lehane view of the larynx is graded III or IV [21] and
the definition based on the intubation difficulty score (IDS) proposed by Adnet [25], which considers an intubation as difficult
if the IDS is greater than 5. We use this broader definition of the
difficult intubation in order to remove, as much as possible, the
subjectivity of using only the laryngoscopic grade, while still
assigning laryngoscopic grades III and IV to the difficult class.
More specifically, the class labels are defined as follows:
easy: IDS = 0, this implies a laryngoscopic grade of I and a
successful intubation at the first attempt;
intermediate: 0 < IDS ≤ 5 and laryngoscopic grade smaller
than III;
difficult: IDS > 5 , or laryngoscopic grades of III or IV.
Out of the 2725 patients who have been recorded, information allowing to compute the IDS is available for 34.4% and
laryngoscopic grade for 51.4% at the time of writing this paper.
Table III(a) shows the distribution of patients according to
the laryngoscopic view for all recorded patients and for the
subset of patients with available ground truth and face detection.
The laryngoscopic view was observed by the anesthesiologist
at the intubation time. It should be noted that the classes are
largely unbalanced, higher laryngoscopic grades being rarely
observed which makes the classification task more challenging.
Table III(b) shows the classification of the recruited patients
according to their IDS score. The same remark applies regarding
high IDS scores.
Table III(c) shows the distribution of each class according
to the classification described above for the 966 patients used
in total. The easy, intermediate, and difficult labels are used as
ground truth. Note that this does not directly correspond to the
IDS because eight patients with IDS ≤ 5 have a laryngoscopic
grade greater than II and are labeled as difficult and 29 other
patients with a laryngoscopic grade greater than II have missing
IDS score.
2) Data Partition for Training and Testing and Class Imbalance Problem: The feature selection, the choice of the hyperparameters, and the training of the classifier are performed on a
subset of patients: the training set. A distinct subset of patients
is then used to test the classifier and compute the different metrics assessing its performance: the test set. The partition of the
original data into those two subsets is random but the original
distribution of classes is maintained (stratified partitioning). In
order to compute proper statistics for the results, those training

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

335

TABLE III
(A) PATIENTS LARYNGOSCOPIC GRADE (LG) DISTRIBUTION AS OBSERVED BY THE ANESTHESIOLOGIST AT INTUBATION TIME, (B) PATIENTS IDS SCORE
DISTRIBUTION, (C) FINAL GROUND TRUTH LABELS DISTRIBUTION

and test sets are generated several times, each time with different
random partitions of the patients.
Note that both the training and the test set follow the same
class distribution as the original dataset. As previously discussed, the occurrence of difficult laryngoscopy has been reported to range from 0.3% to 13% [24]. More recently, the
occurrence of difficult intubation has been reported between
4.5% and 7.5% in the overall population [39]. In the present
dataset, 6.21% of the patients fall in the difficult class. From a
machine learning point of view, skewed distributions of classes
make the learning of concepts more difficult. This is known as
the class imbalance problem. Even a relatively small imbalance
ratio of the order of 10:1, as in our case, is sufficient to hinder
the learning process.
Artificially balancing the classes is possible using sampling methods. However, those methods present some significant drawbacks [51]–[53]. Undersampling from the majority
class(es) allows reducing the imbalance ratio or even totally
compensating for the class imbalance, but removing samples
from class(es) may result in loss of information, thus potentially penalizing the classifier’s performance. In the other case,
oversampling from the minority class(es) allow for the same
reduction of class imbalance but presents a different drawback.
Replicating samples tends to lead overfitting. Even though more
complex techniques exist, several problems prevent from finding a good approximation of the original class density function,
for example, small disjuncts or class overlapping.
In this paper, we consider binary classifiers. To oversome the
class imbalance problem, we use the fact that for each sample,
probabilistic classifiers compute confidence values of belonging to each class. The classifier then usually assigns the most
probable label to each sample by maximizing P (j|x), the posterior probability of classifying a sample x as j. Nevertheless,
in cost-sensitive learning, given a cost matrix defined as C(i, j),
the misclassification cost of classifying an instance from its actual class j into the predicted class i, the minimum expected
loss can be determined as

P (j|x) · C(i, j)
(3)
R(i|x) =
j ∈{0,1}

where R is the Bayes risk and P (j|x) is the posterior probability.
Elkan [54] showed that modifying the classifier’s threshold, that
is choosing the positive class if its confidence value is greater
than a threshold but not necessarily greater than the confidence
value of the other class, has the same effect as sampling in
terms of bias but without the drawbacks mentioned above. Thus,

defining a threshold θ for the classifier allows compensating for
the bias toward the majority class. Specifically, in cost-sensitive
learning, the optimal threshold θ∗ of a classifier with respect to
a given cost matrix is defined as
θ∗ =

C(1, 0)
.
C(1, 0) + C(0, 1)

(4)

In binary classification, C(1, 0) represents FP and C(0, 1) represents false negative (FN). The prior probabilities of the negative
and positive samples (p(0) and p(1), respectively) are proportional to the number of samples in the original training set. As
doubling FN or halving FP has the same effect as doubling p(1),
we train the classifier on the complete (unbalanced) training set,
and when testing it on the test set, the threshold θ is set to the
imbalance ratio between the classes
θ=

FP
FP + FN ·

p(0)
p(1)

=

1
1+

p(0)
p(1)

≈

p(1)
p(0)

(5)

where p(0)
p(1) is bigger than 1 as the positive class, with the label
difficult, is the class for which we have less samples.
As modifying the threshold of the classifier is equivalent to
sampling, we compare three methods of choosing this threshold.
1) The class imbalance ratio method as described above [see
(5)].
2) Minimizing the distance between the corresponding point
on the ROC curve and the (0,1) point (upper left corner)
3) Maximizing the Youden index, i.e., the vertical distance
between the corresponding point on the ROC curve and
the line of no-discrimination.
The latter two methods use fourfold cross validation on the
training set to learn the optimal threshold. In order not to hinder
the learning process when training the classifier on an unbalanced set, we use the area under the ROC curve as criterion.
The ROC curve is generated by plotting the FPR against the
TPR for all values of the classifier threshold. Independently of
what kind of classifier is used, we train it such that the ROC
curve generated from the output confidence values maximizes
the AUC, since AUC is insensitive to the class imbalance. As a
postprocessing step, we then compute the threshold to apply on
the confidence values in order to obtain the final classification
of each sample.
3) Feature Selection and Classification: Feature selection is
performed on the training set. The goal is to determine which
features are the most relevant for difficult intubation prediction. Amongst the complete set of features, only those most

336

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

relevant features are then used to train the classifier. Reducing
the dimensionality of the data, as well as removing noisy irrelevant features from the data helps to improve the classification
performance.
Random forest classifiers provide a feature importance measure, which allows for feature ranking and selection [55]. The
feature importance is measured by randomly permuting the feature in the out-of-bag samples and calculating the percent increase in misclassification rate as compared to the out-of-bag
rate with all variables intact. From the ranking of the features according to their importance, we only keep the k best and discard
all the rest. The parameter k is considered a hyperparameter,
and its best value is found using grid search and K-fold cross
validation on the training set at the same time as the classifier
hyperparameters.
For the final classification, a second random forest classifier
is used. Random forest classifiers are known to be less prown to
overfitting, due to their use of bagging. Indeed, the training algorithm for random forest aims at constructing a forest of trees,
where for each tree it randomly samples with replacement in the
training set and trains the tree by considering only a random subset of the features at each splitting node. The hyperparameters
of the classifier are selected using fourfold cross validation on
the training set. Specifically, those hyperparameters are the following: the number of the k best features to keep (in the range
20–180 by step of 10) and the percentage of features to consider
√ node when looking for the best split (in the range
√ at each
0.5 N –2 N , where N is the total number of features). We use
entropy as the splitting criterion, as it is less sensitive to class
imbalance than the usual accuracy [52]. Our implementation
uses Scikit-learn [56], a python machine learning library.
IV. RESULTS
We present two scenarios: an easy versus difficult classification considering easy control patients and difficult ones, as well
as a more realistic difficult intubation prediction scenario, where
all patients are considered. The second one would correspond
to a real-world scenario, where each and every incoming patient
gets a prediction.
A. Easy Versus Difficult Classification
In this scenario, we followed the same protocol as Naguib did
in his comparative study of three multivariate difficult tracheal
intubation models [38], in which for each difficult patient, an
easy one (control patient) is selected. In our case, we do not
enforce a one to one correspondence, but keep the imbalance
between the classes. Removing the intermediate patients, we end
up with two disjoint classes: the easy and the difficult patients.
We use 80% of the patients for training and 20% for testing.
The partition is repeated 100 times randomly, and the results
are averaged over those different partitions. This results in 496
training patients (448 easy and 48 difficult) and 125 test patients
(113 easy and 12 difficult).
The performances of the classifier are reported in Table IV,
along with the results reported in the literature for three manual
tests [38] and a previous attempt for semiautomatic difficult

TABLE IV
COMPARISON OF OUR RESULTS ON THE EASY VERSUS DIFFICULT PROBLEM
WITH THREE MULTIVARIATE TESTS [38] AND A SEMIAUTOMATIC METHOD [46]
Model

Sens. [95% CI]

Spec. [95% CI]

AUC

Wilson model [36]
Arné model [37]
Naguib model [24]
Connor [46]
Ours
class imbalance
distance to (0,1)
Youden index

40.2 [30.0, 50.0]
54.6 [45.0, 65.0]
81.4 [74.0, 89.0]
90.0

92.8 [88.0, 98.0]
94.9 [90.0, 99.0]
72.2 [63.0, 81.0]
80.0

79.0
87.0
82.0
84.0
81.0

79.7 [77.4, 81.9]
77.1 [74.8, 79.4]
78.9 [76.5, 81.3]

67.4 [66.4, 68.4]
70.6 [69.4, 71.8]
66.7 [64.7, 68.6]

Fig. 5. Mean ROC curve for the easy versus difficult classification and the
best ROC curve out of the 100 runs, with performance obtained using the
class imbalance threshold method compared to the ROC curve obtained on the
validation set in [46].

intubation prediction from [46]. We report the mean values
of the sensitivity and specificity with their 95% confidence
interval (CI).
As can be seen in Table IV, our fully automatic system
achieves comparable performance on the easy versus difficult
intubation classification as compared to manual assessment using state-of-the-art multifactorial tests. In this binary example,
the only metric that can be compared directly is the area under
the ROC curve. All other metrics reported can be tuned by varying the threshold of the classifier, depending on the importance
given to sensitivity or specificity. This can be seen by comparing the three methods to compute an optimal threshold. The
class imbalance method provides the higher sensitivity, which
in this application is an important metric as it is critical to detect as many difficult intubations as possible, even at the cost of
more FP.
Fig. 5 presents the averaged ROC curve over the 100 partitions. In red, it also shows the ROC curve corresponding to the
best run out of 100, i.e., the one with the highest AUC. In blue,
we regenerated the ROC curve corresponding to the validation
set in [46]. We used the values of each samples in the validation
set provided in [46] to compute TPR and FPR for all thresholds.

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

The highlighted performance points on the best run and mean
ROC curve have been obtained by setting the threshold of the
classifier to the class imbalance ratio. On the mean ROC curve,
it corresponds to the results reported in Table IV, whereas the
sensitivity and specificity for the best run are, respectively, 1.0
and 0.65.
As for comparison with the results reported in [46], we would
like to emphasize that such a comparison would not be a fair one
as already mentioned in Section I-A. First, the authors trained
and tested their system only on male caucasian patients, while
we report our results on a much more representative population
(see Table II). Second, Connor and Segal in [46] state that they
perform model selection such that they get the best product
of AUCs on the training and validation sets. In addition, they
do not perform any kind of cross validation and demonstrate
results on a single partitioning. Methodologically, there is no
evidence in their work that similar results would be obtained on
an independent test set or a different partitioning of the data. In
this study, on the other hand, we present our results on multiple
runs, each of them on randomly created independent test sets.
Although in average our AUC score (0.81) is lower than the
AUC calculated on the validation set in [46] (0.84), our results
are better validated in a more generalized way. Fig. 5 shows
indeed that our fully automatic system can achieve better results
on the best run compared to the semiautomatic system in [46].
This shows that a single run is not representative of the merit of
a system.
B. Real-World Difficult Intubation Prediction
In the real-world difficult intubation prediction problem, the
goal is to identify difficult to intubate patients from all the others.
Considering this task, the problem remains a two-class classification problem. Thus, we first group together the easy and
intermediate classes and relabel the new class as easy, which
de facto represents the nondifficult to intubate patients. When
a patient is diagnosed as difficult, it sends a strong signal to
the anesthesiologists on the potential difficulty of that patient,
which is high. Thus, we do not consider only very easy patients
as control patients versus difficult ones, but instead we take into
account all patients, ranging from very easy to impossible to
intubate without gap.
We use 80% of the patients for training and 20% for testing.
The partitioning is repeated 100 times randomly, and the results
are averaged over those different partitions. This results in 772
training patients (724 easy and 48 difficult) and 194 test patients
(182 easy and 12 difficult). Note that in this case, the class
imbalance is more severe, creating an additional challenge to the
fact that there is more variation among the samples as compared
to the previous scenario. The performances of the classifier are
reported in Table V. Fig. 6 presents the averaged ROC curve
over the 100 partitions.
As can be seen in Table V, the performances of the system
drop slightly when considering all patients, without gap between
the classes. We observe a 3.1% decrease on the AUC and between −1.2% and −4.2% on the sensitivity and specificity. By
considering all patients, the variance of the data is larger. Thus,

337

TABLE V
COMPARISON OF OUR RESULTS ON THE REAL-WORLD PROBLEM

Fig. 6.

Model

Sens. [95% CI]

Spec. [95% CI]

Real-world
class imbalance
distance to (0,1)
Youden index

77.7 [75.7, 79.7]
72.9 [70.3, 75.5]
74.8 [72.0, 77.5]

64.1 [63.2, 65.0]
68.4 [67.2, 69.5]
65.5 [63.5, 67.4]

AUC
77.9

Mean ROC curve for the real-world difficult intubation prediction.

the learning of concepts is hindered as this larger variance can
be seen as the noise. Moreover, the absence of gap between the
classes potentially decreases the class separability, again hindering the learning of concepts. Indeed, the classes become less
distinct and when testing on a different dataset than that used for
training, the chances are higher that the classes overlap. Note
that the definition of the ground truth also has an importance
in the performance of the system. More specifically, the subjectivity and poor reproducibility of the Cormack–Lehane grade
make the ground truth label less reliable.
V. CONCLUSION
In this study, we presented a completely automatic
morphology-based method allowing predicting a patient’s difficulty of intubation with performances comparable to stateof-the-art medical diagnosis-based predictions by experienced
doctors. Our method has been validated on more than 900 patients, both in a research oriented scenario with only easy and
difficult patients and in a real-world oriented scenario, where all
patients are considered.
The database used in this study is, to the best of our knowledge, the largest database of images, videos, and ground truth
data related to endotracheal intubation.
The open question of how to quantify a difficult intubation remains a penalizing factor for our results. Indeed, the recognized
subjectivity as well as the large variability of the factors taken
into account in order to quantify the difficulty of intubation of
a patient creates an additional confound for the system. This

338

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 63, NO. 2, FEBRUARY 2016

raises the question of the direct clinical usefulness of such an
automatic tool. Yet, we demonstrate that it can achieve close
to human performance even with such existing limitations. It
is thus encouraging to further investigate the usage of facial
image analysis in the scope of difficult endotracheal intubation
prediction.
Due to the rarity of patients difficult to intubate, obtaining
a reasonable number of them is a long term procedure. Thus,
current and future development include the collection of more
data. Another future research axis is to use other modalities that
may be indicative of intubation difficulty. For this purpose, we
also record the voice of the patient and the depth of the mouth
cavity using the kinect. Further analysis of the data will include
the use of these two modalities.

REFERENCES
[1] G. N. Peterson et al., “Management of the difficult airway: A closed claims
analysis,” Anesthesiology, vol. 103, no. 1, pp. 33–39, 2005.
[2] T. M. Cook and S. R. Macdougall-Davis, “Complications and failure of
airway management,” Brit. J. Anaesthesia, vol. 109, pp. i68–i85, 2012.
[3] J. Metzner et al., “‘Closed claims’ analysis,” Best Pract. Res., Clin. Anaesthesiol., vol. 25, no. 2, pp. 263–276, 2011.
[4] A. M. B. Heard et al., “The formulation and introduction of a ‘can’t
intubate, can’t ventilate’ algorithm into clinical practice,” Anaesthesia,
vol. 64, no. 6, pp. 601–608, 2009.
[5] L. D. Hove et al., “Analysis of deaths related to anesthesia in the period
1996-2004 from closed claims registered by the danish patient insurance
association,” Anesthesiology, vol. 106, no. 4, pp. 675–680, 2007.
[6] M. F. Aziz et al., “Routine clinical practice effectiveness of the glidescope
in difficult airway management: An analysis of 2,004 glidescope intubations, complications, and failures from two institutions,” Anesthesiology,
vol. 114, no. 1, pp. 34–41, 2011.
[7] W. H. L. Teoh et al., “Comparison of three videolaryngoscopes: Pentax
R
airway scope, C-MACT M , glidescope
vs the macintosh laryngoscope
for tracheal intubation,” Anaesthesia, vol. 65, no. 11, pp. 1126–1132,
2010.
[8] G. Serocki et al., “Management of the predicted difficult airway: A comparison of conventional blade laryngoscopy with video-assisted blade
laryngoscopy and the glidescope,” Eur. J. Anaesthesiol., vol. 27, no. 1,
pp. 24–30, 2010.
[9] E. J. Juan et al., “Miniature acoustic guidance system for endotracheal
tubes,” IEEE Trans. Biomed. Eng., vol. 49, no. 6, pp. 584–596, Jun. 2002.
[10] J. O. Räsänen et al., “Effects of diameter, length, and circuit pressure
on sound conductance through endotracheal tubes,” IEEE Trans. Biomed.
Eng., vol. 53, no. 7, pp. 1255–1264, Jul. 2006.
[11] H.-I. A. and R. W. Picard, “Measuring affective-cognitive experience and
predicting market success,” IEEE Trans. Affective Comput., vol. 5, no. 2,
pp. 173–186, Apr.–Jun. 2014.
[12] F. Ringeval et al., “Prediction of asynchronous dimensional emotion ratings from audiovisual and physiological data,” Pattern Recog. Lett., 2014,
doi:10.1016/j.patrec.2014.11.007
[13] Y. Dong et al., “Driver inattention monitoring system for intelligent vehicles: A review,” in Proc. IEEE Intell. Veh. Symp., 2009, pp. 875–880.
[14] H. Gao et al., “Detecting emotional stress from facial expressions for
driving safety,” in Proc. Int. Conf. Image Process., 2014, pp. 5961–5965.
[15] G. Baynam et al., “Intersections of epigenetics, twinning and developmental asymmetries: Insights into monogenic and complex diseases and
a role for 3D facial analysis,” Twin Res. Hum. Genetics, vol. 14, no. 4,
pp. 305–315, 2011.
[16] P. Claes et al., “Dysmorphometrics: The modelling of morphological abnormalities,” Theor. Biol. Med. Model., vol. 9, no. 1, 2012,
doi:10.1186/1742-4682-9-5
[17] Q. Zhao et al., “Automated down syndrome detection using facial photographs,” in Proc. IEEE Annu. Int. Conf. Eng. Med. Biol. Soc., 2013,
pp. 3670–3673.
[18] X. Zhu and D. Ramanan, “Face detection, pose estimation, and landmark
localization in the wild,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recogn., 2012, pp. 2879–2886.

[19] H. Cevikalp et al., “Face and landmark detection by using cascade of
classifiers,” presented at the IEEE 10th Int. Conf. Automatic Face Gesture
Recognition, Shanghai, China, 2013.
[20] X. Xiong and F. De La Torre, “Supervised descent method and its applications to face alignment,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis.
Pattern Recogn., 2013, pp. 532–539.
[21] R. S. Cormack and J. Lehane, “Difficult tracheal intubation in obstetrics,”
Anaesthesia, vol. 39, no. 11, pp. 1105–1111, 1984.
[22] S. M. Yentis and D. J. H. Lee, “Evaluation of an improved scoring system
for the grading of direct laryngoscopy,” Anaesthesia, vol. 53, no. 11,
pp. 1041–1044, 1998.
[23] R. Krage et al., “Cormack-Lehane classification revisited,” Brit. J. Anaesthesia, vol. 105, no. 2, pp. 220–227, 2010.
[24] M. Naguib et al., “Predictive models for difficult laryngoscopy and intubation. A clinical, radiologic and three-dimensional computer imaging
study,” Can. J. Anaesthesia, vol. 46, no. 8, pp. 748–759, 1999.
[25] F. Adnet et al., “The intubation difficulty scale (IDS): Proposal and evaluation of a new score characterizing the complexity of endotracheal intubation,” Anesthesiology, vol. 87, no. 6, pp. 1290–1297, 1997.
[26] P. A. Baker et al., “Thyromental distance measurement—Fingers don’t
rule,” Anaesthesia, vol. 64, no. 8, pp. 878–882, 2009.
[27] B. Krobbuaban et al., “The predictive value of the height ratio and thyromental distance: Four predictive tests for difficult laryngoscopy,” Anesthesia Analgesia, vol. 101, no. 5, pp. 1542–1545, 2005.
[28] S. R. Mallampati et al., “A clinical sign to predict difficult tracheal intubation; a prospective study,” Can. Anaesthetists’ Soc. J., vol. 32, no. 4,
pp. 429–434, 1985.
[29] G. L. T. Samsoon and J. R. B. Young, “Difficult tracheal intubation: A
retrospective study,” Anaesthesia, vol. 42, no. 5, pp. 487–490, 1987.
[30] D. Cattano et al., “Risk factors assessment of the difficult airway: An
italian survey of 1956 patients,” Anesthesia Analgesia, vol. 99, no. 6,
pp. 1774–1779, 2004.
[31] L. H. Lundstrøm et al., “Poor prognostic value of the modified mallampati
score: A meta-analysis involving 177088 patients,” Brit. J. Anaesthesia,
vol. 107, no. 5, pp. 659–667, 2011.
[32] A. Lee et al., “A systematic review (meta-analysis) of the accuracy of
the mallampati tests to predict the difficult airway,” Anesthesia Analgesia,
vol. 102, no. 6, pp. 1867–1878, 2006.
[33] Z. H. Khan et al., “A comparison of the upper lip bite test (a simple new
technique) with modified mallampati classification in predicting difficulty
in endotracheal intubation: A prospective blinded study,” Anesthesia Analgesia, vol. 96, no. 2, pp. 595–599, 2003.
[34] Z. H. Khan et al., “The diagnostic value of the upper lip bite test combined
with sternomental distance, thyromental distance, and interincisor distance
for prediction of easy laryngoscopy and intubation: A prospective study,”
Anesthesia Analgesia, vol. 109, no. 3, pp. 822–824, 2009.
[35] L. H. J. Eberhart et al., “The reliability and validity of the upper lip
bite test compared with the Mallampati classification to predict difficult
laryngoscopy: An external prospective evaluation,” Anesthesia Analgesia,
vol. 101, no. 1, pp. 284–289, 2005.
[36] M. E. Wilson et al., “Predicting difficult intubation,” Brit. J. Anaesthesia,
vol. 61, no. 2, pp. 211–216, 1988.
[37] J. Arné et al., “Preoperative assessment for difficult intubation in general
and ENT surgery: Predictive value of a clinical multivariate risk index,”
Brit. J. Anaesthesia, vol. 80, no. 2, pp. 140–146, 1998.
[38] M. Naguib et al., “Predictive performance of three multivariate difficult
tracheal intubation models: A double-blind, case-controlled study,” Anesthesia Analgesia, vol. 102, no. 3, pp. 818–824, 2006.
[39] T. Shiga et al., “Predicting difficult intubation in apparently normal patients: A meta-analysis of bedside screening test performance,” Anesthesiology, vol. 103, no. 2, pp. 429–437, 2005.
[40] S. Fritscherova et al., “Can difficult intubation be easily and rapidly predicted?” Biomed. Papers, vol. 155, no. 2, pp. 165–172, 2011.
[41] S. M. Yentis, “Predicting difficult intubation—Worthwhile exercise or
pointless ritual?” Anaesthesia, vol. 57, no. 2, pp. 105–109, 2002.
[42] É. Orozco-Dı́az, et al., “Predictive factors of difficult airway with known
assessment scales,” Cirugia y Cirujanos, vol. 78, no. 5, pp. 393–399,
2010.
[43] N. Suzuki et al., “Submandible angle in nonobese patients with difficult
tracheal intubation,” Anesthesiology, vol. 106, no. 5, pp. 916–923, 2007.
[44] S. A. Schendel and D. Hatcher, “Automated 3-dimensional airway analysis
from cone-beam computed tomography data,” J. Oral Maxillofacial Surg.,
vol. 68, no. 3, pp. 696–701, 2010.
[45] O. Langeron et al., “Prediction of difficult tracheal intubation: Time for a
paradigm change,” Anesthesiology, vol. 117, no. 6, pp. 1223–1233, 2012.

CUENDET et al.: FACIAL IMAGE ANALYSIS FOR FULLY AUTOMATIC PREDICTION OF DIFFICULT ENDOTRACHEAL INTUBATION

[46] C. W. Connor and S. Segal, “Accurate classification of difficult intubation
by computerized facial analysis,” Anesthesia Analgesia, vol. 112, no. 1,
pp. 84–93, 2011.
[47] D. Cattano et al., “Anticipation of the difficult airway: Preoperative airway assessment, an educational and quality improvement tool,” Brit. J.
Anaesthesia, vol. 111, no. 2, pp. 276–285, 2013.
[48] Y. Yang and D. Ramanan, “Articulated pose estimation with flexible
mixtures-of-parts,” in Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recogn., 2011, pp. 1385–1392.
[49] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, 2004.
[50] G. L. Cuendet et al., “Automatic Mallampati classification using active
appearance models,” presented at the Int. Workshop Pattern Recognition
Healthcare Analytics, Tsukuba, Japan, 2012.
[51] V. López et al., “An insight into classification with imbalanced data:
Empirical results and current trends on using data intrinsic characteristics,”
Inf. Sci., vol. 250, pp. 113–141, 2013.
[52] H. He and E. A. Garcia, “Learning from imbalanced data,” IEEE Trans.
Knowl. Data Eng., vol. 21, no. 9, pp. 1263–1284, Sep. 2009.
[53] M. Galar et al., “A review on ensembles for the class imbalance problem:
Bagging-, boosting-, and hybrid-based approaches,” IEEE Trans. Syst.,
Man, Cybern. C, Appl. Rev., vol. 42, no. 4, pp. 463–484, Jul. 2012.
[54] C. Elkan, “The foundations of cost-sensitive learning,” in Proc. Int. Joint
Conf. Artif. Intell., 2001, pp. 973–978.
[55] L. Breiman, “Random forests,” Mach. Learn., vol. 42, no. 1, pp. 5–32,
2001.
[56] F. Pedregosa et al., “Scikit-learn: Machine learning in python,” J. Mach.
Learn. Res., vol. 12, pp. 2825–2830, 2011.

Gabriel Louis Cuendet (S’12) received the B.Sc.
and M.Sc. degrees in electrical engineering with
specialization in biomedical engineering from the
Ecole Polytechnique Fédérale de Lausanne, Lausanne, Switzerland, in 2012, where he is currently
working toward the Ph.D. degree in developing facial
image analysis for medical diagnosis applications.
His research interests include computer vision
methods for 2-D and 3-D facial landmarks detection
and tracking.

Patrick Schoettker studied medicine in University of Lausanne, Lausanne, Switzerland, specialized in anesthesia and emergency medicine in 2001,
while doing his clinical activity in Switzerland and
Australia.
He is responsible for the difficult airway curriculum at the University Hospital Lausanne (CHUV),
Lausanne, Switzerland, where he is the Head at the
Neuro, ENT, and Trauma Anesthesia. His research
interests includes airway subject.

Anıl Yüce (S’11) received the B.Sc. degree from
Middle East Technical University, Ankara, Turkey,
in 2008, and the M.Sc. degree in electrical engineering from Ecole Polytechnique Fédérale de Lausanne
(EPFL), Lausanne, Switzerland, in 2010, where he
has been working toward the Ph.D. degree at the Signal Processing Laboratory.
His main research interest includes facial image
analysis for various applications, particularly analysis of facial expressions and their dynamics.

339

Matteo Sorci received the B.Sc. and M.Sc. degrees
from the Faculty of Telecommunication Engineering, University of Siena, Siena, Italy, in 2001, and
the Ph.D. degree degree from the Signal Processing
Laboratory, Swiss Federal Institute of Technology,
Lausanne, Switzerland, under the supervision of Prof.
J.-P. Thiran, in 2009.
He is currently the CTO and the Cofounder at
nViso SA, Lausanne. His main research interests include behavioural modeling, dimensionality reduction, machine learning, and computer vision.

Hua Gao received the Dipl.Inf. and Ph.D. degrees
in computer science from the Karlsruhe Institute of
Technology, Karlsruhe, Germany, in 2008 and 2013,
respectively.
He is currently a Postdoctoral Researcher at the
Swiss Federal Institute of Technology, Lausanne,
Switzerland. His research interests include facial
image processing, e.g., face tracking, 3-D face reconstruction, facial expression recognition, and face
recognition.

Christophe Perruchoud received the medical degree from the University of Lausanne, Lausanne,
Switzerland, in 1998.
He then trained in anesthesiology and pain management and is currently the Medical Chief Officer
at the Hospital of Morges, Morges, Switzerland, and
the Consultant at the University Hospital of Lausanne
(CHUV), Lausanne. His main research interests include evaluation and management of difficult airways
in the perioperative period.

Jean-Philippe Thiran ((S’91–M’98–SM’03) is currently an Associate Professor of image processing
and the Director at the Signal Processing Laboratory,
Swiss Federal Institute of Technology, Lausanne,
Switzerland. He is also an Associate Professor at the
Department of Radiology, University Hospital Center
(CHUV), Lausanne, and the University of Lausanne,
Lausanne. His research interests include image analysis and multimodal signal/image processing, with applications in many domains including medical image
analysis, human–computer interaction, remote sensing of the earth, and surveillance. He is the Author or Coauthor of more than
130 journal papers, nine book chapters, more than 185 papers in peer-reviewed
proceedings of international conferences, and holds four international patents.
He is currently an Associate Editor of the IEEE TRANSACTIONS ON IMAGE
PROCESSING and a Reviewer for many journals and conferences.

