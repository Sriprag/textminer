Journal of Biomedical Informatics 44 (2011) S63–S68

Contents lists available at SciVerse ScienceDirect

Journal of Biomedical Informatics
journal homepage: www.elsevier.com/locate/yjbin

A bootstrapping algorithm to improve cohort identiﬁcation using structured data
Sasikiran Kandula a, Qing Zeng-Treitler a,⇑, Lingji Chen b,1, William L. Salomon c, Bruce E. Bray a
a

Department of Biomedical Informatics, University of Utah, Salt Lake City, UT, United States
Scientiﬁc Systems Company Inc., Woburn, MA, United States
c
Clinical Metrics LLC, Poland, ME, United States
b

a r t i c l e

i n f o

Article history:
Available online 7 November 2011
Keywords:
Cohort analysis
International classiﬁcation of diseases
Machine learning

a b s t r a c t
Cohort identiﬁcation is an important step in conducting clinical research studies. Use of ICD-9 codes to
identify disease cohorts is a common approach that can yield satisfactory results in certain conditions;
however, for many use-cases more accurate methods are required. In this study, we propose a bootstrapping method that supplements ICD-9 codes with lab results, medications, etc. to build classiﬁcation models that can be used to identify cohorts more accurately. The proposed method does not require prior
information about the true class of the patients. We used the method to identify Diabetes Mellitus
(DM) and Hyperlipidemia (HL) patient cohorts from a database of 800 thousand patients. Evaluation
results show that the method identiﬁed 11,000 patients who did not have DM related ICD-9 codes as
positive for DM and 52,000 patients without HL codes as positive for HL. A review of 400 patient charts
(200 patients for each condition) by two clinicians shows that in both the conditions studied, the labeling
assigned by the proposed approach is more consistent with that of the clinicians compared to labeling
through ICD-9 codes. The method is reasonably automated and, we believe, holds potential for inexpensive, more accurate cohort identiﬁcation.
Published by Elsevier Inc. Open access under CC BY-NC-ND license.

1. Introduction
Identiﬁcation of large cohorts of patients with a speciﬁc diagnosis or chosen characteristics is an essential step in many clinical research studies and biomedical applications. It is also an integral
part of pharmacovigilance [1], where the detection of adverse drug
reactions is often achieved through data mining methods. Cohort
identiﬁcation can be carried out manually by researchers with
the assistance of software tools [2], or automatically by algorithms
that, for example, analyze clinician’s notes [3].
A commonly adopted ‘baseline’ method for cohort identiﬁcation
is to use patients’ ICD-9 codes. This leads to levels of accuracy that
may be acceptable for certain diseases but unacceptable for others
[4,5]. For some diseases, researchers have established more sophisticated cohort identiﬁcation criteria, but to strictly follow them may
require manual patient chart reviews and may result in wrongful
exclusion of a large number of subjects due to missing or incomplete data. This could become an issue in detecting relatively rare
events in pharmacovigilance, where a large sample size is required.
More recently, researchers afﬁliated with the eMERGE network
⇑ Corresponding author. Address: Department of Biomedical Informatics,
University of Utah, 26 S 2000 E, HSEB 5700, Salt Lake City, UT 84112, United States.
E-mail address: qing.zeng@utah.edu (Q. Zeng-Treitler).
1
Present address: BAE Systems (Advanced Information Technologies), Burlington,
MA, United States.
1532-0464 Published by Elsevier Inc. Open access under CC BY-NC-ND license.
doi:10.1016/j.jbi.2011.10.013

have proposed rule-based methods using billing-, diagnoses- and
procedure-codes along with laboratory results, medications and
related ﬁndings to diagnose certain conditions to identify a range
of conditions, including peripheral arterial disease, diabetic retinopathy, hypothyroidism and dementia [6–9].
Structured data other than ICD-9 codes as well as narrative data
have been used to develop automated models to identify patients
with cancer [10], rheumatoid arthritis [11], pneumonia [12] and
asthma [13]. The development usually employs supervised learning, which requires some gold standard in forming the needed
training set for machine learning. This could be prohibitively labor
intensive if the identiﬁcation system has to produce cohorts for
many researchers on a diverse range of diseases. The established
models from one data source may also have to be re-trained before
being applied to data from different sources.
In this paper, we describe a bootstrapping learning method that,
starting with an initial classiﬁcation based on ICD-9 codes, iteratively improves cohort accuracy through training on relevant
structured data. The method does not require an expert-created
gold standard.
The research reported in this paper is part of a larger effort to
enhance signal-to-noise ratio in order to achieve better accuracy
of detection in pharmacovigilance applications. There are many
challenges in pharmacovigilance [14–16], and broadly speaking
the rareness of an adverse drug reaction (ADR) leads to weak causal relations between the reaction and any measured signal. We

S64

S. Kandula et al. / Journal of Biomedical Informatics 44 (2011) S63–S68

believe that by grouping related signals together, to ’connect the
dots’ so to speak, we can enhance detection rate and suppress false
alarm rate. Extending the research results from our previous studies [17,18], we developed a method to identify and aggregate related medical data.
In this study, we attempt to demonstrate how related medical
data items can be used to improve classiﬁcation accuracy in disease cohort identiﬁcation tasks. We chose two common diseases
Diabetes Mellitus (DM) and Hyperlipidemia (HL) to test the proposed method. DM and HL are appropriate disorders to use in this
evaluation since they are common disorders which are generally
treated with medications, and frequently monitored by routine
laboratory testing, thus providing structured electronic health record (EHR) data for cohort identiﬁcation. These chronic disorders
are also associated with adverse drug events which are candidates
for pharmacovigilance applications.

2. Materials and methods
2.1. Data set
A de-identiﬁed Logician (now GE Centricity) outpatient EHR
database from the MeritCare Health System (now Sanford Health)
that included patient encounters with related chief complaints,
diagnosis codes, medications, etc. was used as the source of structured data. The database was constructed from transactional EHR
data with an independent laboratory result data feed containing
inpatient and outpatient laboratory values. The transactional data
was uploaded periodically to the GE Medical Quality Improvement
Consortium (MQIC), where the data was de-identiﬁed, transformed, and loaded into the MQIC system.
Transformation of this data included: scrubbing of personal
identiﬁers, deletion of all free-text information in clinical notes,
chief complaints, diagnoses and annotations, medication instructions, and reduction of concepts such as diagnoses, chief complaints
and medications in a form analogous to brief ICD-9, SNOMED-CT,
LOINC and RxNorm descriptions.
The database contains inpatient and outpatient visit data on
833 thousand patients between the years 1998 and 2006, containing 665 thousand chief complaint rows, 830 thousand diagnosis
rows, 4 million medication rows and 45 million observation/lab
rows.
2.2. Bootstrapping algorithm
ICD-9 codes, which are generally used for cohort identiﬁcation,
have been reported to result in a signiﬁcant number of false negatives and a few false positives. In this study, we supplement the
ICD-9 codes with ﬁndings, such as lab results, that are used by clinicians to make diagnoses and other relevant information, such as
medication lists, that are available in the structured database and
can indicate the presence of a condition. The features thus selected
are used to build classiﬁcation models that output an estimate for a
patient to be positive for a given disease.
However, most classiﬁcation algorithms require that we have a
labeled set of instances, i.e. instances whose true-class/label (outcome) is known. This information is not available through the
structured database and hence we propose the use of an iterative
process that is initialized with an approximation of the true class,
i.e. the number of encounters with condition related ICD-9 codes.
Using this approximately labeled training set, we build support
vector machine based classiﬁcation models. The estimate obtained
from the model is used to label the patient. Since, our initial trueclass is not very reliable, this estimate can lead to signiﬁcant num-

ber of misclassiﬁcations. To improve the accuracy of the estimate,
we try to improve the quality of the training set by reﬁning the
true-class. To achieve this, we iteratively update the true-class
with the estimates of the classiﬁcation models built. The iterative
process is terminated when the number of changes in the labels
of patients between two subsequent iterations, satisﬁes a user-deﬁned threshold.
Speciﬁcally, the proposed method has the following steps:
1. For the condition of interest, identify related ﬁndings and calculate the corresponding values for each instance in the patient
sample, W. Let Xj be the vector of values for the related ﬁndings
of patient j.
2. Initialize Dj to the number of encounters with related ICD-9
codes for patient j
3. Identify a training set T from W, containing positive and negative instances.
4. Set Cj (the class of patient j) to Dj
5. Begin iterative process (superscript denotes iteration number):
(a) Using T, build support vector machine based classiﬁcation
b i ¼ f i ðX j Þ, where
models to obtain non-linear relationship C
j
b i is the model’s estimate for Cj
C
j
b i for all
(b) Apply f i ðX j Þ to W and generate histogram over C
j
patients in W;
(c) Select a cut-off threshold gi to separate the positive and
negative populations.
b i > gi and Li ¼ 0 otherwise; here, Li is our esti(d) Set Lij ¼ 1 if C
j
j
j
mate of the patient’s label and a positive label indicates the
patient is positive for the condition. Note that at initialization, Lij ¼ 1 if Cj > 0; else L0j ¼ 0.
(e) Compare Lij to Lji1 and compute Fi, the percentage of
patients for whom the label has changed in the current
iteration.
(f) If Fi < d (where d is an acceptable change threshold), return
b i þ D=ði þ 1Þ for
Lij and terminate; else, update C iþ1
¼ ½i  C
j
j
all j in T
It can be seen from the above (step 5.f) that the inﬂuence of Dj,
tapers down as training progresses, while that of the learned relationship increases. We found a d of 0.5% to give reasonable tradeoff between the accuracy of the method and the computation time.
To identify a good cut-off threshold (step 5.c) the method calculates the global minima of the ﬁrst derivative of a cubic spline funcb vs. frequency; see Fig. 1). The
tion that describes the histogram ( C
histogram of a good estimation function can be expected to have
two peaks representing the two classes: a high peak at a small
b representing the negative patient population and a
value of C
relatively shorter peak(s) representing the positive population separated by a region of relatively low frequency counts. The value of
b that minimizes the ﬁrst derivative of the histogram function
C
identiﬁes this valley between the two patient populations and
can be considered a reasonable cut-off threshold.
2.3. Experiment
We tested the bootstrapping algorithm by identifying DM and
HL cohorts in the MeritCare dataset. Relevant laboratory test results and medications were used as features to build the machine
learning classiﬁers. For DM, the following features were used:
 The number of unique anti-diabetic medications (insulin, insulin supplements, biguanides, sulphonylureas, alpha-glucosidase
inhibitors, etc.) that have been prescribed to the patient.
 The average amount by which the observed hemoglobin A1C
(HbA1C) values were above the normal range (>6%), i.e. the

S. Kandula et al. / Journal of Biomedical Informatics 44 (2011) S63–S68

S65

b is initialized to the number of encounters with DM ICD-9 codes and is updated in each
Fig. 1. Histogram observed for DM patient population. The continuous variable C
iteration.

average amount by which the patient’s test result, if abnormal,
was above the normal range.
 The average amount by which the observed blood glucose values (fasting, random or ﬁnger stick) were above the normal
range (>110 mg/dL).
Similarly, for HL the features identiﬁed were:
 The number of unique anti-hyperlipidemics (HMG-CoA reductase inhibitors, intestinal cholesterol absorption inhibitors,
fenoﬁbrate, gemﬁbrozil, etc.) that have been prescribed to the
patient.
 The average amount by which the observed concentration of
HDL was below 40 mg/dL.
 The average amount by which the observed concentration of
LDL was above 160 mg/dL.
 The average amount by which the observed concentration of
triglycerides was above 200 mg/dL
 The average amount by which the observed concentration of
total cholesterol was above 240 mg/dL
 The number of dates on which at least one abnormal lipid panel
tests were observed.
 The number of normal lipid panel tests.
To prepare the training data for the classiﬁers, we conducted a
controlled sampling of the patient sample. For each disease, we
randomly selected patients who have one or more ICD-9 codes
(i.e. 250.* for DM and 272.* for HL). We also randomly selected
an equal number of patients with no related diagnosis codes
resulting in a total of about 30,000 patients in the training set. Note
that due to the random sampling from the entire negative population, the training set contained instances that had a zero value for
all markers/class.
Using these training sets, we built support vector machine
(SVM) based classiﬁcation models. SVMs are known to exhibit
good performance in classiﬁcation tasks of this nature and we used
a Weka [19] implementation of a polynomial kernel SVM [20] with
normalized attributes for building the classiﬁcation models.

2.4. Evaluation
For each condition, a small sample of 200 patients was selected
from the MeritCare database for manual review. Since in the overall
data set, the prevalence of DM and HL is fairly low, we controlled
the sampling process. Speciﬁcally, for each of the following criteria,
50 patients who satisﬁed the criterion were included:





Labeled positive by the algorithm.
Labeled negative by the algorithm.
Had at least one related ICD-9 code.
Had no related ICD-9 codes.

This ensured that at least half of the 200 patients had a related
ICD-9 code and/or classiﬁed as positive by the algorithm. 47% of
the patients selected for DM and 34.5% of the patients selected
for HL were found to have been labeled negative by the algorithm
and had no related ICD-9 code.
All information (i.e. detailed diagnoses, test results, procedures,
and medications) related to the selected patients was manually reviewed by two clinicians. (There were no free-text reports in the
MeritCare database used.) Each patient was labeled as positive, negative or possible for the condition and these labels were considered
to be the gold standard. The possible category had to be included to
allow the reviewers to indicate that while the patient is likely to
have the condition, additional information would be necessary to
label the patient as positive.
In labeling the patients the clinicians were asked to assess the
patient information as in a clinic visit. To label the patient’s HL status, which is less well-deﬁned than DM, the following criteria were
used: (1) presence of at least one ICD-9 diagnosis of hyperlipidemia; (2) an abnormal laboratory test based on the National Cholesterol Education Program (NCEP) guidelines (total cholesterol
>200, or LDL cholesterol >130[21]); and/or (3) treatment with at
least one antihyperlipidemia medication. Those satisfying only a
single medication criterion were labeled as possible.
Sensitivity, Speciﬁcity and F-measure values for the diagnosis
code and the classiﬁcation models were calculated and compared.

S66

S. Kandula et al. / Journal of Biomedical Informatics 44 (2011) S63–S68

Additionally, results of McNemar’s test for marginal homogeneity
for classiﬁcations of ICD-9 and model are reported.
3. Results

Table 1
Precision, recall and F-measure for DM using ICD-9 and model. Expert assigned label
was considered to be the gold standard. Note recall = sensitivity and precision = positive Predictive Value. The measures in columns with the header ‘possible = positive’
were calculated by treating patients assigned a possible label as positive, and the
measures listed in columns with header ‘possible = negative’ were calculated by
treating possible as negative.

3.1. Diabetes Mellitus
In the DM patient population identiﬁed (at least one non-zero
feature), 15.95% of the patients had at least one encounter with
DM related ICD-9 code, 20.24% were on at least one anti-diabetic
medication and 20.21% had at least one abnormal HbA1C test. Most
(95%) of the patients had one or more abnormal blood glucose
tests.
6.55% of the patients with a DM diagnosis code had neither an
abnormal HbA1C test nor were on any anti-diabetic medication.
17.38% of the patients who had one or more abnormal HbA1C test
and were on anti-diabetic medications had no DM diagnosis code.
At the termination of the iterative process, 26.7% of the patients
were labeled as positive, i.e. predicted to be diabetic. Of these patients, 44% had no encounters with DM related ICD code (Fig. 2).
Most of the patients with one or more encounters with DM ICDcodes were labeled as positive, i.e. 93% of the patients with one
DM coded encounter and 96% of the patients with more than one
DM coded encounters were labeled positive. None of the patients
whose only non-zero marker was an abnormal blood glucose test
were labeled positive. On the whole, the cohort identiﬁed by the
algorithm is 70% larger than what would have resulted from the
use of ICD-9 codes.
The human review of the DM evaluation sample labeled 94 patients as positive, 89 patients as negative and 17 patients as possible.
An analysis of the confusion matrices for ICD-9 codes and model’s
estimates against the reviewer’s labels showed that the labels assigned by the model result in fewer false negatives and slightly
higher false positives, irrespective of whether possible is treated
as positive or negative. The precision of ICD-9 code is slightly better
than that of the model, whereas the model’s recall and F-measure
are better than that of ICD-9 (Table 1).
3.2. Hyperlipidemia
In the HL patient population, 24.33% patients had one or more
encounters with 272.* code, 31.03% patients were on one or more
hyperlipidemic medication and 42.35% had at least one abnormal
lipid panel test. 93.53% of the patients had undergone one or more

Fig. 2. Classiﬁcation of DM patient population. D = X indicates that X number of the
patient’s encounters had a DM ICD-9 code. The percentages in parentheses indicate
the percentage of patients with the X number of encounters that were labeled
positive by the algorithm. For example, the segment labeled D = 1 represents
patients who had one of their encounters coded with a DM code and were labeled
positive by the model. 43% of the patients labeled positive by the model had D = 1
and 93% of patients with D = 1 were labeled positive by the model.

Sensitivity
Speciﬁcity
Positive predictive value
Negative predictive value
F-measure

Possible = positive

Possible = negative

ICD-9

Model

ICD-9

Model

0.71
1.00
1.00
0.74
0.83

0.87
0.93
0.94
0.86
0.91

0.81
0.97
0.96
0.85
0.88

0.97
0.89
0.88
0.97
0.92

lipid panel tests. A third of the patients who had both abnormal lipid panel tests and antihyperlipidemic medications did not have a
diagnosis code. This indicates that in HL, compared to DM, use of
ICD-9 codes alone to identify patient cohorts would result in a
greater number of false negatives.
The bootstrapping process labeled 58.4% of the patients as positive, 62.3% of who had no hyperlipidemia diagnosis code (Fig. 3). As
in DM, most of the patients with one or more HL coded encounters
were labeled positive. i.e. only 9.5% of the patients with a diagnosis
code were labeled as negative and were generally patients who
were not on any related medications and had almost no abnormal
test results.
The number of patients labeled to be positive for hyperlipidemia
by the model (84,000) is more than twice the number of patients
who had one or more ICD-9 codes (35,000). This is also much higher than the cohort size identiﬁed in DM. This probably indicates
that compared to DM, HL is less likely to be explicitly coded.
Of the 200 HL patients selected for review, the expert labeled
109 as positive, 62 as negative and 29 as possible. An analysis of
the confusion matrices showed that when possible is treated as
positive, ICD-9 codes have missed a signiﬁcant number of patients
(64 false negatives) most of who were captured by the models,
thereby considerably improving recall and F-measure (Table 2).
Treating possible as negative, reduced the false negatives for
ICD-9 from 64 to 36 which was still signiﬁcantly higher than those
in models (13). Correspondingly, this improves the performance of
ICD-9 codes slightly, although the recall and F-measure of the
model continues to be better than that of ICD-9 codes.
McNemar’s chi-square statistic for homogeneity was found to
be 17.63 (p < 0.0001) in DM and 32.55 (p < 0.0001) for HL. This
shows that in both disease conditions, the null hypothesis that
there was no difference in the classiﬁcation of the two approaches
can be rejected.

Fig. 3. Classiﬁcation of HL patient population. Label notation is similar to that used
in Fig. 2.

S. Kandula et al. / Journal of Biomedical Informatics 44 (2011) S63–S68
Table 2
Precision, recall and F-measure for HL using ICD-9 and model.

Sensitivity
Speciﬁcity
Positive predictive value
Negative predictive value
F-measure

Possible = positive

Possible = negative

ICD-9

Model

ICD-9

Model

0.54
0.98
0.99
0.49
0.69

0.83
0.89
0.94
0.71
0.88

0.67
0.98
0.97
0.71
0.79

0.88
0.71
0.79
0.83
0.83

4. Discussion
We developed a bootstrapping algorithm that uses structured
data to improve cohort identiﬁcation over ICD-based cohort identiﬁcation. This algorithm conducts iterative learning from the
imperfect ICD-9 codes and does not require expert-annotated data
for training. The method, when applied to two disease conditions,
was able to identify a larger number of patients as positive for the
conditions compared to ICD-9 codes. An initial evaluation indicated that the labeling achieved through the classiﬁcation models
is more consistent with clinical expert assessment – and hence
presumably more accurate – than classiﬁcation based solely on
ICD-9 codes.
Our motivation for this study has been the development of a
method that helps identify patient cohorts in structured data. Natural language processing based methods that extract diagnoses, medications, etc. [22,23], are useful in complementing structured data
and ICD codes. But the accuracy of most of the reported NLP methods
suggests that there is room for supplementary methods, like the
bootstrapping method described here, in cohort identiﬁcation. Our
method does not require a human created gold standard for training
purposes. Its results, however, may need to undergo further human
review for certain types of applications. In the context of automated
pharmaco-surveillance which motivated this work, further human
review may not be possible. In the context of improving the completeness of a problem list, for example, human review would be
appropriate and necessary.
We note that our learning problem is different from that of
learning from positive-only data [24]. The positive-only learning
creates classiﬁers using a gold standard positive sample and an
unlabeled sample that contains negative and positive samples.
Although our algorithm discovers many cases that are positive
but did not have the ICD-9 code, these cases were mislabeled rather
than ‘unlabeled’. This is in contrast to applications, where web documents or protein types have not been processed and therefore are
unlabeled.
We believe that the method discussed here is generalizable to
other conditions and can potentially be completely automated.
The only step in the experiment that relied on human input is the
feature selection process. We believe that this step could be automated either by mining concepts most-frequently co-occurring
with the condition name/synonyms in biomedical literature or
through the integration of a module that can retrieve concepts
semantically related to the condition [17,18]. The integration and
mining of related-concepts were not discussed here in part to evaluate the effectiveness of the bootstrapping algorithm independent
of the quality of feature selection. Though we manually selected the
features in the DM and HL experiments, the algorithm does not require perfect features. To test for this, we intentionally included
features – blood glucose test results in DM and the normal lab
counts in HL – we expected to have low distinguishing power. An
examination of the models’ output estimates showed that the models can learn the relative ‘goodness’ of the features used and blood
glucose in DM and normal lab counts in HL were rightly assigned

S67

very low weight coefﬁcients in the models. This suggests that the
training/bootstrapping is robust enough to ignore bad features
and if the identiﬁcation of the feature set were automated and
the resulting feature set were to include a few irrelevant features,
the training/bootstrapping can still be expected to have relatively
good performance.
We also believe that the assignment of real-valued class estimates to the patients gives the researcher more ﬂexibility in experimenting with different thresholds for cohort identiﬁcation than
ICD-9 codes.
In the initial stages of the current study, we also experimented
with clustering algorithms to identify positive and negative patients. We observed that, either with the ICD-9 codes as a feature
or without, clustering results in a much higher number of wrong
labels than SVM based classiﬁcation models. We believe this is because the clustering algorithms which determine cluster membership using a distance function tend to give equal weight to all
features.
It is possible that the improvement in accuracy achieved through
the use of algorithm could be achieved by having an expert draft a set
of rules that use the same features (labs, medications, etc.) as used
here. A comparison of the performance of our algorithm to such a
set of rules, in these two conditions and in conditions, where the
diagnosis criteria are not as well-deﬁned or agreed upon would be
interesting to investigate. Manually drafting rules, however, is a
method that is difﬁcult to scale up. In addition, human experts do
not typically assign quantitative weights to the different ﬁndings
that contribute to their decisions.
Limitations: The evaluation method used controlled sampling to
ensure that the evaluation set has a good number of true positive
cases. Evaluation with an alternate set, with strict random sampling,
may show a higher number of false positives. The difference in Fmeasure between ICD-9 codes and our bootstrapping method observed in such a case will probably be less than what is reported here.
Moreover, although two clinicians were used for manual evaluation,
each patient’s data was reviewed by only one clinician. Sensitivity/
Speciﬁcity measures computed under an alternate study design that
allowed for overlap and the establishment of inter-reviewer agreement would have been more reliable. The conditions used for the
evaluation here – diabetes and hyperlipidemia – have relatively speciﬁc medications and speciﬁc laboratory ﬁndings associated with
them. This is unusual and most other conditions have less speciﬁc
medications and ﬁndings. For most conditions, the presence of
free-text reports is important to determine a reliable gold standard.
However, the database we used here does not contain free-text reports. We would like to test the algorithm on other conditions using
a different database in which free-text notes are available for chart
review.
We have described one way of identifying the cut-off threshold
to separate positive and negative populations. More optimal selection strategies may be available and will need to be explored
further.
In the data set used here, ICD-9 coding of DM and HL appears to
be highly speciﬁc while low in sensitivity. We know from literature
that depending on the data set, ICD-9 codes could be low in speciﬁcity too [25].
The ICD-9 codes used to identify diagnosis codes for DM and HL
in this study are not comprehensive. For instance, ICD-9 codes
362.0* for retinopathy and 253.5 for polynephritis may implicitly
indicate that the patient is diabetic and using these codes in addition to 250.* could have given a more complete list of patients with
diabetes diagnosis codes. However, we have found only a few patients who had retinopathy and polynephritis ICD-9 codes that
did not have DM ICD-9 codes and including these additional patients may only marginally alter the method’s performance.

S68

S. Kandula et al. / Journal of Biomedical Informatics 44 (2011) S63–S68

Acknowledgments
This work is supported in part by the United States Army Small
Business Innovation Research contract # DAMD17-03-C-0054. We
would like to thank Dr. Trinka Coster for her guidance of the project.
We would also like to thank Mr. Andrew Bigelow for constructing a data
presentation interface that facilitated the clinician review process.
References
[1] World Health Organization. The importance of pharmacovigilance. safety
monitoring of medicinal products. Geneva: WHO; 2002.
[2] Lowe H, Weber S, Ferris T, Hernandez P. Self-service support for research
patient cohort identiﬁcation and review of clinical data in the STRIDE clinical
data warehouse. AMIA CRI Summit; 2010.
[3] Turchin A, Pendergrass ML, Kohane IS. DITTO – a tool for identiﬁcation of patient
cohorts from the text of physician notes in the electronic medical record. In:
AMIA annual symposium proceedings/AMIA symposium; 2005. p. 744–8.
[4] Shea AM, Curtis LH, Szczech LA, Schulman KA. Sensitivity of international
classiﬁcation of diseases codes for hyponatremia among commercially insured
outpatients in the United States. BMC Nephrol 2008;9:5.
[5] Boyd M, Specks U, Finkielman JD. Accuracy of the ICD-9 code for identiﬁcation
of patients with Wegener’s granulomatosis. J Rheumatol 2010;37(2):474.
[6] Kullo IJ, Fan J, Pathak J, Savova GK, Ali Z, Chute CG. Leveraging informatics for
genetic studies: use of the electronic medical record to enable a genome-wide
association study of peripheral arterial disease. J Am Med Inform Assoc
2010;17(5):568–74.
[7] Schildcrout JS, Basford MA, Pulley JM, Masys DR, Roden DM, Wang D, et al. An
analytical approach to characterize morbidity proﬁle dissimilarity between
distinct cohorts using electronic medical records. J Biomed Inform
2010;43(6):914–23.
[8] McCarty CA, Chisholm RL, Chute CG, Kullo IJ, Jarvik GP, Larson EB, et al. The
eMERGE Network: a consortium of biorepositories linked to electronic medical
records data for conducting genomic studies. BMC Med Genom 2011;4:13.
[9] The eMERGE Network. <https://www.mc.vanderbilt.edu/victr/dcc/projects/
acc/index.php/Library_of_Phenotype_Algorithms>.
[10] Friedlin J, Overhage M, Al-Haddad MA, Waters JA, Aguilar-Saavedra JJ,
Kesterson J, et al. Comparing methods for identifying pancreatic cancer
patients using electronic data sources. In: AMIA annual symposium
proceedings/AMIA symposium 2010, 2010. p. 237–41.

[11] Liao KP, Cai T, Gainer V, Goryachev S, Zeng-treitler Q, Raychaudhuri S, et al.
Electronic medical records for discovery research in rheumatoid arthritis.
Arthrit Care Res (Hoboken) 2010;62(8):1120–7.
[12] Lagor C, Aronsky D, Fiszman M, Haug PJ. Automatic identiﬁcation of patients
eligible for a pneumonia guideline: comparing the diagnostic accuracy of two
decision support models. Stud Health Technol Inform 2001;84(Pt 1):
493–7.
[13] Meystre SM, Deshmukh VG, Mitchell J. A clinical use case to evaluate the i2b2
Hive: predicting asthma exacerbations. In: AMIA annual symposium
proceedings/AMIA symposium 2009, 2009. p. 442–6.
[14] Aronson JK, Ferner RE. Joining the DoTS: new approach to classifying adverse
drug reactions. BMJ Clin Res 2003;327(7425):1222–5.
[15] Noren GN, Sundberg R, Bate A, Edwards IR. A statistical methodology for drugdrug interaction surveillance. Stat Med 2008;27(16):3057–70.
[16] Noren GN, Edwards IR:. Modern methods of pharmacovigilance: detecting
adverse effects of drugs. Clin Med (Lond, Engl) 2009;9(5):486–9.
[17] Zeng Q, Cimino JJ. A knowledge-based, concept-oriented view generation
system for clinical data. J Biomed Inform 2001;34(2):112–28.
[18] Zeng Q, Cimino JJ, Zou KH. Providing concept-oriented views for clinical data
using a knowledge-based system: an evaluation. J Am Med Inform Assoc
2002;9(3):294–305.
[19] Witten IH, Frank E. Data mining: Practical machine learning tools and
techniques. Morgan Kaufmann Publications; 2005.
[20] Keerthi SS, Shevade SK, Bhattacharyya C, Murthy KRK. Improvements to Platt’s
SMO algorithm for SVM classiﬁer design, vol. 13. MIT Press; 2001. p. 637–49.
[21] Third Report of the National Cholesterol Education Program (NCEP) Expert
Panel on Detection, Evaluation, and Treatment of High Blood Cholesterol in
Adults (Adult Treatment Panel III) ﬁnal report. Circulation 2002;106(25):
3143–421.
[22] Zeng QT, Goryachev S, Weiss S, Sordo M, Murphy SN, Lazarus R. Extracting
principal diagnosis, co-morbidity and smoking status for asthma research:
evaluation of a natural language processing system. BMC Med Inform Decis
Making 2006;6:30.
[23] Meystre S, Haug PJ. Automation of a problem list using natural language
processing. BMC Med Inform Decis Making 2005;5:30.
[24] El-kan C, Noto K. Learning classiﬁers from only positive and unlabeled data. In:
Proceedings of the 14th international conference on Knowledge discovery and
data mining: 2008. AMC; 2008. p. 213–20.
[25] Benesch C, Witter Jr DM, Wilder AL, Duncan PW, Samsa GP, Matchar DB.
Inaccuracy of the international classiﬁcation of diseases (ICD-9-CM) in
identifying the diagnosis of ischemic cerebrovascular disease. Neurology
1997;49(3):660–4.

