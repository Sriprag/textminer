Journal of Informetrics 1 (2007) 257–268

The estimation of the number of lost multi-copy documents:
A new type of informetrics theory
L. Egghe a,b,∗ , G. Proot c
a
b

Universiteit Hasselt (UHasselt), Campus Diepenbeek, Agoralaan, B-3590 Diepenbeek, Belgium
Universiteit Antwerpen (UA), Campus Drie Eiken, Universiteitsplein 1, B-2610 Wilrijk, Belgium
c Universiteit Antwerpen (UA), Stadscampus, Prinsstraat 13, B-2000 Antwerpen, Belgium

Received 22 December 2006; received in revised form 22 February 2007; accepted 22 February 2007

Abstract
A probabilistic model is presented to estimate the number of lost multi-copy documents, based on retrieved ones. For this, we
only need the number of retrieved documents of which we have one copy and the number of retrieved documents of which we have
two copies. If we also have the number of retrieved documents of which we have three copies then we are also able to estimate the
number of copies of the documents that ever existed (assumed that this number is fixed over all documents). Simulations prove the
stability of the model. The model is applied to the estimation of the number of lost printed programmes of Jesuit theatre plays in
the Provincia Flandro-Belgica before 1773. This Jesuit province was an administrative entity of the order, which was territorially
slightly larger in extent than present day Flanders, the northern, Dutch-speaking part of Belgium. It is noted that the functional
model Pj for the fraction of retrieved documents with j copies is a size-frequency function satisfying (Pj+1 /Pj )/(Pj /Pj−1 )<1 for all j.
It is further noted that the “classical” size-frequency functions are different: Lotka’s function satisfies the opposite inequality and
the decreasing exponential one gives always 1 for the above ratio, hence showing that we are in a new type of informetrics theory.
We also provide a mathematical rationale for the “book historical law” stating that the probability to lose a copy of a multi-copy
document (i.e. an edition) is an increasing function of the size of the edition. The paper closes with some open problems and a
description of other potential applications of this probabilistic model.
© 2007 Published by Elsevier Ltd.
Keywords: Multi-copy document; Book historical law

1. Introduction
Most printed documents appear in several copies. The number of these copies is usually high. Indeed: books usually
appear in hundreds or thousands of copies and the same is true for printed journals. They are spread out over the
world (for international literature) or over one country or region (for literature with local interest). Other examples of
multi-copy documents are (non-exhaustively!): newspapers, music scores, “In Memoriam”-cards, theatre plays, . . ..
Even non-documentary objects fall in the category of having multiple copies: engravings, etches, pieces of art produced
by a factory, furniture, tools, cars, stamps and many other collector’s items. To fix the idea we will, however, continue
to use the terminology “multi-copy documents”.
Typical for these multi-copy documents is that, at the time of production, we have a “complete” set of copies
(whatever their amount is) and that from that time on copies can be lost: the further we are away from the production
∗

Corresponding author.
E-mail addresses: leo.egghe@uhasselt.be (L. Egghe), goran.proot@ua.ac.be (G. Proot).

1751-1577/$ – see front matter © 2007 Published by Elsevier Ltd.
doi:10.1016/j.joi.2007.02.003

258

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

time (i.e., t = 0) the higher the probability that a copy of a document is lost. Here, we look at a cumulative time-period
[0, t] and we do not claim anything about the probability, in a time-period [t, t + t], that a copy will be lost: this can
increase in t, say in the case of old material which has not much value (e.g. newspapers, commercial printings, . . .),
but this probability can also decrease in t at a certain moment as is, e.g. the case for precious materials as old printings
kept in a museum or a library.
Lost copies do not always imply that the document as such is lost: the latter means that all copies of the document are
lost. Dependent on the application we can have that the majority of documents are lost or that lost documents are very
rare. Of very precious old printings (e.g. of books) it will occur only very rarely that all copies are lost (or destroyed,
e.g. by fire). The topic that led us to this article is an example where the majority of multi-copy documents are lost,
namely in the case of printed programmes of Jesuit theatre plays edited before 1773 in the Provincia Flandro-Belgica.
This Jesuit province encompassed secondary schools in 18 cities, which nowadays belong to the Nord de France
(Dunkerque, Cassel, Bergues and Bailleul), the southern part of the Netherlands (Breda, Maastricht, ’s-Hertogenbosch
and Roermond) and present-day Flanders (Aalst, Antwerp, Bruges, Brussels, Ghent, Halle, Ieper, Kortrijk, Mechelen
and Oudenaarde).
These printed programmes of theatre plays were not considered as precious (certainly at the time of their performance) and many copies are lost. For a certain number of theatre programmes, all copies are lost or destroyed, which
in most cases means that all information about the play itself is lost too. One reason for this can be that, in those times,
paper was very scarce and one re-used the paper of many of these printed copies of theatre plays. For more on this
historical problem, we refer the reader to Proot and Egghe (in press).
This intriguing historical case was the origin for this paper which will treat this problem in a general way: based on
“what we have”, i.e. some found copies of multi-copy documents, is it possible to predict the number of lost documents,
i.e. of which we do not have a single printed copy anymore? It will turn out that only the knowledge of the number of
documents of which we have one copy and the knowledge of the number of documents of which we have two copies,
is already enough to estimate the number of lost documents. This will be done in the next section where we will also
show that the method is very stable: this will be done by performing simulations of lost copies on a corpus of which
we know the size. In the same section, the model will be applied to the data that we have on found Jesuit theatre plays.
Since these plays originally were printed in at least 150 copies (going up to 850 copies—see further) this application
shows that in this case the results are almost independent of this (unknown) number a of copies.
Section 3 is then devoted to establishing a model to estimate this unknown number a of copies (especially for low
values of a this is needed in order to apply the model in Section 2). It turns out that we now also need the number of
documents of which we found three copies: this number is of course known but needs to be large enough in order to
yield reliable estimates for the number a.
Section 4 considers the formulae for the fraction Pj of documents of which we have j copies (j = 1, 2, 3, . . .) which
were proved in Section 2. Here we show that this size-frequency function satisfies
Qj =

Pj+1 /Pj
<1
Pj /Pj−1

for all j ≥ 2. It is noted that for Lotkaian functions j → Pj (which is not the case here) we always have Qj > 1. The
decreasing exponential function is between these two types of informetrics theories since here we have Qj = 1 for all
j. This section concludes that we encountered a new type of informetrics theory.
Section 5 gives a (partial) explanation of the so-called book historical law (see Proot and Egghe, in press for some
historical references) stating that the probability to lose a copy of a document is an increasing function of the size of
the edition.
Section 6 formulates some open problems concerning this model and discusses some possible applications to
examples of multi-copy documents (or even not-printed objects), which were briefly mentioned in the beginning of
Section 1.
2. The model
The model is not time-dependent. We suppose that we have a situation of documents of which a copies were
produced (printed) some time ago (the precise timing of this is not needed in the model). We do not need to know the
exact value of the variable a: we will treat a as a parameter and we will evaluate the results (and the possible need to

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

259

know the value of a) later. Now, we look at the present time and count the number of found documents of which we
have i copies (i = 1, 2, 3, . . .). Can (some of) these numbers predict the number of lost documents, i.e. documents of
which all a copies are lost?
The used probabilistic methods are elementary and can, e.g. be found in Canavos (1984) or Grimmett and Stirzaker
(1985).
The basic (unknown) number is p which we define as the probability for a copy to be lost (0 < p < 1). It is the unknown
number being the division of the number of lost copies (in total) by the total number of copies that ever existed.
Since p is the probability for a copy to be lost we can, using this unknown number, determine the fraction of the
documents of which we still have j ∈ {0, 1, 2, . . ., a} copies left. This is denoted by Pj and equals
 
a
Pj =
pa−j (1 − p)j
(1)
j
Note that this formula also comprises the cases where no copies are left, i.e. the fraction P0 of lost documents (i.e.
the proportion of the original documents that have been lost)
P0 = pa

(2)

and also comprises the case where no copies are lost, i.e. the fraction Pa of documents of which we have all copies:
Pa = (1 − p)a

(3)

Note that Eq. (1) treats all document probabilities since
a


Pj = 1

(4)

j=0

as is readily seen.
As both referees pointed out, this simple binomial model presupposes that the probability p for a copy to be lost is
a constant, hence that each copy has an equal chance to be lost (in other words that copies are lost independently). It
hence presupposes that enough copies are sufficiently disseminated over a wide enough area such that natural disasters
or other causes do not claim large parts of the original documents.
Note that P0 is the proportion of the original documents that have been lost (unknown but this is the fraction we
are looking for) and that P1 and P2 are also unknown but that P2 /P1 is known: indeed, denote by N the unknown total
number of documents that ever existed, then
P2
NP2
=
P1
NP1

(5)

which is the division of two known numbers: the number of documents of which we have two copies found and the
number of documents of which we have found one copy. Note that the unknown N cancels in Eq. (5). But, using Eq.
(1) for j = 1 and 2 we find
P1 = apa−1 (1 − p)
P2 =

a(a − 1) a−2
p (1 − p)2
2

hence Eq. (5) reduces to



P2
a−1
1−p
=
P1
2
p

(6)
(7)

(8)

Solving Eq. (8) for p gives
p=

1
1 + 2P2 /(a − 1)P1

(9)

260

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

Formula (9) in Formula (2) for P0 yields
a

1
P0 =
1 + 2P2 /(a − 1)P1

(10)

In this formula, as said above, P2 /P1 is known but the parameter a is unknown. How to determine the value of a
will be the topic of Section 3? Now we will apply this model to the case of Jesuit theatre programmes and it will turn
out (lucky as we are!) that, with these practical data (and probably in much more occasions), P0 is almost constant in
a. The data are as follows:
•
•
•
•
•
•

we have 714 documents (editons of theatre programmes) with 1 copy;
we have 82 documents with 2 copies;
we have 4 documents with 3 copies;
we have 3 documents with 4 copies;
we have 1 document with 5 copies;
we have no documents with 6 or more copies;

totalling to 804 found documents (theatre plays). Hence, based on Eq. (5) we have
P2
82
=
P1
714

(11)

It is historically known that small Jesuit colleges printed between 150 and 200 copies of each theatre programme while
large Jesuit colleges printed between 680 and 850 copies of the programmes for their theatrical performances. Though
these are large differences in the value of a it will turn out that, due to the fact that a, in any case, is large, it has almost
no influence on the value of P0 . Indeed, using Eq. (11) we find for P0 (Formula (10))—for a = 150:

150
1
= 0.7936955
P0 =
1 + (2/149) (82/714)
hence, 79.4% of all plays is lost. For a = 200 this gives
200

1
P0 =
= 0.7939673
1 + (2/199) (82/714)
hence still 79.4%. Even for a = 750 we have

750
1
P0 =
= 0.7945627
1 + (2/749) (82/714)
being 79.5% and the same for a = 850.
So P0 is very stable in a and we can conclude that we lost about 79.4% (or 79.5%) of all editions of theatre
programmes. Even for a → ∞ we can calculate P0 based on the general Formula (10).
Proposition.
lim P0 = e−2P2 /P1

(12)

a→∞

Proof. By Eq. (10):
lim P0

a→∞



a
1
= lim
a→∞ 1 + (2P2 /P1 ) (1/(a − 1))

b
1
= lim
,
b→∞ 1 + (2P2 /P)1 (1/b)

denoting b = a − 1 and remarking that
lim

1

a→∞ 1 + (2P2 /P1 ) (1/(a − 1))

=1

(13)

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

Denoting B = 2P2 /P1 it follows that
	
b 

ln(1/(1 + B/b))
1
= lim
lim ln
= −B
b→∞
b→∞
1 + (2P2 /P1 ) (1/b)
1/b

261

(14)

by the l’Hôspital’s rule. By Eqs. (13) and (14) we now have
lim P0 = e−B = e−2P2 /P1 .

a→∞



For the value (Formula (11)) of P2 /P1 this gives
lim P0 = e−2(82/714) = 0.7947785

a→∞

hence still under 79.5%, so very stable!
These calculations show that only low values of a have an influence on P0 , e.g. for a = 5 we have, with Eq. (11):
P0 = 0.7564083, hence about 75.6%. In this case, it might be necessary to calculate a from the data. This will be
executed in Section 3.
Note. The method established here works for all a ∈ N from a = 2 onwards, i.e. for real multi-copy documents. Indeed,
if a = 1 we only have P0 = p and P1 = 1 − p which is not enough to determine P0 . Already from a = 2 onwards is the
method working: for a = 2 we have P0 = p2 , P1 = 2p(1 − p), P2 = (1 − p)2 , hence the model, using P2 /P1 , can be executed
(deriving p from P2 /P1 and then putting p in P0 = p2 ).
Finally, we have to estimate the number of lost documents. Let us denote by N this number and by Nf the number
of found documents. Since we denoted by N the total number of documents that ever existed, we clearly have
N = Nf + N

(15)

We have
NP0 = N

(16)

N(1 − P0 ) = Nf

(17)

by definition, hence
N = Nf

P0
1 − P0

(18)

This general formula can be applied to our case of Jesuit theatre plays where we found P0 ≈ 0.794. Recall that Nf = 804.
Hence,
N ≈ 804(0.794/0.206) pieces (i.e. editions of theatre programmes)
or
N ≈ 3099 pieces

(19)

The total number of pieces that ever existed is, hence, estimated by Eq. (15):
N ≈ 804 + 3099 pieces
N ≈ 3903 pieces

(20)

We leave open a mathematical theory to calculate the confidence intervals for P0 (or N or N), but we have executed
several simulations of random samples in abstract “copies” of “pieces” (i.e. editions of theatre programmes). The
experiments show that we can have a high confidence in the numbers above. The results are as follows. We explored
the reliability of the mathematical model as follows. In a database, we created 10 different fictitious corpora, each
corpus containing a different number of editions (from 1000 until 10,000). Every edition is present in 150 copies
(a = 150). Every copy of each edition is represented by one record in the database. The largest corpus of 10,000 edition

262

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

Table 1
Distribution of editions and copies (corpus = 10,000 editions, a = 150, n = 1000)

P1
P2
P3
P4 . . . Pa

Number of editions

Number of copies

906 pieces are found with
44 pieces are found with
2 pieces are found with
No pieces are found with

1 copy
2 copies
3 copies
4 or more copies

Table 2
Estimation of N based on 30 random samples with n = 1000 (a = 150)
Size corpus

Average result for N

Minimum value for N

Maximum value for N

Standard deviation

10,000
9,000
8,000
7,000
6,000
5,000
4,000
3,000
2,000
1,000

10,073
9,007 (8,801)
7,890
6,947
6,126
5,102
4,059
3,010
2,011
998

8,074
6,437 (6,437)
6,520
5,070
5,238
4,165
3,183
2,449
1,796
873

12,178
14,978 (11,096)
11,278
9,099
8,117
6,485
5,456
3,389
2,425
1,106

1,278
1,563 (1,101)a
971
753
644
530
469
215
143
56

a

These values are due to one sample. The numbers between brackets give the results when this sample is omitted.

counts therefore 1,500,000 unique records, the smallest one 150,000 records or “copies of editions”. Firstly, we had the
computer pick out 1000 records (or copies) at random from every corpus. Then the sample was analysed: how many
editions were presented by one copy, how many by two copies and so on (see Table 1). This exercise provided us with
the values for P1 and P2 , needed in Formula (12).
The total number of found editions is 952. These numbers result for P0 in

150
1
P0 =
= 0.90867535
1 + (2/149)(44/906)
Using this result in Formulae (15) and (18) results in an estimated total number (N) of 10,223 editions. As the corpus
from which the sample is taken counts 10,000 editions, this sample gives a very precise idea of its size.
For each of these 10 corpora, we repeated this exercise 30 times and took 30 samples of 1000 records (copies) in
order to get an estimation of the precision of the estimations in practise. The results of the tests are presented in Table 2.
The size of the samples being constant (n = 1000), it is obvious that they are more exact, the more the size of the
corpus decreases. For corpora between 1000 and 3000 editions, the standard deviation is very low and even the found
minimum and maximum values for N give a reasonable indication of the real size of the corpus.1 Of course, these
results are each the average results of 30 samples. Figs. 1 and 2 show the individual results of each sample for a corpus
of 3000 editions (left) and 10,000 editions.
Overall, the results of the individual estimations are very satisfactory.
In practice, it is most of the time impossible to control the size of the sample. In general, scholars try to take the
sample as large as possible. It is useful to know when the size of a sample is sufficient. We tested the variation of the
size of the sample in relation to a constant corpus of 4000 editions each consisting of 150 copies.
The greater the sample sizes, the more exact the estimations. Table 3 shows that the exactness of random samples
between 700 and 1000 is quite high: the standard deviation is between 9.7 and 14.9% of the found average result for N.
Decreasing the sample to 600, 500 or 400 has already tangible consequences: the estimated size of the corpus shows
1

Only one sample out of 300 gave a totally wrong image of the estimated size, see Table 2. When we leave that sample out, we get a normal series
for the corpus containing 9000 editions.

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

263

Fig. 1. Corpus of 3000 editions.

Fig. 2. Corpus of 10,000 editions.

Table 3
Estimation of N based on samples with a decreasing size (corpus of 4000 editions, a = 150)
Size sample

Average result for N

Minimum value for N

Maximum value for N

Standard deviation

1,000
900
800
700
600
500
400
300
200
100a

4,059
4,022
3,915
3,902
4,059
4,254
3,829
4,470
4,842
–

3,183
3,286
3,176
3,186
2,895
2,856
2,933
2,345
1,616
–

5,456
5,034
6,000
4,833
5,943
6,284
5,727
8,647
19,670
–

469
392
582
428
652
848
770
1,342
3,417
–

a

Samples with n = 100 result sometimes in P2 = 0, where the model requires a value P2 > 0.

264

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

Table 4
Result for N: each the average results of 30 samples taken at random with a = 150
Size corpus

1,000
2,000
3,000
4,000
5,000
6,000
7,000
8,000
9,000
10,000
a

Size sample
1000

900

800

700

600

500

400

300

200

998
2,011
3,010
4,059
5,102
6,126
6,947
7,890
9,007
10,073

990
2,023
3,017
4,022
5,013
6,171
7,190
8,226
9,379
10,242

1,001
2,016
3,045
3,915
5,253
5,983
7,661
8,716
9,230
10,162

1,030
2,028
3,187
3,902
5,151
6,304
6,962
8,792
9,503
10,516

1,013
2,048
2,969
4,059
4,977
6,221
7,208
8,099
9,426
11,749

1,006
1,962
3,087
4,254
5,106
6,476
7,754
8,036
10,357
10,937

1,024
2,031
3,251
3,829
5,517
6,218
7,745
9,165
9,681
12,136

1,026
2,095
3,421
4,470
5,093
7,023
8,978
9,571
12,058
12,705

1,060
2,308
4,021
4,842
a

7,261
a
a
a
a

Some samples have P2 = 0.

an average mistake of ca. 20% of the found average result for N, which is, – under circumstances – acceptable. Smaller
samples result in unreliable (200 < n < 300) or even unworkable (n = 100) estimations.
Tables 4 and 5 can serve as a guideline for scholars to evaluate the precision of a concrete sample.
Each number in these tables is the result of 30 samples. In the upper left edge of Table 4, the average estimation of
N is presented for the smallest corpus (i.e. 1000 editions of each 150 copies) and the largest sample (i.e. 1000 copies
taken at random): 998. The corresponding number in Table 5 indicates the standard deviation of that estimation: 56,
or 5.6% of the found average result for N. That means that a sample of 1000 copies of a corpus of 1000 editions gives
a very good indication of N.
On the other hand, the tables indicate that a sample of 300 copies for an estimated corpus of 10,000 editions is
very unreliable. The average result of 30 samples for N still gives 12,705 editions, but the standard deviation of these
estimations is 9763!
These simulations provide us with a guideline for the interpretation of the result based on our sample of Jesuit
theatre programmes. The estimation of the total production of theatre programmes bearing the same characteristics
as the programmes in the sample, led to the number of 3903 editions (cf. Eq. (20)). Given the fact that the sample
consisted of 907 single copies (714 + 2 × 82 + 3 × 4 + 4 × 3 + 5 copies, see above), this estimation is probably quite
exact (cf. Table 5). From a methodological point of view, this information is of high importance. It means that, if we
analyse the theatrical production of the Jesuits in Flanders before 1773 on the basis of retrieved programmes, we may
extrapolate the results to a group of pieces five times larger than the retrieved number.
The next section is devoted to a mathematical – probabilistic model to determine a, the number of copies per
document.
Table 5
Standard deviation of N mentioned in Table 4
Size corpus

1,000
2,000
3,000
4,000
5,000
6,000
7,000
8,000
9,000
10,000
a

Size sample
1000

900

800

700

600

500

400

300

200

56
143
215
469
530
644
753
971
1,563
1,278

65
187
234
392
645
830
775
1,229
1,468
1,704

76
213
384
582
887
913
1,684
1,659
1,777
1,723

72
245
440
428
939
1,318
1,197
1,447
2,845
2,137

102
200
292
652
967
1,029
1,331
1,720
2,538
3,421

97
265
509
848
1,399
1,763
1,636
2,127
3,022
3,750

134
417
648
770
1,434
1,246
3,478
3,823
4,381
4,743

179
432
1,022
1,342
1,763
2,946
7,703
4,933
4,223
9,763

245
835
2,308
3,417

Some samples result sometimes in P2 = 0, where the model requires a value P2 > 0.

a

4,103
a
a
a
a

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

265

3. Determination of the number of copies per document
In this section, we assume that a, the number of copies per document, is small enough to have an influence on P0
(see Formula (10)) and hence should be calculated. Clearly, besides Formula (8) (to determine p) we need another
equation. This is best done by calculating the value of P3 /P2 , from Formula (1). We have Formula (7) for P2 and
Formula (1) yields
P3 =
So
P3
=
P2

a(a − 1) (a − 2) a−3
p (1 − p)3
6


a−2
3



1−p
p

(21)


(22)

Results (8) and (22) yield:
P3 /P2
2a−2
=
P2 /P1
3a−1
from which a easily follows:
a=

(3P1 P3 /P22 ) − 4
(3P1 P3 /P22 ) − 2

(23)

Note that P1 , P2 , P3 are not known but that P2 /P1 (see Formula (5)) and similarly, P3 /P2 are known from the retrieved
data:
P3
NP3
=
(24)
P2
NP2
is the division of the known number of documents of which we have three copies found by the known number of
documents of which we have two copies found. The determination of p is as in the previous section (Formula (9)) and
hence P0 (Formula (10)) is completely determined.
Unfortunately, in our example of Jesuit theatre programmes, the number NP3 = 4 is too low to be a stable value in
these formulae. We have
P3
4
=
(25)
P2
82
and this yields, together with Formula (11):
a = 3.7557376
an unrealistic number. Still, Formula (10) gives P0 = 0.7403156, about 5% lower than in the previous section (based
on higher values of a) which still can be considered as rather stable.
We want to underline that the above model certainly is useable in case one has not lost many documents in which
case the number NP3 will be high and trustable. Note that in the case of Jesuit theatre programmes the number p is
extremally high, being around p = 0.999 for whatever value of a ≥ 200 and around p = 0.998 for a = 150 (use Formulae
(9) and (11) to establish this). This is the reason why, although at least 150 copies of Jesuit theatre programmes existed
(and in many cases even up to 850), we hardly found any plays with 3 or more copies: in short: “almost all copies have
been destroyed”.
4. Informetric properties of the function j → Pj
The function j → Pj as given by Formula (1) is what we call in informetrics a size-frequency function (cf. Egghe
(2005)): it expresses the number (or rather the fraction, the difference is only a factor N as expressed in Formula (5)) of
documents of which we still have (or found) j ∈ {1, 2, 3, . . .} copies (here we do not consider the case j = 0 anymore).
In general informetrics terminology, we could say that Pj expresses the fraction of sources with j items (cf. again Egghe
(2005)).

266

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

In classical informetrics one is then automatically thinking of a “classical” size-frequency model, e.g. the law of
Lotka
C
Pj = α
(26)
j
(C > 0, α > 1) or of a decreasing power law
Pj = b0 bj

(27)

(b0 > 0, 0 < b < 1). It is clear that relation (1) is not of these types (it is the binomial distribution)! But here, a more
fundamental result can be derived.
Let us look at the indicators Pj+1 /Pj (j = 1, 2, 3, . . .). For the model developed in this article we find readily



a−j
1−p
Pj+1
=
(28)
Pj
j+1
p
Let us define the relative indicators
Pj+1 /Pj
Pj+1 Pj−1
Qj =
=
Pj /Pj−1
Pj2

(29)

for j = 2, 3, . . .. Hence, we have, for our model here, by Formula (28):
Qj =

j
a−j
j+1a−j+1

(30)

Since here the requirement j + 1 ≤ a < ∞ is clear (since a is the maximum number of copies, available at the start), we
see, by Formula (30) that
j
j
≤ Qj <
2(j + 1)
j+1

(31)

for all j = 2, 3, . . .. We find, e.g. Q2 ∈[1/3, 2/3[ and so on, but for all j we have that
1
≤ Qj < 1
3
If we calculate Qj for the law of Lotka (26) we have, as is readily seen
Qj =

(32)

(C/(j + 1)α ) (C/(j − 1)α )
(C/j α )2


Qj =

j2
j2 − 1

α
>1

(33)

for all j. Hence, Qj occupies a disjoint range when compared with Formula (32)!
For the decreasing exponential function (27), we readily see that
Qj = 1

(34)

for all j, hence being (strictly) between cases (32) and (33).
The inequality (Formula (32)) expresses that the decline in Pj+1 /Pj as a function of j is much faster for our present
model than for the Lotkaian or exponential model. This is a remarkable conclusion: finding missing copies of documents,
no matter what p (the probability for a copy to be lost) is, is an activity which leads to very fast declining values of Pj ,
the fraction of documents with j copies recovered. This means that it is, relatively, very hard to find multiple copies of
a single document.
The above shows that our present model belongs to a new type of two-dimensional informetrics theory. While
Lotkaian informetrics describes a two-dimensional informetrics theory of growth of sources and items (cf. Egghe
(2005)), the present model describes a two-dimensional informetrics theory of “what is left”, hence a two-dimensional
informetrics theory of aging (or obsolescence) – here in the sense of recovering copies of documents, hence also
describing the loss of copies and, consequently, when all copies of a document are lost, the loss of documents. Whether

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

267

or not this two-dimensional model of aging can also be applied to the more “classical” topic of aging in terms of
citation analysis, is left as an open problem.
5. A rationale for the book historical law
The book historical law says (Willard (1943) and other references in Proot and Egghe (in press)): the probability to
save a copy of an edition is inversely proportional to the size of the edition. We carefully checked the literature on this
subject and noticed that this law has not been formulated in a more accurate way, let alone that it has been proved. We
therefore formulate the above “law” as follows: the probability to save a copy of an edition is a decreasing function of
the size of the edition. Equivalently, and using the parameters p and a in this article, we can state the book historical
law as below.
Book historical law: The probability p to lose a copy is an increasing function of the size a of the edition.
We will now give a partial explanation of this expected regularity, not taking into account other variables such as
temporary interest of documents or, simply, the money value of documents. In the sequel we will show that p is an
increasing relation (to be explained further) of a.
Denoting P2 /P1 = x, we have, by Formula (9):
p=

1
1 + (2/(a − 1)) x

(35)

In practise, we can assume that x < 1; in fact, Formula (11) shows that x 	 1, a logical fact. So, if we let x vary in ]0, 1[,
we have that
1
≤p≤1
(36)
1 + (2/(a − 1))
showing that p has an (evident) upper bound in 1 and a lower bound
f (a) =

1
1 + (2/(a − 1))

(37)

which is a concavely increasing function since f
 (a) > 0, f

 (a) < 0 for all a ≥ 2, the absolute lower bound of a (since we
deal with P2 ). Formulae (36) and (37) imply that the relation between p and a (describing the book historical law) is
as in Fig. 3.
Fig. 3 shows that, when a is low, we can have values of p in the range [1/3, 1] (maximally) but for larger values of a
we see that p can only be large (close to 1). So, the higher a, the more limited is the range in which p can vary and the
higher this range is situated, giving a partial explanation of the book historical law. Note that this explanation could
only be given based on the boundedness of P2 /P1 (here by 1, but higher bounds could serve as well). Hence, a high
value of a (the size of the collection) forces p (the probability to lose a copy) to be high. The intuition for this is clear:
a high value of a implies that it will be difficult to have pieces with a low number of copies, unless p is very high (close
to 1). The fact that P2 /P1 	 1 expresses that we have relatively more pieces with 1 copy than with 2 copies which can
only be understood when p is large.

Fig. 3. The relation between p (the probability to lose a copy) and a (the size of the edition) is given by the shaded area.

268

L. Egghe, G. Proot / Journal of Informetrics 1 (2007) 257–268

6. Conclusions and open problems: suggestions for further research
By means of found copies of multi-copy documents we were able to estimate the number of lost documents and
hence also to estimate the total number of multi-copy documents that ever existed. This probabilistic theory shows that
the numbers are relatively independent on the number a of copies per document as long as a is not very small: in the
other case the theory is complemented with a formula to estimate the value of a.
Simulations show that the estimated number of lost documents is very stable. These simulations are executed by
random sampling in the copies, where we know in advance the total number of documents.
We applied the model to the case of Jesuit theatre programmes in which case a ≥ 150 (and where a can even go up
to 850). As mentioned above, these large values of a (number of printed copies of theatre plays) guarantee a stable
percentage of lost plays, estimated in this case around 80%.
It is clear that this theory could be applied to other cases of multi-copy documents. One could study the problem
of estimating the number of lost documents in case the documents are precious. Here, a will be smaller in which case
Section 3 can be used to estimate a (needed since, for smaller values of a, P0 , being the proportion of the original
documents that have been lost, is more dependent on a). But in this case, p will also be smaller (being the probability
to lose a copy) implying that, in this case, one has more documents (than in the case of Jesuit theatre programmes) of
which more (i.e. 3, 4, . . .) copies are found (i.e. not lost), making the estimate of a more reliable (see Section 3).
We remarked that the size-frequency function j → Pj (fraction of documents for which we found j copies) that we
encountered in this theory satisfies the inequality (for all j ≥ 2)
Qj =

Pj+1 /Pj
<1
Pj /Pj−1

while we have the opposite inequality for Lotkaian size-frequency functions and while we always have Qj = 1 for
decreasing exponential size-frequency functions, hence noting that we are in a new type of informetrics theory, describing loss (or rather recovery) of items of sources. We leave it for further study whether this model can also be used for
the description of two-dimensional aging in the “classical” sense: the decline of citations in time.
We also gave a partial rationale for the book historical law: the probability to lose a copy of a document is an
increasing function of the size of the edition.
It is our hope that this model will be applied in many other (varying) examples of multi-copy documents (and even
multi-copy objects as described in Section 1), hereby further testing the stability of the probabilistic model. The further
development of this non-Lotkaian informetrics theory is also a challenge.
References
Canavos, G. C. (1984). Applied probability and statistical methods. Boston, USA: Little, Brown and Company.
Egghe, L. (2005). Power laws in the information production process: Lotkaian informetrics. Oxford, UK: Elsevier.
Grimmett, G. R., & Stirzaker, D. R. (1985). Probability and random processes. Oxford, UK: Clarendon Press.
Proot, G., & Egghe, L. (in press). The estimation of editions on the basis of survivals: Printed programmes of Jesuit Theatre Plays in the Provincia
Flandro-Belgica (before 1773). With a note on the “bookhistorical law”, Papers of the Bibliographical Society of America, to appear.
Willard, O. M. (1943). The survival of English books printed before 1640: a theory and some illustrations. The Library, Fourth Series, 23, pp.
171–190.

