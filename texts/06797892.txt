IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

2141

Automated Mosaicing of Feature-Poor Optical
Coherence Tomography Volumes With
an Integrated White Light Imaging System
Kristen L. Lurie, Roland Angst, and Audrey K. Ellerbee∗

Abstract—We demonstrate the first automated, volumetric mosaicing algorithm for optical coherence tomography (OCT) that
both accommodates 6-degree-of-freedom rigid transformations
and implements a bundle adjustment step amenable to generating
large fields of view with endoscopic and freehand imaging systems.
Our mosaicing algorithm exploits the known, rigid connection between a combined white light and OCT imaging system to reduce
the computational complexity of traditional volumetric mosaicing
pipelines. Specifically, the search for 3-D point correspondences is
replaced by two, 2-D processing steps: We first coregister a pair of
white light images in 2-D and then generate a surface map based
on the volumetric OCT data, which is used to convert 2-D image
homographies into 3-D volumetric transformations. A significant
benefit of our dual-modality approach is its tolerance for featurepoor datasets such as bladder tissue; in contrast, approaches to
mosaic feature-rich volumes with significant variations in the local
intensity gradient (e.g., retinal data containing prolific vasculature)
are not suitable for such feature-poor datasets. We demonstrate the
performance of our algorithm using ex vivo bladder tissue and a
custom tissue-mimicking phantom. The algorithm shows excellent
performance over the range of volume-to-volume transformations
expected during endoscopic examination and comparable accuracy with several orders of magnitude superior run times than an
open-source gold-standard algorithm (N-SIFT). We anticipate the
proposed algorithm can benefit bladder surveillance and surgical
planning. Furthermore, its generality gives it broad applicability
and potential to extend the use of OCT to clinical applications
relevant to large organs typically imaged with freehand, forwardviewing endoscopes.
Index Terms—Bladder, endoscopy, optical coherence tomography (OCT), volumetric mosaicing.

I. INTRODUCTION
PTICAL coherence tomography (OCT) is a biomedical imaging technique that acquires in-vivo, volumetric
images of biological tissue with micrometer-scale resolution.

O

Manuscript received December 27, 2013; revised April 4, 2014; accepted
April 6, 2014. Date of publication April 14, 2014; date of current version June
14, 2014. The work of K. L. Lurie was supported by a National Defense Science and Engineering Graduate Fellowship and a National Science Foundation
Graduate Research Fellowship. Asterisk indicates corresponding author.
K. L. Lurie is with the Department of Electrical Engineering, Stanford University, Stanford, CA 94305 USA (e-mail: klurie@stanford.edu).
R. Angst is with the Department of Computer Science, Stanford University,
Stanford, CA 94305 USA (e-mail: rangst@stanford.edu).
∗ A. K. Ellerbee is with the Department of Electrical Engineering and the E. L.
Ginzton Laboratory, Stanford University, Stanford, CA 94305 USA (e-mail: audrey@ee.stanford.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2316535

Given the relevance of microscale tissue architecture to disease,
endoscopic implementations of OCT are emerging as powerful
tools for the diagnosis of diseases such as cancer of the gastrointestinal [1], [2] and urinary tracts [3], [4]. Unfortunately,
the high resolution of OCT comes at the cost of a small field of
view (FOV), which is typically limited by the size of the objective lens and the diameter of the endoscopic working channel to
a few cubic millimeters for a fixed instrument position. Large
FOVs are desirable, however, for surveillance (i.e., examination
of tissue appearance) and guidance during surgery of internal
organs such as bladder, stomach, gallbladder, and intestines.
Well-controlled, helical pull-back scanning patterns work
well to construct large FOVs of internal luminal structures such
as the upper urinary tract [4], [5], but endoscopic examination
of irregularly shaped organs is generally done by freehand scanning, which causes the exact scan pattern to be unknown a priori. Thus, creating a large FOV is more complicated than with a
well-controlled scanning pattern; in such cases, mosaicing may
be a suitable alternative to create large FOVs. In the bladder, for
instance, OCT has been proposed as a complement to the standard imaging modality, white light cystoscopy (WLC) [3], [6]:
OCT can both identify tumors invisible to WLC and stage early
cancers, so a combined WLC and large-area OCT imaging scope
(enabled by mosaicing) would help to monitor and treat bladder
cancer. We envision that such a scope could improve early detection rates, verify morphological changes occurring in subsurface
layers, assist with surgical planning, and provide quasi-real-time
delineation of tumor margins. Importantly, the clinical utility of
such large mosaics depends on the speed and efficiency with
which they can be generated. For example, the time-sensitive
demands of surgical guidance requires quasi-real-time generation to minimize operating room costs. Similarly, comprehensive, high-resolution, diagnostic imaging of even small areas,
tumors or organs generates large datasets that task even the best
computers, underscoring the need for computational efficiency.
Conventional volume mosaicing pipelines are straightforward
extensions of standard 2-D image mosaicing pipelines and typically proceed as follows [7]: 1) Detect interest points—locations
with rich local intensity gradient variation—in each volume. 2)
Extract a descriptor for each interest point that describes the
local gradients. 3) Coregister pairs of volumes based on correspondences (i.e., one-to-one relationships between image points
of different images) generated between locations having similar descriptors. 4) Bundle adjust the positions of the volumes
with respect to the observed correspondences, effectively distributing the cumulative error from pairwise (PW)-registered

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

2142

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

volumes across multiple volumes collected in a loop [8]. Close
variations of this pipeline have been applied to mosaic OCT
and ultrasound datasets. Some of these approaches, however,
lack the bundle adjustment (BA) step needed to mitigate severe drift when handling mosaics composed of large numbers
of registered volumes [9]–[14]; others consider only a subset of
the six degrees of freedom (DOF) needed (three translational,
three rotational) to represent rigid volume transformations in
endoscopes [10].
A more general problem with this pipeline and its variants
is its low computationally efficiency. Modified pipelines have
attempted to reduce the 3-D registration problem to two dimensions, but the methods used (e.g., summed voxel projections or
SVPs) rely on significant feature contrast, often from a dense
layer of vessels, [15], [16] and are therefore not suitable for
organs like the bladder that possess few features in OCT SVPs.
Moreover, such approaches can coregister volumes with a limited number of DOF but are not sufficient for building mosaics of
endoscopic volumes because the use of planar SVPs leads to ambiguity when trying to account for out-of-plane transformations.
The main contribution of this paper is a computationally efficient algorithm for fully automated mosaicing of a large number
(>30) of OCT volumes acquired with a handheld OCT probe
that simultaneously captures white light images (e.g., a WLC
+ OCT scope). The incorporation of white light data is a critical, enabling strategy to overcome the challenge of registering
feature-poor OCT images and additionally allows us to reduce
the most time-consuming step in the volumetric registration
problem (i.e., the 3-D–3-D correspondence search) to two dimensions, significantly accelerating computation compared to
standard volumetric mosaicing approaches. The performance of
the algorithm and its utility to create a large FOV mosaic is validated, evaluated, and demonstrated using ex-vivo bladder tissue
and a custom tissue phantom.
This algorithm, to our knowledge, is the first automated approach to mosaicing multiple OCT volumes that both accommodates 6-DOF rigid transformations and implements a BA step,
making it more capable to handle the imaging conditions of endoscopic systems [9], [11], [12] and more easily scalable to large
numbers of volumes than previously demonstrated works [11],
[13], [17]. The generality of the algorithm, furthermore, gives
it broad applicability and potential to extend the use of OCT
to other clinical applications, such as large, irregularly shaped
organs, whether feature-rich or feature-poor.
II. MOSAICING ALGORITHM
A. Algorithm Overview
Our basic strategy is to register pairs of feature-poor OCT
volumes by exploiting the additional, feature-rich information
contained in WL images acquired simultaneously from a rigidly
attached camera head. The underlying assumption is that the
local curvature of the surface of interest is fairly small and can
thus be well approximated by a tangent plane. Hence, an efficient
2D–2D correspondence search in this tangent plane provides a
set of high-quality 2-D feature point correspondences with few
outliers. With the help of the extracted surface from the OCT

Fig. 1. Representative images (white light, SVP, and B-scans) of bladder
tissue. WL and OCT images are 12.5 mm × 9.4 mm and 2.57 mm2, respectively.
Green and yellow lines indicate locations of SVP and B-scan slices, respectively.
The lack of prominent vascular features within the OCT FOV in some regions
of the bladder makes this tissue difficult to stitch using existing algorithms.

images, this set of 2-D feature point correspondences can then
be lifted to 3-D correspondences, which enables a full 6-DOF
PW registration of the volumes.
This strategy enables us to both replace the expensive computations of 3-D interest points and descriptors used in standard
3-D pipelines (e.g., N-SIFT, or n-dimensional scale-invariant
feature transform) with efficient and well-understood 2-D counterparts and to use the broader FOV of the WL images (compared
to that of the OCT data alone) to obtain a sufficient number of
suitable interest points to generate good correspondences. We
call the bladder “feature-poor” in the context of OCT because
the typical FOV of a SVP has few image features (i.e., variations of the local intensity gradient) that can be extracted with
standard computer vision 2-D feature matching techniques due
to the sparsity and relatively large dimensions of its vasculature (e.g., compared with the retina). In contrast, the larger
FOV of WL images acquired during endoscopy of the bladder includes more vascular content and surface texture than is
present in the OCT volume alone, and these features can be
reliably detected with such techniques [18]–[20] (see Fig. 1).
OCT volumes of the diseased bladder also can be featurepoor in depth because the loss of normally pervasive features
such as the fibers from the muscularis propria is a hallmark of
disease.
The proposed algorithm proceeds as follows (see Fig. 2), and
relevant variables are described in Table I.
1) Coregister pairs of volumes acquired sequentially or having a high degree of overlap.
a) Generate a 2-D homography (HW L(i)→W L(j ) ) to
register a pair of WL images using a set of 2-D
feature point correspondences.
b) Compute the homography (HOCT(i)→OCT(j ) ) and
2-D point correspondences for the projected surface
plane (PSP) of the associated OCT volumes using
the static relative transformation between the WL
and OCT cameras (TW L→OCT ) and the previously
computed homography between the WL images. We

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

2143

TABLE I
ABBREVIATIONS AND SYMBOLS USED IN THE TEXT

2) Refine all transformations describing PW-registered volumes in a global optimization framework (i.e., BA), building a multivolume, large FOV mosaic.
B. General Assumptions and Justifications
Fig. 2. Pipeline for volumetric mosaicing comprising PW volume registration
and BA steps. The steps are labeled with identifying letters and representative
symbols that parallel the text.

define the PSP as a plane of the OCT volume that
is considered tangent to the WL image plane and
contains the surface of the imaged sample projected
onto this plane.
c) Extract the profile of the tissue surface in each OCT
volume [i.e., create a surface map as a depth field,
S(x, y)].
d) Lift the 2-D point correspondences to 3-D point
correspondences using the surface map and compute an initial estimate of the 6-DOF transformation between the two volumes. Apply an iterativeclosest-point (ICP) algorithm to refine the rigid
transformation between the two surface maps
(TOCT(i)→OCT(j ) ), thereby registering the two volumes.

The WL and OCT cameras are rigidly attached; hence, we assume that we can estimate the relationship between the positions
of the cameras with a calibration transformation, TW L→OCT ,
that describes the relative scaling and 2-D translation between
them. In general, points seen in one camera cannot be mapped
directly to points in the other camera without knowing their zpositions (i.e., a point can only be mapped to its epipolar line
in the other view, see [21]). However, if the points lie on a
plane, then a homography (a regular 3×3 transformation matrix that maps a plane to a plane and has 8 DOFs because it
is being defined only up to scale) can map any point on this
plane as seen from one camera position to its corresponding
point as seen from the other camera position (a so-called planeinduced homography). We subsequently argue that our volumetric OCT data sample, positioned within the WL image FOV, can
be approximated with a tangent plane and that this approximation is accurate enough to establish an initial set of outlier-free
2-D–2-D feature point correspondences that can be refined in a
subsequent iterative adjustment step.

2144

The homography approximation for the camera relationship
is justified for our imaging conditions by the following three
observations: 1) The WL and OCT cameras have parallel optical axes and the depth variation of the surface of the sample is
small over the FOV of the WL camera (12.5 mm × 9.4 mm)
with respect to the focal length of the camera (50 mm). 2) The
curvature of the tissue surface is small and can be well approximated locally (i.e., within the FOV of the imaging system) by its
tangent plane. 3) The tangent plane of the tissue is rarely viewed
from a steep angle (i.e., the angle between the optical axes and
the normal of the tangent plane is never close to being orthogonal). These observations together imply that the perspective
effect and parallax due to the depth variation of the tissue surface is small around a fairly large region of the tangent plane
of the tissue surface. Therefore, we propose to approximate the
calibration matrix between the two cameras by the homography
that maps the WL camera plane to the OCT PSP. We can further
reduce the homography to a scaling factor and 2-D translation
(i.e., three DOFs in total) because the image axes in the WL and
OCT imaging systems are aligned. In practice, the calibration
matrix is computed by measuring the FOV of both imaging systems to determine the scaling factor and by localizing a common
point to determine the translation.
We assume the radial distortion of the camera is minimal,
but note that a standard process to calibrate and correct for this
distortion exists [22]. We also assume that data are collected
such that 1) sequentially acquired volumes overlap, 2) the relative motion between these volumes can be expressed by a rigid
transformation (which mimics the three translational and three
rotational DOFs of an endoscope), and 3) each volume contains
minimal motion artifacts (i.e., motions within the acquisition
of an individual volume). This latter point is reasonable in the
context of endoscopic imaging given emerging trends in highspeed OCT that enable sparsely sampled volumes to be acquired
rapidly (15 Hz) [23]–[25]. Finally, we assume that the index of
refraction of the sample is comparable to the index of the surrounding medium, as is the case when imaging the bladder in
saline. This assumption ensures that relative rotations of the two
volumes out of the plane (i.e., rotations not about the surface
normal vector or x- and y-axis rotations in our notation) with respect to each other do not cause gross misalignments of features
below the surface.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

these approaches. As argued previously, the smoothness and
low curvature of the visible 3-D surface in the image FOV can
be approximated sufficiently closely by a tangent plane; hence,
the mapping of the relative transformation between two images
can be described by a homography [21]. This strategy of using
homographies is computationally efficient and beneficial for
feature-poor datasets because it requires fewer correspondences
than if fundamental or essential matrices were employed (i.e.,
only four correspondences are needed for 2-D homographies
compared to eight with fundamental and five with essential matrices). This is noteworthy considering that the small FOV of
OCT volumes and the relative featurelessness of the data can
limit the number of correspondences that can be identified in
2-D. Finally, homographies map points to points, enabling computation of a more accurate inlier set.
We used SIFT [26], [27], a standard 2-D, image-featurematching technique, to detect and describe features and identify point correspondences between two WL images: pW L(i) =
(ui , vi , 1)T ↔ pW L(j ) = (uj , vj , 1)T . We define a correspondence as a mapping of two points between images, where ↔
can be read as “correspondence between.” The set of all correspondences was filtered using RANSAC [28], [29], a geometric
verification step, yielding an optimal homography between the
two WL images based on the determined inlier correspondences.
In brief, RANSAC is a robust procedure to estimate a parametric
model (e.g., a homography). In its innermost loop, a so-called
minimal sample set is randomly picked from the set of potential
correspondences; a hypothesis of the homography is computed
from this sample set and verified on the remaining correspondences. The hypothesis with the largest support will be returned
as the estimated model. Because a successful hypothesis can
only be generated from an outlier-free minimal sample set,
larger minimal sample sets necessarily reduce the probability
of generating a good hypothesis, hence the efficiency advantage
of homography estimation over fundamental or essential matrix
estimation. We denote the homography returned by RANSAC
as HW L(i)→W L(j ) , where pW L(j ) = HW L(i)→W L(j ) pW L(i) .
While the nonvanishing curvature of the 3-D surface can lead
to a small inaccuracy in the homography on the order of a
few pixels, we nonetheless expect that this error is sufficiently
small so that the selected homography allows for separation of
the inliers from outliers. Thus, the end result of this step is an
outlier-free set of 2-D correspondences.

C. Step 1A: Generate 2-D Homographies
From Outlier-Free Correspondences
Traditional 2-D feature-matching pipelines, as used in computer vision, implicate the fundamental or essential matrix to
compute the relative transformation that registers two images.
Unfortunately, an approach based on those matrices has drawbacks because they only provide the direction, but not the actual
length, of the translation. More importantly, the essential and
fundamental matrix distinguish between inlier and outlier correspondences based on a point-to-epipolar-line distance, which
in practice often leads to outliers being considered as inliers.
Hence, a time-consuming, three-view verification with pointto-point distances must usually be applied when implementing

D. Step 1B: Compute Point Correspondences for OCT PSPs
Given our assumptions about the imaging systems and
sample (see Section II-B), we express the relationship between the two imaging systems as the transformation matrix TW L→OCT . As such, the homography describing the relationship between the two PSPs of the OCT volumes can
be determined from the homography for their associated WL
images computed in the previous step: HOCT(i)→OCT(j ) ∼
=
.
Using
the
PSP
hoTW L→OCT HW L(i)→W L(j ) T−1
W L→OCT
mography, the feature correspondences identified for images WL(i) and WL(j) can be transformed into

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

correspondences between pixels in the PSPs of volumes OCT(i)
and OCT(j).
Although WL point correspondences can be transformed
directly into OCT point correspondences (e.g., pOCT =
TW L→OCT pW L ), in practice the number of correspondences
in the original set that fall within the FOV of the OCT volume can be too small to determine a rigid 3-D transformation between the volumes (i.e., the OCT volume is featurepoor and the overlapping FOV of the WL image may also
contain few features). Instead, we utilize the OCT homography to define a new set of 2-D correspondences between the
two PSPs: pOCT(i) = (xi , yi , 1)T ↔ pOCT(j ) = (xj , yj , 1)T ,
where the set of pOCT(i) s contains randomly selected points
from within OCT(i) that map to within the size of volume j
(ensured by transforming pOCT(i) by HOCT(i)→OCT(j ) ).
E. Step 1C: Convert 2-D Correspondences to 3-D
With Surface Maps
The surface of the data in each OCT volume, S(x, y), is extracted to enable conversion of the 2-D point correspondences
into 3-D point correspondences in a future step (see Section IIF). The extraction proceeds one B-scan at a time; the surface
from each B-scan consists of a 1-D array of points that minimizes a cost function applied to contiguous A-scans using a
greedy algorithm. The cost function comprises a linear weighted
combination of three energy terms, for which the cost vector of
A-scan ican be expressed as
costi (z) = [c1 di (z) + c2 li (z) + c3 si (z)]

(1)

where cj represents scalar weighting coefficients and di (z),
li (z), and si (z) represent the three energy terms. The energy terms are defined as follows: 1) The 2-D image derivative (di (z)) is computed as the convolution of the log-scale
intensity B-scan (I (x, z)) with a first-derivative Gaussian kernel (N (0, σ)) with a standard deviation of σ = 10. Namely,
D(x, z) = [d1

d2

···

dN ] = |I(x, z) ∗ N (0, σ)| . (2)

This energy term prioritizes points with strong edges, such
as points on the sample surface. 2) The distance from the
expected surface location (li (z)) anticipates the position of
the surface in a specific A-scan based on its likely position
given all A-scans. The expected surface location is approximated as a constant for all A-scans and can be expressed by
l0 = argminz | I(z) − [Im in + c(Im ax − Im in )] |, where I(z) is
the averaged log-scale intensity A-scan, c is a thresholding factor, and Im ax and Im in are the maximum and minimum intensity in average A-scan, respectively. This energy term can be
expressed as
li (z) = |z − l0 |

(3)

and is designed to bias the position of the surface toward the top
of the image to reduce potentially large edge weights associated
with muscle fibers. 3) The distance from the previous surface
position (si (z)) can be expressed as
si (z) = |z − zi−1 |

(4)

2145

where zi−1 is the position of the surface extracted for the previous A-scan. This parameter contributes to the extraction of a
smooth surface by prioritizing depths in a particular A-scan that
are in close proximity with those already identified as surface
locations in adjacent A-scans.
For each B-scan, the algorithm is run first from left to right
and then from right to left, using the surface depth extracted for
the last A-scan in the first iteration to initialize the second. This
two-step process reduces inaccuracies of the surface extraction
of the initial A-scans of the first iteration. When run over all Bscans in a volume, the resulting 2-D array of points represents
the depth of the surface for each lateral position on the PSP.
In principle, a more accurate and smooth surface could be
achieved by extracting the surface of the entire volume at once;
however, this is computationally expensive, and in practice, the
extraction of the surface for each B-scan independently is sufficient for our application: we observed that the variability between the two approaches is typically only a couple of pixels
out of the majority number of surface pixels (512 × 512). The
current approach also has the benefit of being “pleasingly parallel,” allowing future implementation on parallel hardware that
can permit further gains in computational efficiency.
F. Step 1D: Refine Volume Registration With 3-D ICP
A 3-D ICP algorithm [30] is used to compute the best
rigid transformation between the two surfaces (and, thereby,
between the corresponding volumes) given an initial estimation. We denote the rigid transformation as TOCT(i)→OCT(j ) ,
or Ti→j for short. To compute the initial estimate of
the rigid transformation, T0i→j , the 2-D point correspondences from the previous step are first lifted into 3D correspondences, pOCT(i) = (xi , yi , zi , 1)T ↔ pOCT(j ) =
(xj , yj , zj , 1)T , by assigning each 2-D correspondence the
z-value that corresponds to the depth of the tissue surface at that
lateral location given by the surface map, S(x, y). The lifting of
the correspondences to 3-D in this manner is reasonable because
the OCT camera is affine. The inputs to the ICP algorithm are the
points of the 3-D correspondences (pOCT(i) and pOCT(j ) ) and
T0i→j , which are set to the best 3-D rigid transformation between
these correspondences based on singular value decomposition.
Each iteration of ICP through the following two steps yields
an updated transformation (TPW
i→j ), which is the output of the
PW volume registration step : 1) Create a new set of correspondences between each point in volume i (pOCT(i) ) and that point
in volume j (pOCT(j ) ) which is physically closest to the value
of the current transformation estimate (i.e., TPW
i→j pOCT(i) ).
2) Update the rigid transformation using a mean-square cost
function given the current set of correspondences. The algorithm
converges when the average distance between TPW
i→j pOCT(i)
and pOCT(j ) (where pOCT(i) ↔ pOCT(j ) ), for all current correspondences, falls below a fixed threshold.
Although in practice our initialization is very close to the
final transformation, the ICP step is still helpful to refine inaccuracies that may exist in the WL-to-OCT calibration, which
is only an approximation of the true transformation describing
the relationship between the cameras. Inaccuracies in our

2146

Fig. 3.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

Representative steps for fabrication of a custom two-layer phantom with a well-known pattern (“SBO,” our laboratory logo).

estimation of the calibration matrix may stem from variations
due to WL radial distortions or out-of-plane rotations, leading
to a systematic error that is compensated in the ICP step.
G. Step 2: Bundle Adjust PW-Registered Volumes
Large mosaics can be constructed from a sequence of volumes
by repeated, PW registration of new volumes with previously
registered volumes (i.e., repeated application of Step 1); however, as more volumes are added, positional error accumulates
from the PW transformations. Thus, the more images used for
the mosaicing, the more important loops (i.e., volume pairs that
are not sequentially acquired but are overlapping) become: loops
provide strong constraints that force chained relative transformations to equal the identity transformation. These constraints
enable correction of accumulated errors, which would otherwise
result in a significant drift of the registered volumes when connected on a large scale. We therefore include a state-of-the-art
BA step that refines all of the PW transformations.
This task is commonly framed as a graph optimization problem, wherein each volume is represented as a vertex of a graph,
and each vertex has a pose that describes the position and
orientation of the volume in a global coordinate system [31].
PW rigid transformations between two volumes are described
as edges. Ours is a nonlinear graph optimization problem because we assume there can be rotations between volumes [32].
We invoke the Tree-based netwORk Optimizer (TORO) [33],
an open-source, nonlinear graph optimization solver: this algorithm converges rapidly to a good approximation of the global
minimum to find the pose of each volume given observed edges.
The volume-transformation graph, which is the input graph
to TORO, is generated from the rigid transformations between
sequential volume pairs and highly overlapped, nonsequential
volume pairs (i.e., loops). First, the sequential rigid transformations (TPW
i−1→i , the output of Step 1) are added as edges to the
graph. By default, the pose of the first volume is used as the
origin of the global coordinate system and deemed to be aligned
with its axes (i.e., no rotation). The initial poses of the remaining
volumes are defined sequentially: specifically,
pose i is defined
k =i PW
=
T
by the transformation matrix Tinit
0→i
k =1 k −1→k , where the
translation and rotation of T0→i describe the position and orientation of the volume in the global coordinate system. Next, additional edges describing loops (e.g., nonsequential, overlapping
volume pairs) are added to the volume-transformation graph.
Specifically, to determine if two volumes overlap sufficiently to
 i→j = Tinit (Tinit )−1 and use it to
form a loop, we compute T
0→j
0→i
estimate the overlap between the two volumes i and j. If the
estimated overlap exceeds 5%, the actual rigid transformation
TPW
i→j and its corresponding volume overlap are computed using

the full PW registration algorithm (Step 1). Further, if the volumes overlap by at least 20%, an edge between these volumes
is added to the graph. Note that this latter threshold was chosen
empirically to ensure high accuracy. The output of TORO produces a good approximation to the “best” poses (Tﬁnal
0→i ) for the
volumes given the observed edges.
III. DATA COLLECTION
A. Samples
We used two types of samples to validate our algorithm: ex
vivo bladder tissue and a custom tissue-mimicking phantom.
Healthy bladder tissue was harvested from a female pig euthanized as part of an unrelated study. Samples of tissue from
different sections of the bladder wall (e.g., dome, anterior wall,
trigone) were excised, stretched to mimic normal bladder distention and submersed in a 0.9% w/v saline solution to mimic
the aqueous conditions of cystoscopy; the saline level was sufficiently above the tissue so that the air–water interface was not
evident in the OCT B-scan (see Fig. 1). When stretched, as during distention, tissue has a rough texture due to the presence
of folds, or rugae, in the bladder wall. This surface texture can
vary in height up to a few hundred microns within the FOV of
the OCT system.
A two-layer, tissue-mimicking phantom was created with an
imprinted pattern (“SBO,” our laboratory logo). The use of an
easily recognizable structure was intended to facilitate visualization of mosaicing error, because accurate mosaicing of the
bladder is hard to discern. Like the bladder tissue, the phantom
is feature-poor because the scattering is homogenous in each
layer, and the features of the imprinted pattern are large with
respect to our FOV, limiting the number of areas containing
variations in the local intensity gradient.
To fabricate the phantom (see Fig. 3), a 3-D printed mold from
Visijet Crystal plastic (3-D Systems) was created with the desired pattern for the first layer protruding approximately 500 μm
from its surface. The mold was treated with an acetone, acrylic
spray (ModPodge spray adhesive, Plaid Enterprises, Inc.). Polydimethylsiloxane (PDMS, Sylgard 184) was hand-mixed at a
ratio of 20:1 PDMS base to curing agent. The PDMS was then
degassed, poured onto the mold, and allowed to cure for 4 h
at 60 ◦ C. The second layer consisted of 20:1 PDMS with the
addition of 0.1486% mass percent of titanium dioxide (anatase,
Sigma Aldrich); the addition of scattering particles provides
contrast between the first and second layers. The titanium dioxide and curing agent were mixed with magnetic spin bars for
an hour prior to combining with the PDMS base. A thin layer
of the scatterer-infused PDMS (˜500 μm) was poured over the
first layer. To induce a rugged surface texture that mimics the

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

Fig. 4. Example images (white light, en face, and B-scans) from a custom
phantom. WL and OCT images are 12.5 mm × 9.4 mm and 2.57 mm2 , respectively. Green and yellow lines indicate the locations of en face and B-scan slices,
respectively.

surface of the bladder, a crumpled piece of aluminum foil was
pressed on top while the PDMS was cured for 3 h in an oven at
60 ◦ C [34].
Example white light images and OCT cross sections of the
custom phantom are shown in Fig. 4. The troughs between
the letters of the SBO logo are approximately 500 μm deep,
and the texture imparted by the crumpled aluminum foil adds
approximate 500 μm of surface variation (peak to trough) over
the FOV of the white light images. The image of the SBO logo,
which intentionally does not appear at all depths within the
phantom, manifests in the OCT en face images as areas of high
intensity (due to the scattering particles) surrounded by clear
PDMS.
B. Imaging
We evaluated our algorithm using volumetric OCT and WL
image data collected with a ThorLabs TELESTO SD-OCT system. This system has a center wavelength of 1325 nm and axial
and lateral resolutions of 7.5 and 15 μm in air, respectively.
We determined the calibration between the OCT and WL
imaging systems by imaging a test target in a region with a
sharp corner. For each imaging system, the location of the corner was measured by fitting two perpendicular lines to points
manually selected at the edges of the test target; the intersection
of these lines were considered to be the location of the corner. The locations of the corners in the OCT and WL datasets
were used to determine the translation between the origins of
the two systems. The scale factor was determined directly from
the TELESTO software based on the reported dimensions of the
OCT scan area and the percentage of the WL image consumed
by the overlay corresponding to the position of the OCT scan,
provided by the manufacturer.
The TELESTO probe was stationary and mounted to the table. The sample, placed on a translation stage attached to a
rotation mount, was made to move relative to the probe. The
absolute position of the focal plane of the TELESTO, as well
as the position of the OCT reference mirror, were controlled
by knobs on the TELESTO probe. This setup mimics the mo-

2147

tion of a standard endoscope, where motion of the scope during
endoscopy is replaced by relative motion of the sample in our
experiment.
Coregistered white light images and OCT volumes were collected using commercial TELESTO software augmented with
a proprietary MATLAB GUI. Raw interferometric data were
processed into OCT volumes with the commercial TELESTO
software that performs apodization and a nonuniform discrete
Fourier transformation. Both white light (640 × 480 pixels,
12.5 mm × 9.4 mm) and OCT volumes (512 voxels3 , 2.57 mm3 )
were collected for each position of the sample and imaging
probe.
Data for mosaics were accumulated by collecting multiple
image–volume pairs, between which the sample was displaced
by rotation or translation using the aforementioned stage, mount
and knob. Fig. 1 and 4 show examples of white light and OCT
cross sections captured from our two samples.
IV. EXPERIMENTAL VALIDATION
A. Comparison With Gold Standard: N-SIFT
The accuracy and timing of our algorithm was evaluated relative to n-dimensional SIFT (N-SIFT), a well-known multidimensional registration algorithm used in the computer vision
community. N-SIFT is analogous to the standard 2-D SIFT algorithm, but is able to detect and describe features in n dimensions
(in our case, three). Like its 2-D counterpart SIFT, N-SIFT can
be invariant to scale, translation, and rotation. Note that while
SIFT is invariant to translations and in-plane rotations, it is
only robust to out-of-plane rotations; in contrast, N-SIFT can
be invariant to translations and rotations in all three dimensions,
including rotation out of the plane [35]. We utilized the only
known open-source version of the algorithm [36], which has
poor rotational invariance.
In place of our multistep pipeline, unmodified open-source
N-SIFT code was used to detect, describe, and match feature
points in 3-D. The generated correspondences were then used
to determine the best rigid transformation from the extracted
correspondences using RANSAC. This was followed by BA
using an identical algorithm to that described in Section II-G.
We measured the time for PW registration between the the NSIFT variants and our algorithm using the standard stopwatch
timer in MATLAB and the time.h library in C++. The latter
was used to time the feature extraction and matching times in
N-SIFT. All experiments were run on a 3.6-GHz Intel Core i5
with 8 GB of RAM.
We also measured the quantitative accuracy of N-SIFT and
compared it with our algorithm using the method described in
Section IV-B. Because of the significant runtime associated with
N-SIFT for a volume size of 5123 voxels, we used a variant of
N-SIFT called white-light-enhanced N-SIFT (WhiLE-NS) [37]
to achieve the same results while expediting the computation.
WhiLE-NS uses coregistration between white light images to
preselect small, highly overlapping subvolumes (in our case, of
size 1282 × 256 voxels) over which to run N-SIFT; the results
from the overlapping subvolumes are extrapolated to the entire
volume. This method reduces the computation time because

2148

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

the size of the subvolumes for which features are detected and
described is significantly smaller.
B. Assessment of PW Registration Accuracy and
Performance Boundaries

=

To determine the range of freehand motion patterns over
which our algorithm can perform accurate PW registrations,
we devised an approach in which the positional accuracy of the
registration was assessed by comparing two transformations that
describe the relationship between the same pair of volumes, but
are arrived at by registering different volume pairs. In essence,
we evaluated the difference in the transformations determined
by a direct trajectory and a loop trajectory. This method allowed
us to evaluate the robustness of our algorithm to transformations
associated with each of the possible 6 DOF.
Specifically, for each transformation, we collected at least 13
sets of OCT volumes. A set refers to three OCT volumes taken
from different positions on the sample, such that all volumes in
the set overlap. After the first volume was collected, the sample
was displaced by a small amount for collection of the second volume: the displacement took the form of a small rotation and/or
translation along any dimension. A third volume was collected
after further displacing the sample by a large amount relative
to the previous position; this displacement was primarily along
one of the six DOF. This acquisition protocol creates a loop
between the three volumes (i.e., sequential volumes overlap, as
do the first and last volumes). In the ideal case, the transformation between Volumes 1 and 3 should be the same independent
of whether that transformation is arrived at by applying T1→3
directly or by applying T1→2→3 = T2→3 T1→2 . The accuracy
of the registration was assessed as the displacement error between these two volumetric transformations. Namely, let the
displacement error be defined as
=

N
1 
(T1→3 − T2→3 T1→2 )pn 2
sN n =1

error between the relative poses of two volumes (T0→j T−1
0→i )
):
and the PW volume transformation (TPW
i→j

(5)

where s is the number of voxels along an edge of the volume
and pn is any point contained within a volume (e.g., s = 512
and the coordinates of pn are between 0 and 511 for volumes
comprising 5123 voxels, as in our collected datasets). To converge close to the actual average displacement error, we choose
to randomly sample N = 50 000 points. A similar computation
for the image registration accuracy can be implemented by replacing a rigid transformation, Ti→j , with the corresponding
OCT homography, HOCT(i)→OCT(j ) .
Volumes were collected from ex vivo pig bladder for our evaluation. We acquired at least 13 sets (comprising three volumes
each) for each DOF. For each DOF, the first two volumes in
each set were the same for all sets and differed only by a small
displacement; the displacement of the third volume was selected
to span the range of desired translations or rotations.
C. Assessment of Bundle Adjustment
We defined a quantitative error metric to demonstrate the effect of the BA step. The error metric describes the displacement

N

1 
−1
(TPW

i→j − T0→j T0→i )pn 2 .
sN n =1

(6)

We evaluated this displacement error for all overlapping volume
ﬁnal
pairs for poses defined before (Tinit
0→i ) and after BA (T0→i ).
The evaluation was performed for two datasets: biological
and phantom data each comprising 30 or more volumes. The
motion between sequential volumes in a given set was primarily
translational, which represents the expected dominant motion
of a cystoscope over a small region of the bladder similar in
size to the dimensions of the mosaic we acquired. The translations were on the order of 1 mm, a magnitude that provides
sufficient overlap between sequential volumes and comparable
displacements to other handheld probes for in vivo mosaicing
of microscopic images [32], [38].
D. Assessment of Visual Quality of Mosaics
We evaluated the effects of BA qualitatively by displaying
cross sections of the 3-D mosaic before and after BA using the
dataset described in Section IV-C. We rendered the mosaics as
follows. The first volume was positioned at the origin of a clean
canvas. Subsequent volumes were inserted sequentially into the
canvas based on their positions in the global coordinate system.
A simple averaging technique was used to achieve a blended visual appearance upon the addition of each new volume: 1) Two
canvases sized to hold the updated mosaic were created containing the current mosaic and the new volume in its proposed
global position, respectively. 2) Pixels with nonzero values in
both canvases were averaged (i.e., locations where the new volume overlaps with existing volumes in the mosaic), while pixels
with a nonzero value in only one of the volumes maintained their
grayscale value. Other more complex techniques for blending
volumes exist (e.g., multiband blending [7]) but are more computationally expensive; the exploration of alternative blending
algorithms is outside the scope of this paper. We evaluated the
visual quality of mosaics using the same dataset as described in
Section IV-C.
V. RESULTS AND DISCUSSION
A. Run-Time Comparison
Table II compares the run time for PW volume registration
with our method and N-SIFT for three volume sizes: 1282 ×256,
2562 ×512 and 5122 ×512. The reported timing is representative
of the time to compute the transformation between an arbitrary
pair of sequential volumes: for every pair excluding the first pair,
features from the first volume of the pair are computed during
the prior PW registration; thus, one need only compute features
for the second volume in the pair.
For all volume sizes, our results demonstrate orders of magnitude lower run time compared to the gold standard, N-SIFT
(80–2000× faster), with better improvements demonstrated for
larger volumes. The significant run time of N-SIFT can be attributed to the high computational cost of computing 3-D feature

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

TABLE II
PROCESSING TIMES FOR PW REGISTRATION (STEP 1) BROKEN DOWN BY
INDIVIDUAL PIPELINE STEPS FOR OUR APPROACH AND N-SIFT ALGORITHMS
FOR THREE VOLUME SIZES

2149

optimized approach runs in about one-fifth of the time. This
result underscores the computational benefits of replacing the
3D-correspondence search with a 2-D one, as we do.
B. Quantitative Assessment of PW Registration and
Transform Tolerances

descriptors: for example, the amount of time to match features
for the smallest volume size is approximately 1 min, while it
takes 15 min to extract features and compute their descriptors. Note, however, that only a subset of the correspondences
determined from the largest volume size is necessary to accurately determine the rigid transformation between two fullsized volumes. We experimentally determined that a volume
size of 1282 × 256 provides a sufficient number of inlier correspondences to generate an accurate rigid transformation with
N-SIFT. Thus, a modified version of N-SIFT requiring reduced
computation time could be devised by only detecting features in
a well-selected smaller subvolume (i.e., a sub-volume with high
overlap with other subvolumes, such as WhiLE-NS); however,
our approach run on the largest volume size is still more than an
order of magnitude faster than when the modified N-SIFT approach is applied to a smaller subvolume (i.e., 54.2 s compared
with 16 min).
Table II also reveals that the bottleneck step in our approach
is the surface extraction time, which accounts for nearly 75% of
the algorithm run time. We made no conscious attempt to optimize this step (or others); hence, future improvements to this
step to optimize its performance or exploit its “pleasingly parallel” nature will lead to drastic improvements in the algorithm
performance overall.
Accurate comparisons of the run-time performance of our
method with other proprietary algorithms reported in the literature is difficult due to differences in the sizes and quality of
samples (e.g., the quantity of features may vary) in the datasets
and differences in the hardware, suggesting a need for the research community to establish standardized datasets and hardware for comparing algorithms. Nevertheless, the run time of
our approach is superior to results reported for other ultrasound
and OCT volumetric registration works [10], [13], [15]. To our
knowledge, one of the fastest registration approaches, Ni et.
al., reports about 1 min of processing time per volume pair
for a volume size of 256 × 191 × 248 using a proprietary
version of 3-D-SIFT [13]; for a similarly sized dataset, our un-

We assessed the quantitative accuracy of the algorithm to determine how well our algorithm compares with N-SIFT (using
WhiLE-NS). Although N-SIFT is supposed to be robust to rotational translations, we and others have noted that the available
open-source version is not [35]. We also noted that this version
of N-SIFT performed poorly over z-axis translations, likely due
to nonuniform changes in intensity associated with the shifting
focus and further suggesting poor illumination invariance of the
open-source N-SIFT. Thus, to avoid exaggerating the performance of our algorithm, we only report the results for x- and
y-axis translations, where the open-source version of N-SIFT
was noted to be well-behaved. We measured the displacement
error for 14 three-volume sets representing either x-axis or yaxis translations with magnitudes under 60% translation and
report the average of displacement error over these sets. The
WhiLE-NS algorithm had a displacement error of 3.0 ± 1.4
voxels (or 0.59 ± 0.27% of the total 512 voxels in any axial or
lateral dimension), while our algorithm had an error of 1.9 ±
1.2 voxels (or 0.37 ± 0.23%). Thus, even in regions where the
open-source N-SIFT algorithm performs well, the magnitude of
the error is comparable to that obtained with our approach, with
our approach performing slightly better.
We also assessed the quantitative accuracy of the algorithm
to determine the range of transformations within which our PW
registration algorithm works best. Fig. 5 shows how the relative
displacement error [see (5)] changes as a function of increasing
translation or rotation from Volume 3 to Volumes 1 and 2 for
both the 2-D OCT homography (result of white light registration
after Step 1B) and the 3-D rigid volume transformation (result
of ICP after Step 1D). In each case, the transformation applied
to Volume 3 was primarily along a single DOF. The magnitude
of the translation or rotation is computed as the average value
of the DOF calculated from the transformation matrices T1→3
and T2→3 . The magnitude of the transformation is reported as
a function of angle for rotations about the given axis and as a
function of the percentage of the translation to the edge length
of a volume for translations.
Note that the reported error is a relative measurement between two transformation sequences [see (5). As such, it is not
possible to attribute all of the error to a particular transformation
term (i.e., T1→2 , T2→3 , or T1→3 ). However, because Volumes
1 and 2 remain the same for all data points in a given subplot,
any change in the displacement error (i.e., difference between
individual points on a given subplot) can be attributed to transformations involving Volume 3 and could reflect the quality of
the features in that volume, among other things. The reported
error is also a function of the volume size because it is a function of the inverse of the edge length; as well, the magnitude
of errors associated with rotations scales with radial distance
from the pivot point. All reported errors are for the full volume

2150

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

TABLE III
DISPLACEMENT ERROR (IN VOXELS) BETWEEN RELATIVE POSES AND PW
TRANSFORMATIONS (6) BEFORE AND AFTER BA

Fig. 5. Displacement error associated with 2-D image registration (after Step
1B) and 3-D volumetric registration (after Step 1D) for a bladder tissue sample
as a function of angle of rotation (a)–(c) and percent translation (d)–(f). Volume
edge length = 512 pixels (100%). Each data point represents the displacement
error from a single three-volume set, and the data from each subplot are captured
from the same location in the tissue. The displacement error is computed as the
fraction of voxel position mismatch to volume edge length between two transformations that comprise a loop using (5). The range of transformations presented
gives a suggested maximum tolerance for the specified DOF (1% error).

size we collected (5122 × 512), although smaller errors would
result for smaller volumes. Nonetheless, we can still define a
size-independent, absolute error threshold to illustrate that volumes appear accurately registered for transformations within
the performance boundaries. Based on pilot experiments, the
mismatch between two volumes becomes visibly noticeable in
a B-scan only if greater than 5–10 pixels. Hence, 5 pixels of
error, which corresponds to a roughly 1% of the edge length
of the volume (512), seems tolerable. Furthermore, five pixels
of error corresponds to approximately 25 μm, which is half the
thickness of the urothelium (the thinnest bladder wall layer).
Thus, even with the maximum displacement error of 25 μm, the
individual bladder wall layers will still be distinct.
At least a portion of the observed 3-D displacement error can
be attributed to inaccurate registration of the 2-D images through
SIFT. As image registration through SIFT is one of the initial
steps in the pipeline, we expect its limited accuracy to propagate
to the volumetric registration step. In particular, the observed
lack of invariance to rotations out of the plane (i.e., rotations
about the x-and y-axes),which manifests as rising displacement
error as a function of degree of rotation, is characteristic of SIFT
and occurs both for the 3-D rigid transformations and the the
2-D homography [see Fig. 5(a) and (b)].
The invariance of SIFT to the other four DOF suggests that
the observed errors for large transformations for these DOF
must originate elsewhere. Our algorithm is seemingly invariant
to z-axis translations over a reasonable range of displacements
[see Fig. 5(f)], and the large error that appears when there is less
than 55% overlap (i.e., 45% translation or 1.15 mm) can likely
be attributed to the loss of signal quality in the OCT volume due

to the finite depth of focus, which leads to difficulty in extracting
the surface in Step 1C. Similarly for in-plane translations [see
Fig. 5 (d) and (e)], the error creeps when the percent translation approaches 80–90%, meaning the overlap between the two
volumes—particularly their surfaces—is small (<20%). Note
that the overlap is still quite large for the white light images in
this case because of its large FOV, so the error for the image
registration remains low.
In general, we observe that the displacement error for the
image registration step acts as a lower bound for the volumetric displacement error. While the ICP step should correct for
minor inaccuracies from image registration, the performance
of ICP is largely dependent on the quality of the surface extraction, the initial estimate of the rigid transformation, and the
amount of texture of the sample (e.g., it would be challenging
to correct initial misalignments in a flat sample). All told, the
algorithm shows invariance to in-plane transformations when
the overlap is sufficient (e.g., at least 20%), and robustness to
small out-of-plane translations and rotations (e.g, 1 mm and
16◦ , respectively). We expect the majority of transformations in
endoscopy to be translations with small rotations—suggesting
that the performance of our algorithm is sufficient to mosaic
endoscopic frames robustly.
The current evaluation does not strictly account for transformations comprising multiple DOF simultaneously; however,
our transformation matrices consider each DOF as a separable
effect, so our analysis of the performance for individual DOFs
well represents the anticipated result for transformations involving multiple, simultaneous DOF expected in true freehand
motion.
C. Quantitative Assessment of Bundle Adjustment
Table III shows the displacement error [see (6)] between the
computed PW transformations (TPW
i→j ) and relative poses before
ﬁnal
(Tinit
)
and
after
(T
)
BA.
Prior
to BA, poses are determined
i→j
i→j
exactly by the sequential PW transformations computed in Step
1. As a result, the displacement error between the relative pose
and PW transformation from sequential volumes pairs (“sequential error”) is identically zero. The loop error represents the
displacement error between nonsequential, overlapping volume
pairs, and is relatively large prior to BA. As expected, BA reduces the loop error to yield modest outcomes for both the loop
and sequential errors: on average less than two pixels (or 10 μm
with our dataset). This reduced error is attributed to the fact that
BA optimizes the poses of each volume by accounting for all
computed PW transformations, which effectively distributes the

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

2151

Fig. 6. Phantom mosaic before and after BA. White arrows indicate areas that are noticeably improved by BA. The mosaic contains 30 volumes and is 13.3 mm
× 6.6 mm × 3.2 mm. Yellow and green lines indicate the location of the B-scan and range of en face images that are averaged to create the SVP, respectively.
Locations of the magnified SVP images are indicated with blue boxes.

error between loop volume pairs across the entire loop. Hence,
post-BA, the loop and sequential errors are nearly the same.
D. Automated Large-Area Mosaics
Figs. 6 and 7 demonstrate the ability of our algorithm to generate large-area mosaics of feature-poor samples. For the phantom, note that imperfections due to the limited resolution of the
3-D printer contribute to some blurriness and other nonidealities in both pre- and postbundle-adjusted images (e.g., spokes
of the “O,” bulge on upper right of “B”). Furthermore, the rough
surface texture of the layer above the embedded “SBO” pattern
contributes to nonuniform shading of the logo (due to variations in the total attenuation caused by local variations in the
thickness of the scattering layer). Close inspection of the SVP
and selected B-scans reveals some locations where there is poor
alignment between the volumes prior to BA. For example, these
misalignments can manifest as ghosting (“S”) and apparent discontinuities (“B” and “O”) of the letter boundaries in the SVP
(see Fig. 6, arrows). As expected, such mismatches are improved
after BA, which validates the effectiveness of the BA algorithm
on mosaic appearance.
Similarly, Fig. 7 (a) and (b) shows an example B-scan and representative en face slice of bladder tissue after BA. The B-scan
shows a clear delineation between the second bright layer of
the bladder (lamina propria) and the striated muscularis propria
layer below it, as well as continuous striations in the muscularis propria. The clear appearance of these anatomical features
implies that the mosaicing algorithm maintains the qualitative
appearance of OCT bladder images. The en face slice also shows
hints of these individual layers where the tissue is tilted on the
sample stage: the upper left corner is urothelium while the lower
right corner is muscularis propria. Overall, the images appear

well registered; however, the limited number of loops acquired
with the dataset limits the effectiveness of the BA step in this region (i.e., there were few nonsequential, overlapping volumes).
Fig. 7(c) shows an example of two nonsequential volumes that
are derived from an area of the mosaic with “poorly constrained”
volumes, while Fig. 7(d) displays a contrasting example of
two nonsequential volumes from “well-constrained” volumes.
A poorly (well)-constrained volume is one with few (many)
loop constraints or low (high) connectivity to the volumetransformation graph described in Section II-G. As expected,
the volume pair from the constraint-poor region shows minimal
improvement after BA; however, the improvement is evident
for the pair from the constraint-rich region. These results underscore the importance of collecting data using scan patterns with
frequent loops. Further investigation is needed to understand
the requirements for loop constraints to limit perceptual errors.
Additionally, it may be possible to use white light data to further
refine the BA step to overcome the limitations of mosaics that are
poorly connected on the basis of their volumetric datasets alone.
VI. CONCLUSION
We present an automated mosaicing algorithm for generating
large FOV for OCT volumes with 6 DOF and a BA step. We
demonstrate the performance of the algorithm using bladder
tissue and a custom phantom to create mosaics comprising up to
33 volumes spanning an area of 13.0 mm × 10.1 mm × 3.9 mm,
with lateral and axial resolutions of 15 and 7.5 μm, respectively.
The total time to generate the mosaics (excluding rendering)
was approximately 33 min each.
Our method reduces the computational complexity of the
volumetric (3-D) registration problem to two dimensions
by using coregistered white light data and a unique OCT

2152

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 7, JULY 2014

Fig. 7. Bladder tissue mosaic after BA: (a) representative B-scan and (b) en face images. Green and yellow lines indicate the location of en face and B-scan
slices, respectively. The mosaic consists of 33 volumes and is 13.0 mm × 10.1 mm × 3.9 mm in size. Examples of the effect of BA on PW volume registration of
B-scans (top) and en face images (bottom) in (c) poorly constrained versus (d) well-constrained locations comprising few or many loops, respectively. Locations
of these example volume pairs are indicated with blue boxes. Arrows indicate areas of mismatch that are noticeable prior to BA.

surface-extraction step. We demonstrate that this reduction in
computational complexity manifests as a significant speedup
in processing time in comparison with an open source N-SIFT
package, and is also faster than other N-SIFT-based results reported in the literature. Furthermore, our method can accurately
register feature-poor volumetric pairs with translations along
and rotations about all three axes and can tolerate modest outof-plane transformations over a range of motions expected for
endoscopic examination of tissue.
The currently described algorithm does not account for motion artifacts (e.g., motions within the acquisition of an individual volume). If necessary, future work may seek to augment our
algorithm to also include steps that register individual A-scans
or B-scans within a volume affected by motion artifacts [15],
[39], [40] or augment BA with a local optimization step [32].
Additionally, the improvements gained from our BA step are
currently limited by the connectivity of the graph generated
from the volumetric data, but further improvements are likely
possible by using additional information from the white light
image registration step to provide a better initialization to the BA
step.
In conclusion, we demonstrate the ability to automatically
create large FOVs for feature-poor biological tissue, which is
an important step toward extending the use of OCT to a number
of new and emerging clinical applications, such as in large,
irregularly shaped organs like the bladder, stomach, and colon.

ACKNOWLEDGMENT
The authors would like to thank Dr. R. Luong and E. Godoy
for access to the pig bladder samples, G. T. Smith for assistance
with the phantom fabrication, and Dr. J. C. Liao, Dr. S. Farsiu,
and Dr. D. L. Rubin for valuable discussions.
REFERENCES
[1] X. D. Li, S. A. Boppart, J. V. Dam, H. Mashimo, M. Mutinga, W. Drexler,
M. Klein, C. Pitris, M. L. Krinsky, M. E. Brezinski, and J. G. Fujimoto,
“Optical coherence tomography: advanced technology for the endoscopic
imaging of Barrett’s esophagus,” Endoscopy, vol. 32, no. 12, pp. 921–930,
Dec. 2000.
[2] C. Pitris, C. Jesser, S. A. Boppart, D. Stamper, M. E. Brezinski, and
J. G. Fujimoto, “Feasibility of optical coherence tomography for highresolution imaging of human gastrointestinal tract malignancies,” J. Gastroenterol, vol. 35, no. 2, pp. 87–92, Jan. 2000.
[3] S. P. Lerner, A. C. Goh, N. J. Tresser, and S. S. Shen, “Optical coherence
tomography as an adjunct to white light cystoscopy for intravesical realtime imaging and staging of bladder cancer,” Urology, vol. 72, no. 1,
pp. 133–7, Jul. 2008.
[4] M. T. J. Bus, B. G. Muller, D. M. de Bruin, D. J. Faber, G. M. Kamphuis,
T. G. van Leeuwen, T. M. de Reijke, and J. J. M. C. H. de la Rosette,
“Volumetric in vivo visualization of upper urinary tract tumors using
optical coherence tomography: A pilot study,” J. Urol., vol. 190, no. 6,
pp. 2236–2242, Dec. 2013.
[5] H. Wang, W. Kang, H. Zhu, G. MacLennan, and A. Rollins, “Threedimensional imaging of ureter with endoscopic optical coherence tomography,” Urology, vol. 77, no. 5, pp. 1254–1258, 2011.
[6] H. Ren, W. C. Waltzer, R. Bhalla, J. Liu, Z. Yuan, C. S. D. Lee, F. Darras,
D. Schulsinger, H. L. Adler, J. Kim, A. Mishail, and Y. Pan, “Diagnosis of

LURIE et al.: AUTOMATED MOSAICING OF FEATURE-POOR OPTICAL COHERENCE TOMOGRAPHY VOLUMES

[7]
[8]
[9]
[10]

[11]

[12]

[13]

[14]
[15]

[16]
[17]

[18]
[19]

[20]

[21]
[22]
[23]

bladder cancer with microelectromechanical systems-based cystoscopic
optical coherence tomography,” Urology, vol. 74, no. 6, pp. 1351–1357,
Dec. 2009.
M. Brown and D. G. Lowe, “Automatic panoramic image stitching using
invariant features,” Int. J. Comput. Vis., vol. 74, no. 1, pp. 59–73, Dec.
2006.
B. Triggs and P. McLauchlan, “Bundle adjustment: A modern synthesis,”
Vis. Algorithms, vol. 1883, pp. 298–372, 2000.
A. Pavaskar, “Tools for creating wide-field views of the human retina
using Optical Coherence Tomography,” Master’s thesis, 2009. Available:
http://epublications.marquette.edu/theses_open/106
M. Niemeijer, M. K. Garvin, K. Lee, B. van Ginneken, M. D. Abramoff,
and M. Sonka, “Registration of 3-D spectral OCT volumes using 3-D
SIFT feature point matching,” in Proc. SPIE, 2009, vol. 7259, pp. 72591I1–72591I-8.
M. Niemeijer, K. Lee, M. K. Garvin, M. D. Abràmoff, and M. Sonka,
“Registration of 3-D spectral OCT volumes combining ICP with a graphbased approach,” Proc. SPIE, vol. 8314, pp. 83141A-1–83141A-9, Feb.
2012.
R. Zawadzki and A. Fuller, “Improved representation of retinal data acquired with volumetric FD-OCT: Co-registration, visualization and reconstruction of a large field of view,” Proc. SPIE, vol. 6844, pp. 68440C-1–
68440C-8, 2008.
D. Ni, Y. Qul, X. Yang, Y. P. Chui, T.-T. Wong, S. S. M. Ho, and P. A. Heng,
“Volumetric ultrasound panorama based on 3-D SIFT,” in Proc. Med.
Image Comput. Comput.-Assist. Intervention, Jan. 2008, vol. 11, no. Pt 2,
pp. 52–60.
T. C. Poon and R. N. Rohling, “Three-dimensional extended field-of-view
ultrasound,” Ultrasound Med. Biol., vol. 32, no. 3, pp. 357–369, Mar. 2006.
H. C. Hendargo, R. Estrada, S. J. Chiu, C. Tomasi, S. Farsiu, and J. A. Izatt,
“Automated non-rigid registration and mosaicing for robust imaging of
distinct retinal capillary beds using speckle variance optical coherence
tomography,” Biomed. Opt. Exp., vol. 4, no. 6, pp. 803–821, May 2013.
Y. Li, G. Gregori, B. L. Lam, and P. J. Rosenfeld, “Automatic montage
of SD-OCT data sets,” Opt. Exp., vol. 19, no. 27, pp. 26239–26248, Dec.
2011.
R. J. Zawadzki, S. S. Choi, A. R. Fuller, J. W. Evans, B. Hamann, and
J. S. Werner, “Cellular resolution volumetric in vivo retinal imaging with
adaptive optics-optical coherence tomography,” Opt. Exp., vol. 17, no. 5,
pp. 4084–4094, Mar. 2009.
T. D. Soper, M. P. Porter, and E. J. Seibel, “Surface mosaics of the bladder
reconstructed from endoscopic video for automated surveillance,” IEEE
Trans. Biomed. Eng., vol. 59, no. 6, pp. 1670–1680, Jun. 2012.
R. Miranda-Luna, C. Daul, W. C. P. M. Blondel, Y. Hernandez-Mier,
D. Wolf, and F. Guillemin, “Mosaicing of bladder endoscopic image sequences: Distortion calibration and registration algorithm,” IEEE Trans.
Biomed. Eng., vol. 55, no. 2 Pt 1, pp. 541–553, Feb. 2008.
Y. Hernández-Mier, W. C. P. M. Blondel, C. Daul, D. Wolf, and
F. Guillemin, “Fast construction of panoramic images for cystoscopic
exploration,” Comput. Med. Imag. Graph., vol. 34, no. 7, pp. 579–592,
Oct. 2010.
R. Hartley and A. Zisserman, Multiple View Geometry in Computer Vision,
Cambridge Univ. Press, 2000.
J. Heikkila and O. Silven, “A four-step camera calibration procedure with
implicit image correction,” in Proc. IEEE Comput. Vis. Pattern Recog.,
1997, pp. 1106–1112.
M. Gora, K. Karnowski, M. Szkulmowski, B. J. Kaluzny, R. Huber,
A. Kowalczyk, and M. Wojtkowski, “Ultra high-speed swept source OCT
imaging of the anterior segment of human eye at 200 kHz with adjustable
imaging range,” Opt. Exp., vol. 17, no. 17, pp. 14880–14894, Aug. 2009.

2153

[24] H. Y. Lee, H. Sudkamp, T. Marvdashti, and A. K. Ellerbee, “Interleaved
optical coherence tomography,” Opt. Exp., vol. 21, no. 22, pp. 26542–
26556, Oct. 2013.
[25] W. Wieser, B. R. Biedermann, T. Klein, C. M. Eigenwillig, and R. Huber,
“Multi-megahertz OCT: High quality 3-D imaging at 20 million A-scans
and 4.5 GVoxels per second,” Opt. Exp., vol. 18, no. 14, pp. 14685–14704,
Jul. 2010.
[26] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
Int. J. Comput. Vis., vol. 60, no. 2, pp. 91–110, Nov. 2004.
[27] A. Vedaldi, B. Fulkerson, and B. Fulerson, “VLFeat: An open and portable
library of computer vision algorithms,” 2008.
[28] M. Fischler and R. Bolles, “Random sample consensus: A paradigm for
model fitting with applications to image analysis and automated cartography,” Commun. ACM, vol. 24, no. 6, pp. 381–395, 1981.
[29] P. Kovesi. MATLAB and Octave functions for computer vision and
image processing. [Online]. Available: http://www.csse.uwa.edu.au/∼
pk/research/matlabfns
[30] S. Rusinkiewicz and M. Levoy, “Efficient variants of the ICP algorithm,”
in Proc. IEEE Int. Conf. 3-D Digit. Imag. Model., 2001, pp. 145–152.
[31] T. Duckett, S. Marsland, and J. Shapiro, “Fast, on-line learning of globally
consistent maps,” Auton. Robot., vol. 12, no. 3, pp. 287–300, 2002.
[32] K. E. Loewke, D. B. Camarillo, W. Piyawattanametha, M. J. Mandella,
C. H. Contag, S. Thrun, and J. K. Salisbury, “In vivo micro-image mosaicing,” IEEE Trans. Biomed. Eng., vol. 58, no. 1, pp. 159–171, Jan.
2011.
[33] G. Grisetti and C. Stachniss, “A tree parameterization for efficiently computing maximum likelihood maps using gradient descent,” Robot., Sci.
Syst., 2007.
[34] K. L. Lurie, G. T. Smith, S. A. Khan, J. C. Liao, and A. K. Ellerbee,
“Three-dimensional, distendable bladder phantom for optical coherence
tomography and white light cystoscopy,” J. Biomed. Opt., vol. 19, no. 3,
p. 036009, Mar. 2014.
[35] S. Allaire, J. J. Kim, S. L. Breen, D. A. Jaffray, and V. Pekar, “Full orientation invariance and improved feature selectivity of 3-D SIFT with
application to medical image analysis,” in Proc. Comput. Vis. Pattern
Recog. Workshop, 2008, pp. 1–8.
[36] W. Cheung and G. Hamarneh, “n-SIFT: n-dimensional scale invariant
feature transform,” IEEE Trans. Image Process., vol. 18, no. 9, pp. 2012–
2021, 2009.
[37] K. L. Lurie and A. K. Ellerbee, “Volumetric mosaicing for optical coherence tomography for large area bladder wall visualization,” Proc. SPIE,
vol. 8926, pp. 89261P-1–89261P-9, Mar. 2014.
[38] T. Vercauteren, A. Perchant, X. Pennec, and N. Ayache, “Mosaicing of
confocal microscopic in vivo soft tissue video sequences,” Med. Image
Comput. Comput.-Assist. Intervention, vol. 8, no. Pt 1, pp. 753–760, Jan.
2005.
[39] S. Ricco, M. Chen, H. Ishikawa, G. Wollstein, and J. Schuman, “Correcting motion artifacts in retinal spectral domain optical coherence tomography via image registration,” Med. Image Comput. Comput.-Assist. Interv.,
vol. 12, no. Pt 1, pp. 100–107, Jan. 2009.
[40] M. F. Kraus, B. Potsaid, M. A. Mayer, R. Bock, B. Baumann, J. J. Liu,
J. Hornegger, and J. G. Fujimoto, “Motion correction in optical coherence tomography volumes on a per A-scan basis using orthogonal scan
patterns,” Biomed. Opt. Exp., vol. 3, no. 6, pp. 1182–1899, Jun. 2012.

Authors’, photographs and biographies not available at the time of publication.

