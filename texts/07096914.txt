794

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Imitation of Dynamic Walking With BSN
for Humanoid Robot
Krittameth Teachasrisaksakul, Zhi-Qiang Zhang∗ , Guang-Zhong Yang, and Benny Lo

Abstract—Humanoid robots have been used in a wide range
of applications including entertainment, healthcare, and assistive
living. In these applications, the robots are expected to perform
a range of natural body motions, which can be either preprogrammed or learnt from human demonstration. This paper proposes a strategy for imitating dynamic walking gait for a humanoid
robot by formulating the problem as an optimization process. The
human motion data are recorded with an inertial sensor-based
motion tracking system (Biomotion+). Joint angle trajectories are
obtained from the transformation of the estimated posture. Key
locomotion frames corresponding to gait events are chosen from
the trajectories. Due to differences in joint structures of the human
and robot, the joint angles at these frames need to be optimized
to satisfy the physical constraints of the robot while preserving
robot stability. Interpolation among the optimized angles is needed
to generate continuous angle trajectories. The method is validated
using a NAO humanoid robot, with results demonstrating the effectiveness of the proposed strategy for dynamic walking.
Index Terms—Balance control, body sensor network, dynamic
walking, human motion imitation, humanoid robot, wearable
sensors.

I. INTRODUCTION
UMANOID robots have been used in a wide range of applications that require human–robot interaction including
entertainment [1]–[3], healthcare [4], [5], and assistive living
[6]. They have also been used for assisting children with learning difficulties [7] to encourage social interaction [8], [9] and
provide verbal and/or physical encouragement [10]. One of the
challenges for such applications is how to make the humanoid
robots perform different human movements naturally. In practice, the imitation of upper limb movement is relatively simple,
since robot stability is easy to control if light weight arms are
used [11]. For whole body movements, however, motion imitation is more challenging. Dynamic walking, for example, is
difficult for the humanoids to imitate, as existing robots lack a
human’s complex set of autonomic sensorimotor balance and
control.
Thus far, human walking reproduction on humanoid robots
has been mainly conducted through motion generation. For
example, Gouaillier et al. [12] generated an omni-directional
walking motion for NAO robot based on a preview controller.

H

Manuscript received November 8, 2014; revised March 27, 2015; accepted
April 16, 2015. Date of publication April 28, 2015; date of current version May
7, 2015. Preliminary results of this paper were presented at BSN 2014, Zurich,
Switzerland. (Corresponding author: Zhi-Qiang Zhang).
The authors are with the Hamlyn Centre, Imperial College London, London,
SW7 2AZ, U.K. (e-mail: kt1312@imperial.ac.uk; z.zhang@imperial.ac.uk;
benny.lo@imperial.ac.uk; g.z.yang@imperial.ac.uk).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2015.2425221

Massah et al. [13] introduced a method for planning the robot’s
walking on slopes. Czarnetski et al. [14] controlled NAO robot’s
walking by using the preview controller and closed-loop control
with sensor feedback. Li et al. [15] generated walking patterns
for COMAN, a compliant humanoid robot, by controlling the
center of mass (CoM). However, these methods involve computation of joint trajectories for robot’s motion by imposing stability constraints, but the resulting movement may not be natural
or human-like because not all degrees of freedom (DOFs) are
used. Furthermore, generating sophisticated motion sequences
is challenging due to the large number of DOFs involved.
To overcome these problems, motion imitation, which employs human motion capture to acquire motion data and joint
angle trajectories directly, has been explored recently. However,
one-to-one direct mapping of joint angle of a human to a humanoid robot is not practical because of significant differences
in joint configurations between human and a humanoid robot.
Moreover, instability and motion beyond the physical capability
of the robot need to be considered when a humanoid performs
the mapped motion. Desirable robot motion must have intrinsic
resemblance to human movement while maintaining the balance
of the robot and respecting its mechanical constraints. To this
end, several solutions have been proposed. For instance, gesture
replication by Gaertner et al. [16] employed nonlinear optimization to satisfy mechanical limits of the robot and desired hand
positions and maximize angular similarity between human and
a robot. Pollard et al. [17] scaled each joint’s angular trajectory
locally in order to satisfy joint limits of the robot. Ott et al.
[18] achieved motion imitation by using Cartesian tracking.
In this method, a set of control points on the humanoid robot
were virtually linked to measured marker positions on a subject’s body via springs. Dariush et al. [19] enabled upper-body
motion replication by a humanoid robot by tracking motion descriptors defined in Cartesian space while fulfilling the required
constraints. Suleiman et al. [20] represented mechanical limits
by minimizing differences in joint angles of human and robot
based on recursive dynamics. Jingru and Hauser [21] converted
human motion into a robot’s motion through minimization of
time and angular differences between trajectories of a human
subject and a robot. All of the above methods dealt with only
upper body movements, and they did not consider robot stability
due to the exclusion of lower body movements.
For imitation of full body human motion, the main focus
of existing methods is on balance maintenance. Two stability
criteria which are used in these studies include Zero Moment
Point (ZMP) [22] and CoM. The general pipeline of balance
maintenance involves designing ZMP trajectory for a humanoid
robot, computing reference CoM trajectory from the ZMP

2168-2194 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

TEACHASRISAKSAKUL et al.: IMITATION OF DYNAMIC WALKING WITH BSN FOR HUMANOID ROBOT

trajectory, and constraining a humanoid robot to follow the reference CoM trajectory. Kim et al. [23] proposed a method for
imitating full body dance movements. The ZMP trajectory of
the robot was generated based on the support region and used
to compute reference CoM trajectory by recursive equations.
Under this scheme, the robot’s pelvis was forced to follow the
reference CoM to maintain balance. Hu and Lee [24] used human walking data to allow walking replication by a humanoid
robot. The robot’s ZMP trajectory was designed by projecting
pelvis position according to support area then reference CoM
trajectory was obtained by a preview controller. Closed-loop
inverse kinematics was applied to follow human end-effector
positions. Koenemann and Bennewitz [25] performed whole
body motion imitation by finding valid foot positions and applying inverse kinematics to compute lower body joint angles.
However, the method was validated using standing on one leg
motion, not walking motion and considered only static stability, not dynamic stability. Boutin et al. [26] imitated human
walking. The ZMP trajectory of the robot was derived based
on foot trajectories, and the CoM trajectory was also generated
by a preview controller. The optimization algorithm for inverse
kinematics was employed to find the joint angles that satisfy
constraints on the swing foot and the reference CoM. However,
these methods are all based on optical motion tracking, such as
BTS [27] and Vicon [28]. Despite the highly accurate results
they can provide, the use of optical tracking involves expensive cost and complex installation, limiting them to laboratory
environment only. In addition, the use of optical motion data
complicates the process of motion imitation because marker locations acquired from the optical motion tracker has to be scaled
to fit to a humanoid robot before transformed into joint angles
by inverse kinematics [29], [30]. Furthermore, all of these approaches require complex computations of the stability criteria
and consideration of all motion frames in order to generate robot
motion, which involves extensive computation load in practice.
To this end, several methods which performed optimization to
minimize error between movements of human and a humanoid
robot and interpolation between discrete time points have been
proposed [16], [19], [20], [21]. However, these methods applied
both optimization and interpolation to upper body joint angles
only.
In this paper, we applied wearable inertial sensor-based systems to acquire human motion, which can offer a similar tracking accuracy of optical system, but overcomes the limitation on
cost and usability. We further extend our initial results reported
in [11], and focus on dynamic walking motion production by
performing optimization and interpolation to lower body joint
angles. The main contribution of this paper includes: 1) the task
of dynamic walking imitation is deemed an optimization process that seeks for joint configurations. Owing to inconsistencies
in the joint structures of human and a humanoid robot, a cost
function for minimizing angular differences between human trajectories and robot trajectories is proposed; 2) to maintain the
robot balance during dynamic waking, the CoM can be outside the support area for a short duration rather than keeping its
projection inside the foot support area throughout the motion.
A novel and simple CoM trajectory scheme is thus designed

Fig. 1.

795

Kinematic representation of the joints of a humanoid robot NAO H25.

to make the robot walking imitation dynamically stable; 3) to
simplify replication of walking, key motion frames in a gait
cycle are selected and the proposed optimization approach is
applied to these key frames only. Interpolation among the key
frames is then employed to generate continuous angle trajectories. The method is validated using a NAO humanoid robot, with
results demonstrating the effectiveness of the proposed strategy
for walking imitation.
II. DYNAMIC WALKING IMITATION METHOD
A. Biomotion+ and NAO Robot
For motion capture, the Biomotion+ platform developed by
the Hamlyn Centre has been used. It is an inertial sensor-based
motion tracking system involving sensor calibration, sensor to
body segment alignment, and body segment orientation estimation [31]–[33]. Capturing full body movement requires the
placement of nine sensor nodes on the sternum, upper arms,
forearms, thighs, and shanks of the subject. Each wearable node
includes a triaxis magnetometer, triaxis accelerometer, and gyroscope. The locations and the measurements of each node enable the estimation of segmental orientation. The orientations of
all segments allow the reconstruction of full body motion. The
humanoid robot used for this study is a NAO H25 robot. The
NAO robot has a height of 58 cm and the kinematic representation of the joints of NAO H25 is illustrated in Fig. 1. More
specifically, the neck of the robot has 2 DOFs. Each arm has
5 DOFs: 2 DOFs, 2 DOFs, and 1 DOF at the shoulder, elbow,
and wrist, respectively. Each leg has 5 DOFs: 2 DOFs, 1 DOF,
and 2 DOFs at the hip, knee, and ankle, respectively. The NAO
robot contains a total of 23 DOFs and the kinematic model of
the robot, however, contains 22 DOFs due to the exclusion of
hip yaw pitch joint axis [34].
B. Generation of Joint Angle Trajectory
For the Biomotion+ platform, quaternion with respect to the
global reference frame is used to represent orientations of the
segments, which are converted into joint angles. The conversion of the orientations into joint angles requires orientation
difference between any two adjacent body segments.

796

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Fig. 2. Constraints of the humanoid robot required in the proposed framework
for human walking imitation.

Given any two quaternion qm and qn , the orientation difference Δq from qm to qn is defined as
Δq = qn ⊗

−1
qm

(1)

where ࣹ is the quaternion multiplication [35]. The conversion
of orientation difference Δq into the rotational angles requires
the direction cosine matrix representation C(Δq):


C (Δq) = q42 − eT e I3×3 + 2eeT − 2q4 e×
(2)
where e and q4 are the vector and the scalar component of the
quaternion Δq, and e× is an operator for cross product. The
transformation of the rotation matrix into Euler angles is specific
to each joint. The angles of shoulder, elbow, hip, and knee joints
can be computed from the motion capture data. The rotation
sequence for computing the angles of shoulder, hip, and knee is
XZY. The sequence XZY defines rotations of a rotating reference
frame about its Z-axis, Y-axis, and X-axis, respectively. Shoulder
Pitch and Shoulder Roll, axes of a shoulder joint, are set to align
with the X and Z axes of the reference frame. Hip pitch and hip
roll, axes of a hip joint, align with the X and Z axes. Knee Pitch
of a knee joint aligns with the X-axis. Additionally, YZX and
YZY are the rotation sequences for computing elbow angles. To
acquire the angles of Elbow Yaw and Elbow Roll joint axes,
Y-axis angle of YZX and Z-axis angle of YZY are computed.
C. Walking Motion Imitation
1) Formulation of Walking Imitation Task as an Optimization
Problem: The task of motion imitation is deemed an optimization process that seeks for joint configurations. Maintaining
resemblance between robot motion and human motion is the
key objective of motion imitation. Therefore, the estimations of
robot joint angles vector β̂t at time t should satisfy
β̂t = argmin L(βt )

(3)

βt

where
L(βt ) = βt − αt 2

(4)

and βt and αt are the humanoid robot’s and a human demonstrator’s joint angle configurations at time t, respectively. Several
constraints, including mechanical constraint and stability constraints, as shown in Fig. 2, should be satisfied as well during the

Fig. 3. Development of reference CoM position in y-axis. The four dots
represent reference CoM positions in y-axis at the four key locomotion frames.
L is left foot and R is right foot.

optimization process. In the rest of this section, we will explain
these constraints in detail.
2) Balance Constraint: To maintain the robot balance during
static walking—the topic focused by our previous work [11]—
the projection of CoM on the ground should be inside the foot
support area. However, for dynamic walking robots, the CoM
can be outside the support area, which mainly happens when
the support leg changes from left to right or from right to left.
Therefore, a simple CoM y-axis trajectory scheme for a gait cycle is designed as shown in Fig. 3. In this figure, each gait cycle
is segmented into four phases: left-to-right transition, right foot
support, right-to-left transition, and left foot support. The gait
segment is based on four key locomotion frames, tKF1 , tKF2 ,
tKF3 , tKF4 , detected from human hip flexion angle and shank
angular velocity. To take the walking dynamics into consideration, the right-to-left (or left-to-right) transition should start
before the left (or right) heel touches the ground. In our implementation, tKF1 and tKF3 are taken as the time points when
maximum right and left hip flexion angles exist, as shown in
Fig. 4. Meanwhile, the right-to-left (or left-to-right) transition
should end before the right (or left) toe leaves the ground. Here,
we choose the left and right toe off points as the key frame
tKF2 and tKF4 , respectively. Toe off point is selected as a frame
boundary because it represents the ending of left foot support or
right foot support and it occurs after the point when foot is flat
so it allows more time for the transition of the CoM. Heel strike
is selected as a frame boundary because it represents the beginning of left foot support or right foot support. The derivation
of the y-axis of reference CoM point ytc,ref is shown in Fig. 3.
Because tKF1 and tKF3 are the moment prior to right heel strike
and left heel strike, respectively, ytc,ref is transferred from the
left to the right foot during the period between tKF1 and tKF2 ,
and vice versa during the period between tKF3 and tKF4 . Thus,
the y-axis of reference CoM point can be written as
⎧
y rf − ytlf
⎪
⎪
⎪
ytlf + t
, if tKF1 < t < tKF2
⎪
⎪
t − tKF1
⎪
⎪
⎨ y rf ,
if tKF2 ≤ t ≤ tKF3
t
(5)
ytc,ref =
lf
rf
⎪
yt − yt
⎪
rf
⎪
yt +
, if tKF3 < t < tKF4
⎪
⎪
⎪
t − tKF3
⎪
⎩ lf
yt ,
if tKF4 ≤ t ≤ tKF1

TEACHASRISAKSAKUL et al.: IMITATION OF DYNAMIC WALKING WITH BSN FOR HUMANOID ROBOT

797

Fig. 5. Development of x-axis reference CoM position. The dots represent the
reference CoM positions at the key locomotion frames. L is left foot and R is
right foot.

where the center position of the left foot xlfc
t and the center
position of the right foot xrfc
t are defined as
xlfc
t =

4


xlft ,u

u =1

Fig. 4. Criteria for selection of four key locomotion frames tK F 1 , tK F 2 ,
tK F 3 , tK F 4 . Left/Right hip flexion angles in degrees derived from the wearable
inertial motion capture system (upper) and left/right shank angular velocity
around x-axis in radians/second (lower) measured directly by the gyroscope in
a gait cycle. The angles and angular velocity of left and right leg are represented
by the normal lines and the dashed lines, respectively. The horizontal axes of
both plots represent the frame index of motion. Heel strikes of left and right feet
are represented by diamond-shaped points.

where ytlf and ytrf are the y-coordinates of the center of the left
foot and the right foot, respectively, at time t. tKF1 is the first key
frame of the next gait cycle. According to Fig. 4, the selection
of the key frames requires detection of toe-off event and the
moment when maximum hip flexion angles occur. The former,
i.e., tKF2 and tKF4 , are detected from negative peaks of shank
angular velocity, circular markers in Fig. 4(bottom). The latter,
i.e., tKF1 and tKF3 , are detected from maximum of hip flexion
angles, circular markers in Fig. 4 (top). These points occur after
the mid-swing phase and before a heel-strike event. The point
of mid-swing phase is characterized by positive peak of shank
angular velocity [36], diamond markers in Fig. 4(bottom).
The reference CoM position of the x-axis is also chosen based
on the four phases. For the left foot support or the right foot
can be
support phase, the reference CoM x-axis position xc,ref
t
taken as the support foot’s center:
xc,ref
=
t

1
4

4


xsp,k
t

(6)

k =1

where xsp,k
is the kth corner position of the support foot at time
t
can
t. As shown in Fig. 5, during the transition periods, xc,ref
t
be written as
⎧
rfc
lfc
⎪
⎪ xlfc + xt − xt , if tKF1 < t < tKF2
⎪
⎨ t
t − tKF1
=
(7)
xc,ref
t
⎪ rfc
xlfc
− xrfc
⎪
t
t
⎪
⎩ xt +
, if tKF3 < t < tKF4
t − tKF3

xrfc
t =

4


,v
xrf
t

(8)

v =1
,v
where xlft ,u is the uth corner position of the left foot and xrf
is
t
the vth corner position of the right foot. In our implementation,
we use the trunk or torso position as the CoM since it contains
the majority of the robot’s mass. The actual torso position is
computed by forward kinematics of the robot:

pct = Trans (Fr (βt , 0, 0))

(9)

where Trans( · ) is an operator for obtaining the translational
part of a transformation matrix and Fr (·) is the forward kinematics function, which can be written as
Fr (α, w1 , w2 ) =

w2
	

j −1

Tj (αj )

(10)

j =w 1

where w1 is the index of a base frame, w2 is the index of an endeffector frame, j −1 Tj is a transformation matrix from the jth link
frame to the (j − 1)th link frame of the robot’s kinematic model,
and αj is the angle of the jth joint. The stability constraint can
thus be written as
 ≤ εcp
pct − pc,ref
t

(11)

where εcp is a small threshold.
3) Foot Constraints: To make the robot walk more stably,
the swing foot should not be lower than the ground or lifted
too high. Therefore, the height of the swing foot psw
z ,t (βt ) must
satisfy
sw ,m ax
0 ≤ psw
z ,t (βt ) ≤ pz

(12)

,m ax
is the maximum swing foot height which is an
where psw
z
empirically chosen parameter. It is based on the default value of
the parameter used in the walking controller of the NAO robot
[37]. Actual swing foot height can be computed by forward

798

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

TABLE I
SPECIFIED VALUES OF THE CONSTRAINTS

kinematics of a kinematic model of the robot:
psw
t (βt ) = Trans (Fr (βt , b1 , b2 ))

(13)

where βt is the robot’s joint angle configuration at time t, and b1
and b2 are the base frame index and the swing foot frame index
of the robot’s model. According to Fig. 1, b1 and b2 are 1 and 5
for the left foot and 6 and 10 for the right foot.
4) Mechanical Constraints: The joint angles of the robot
have to be within its mechanical limits particularly the limits of
joint angles. Thus, βt must satisfy
β m in ≤ βt ≤ β m ax

Threshold
ε sp w (m)
ε sa w (degree)
ε cp (m)
ε ca (degree)

The values of the constraints
[5 × 10 −3 5 × 10 −3 5 × 10 −3 ]T
[2 0.5 4]T
[1 × 10 −2 1 × 10 −3 3 × 10 −2 ]T
[13 3 10]T

(14)

where β m in and β m ax are minimal and maximal angle configurations of the robot’s joint.
5) Artificial Constraints: In addition to mechanical constraints, several artificial constraints can also be considered to
make the robot walk more naturally and improve balance maintenance. Since significant changes in KneePitch angles between
two motion frames can pose an abrupt change in torso height and
thus instability; therefore, KneePitch angle βKP,t must satisfy
βKP,t − γKP  ≤ εKP

(15)

where γKP denotes a desired KneePitch angle and εKP denote
a threshold for Knee Pitch angle error In the real implementation, γKP and εKP are set to 0.95 and 0.05 radians empirically.
Furthermore, to limit the robot’s body sway motion, the torso is
controlled to align with the global frame; thus an actual orientation act of the torso must satisfy:
act  ≤ εca

(16)

where εca is a threshold for orientation error of the torso and act
is an actual torso orientation that can be computed by forward
kinematics:
act = Rot (Fr (βt , 0, 0))

(18)

εsw
a

where
is a threshold for orientation deviation of the swing
foot. Real orientation of the swing foot can be computed by
forward kinematics:
asw
t = Rot (Fr (βt , b1 , b2 )) .

III. EXPERIMENTAL RESULTS AND DISCUSSIONS
A. Experimental Setup
Human walking was used for validating the proposed framework for motion imitation. To produce robot motion, the proposed framework was employed to adapt the captured human
motion data by using the constraints listed in TABLE I.

(17)

where Rot(·) is an operator for obtaining the rotational part of a
transformation matrix. Forcing the alignment of the swing foot
with the ground plane is also considered to prevent stumbling
of the robot. Thus, the swing foot’s real orientation asw
t must
satisfy
sw
asw
t  ≤ εa

Fig. 6. (a) Human walking gait acquired from the Biomotion+ platform and
(b) the robot walking reproduced from the Biomotion+ data.

(19)

Finally, the sequential quadratic programming can be applied
to solve the formulated optimization problem with nonlinear
cost function and constraints for each frame separately [38],
[39]. For the ease computation, only the four key frames are sent
to the proposed optimization framework to obtain the robot’s
optimal joint configurations at these frames. Interpolation based
on Piecewise Cubic Hermite Interpolating Polynomial [40] is
applied to the joint configurations in order to obtain intermediate
joint angles among these frames and to complete a gait cycle of
the robot.

B. Experimental Results
To examine whether the humanoid robot is able to perform
the generated movement, the optimized joint angles were transmitted to NAO robot for joint rotations. The successful imitation
of human gait by the robot is illustrated in Fig. 6(b). The robot
was able to perform similar postures compared to human and
follow three gait cycles in the sagittal plane by executing repetitions of the generated movement. Moreover, the balance of the
robot was maintained throughout the period of motion.
The proposed dynamic walking imitation framework can be
readily generalized to any generic motion capture systems.
Fig. 7(b) shows the imitation results of walking data captured
with Vicon, the optical motion capture platform, which is the
dataset of subject 7 (trial 2) from Carnegie Mellon University
(CMU) motion capture database. It is evident that the dynamic
walking can also be imitated by the Nao robot. The proposed
framework was also applied to other nine walking datasets of
different subjects and similar results were obtained. The mean
and standard deviation of CoM velocity of ten CMU datasets
were 1.1212 ± 0.3295 m/s. The CoM velocity was estimated
from the displacement of the centroid of four hip markers over
time.

TEACHASRISAKSAKUL et al.: IMITATION OF DYNAMIC WALKING WITH BSN FOR HUMANOID ROBOT

799

Fig. 7. (a) Human walking gait acquired from the optical motion tracker and
(b) the robot walking reproduced from the optical data.

C. Quantitative Assessment
Three requirements selected to assess the effectiveness of
the proposed strategy for motion imitation include: similarity
between movements of human and robot, motion feasibility,
and the balance of the robot during motion.
1) Similarity Between Human Motion and Robot Motion:
The main indicator for accomplishing motion imitation is that
the robot motion must bear a close resemblance to human motion. In this study, similarity between the two movements is enforced by the cost function of the proposed framework that maximizes similarity between human angle trajectories and robot
trajectories as described in (3) and (4). Thus, the similarity between the angle trajectories of human and robot is examined in
order to assess the similarity between the motions. As shown in
Fig. 8(a) and Fig. 8(b), the hip pitch angles interpolated among
optimized key frame angles have the corresponding trend to human angles of the same joints. Despite the similarity observed
through visual inspection, quantification of the similarity between human and robot walking requires inspection of mean
angle errors as listed in TABLE II. These values are the errors
between human and optimized angles at the key frames. The
errors of both Knee Pitch joints are, however, less than 14◦ due
to the artificial constraint which maintains Knee Pitch angles at
specific values in order to avoid immediate change in the robot’s
torso height. The errors of both hip roll joints are relatively small
as these errors are less than 4.5°. The errors of left and right hip
pitch joints are lower than 8° which is acceptable. Although the
differences in Knee Pitch joints are relatively large, hip pitch
joints have a greater effect on foot positions than knee pitch
joints because positions of the former joints are higher than
those of the latter joints. Moreover, the mean error of all hip
pitch, hip roll, and knee pitch joints is 7.0296◦ . Therefore, the
optimization framework is able to reproduce robot walking that
is similar to human walking.
The mean values of joint angle errors of human and the robot
for each of the ten datasets from the CMU database are shown
in TABLE III. The overall mean errors of all joints are similar
to the mean errors of the dataset 10 of which imitation result
is shown in Fig. 7. Moreover, the trend of the mean errors of
the optical data is corresponding to the trend of mean errors

Fig. 8. Joint angle trajectories of (a) left hip pitch joint and (b) right hip
pitch joint during a walking cycle. The trajectories include the trajectories of
human (gray) and the trajectories of the robot (black). The robot trajectories
were interpolated among the optimized angles at the key frames

TABLE II
MEAN AND STANDARD DEVIATION OF ANGULAR ERRORS (IN DEGREES) OF
HUMAN JOINT ANGLES AND OPTIMIZED ANGLES OF THE ROBOT’S JOINTS IN
WALKING MOTION
Joints
Left hip roll
Left hip pitch
Left knee pitch
Right hip roll
Right hip pitch
Right knee pitch

Mean

SD

4.0682
6.1685
13.3663
3.6242
7.8396
7.1109

4.4602
3.6835
6.9338
4.4084
5.7270
2.1900

of the inertial sensor data. The errors of knee pitch joints are
larger than the errors of hip pitch joints due to the use of the
constraint limiting knee pitch angles. The mean errors of knee
pitch angles are lower than 4.5° and the mean errors of hip
pitch angles are lower than 2◦ . The relative low mean errors in
hip pitch and knee pitch angles also demonstrate the similarity
between human motions and robot motions. The mean values of
left and right ankle pitch angle error are 10.6430° and 9.5763°,
respectively, which are relatively large. The trend of human and
the robot’s angles of knee pitch and ankle pitch joints are also
not similar due to the use of two artificial constraints on knee
pitch angles and swing foot orientation. The former maintains
knee pitch angles at the constant desired values; therefore, the
robot’s knee during toe off is not flexed backward as much as
that of human. The latter constrains swing foot orientation to

800

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

TABLE III
MEAN OF ANGULAR ERRORS (IN DEGREES) OF HUMAN JOINT ANGLES AND OPTIMIZED ANGLES OF THE ROBOT’S JOINTS IN WALKING MOTION.

The angles were obtained from CMU motion capture database.

align with the ground; therefore, during toe off, the robot’s foot
is not tilted downward and its ankle is not extended as much as
that of human.
The main metric for motion similarity in this paper is mean
joint angle error. The mean values of the hip pitch angle error can
indicate similarity between the movements of a human subject
and the robot because the knee pitch angles were constrained to
be within a predefined range by one of the artificial constraints
in (15). Therefore, without significant changes in the knee pitch
angles, leg movements were mainly affected by the changes in
the hip pitch angles. As shown in TABLE III, the mean values
of the hip pitch joint angle errors were lower than 2°. This
demonstrated similar trends of hip pitch joint angles of both
human and the robot and thus the similar movements.
2) Feasibility of Motion: It is feasible for the robot to imitate
human motion only if the joint angle is within the robot’s joint
limits because motion can be produced through joint actuation.
Therefore, in order to satisfy this requirement, the proposed
approach includes the mechanical constraint defined in (14). To
evaluate the feasibility of motion, optimized joint trajectories are
tested whether they are within the mechanical limits. As shown
in Fig. 8, the trajectories of left hip pitch and right hip pitch
are within the robot’s joint limits. Particularly during the period
from 1 to 1.5 s, angles of right hip pitch do not go beyond the
lower limits of the joint due to the optimization framework and
interpolation method which preserves the shape of the trajectory.
Thereby, motion feasibility is achieved.
3) Robot Balance During Walking Imitation: The condition
that determines the success of walking and other whole body
movements is the robot balance. The static equilibrium can be
maintained if actual CoM position is inside the support area.
However, for dynamic stability, CoM position can be outside
the support region for a short duration during walking. Since
the robot’s torso position is used to approximate the CoM position, actual torso positions were computed by using angle
trajectories and forward kinematics. Angle trajectories were
derived by cubic polynomial interpolation among optimal angle configurations at the key locomotion frames. The robot’s
torso trajectories in x-axis and y-axis, when the threshold of the
stability criterion, as defined in (11) are 0.5 and 0.5 cm, are
shown in Fig. 9. The x-axis torso positions are within support
region for the whole period of walking. The majority of y-axis
torso positions are within support region. However, the parts of

y-axis torso positions which account for approximately 9.8039%
of a gait cycle time are outside of the support region. This occurred during transferring the projection of mass between two
feet. This gait was considered dynamically balanced because,
by definition, dynamically balanced gait occurs even if the floor
projection of the CoM is outside the support region while the
ZMP is within the support region [41]. Moreover, torso trajectories in both axes are in proximity to the middle positions of
the support region because the reference torso positions are set
to be the center of the foot during left foot support and right
foot support. This trend of the trajectories indicates a relatively
high degree of stability. If there is any error in controlling the
torso position, actual torso position close to the middle position
will still be within the support region compared to the position
that is close to the boundary of the support region. Accordingly,
the torso trajectories exhibit the effectiveness of the proposed
scheme in maintaining torso position inside the support area and
preserving the robot balance.
The threshold on stability criterion, as defined in (11), was selected for evaluation because the threshold may affect the stability of the robot. In order to investigate the effect of the threshold
on stability criterion on the robot’s stability, the threshold was
set to different values and the resulting torso positions during
walking were inspected. Two groups of parameter settings as
reported in Fig. 9 were used in this investigation. In Fig. 9(a),
as the x-axis value of the threshold was increased, the torso positions in x-axis deviated from the reference positions by larger
extent, particularly during the period between 0.5 and 1 s and the
period between 1.5 and 2 s. In Fig. 9(b), as the y-axis value of the
threshold was increased, the deviation of the torso positions in
y-axis from the reference positions was larger, especially during
the period from 1.5 to 2 s. Therefore, increasing the threshold on
stability criterion causes greater errors between torso positions
and reference positions and increases the chance of the actual
position being outside the support region which may lead to a
higher degree of instability.
Generating dynamically stable gait is one of the contributions
of this paper. Dynamically stable walk is defined as walking
motion with the ZMP residing within the support region while
the projection of the CoM on the floor being outside the support
region [41]. The experimental results as illustrated in Fig. 9
demonstrated that the robot’s gait was dynamically stable and
the robot does not experience a fall even though the projection

TEACHASRISAKSAKUL et al.: IMITATION OF DYNAMIC WALKING WITH BSN FOR HUMANOID ROBOT

801

Fig. 9. Torso positions of robot walking: (a) in x-axis when the x-axis value of the threshold in (11) is altered; and (b) in y-axis when the y-axis value of the
threshold in (11) is altered. The values of the threshold on stability criterion in (11) are shown in the legends. The lower and upper borders of the robot’s support
region are represented by the two external dashed lines. Reference torso trajectories are represented by the middle dashed lines.

of CoM was outside of the support region for approximate 9.8%
of the entire walking period. This supports the contribution that
the gait is dynamically stable.
4) Importance of Artificial Constraints: Artificial constraints
including the constraints on knee pitch angles, torso orientation
and swing foot orientation were also required in our method.
Constraining knee pitch angle of the robot, as described in (15),
is needed because human walking requires considerable variation in knee angles in order to tilt a foot in a toe-off posture
and detach the swing foot from the ground while the robot
walking requires a foot to be flat, i.e., aligned with the ground,
but does not require toe-off. Therefore, for the robot walking,
the changes in knee pitch angle should be kept to be within
a predefined range. Forcing the torso orientation to be within
a predefined range, as described in (16), reduces the chance
of falling because leaning the body too much in any direction
would cause the CoM to be moved further from the center of
the support polygon. Constraining the orientation of swing foot
to be aligned with the ground, as described in (18), is necessary since tilted swing foot would lead to undesirable contact
between a swing foot and the ground, stumbling, and loss of
balance.
IV. CONCLUSION AND FUTURE WORK
In this paper, we have presented a method for imitation of
dynamic human gait by a humanoid robot. We modeled the task
of dynamic motion imitation as an optimization process which
minimizes differences between joint angle trajectories of human
and robot. Constraints to the minimization were also considered
to maintain the feasibility of the robot imitation. To maintain
the robot balance, we presented a novel CoM trajectory strategy, which did not restrict the projection of the CoM within
the foot support area, to make the robot imitation dynamically
stable. Key motion frames in any gait cycle were also extracted.
Only the angular data of key motion frames were optimized
and continuous angle trajectories among these key frames were
generated by interpolation. The method was verified by using
both the wearable inertial sensor-based motion data and optical motion data. The experimental results have demonstrated
the effectiveness of the proposed strategy for dynamic walking
motion imitation.

The future work could involve extending the current method
to deal with fast dynamic motions including running and full
body movements with an intense angular momentum created by
arm motion. The future work could also relate to improvement
on the speed of walking. The future work could involve considering energy minimization in the cost function of the optimization
framework and working on stabilization strategy without the
artificial constraint on knee pitch angles in order to obtain more
humanlike motion for a humanoid robot. Human motion is resulted from a weighted combination of several motion criteria.
The effect of using the combination as a cost function could be
investigated in the future.
REFERENCES
[1] M. Fujita, Y. Kuroki, T. Ishida, and T. T. Doi, “Autonomous behavior
control architecture of entertainment humanoid robot SDR-4X,” in Proc.
IEEE/RSJ Int. Conf. Intell. Robots Syst., 2003, vol. 1, pp. 960–967.
[2] Y. Sakagami, “The intelligent ASIMO: System overview and integration,”
in Proc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2002, pp. 2478–2483.
[3] N. Yunjun, K. Bonkon, A. Cichocki, and C. Seungjin, “GOM-Face: GKP,
EOG, and EMG-based multimodal interface with application to humanoid
robot control,” IEEE Trans. Biomed. Eng., vol. 61, no. 2, pp. 453–462,
Feb. 2014.
[4] J. Wainer, K. Dautenhahn, B. Robins, and F. Amirabdollahian, “Collaborating with Kaspar: Using an autonomous humanoid robot to foster
cooperative dyadic play among children with autism,” in Proc. IEEE-RAS
Int. Conf. Humanoid Robots, 2010, pp. 631–638.
[5] E. T. Bekele, U. Lahiri, A. R. Swanson, J. A. Crittendon, Z. E. Warren, and
N. Sarkar, “A step towards developing adaptive robot-mediated intervention architecture (ARIA) for children with autism,” IEEE Trans. Neural
Syst. Rehabil. Eng., vol. 21, no. 2, pp. 289–299, Mar. 2013.
[6] J. M. I. Zannatha, A. J. M. Tamayo, A.D. Sánchez, J. E. Delgado,
L. E. Cheu, W. A. Arévalo, “Development of a system based on 3D
vision, interactive virtual environments, ergonometric signals and a humanoid for stroke rehabilitation,” Comput. Methods Prog. Biomed., vol.
112, pp. 239–249. 2013.
[7] J. J. Diehl, L. M. Schmitt, M. Villano, and C. R. Crowell, “The clinical
use of robots for individuals with Autism Spectrum Disorders: A critical
review,” Res. Autism Spectrum Disorders, vol. 6, pp. 249–262. 2012.
[8] A. Billard, B. Robins, J. Nadel, and K. Dautenhahn, “Building Robota,
a mini-humanoid robot for the rehabilitation of children with autism,”
Assistive Technol., vol. 19, pp. 37–49, Mar. 2007.
[9] B. Robins, K. Dautenhahn, T. Boekhorst, and A. Billard, “Robotic assistants in therapy and education of children with autism: Can a small
humanoid robot help encourage social interaction skills?” Univers. Access Inf. Soc., vol. 4, pp. 105–120. 2005.
[10] P. Standen, D. Brown, J. Roscoe, J. Hedgecock, D. Stewart, M. Galvez
Trigo, and E. Elgajiji, “Engaging students with profound and multiple disabilities using humanoid robots,” in Universal Access in Human-Computer
Interaction: Universal Access to Information and Knowledge (Lecture

802

[11]
[12]
[13]
[14]
[15]
[16]

[17]
[18]
[19]
[20]
[21]

[22]
[23]
[24]

[25]

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 19, NO. 3, MAY 2015

Notes in Computer Science), vol. 8514, C. Stephanidis and M. Antona,
Eds. New York, NY, USA: Springer, 2014, pp. 419–430.
K. Teachasrisaksakul, Z.-Q. Zhang, and G.-Z. Yang, “The use of BSN for
whole body motion training for a humanoid robot,” presented at the Int.
Conf. Body Sensor Networks, Zurich, Switzerland, 2014.
D. Gouaillier, C. Collette, and C. Kilner, “Omni-directional closedloop walk for NAO,” in Proc. IEEE-RAS Int. Conf. Humanoid Robots,
Nashville, TN, USA, 2010, pp. 448–454.
A. Massah, A. Sharifi, Y. Salehinia, and F. Najafi, “An open loop walking
on different slopes for NAO humanoid robot,” in Proc. Int. Symp. Robotics
and Intelligent Sensors, Kuching, Malaysia, 2012, pp. 296–304.
S. Czarnetzki, S. Kerner, and O. Urbann, “Observer-based dynamic walking control for biped robots,” Robot. Auton. Syst., vol. 57, pp. 839–845,
Jul. 2009.
Z. Li, N. Tsagarakis, and D. Caldwell, “Walking pattern generation for a
humanoid robot with compliant joints,” Auton. Robot, vol. 35, pp. 1–14,
Jul. 2013.
S. Gaertner, M. Do, T. Asfour, R. Dillmann, C. Simonidis, and
W. Seemann, “Generation of human-like motion for humanoid robots
based on marker-based motion capture data,” in Proc. Int. Symp. Robot.,
Munich, Germany, 2010, pp. 1–8.
N. S. Pollard, J. K. Hodgins, M. J. Riley, and C. G. Atkeson, “Adapting
human motion for the control of a humanoid robot,” in Proc. IEEE Int.
Conf. Robotics Autom., Washington, DC, USA, 2002, pp. 1390–1397.
C. Ott, L. Dongheui, and Y. Nakamura, “Motion capture based human
motion recognition and imitation by direct marker control,” in Proc. IEEERAS Int. Conf. Humanoid Robot., 2008, pp. 399–405.
B. Dariush, M. Gienger, J. Bing, C. Goerick, and K. Fujimura, “Whole
body humanoid control from human motion descriptors,” in Proc. Int.
Conf. Robotics and Automation, 2008, pp. 2677–2684.
W. Suleiman, E. Yoshida, F. Kanehiro, and J.-P. Laumond, “On human
motion imitation by humanoid robot,” in Proc. IEEE Int. Conf. Robotics
and Automation, Pasadena, CA, USA, 2008, pp. 2697–2704.
L. Jingru and K. Hauser, “Interactive generation of dynamically feasible
robot trajectories from sketches using temporal mimicking,” in Proc. IEEE
Int. Conf. Robotics and Automation, St. Paul, MN, USA, 2012, pp. 3665–
3670.
M. Vukobratovic, B. Bomvac, D. Surla, and D. Stokic, Biped Locomotion: Dynamics, Stability, Control and Application. New York, NY, USA:
Springer, 1990.
S. Kim, C. H. Kim, B. You, and S. Oh, “Stable whole-body motion generation for humanoid robots to imitate human motions,” in Proc. IEEE/RSJ
Int. Conf. Intell. Robots Syst., St. Louis, MO, USA, 2009, pp. 2518–2524.
K. Hu and D. Lee, “Prediction-based synchronized human motion imitation by a humanoid robot,” Automatisierungstechnik Methoden und
Anwendungen der Steuerungs-, Regelungs- und Informationstechnik,
vol. 60, pp. 651–720. 2012.
J. Koenemann and M. Bennewitz, “Whole-body imitation of human motions with a Nao humanoid,” presented at the ACM/IEEE Int. Conf.
Human-Robot Interact., Boston, MA, USA, 2012.

[26] L. Boutin, A. Eon, S. Zeghloul, and P. Lacouture, “From human motion
capture to humanoid locomotion imitation: Application to the robots HRP2 and HOAP-3,” Robotica, vol. 29, pp. 325–334, Mar. 2011.
[27] BTS. (2013).BTS Bioengineering. [Online]. Available: http://www.
btsbioengineering.com/.
[28] Vicon. (2013). Vicon. [Online]. Available: http://www.vicon.com/
[29] K. Yamane and J. Hodgins, “Simultaneous tracking and balancing of
humanoid robots for imitating human motion capture data,” in Proc.
IEEE/RSJ Int. Conf. Intell. Robots Syst., St. Louis, MO, USA, 2009,
pp. 2510–2517.
[30] C. D. Metcalf, R. Robinson, A. J. Malpass, T. P. Bogle, T. A. Dell,
C. Harris, and S. H. Demain, “Markerless motion capture and measurement of hand kinematics: Validation and application to home-based upper
limb rehabilitation,” IEEE Trans. Biomed. Eng., vol. 60, no. 8, pp. 2184–
2192. Aug. 2013.
[31] Z.-Q. Zhang and G.-Z. Yang, “Calibration of miniature inertial and magnetic sensor units for robust attitude estimation,” IEEE Trans. Instrum.
Meas., vol. 63, no. 3, pp. 711–718, Mar. 2014.
[32] Z.-Q. Zhang, X.-L. Meng, and J.-K. Wu, “Quaternion-based Kalman filter with vector selection for accurate orientation tracking,” IEEE Trans.
Instrum. Meas., vol. 61, no. 10, pp. 2817–2824, Oct. 2012.
[33] Z. Zhang, A. Panousopoulou, and G.-Z. Yang, “Wearable sensor integration and bio-motion capture: A practical perspective,” in Body Sensor
Networks, 2nd ed. London, U.K.: Springer, 2014, pp. 495–526.
[34] D. Gouaillier, V. Hugel, P. Blazevic, and C. Kilner, “Mechatronic design
of NAO humanoid,” in Proc. IEEE Int. Conf. Robot. Autom., Kobe, Japan,
2009, pp. 769–774.
[35] S. L. Altmann, Rotations, Quaternions, and Double Groups. Oxford, U.K.:
Clarendon, 1986.
[36] A. Salarian, H. Russmann, F. J. Vingerhoets, C. Dehollain, Y. Blanc,
P. R. Burkhard, and K. Aminian, “Gait assessment in Parkinson’s disease:
Toward an ambulatory system for long-term monitoring,” IEEE Trans.
Biomed. Eng., vol. 51, no. 8, pp. 1434–43, Aug. 2004.
[37] Aldebaran. (2015). Locomotion control. [Online]. Available:
http://doc.aldebaran.com/1–14/naoqi/motion/control-walk.html.
[38] M. J. D. Powell, “Variable metric methods for constrained optimization,”
in Mathematical Programming the State of the Art, A. Bachem, B. Korte,
and M. Grötschel, Eds. New York, NY, USA: Springer-Verlag, 1983,
pp. 288–311.
[39] R. Fletcher, Practical Methods of Optimization, 2nd ed. New York, NY,
USA: Wiley-Interscience, 1987, pp. 304–316.
[40] F. Fritsch and R. Carlson, “Monotone piecewise cubic interpolation,”
SIAM Journal on Numerical Analysis, vol. 17, pp. 238–246, Apr. 1980.
[41] M. H. P. Dekker, Zero-Moment Point Method for Stable Biped Walking.
Eindhoven, Netherlands: Eindhoven Univ. Technol., 2009.

Authors’ photographs and biographies not available at the time of publication.

