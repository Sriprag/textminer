2434

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

A Stereo Music Preprocessing Scheme for Cochlear
Implant Users
Wim Buyensâˆ— , Bas van Dijk, Jan Wouters, and Marc Moonen

Abstractâ€”Objective: Listening to music is still one of the more
challenging aspects of using a cochlear implant (CI) for most users.
Simple musical structures, a clear rhythm/beat, and lyrics that are
easy to follow are among the top factors contributing to music
appreciation for CI users. Modifying the audio mix of complex
music potentially improves music enjoyment in CI users. Methods:
A stereo music preprocessing scheme is described in which vocals,
drums, and bass are emphasized based on the representation of
the harmonic and the percussive components in the input spectrogram, combined with the spatial allocation of instruments in typical stereo recordings. The scheme is assessed with postlingually
deafened CI subjects (N = 7) using pop/rock music excerpts with
different complexity levels. Results: The scheme is capable of modifying relative instrument level settings, with the aim of improving
music appreciation in CI users, and allows individual preference
adjustments. The assessment with CI subjects confirms the preference for more emphasis on vocals, drums, and bass as offered
by the preprocessing scheme, especially for songs with higher complexity. Conclusion: The stereo music preprocessing scheme has the
potential to improve music enjoyment in CI users by modifying the
audio mix in widespread (stereo) music recordings. Significance:
Since music enjoyment in CI users is generally poor, this scheme
can assist the music listening experience of CI users as a training
or rehabilitation tool.
Index Termsâ€”Cochlear implants (CIs), music processing, sound
separation.

I. INTRODUCTION
cochlear implant (CI) is a medical device enabling people with severe-to-profound sensorineural hearing loss to
perceive sounds by electrically stimulating the auditory nerve
using an electrode array implanted in the cochlea [1]. This type
of hearing loss is mostly caused by malfunctions in the hair cells
of the cochlea and can be congenital or acquired after birth.
Although CI users reach good speech understanding in quiet
surroundings, music perception and appreciation generally remain poor [2]. Simple musical structures, a clear rhythm/beat,
and lyrics that are easy to follow were reported amongst the
top factors contributing to music appreciation in CI users [3]. A
negative correlation was found between (subjective) complexity and appreciation, studied with pop, country, and classical

A

Manuscript received January 6, 2015; revised March 27, 2015; accepted April
25, 2015. Date of publication May 8, 2015; date of current version September
16, 2015. This work was supported by a Baekeland Ph.D. grant of the Institute
for the Promotion of Innovation through Science and Technology in Flanders
(IWT090274) and Cochlear Technology Centre Belgium. Asterisk indicates
corresponding author.
âˆ— W. Buyens is with the Cochlear Technology Centre Belgium, 2800
Mechelen, Belgium, and also with the Department of Neurosciences (ExpORL)
and Department of Electrical Engineering (ESAT-STADIUS), KU Leuven, 3000
Leuven, Belgium (e-mail: wim.buyens@esat.kuleuven.be).
B. van Dijk is with the Cochlear Technology Centre Belgium.
J. Wouters and M. Moonen are with KU Leuven.
Digital Object Identifier 10.1109/TBME.2015.2428999

music [4]. CI users were asked to rate complexity and appraisal
for different music excerpts on a scale from 0 to 100. Classical music was rated as more complex than pop and country
music. Several plausible explanations were provided including
the presence of simple musical structures and lyrics in pop and
country music. Since CIs were mainly developed for transmitting speech sounds, the presence of lyrics may make it easier
for CI users to follow the sequence of events in complex music.
In addition, both pop and country music often contain a strong
simple beat, which is well encoded in the electrical stimulation
pattern of current CIs. Moreover, the performance for CI subjects on rhythmic pattern perception tasks is nearly as good as
for normal hearing (NH) subjects (e.g., [2]â€“[6]). On the other
hand, the accurate transmission of spectral and fine-structure information (as is often the case in instrumental classical music)
remains challenging due to channel interactions, limited number
of stimulation channels, and limited dynamic range.
The preference for clear vocals and a strong rhythm/beat in CI
users was demonstrated before by modifying relative instrument
level settings in pop music [7]. A significant difference in preference rating scores was found between NH and CI subjects. For
the pop songs provided, CI subjects preferred an audio mix with
higher vocals-to-instruments ratio compared to NH subjects.
In addition, given an audio mix with clear vocals and attenuated instruments, CI subjects preferred the drums/bass track to
be louder than the other instrument tracks. Although individual
differences occurred across subjects, the potential for improving
music appraisal by modifying the audio mix is apparent. The relative instrument level settings were modified by altering the levels of the different separately recorded instrument tracks, which
are, however, not widely available for most music. To accomplish the same modification in relative instrument levels with
mono or stereo music recordings, a specific signal processing
scheme is needed. By using sound source separation techniques,
vocals, drums, and bass can be separated out, and then, the residual signal is mixed in at a different level to provide the output
of this specific signal processing scheme. Several approaches
have been studied to tackle the sound source separation problem, and can be divided in two main categories: single-channel
methods and multichannel methods [8]. The approaches used
in a single-channel sound source separation can be categorized
roughly into model-based inference, unsupervised learning, and
psychoacoustically motivated methods. Mostly, a combination
of these approaches is used in practice. In a model-based inference, a parametric model of the sound sources to be separated
is employed in which the model parameters are estimated from
the observed mixture signal. The sinusoids plus noise model
is the most commonly used model, introduced in music signal
processing by Smith and Serra [9]â€“[11]. Unsupervised learning

0018-9294 Â© 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

BUYENS et al.: STEREO MUSIC PREPROCESSING SCHEME FOR COCHLEAR IMPLANT USERS

methods use a simple nonparametric model, and estimate the
sound source characteristics from the data based on an independent component analysis (ICA), nonnegative matrix factorization (NMF), or sparse coding. Uhle et al. applied an ICA
to the spectrogram, and classified the extracted independent
components into a harmonic and a percussive group based on
features, like percussiveness, noise-likeness, etc. [12]. Helen
and Virtanen utilized NMF for decomposing the spectrogram
into elementary patterns, and classified them by a pretrained
support vector machine [13]. Sparse coding methods represent
a mixture signal in terms of a small number of active elements
chosen from a larger set [14], and were used in the analysis of
music signals by Abdallah and Plumbley [15] and Virtanen [16].
Although good performance is demonstrated for many different
signals, the aforementioned approaches often have their limitations. First, some sound sources are hard to model with a few
fixed spectral templates, and in practice require careful tuning
of sophisticated models [17], [18]. Second, the typical assumption that different sources are characterized by different sets of
spectra may not be realistic in many cases in music signals.
Instead of decomposing the sound sources as a combination
of fixed patterns, in psychoacoustically motivated methods, the
elementary timeâ€“frequency components of the incoming signal
are categorized into their respective sound sources based on
association cues, such as spectral proximity, harmonic concordance, synchronous changes, and spatial proximity [19]. A simple and fast algorithm to perform a harmonic/percussive sound
separation (HPSS) is based on the â€œanisotropic smoothnessâ€ of
the harmonic and percussive components in the spectrogram
[20]. Harmonic components appear smooth in the temporal direction, whereas percussive components appear smooth in the
frequency direction in the spectrogram. A (mono) music preprocessing scheme for CI users based on the HPSS was described
in [21] for a single-channel input. This scheme emphasized vocals and drums in complex pop music in order to improve music
appreciation in CI users.
In contrast to single-channel methods, multichannel methods
can take advantage of the availability of spatial information, e.g.,
from recordings with multiple microphones placed at different
positions, enabling acoustic beamforming, or blind separation of
convolutive mixtures to recover the sound sources. The typical
karaoke problem to remove vocals from background music also
exploits the spatial information of stereo recordings [22].
In this paper, a stereo music preprocessing scheme for CI
users is described, which is a stereo extension of the mono music preprocessing scheme from [21] with improved performance
based on exploiting the spatial information in stereo recordings. The building blocks for this stereo music preprocessing
scheme are described in detail in Section II. The evaluation of
the scheme with CI users and pop music excerpts is presented in
Sections III and IV.
II. STEREO MUSIC PREPROCESSING SCHEME
The stereo music preprocessing scheme, which performs vocals, drums, and bass enhancement on complex stereo music,
is shown in Fig. 1. The scheme contains a â€œvocals and drums
extractionâ€ block applied on the input spectrogram and a â€œstereo

2435

Fig. 1. Schematic of the stereo music preprocessing scheme for CI users,
which is enhancing vocals/drums/bass, while attenuating the â€œotherâ€ instruments
with parameter â€œAttenuation.â€ It is based on â€œvocals and drums extractionâ€ of
the input spectrogram (Input_L + Input_R with frequency > cutoff frequency),
and a â€œstereo binary maskâ€ to exploit the spatial information in stereo recordings
(based on Input_L, Input_R, Input_L-Input_R).

binary maskâ€ block to promote components that are located in
the center of the stereo image. The output of the scheme contains the extracted signal (indicated as â€œvocals, drums, bassâ€)
mixed together with an attenuated version of the residual signal
(indicated as â€œotherâ€).
In Section II-A, the â€œvocals and drums extractionâ€ is described in detail, performing vocals and drums enhancement on
a mono input signal which isâ€”in the case of a stereo inputâ€”the
sum of the left and the right channel. The processing of the
bass frequencies is explained in Section II-B. In Section II-C,
the typical mixing settings in stereo recordings are explained,
and the inclusion of the stereo mixing property is described as
a constraint in the optimization problem with the â€œstereo binary maskâ€. Section II-D concludes with the computation of the
output of the stereo music preprocessing scheme.
A. Vocals and Drums Extraction
The â€œvocals and drums extractionâ€ block is adopted from
the mono music preprocessing scheme for CI users in [21]
and is based on the HPSS in [20], which separates harmonic (H) and percussive (P) components by exploiting the
â€œanisotropic smoothnessâ€ of these components in the spectrogram. â€œAnisotropic smoothnessâ€ is defined based on partial differentials of the spectrogram in the temporal or the frequency
direction: harmonic components are â€œsmooth in the temporal
directionâ€ because they are sustained and periodic; percussive
components are â€œsmooth in the frequency directionâ€ because
they are instantaneous and aperiodic [23]. The input spectrogram WÏ„ ,Ï‰ = STFT(w (t)), which is calculated using the
short-time Fourier transform (STFT), and a Hamming window
is decomposed into the harmonic components HÏ„ ,Ï‰ and the percussive components PÏ„ ,Ï‰ . The indices Ï„ and Ï‰ represent time
and frequency, respectively. The L2 norms of the spectrogram
gradients are used as a metric for the anisotropic smoothness,
that is, HÏ„ ,Ï‰ and PÏ„ ,Ï‰ are found by minimizing
J (H, P ) =

1 
(HÏ„ âˆ’1,Ï‰ âˆ’ HÏ„ ,Ï‰ )2
2
2ÏƒH
Ï„ ,Ï‰
+

1 
(PÏ„ ,Ï‰ âˆ’1 âˆ’ PÏ„ ,Ï‰ )2
2ÏƒP2 Ï„ ,Ï‰

(1)

2436

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

under the constraint of
HÏ„2,Ï‰ + PÏ„2,Ï‰ = |WÏ„ ,Ï‰ |2

(2)

HÏ„ ,Ï‰ â‰¥ 0, PÏ„ ,Ï‰ â‰¥ 0

(3)

where H and P are sets of HÏ„ ,Ï‰ and PÏ„ ,Ï‰ , respectively, and
ÏƒH and ÏƒP are parameters to control the weights of the horizontal and vertical smoothness. This optimization problem can
be solved numerically, which results in the following iteration
formulae [24]:
(j )

(j + 1)

HÏ„2,Ï‰

Î±Ï„ ,Ï‰ |WÏ„ ,Ï‰ |2

= 
(j )
(j )
Î±Ï„ ,Ï‰ + Î²Ï„ ,Ï‰

(4)

(j )

(j + 1)

PÏ„2,Ï‰

Î²Ï„ ,Ï‰ |WÏ„ ,Ï‰ |2

= 
(j )
(j )
Î±Ï„ ,Ï‰ + Î²Ï„ ,Ï‰

where j is the iteration index, and
2

(j )
(j )
Î±Ï„(j,Ï‰) = HÏ„ +1,Ï‰ + HÏ„ âˆ’1,Ï‰

2
(j )
(j )
Î²Ï„(j,Ï‰) = Îº2 PÏ„ ,Ï‰ +1 + PÏ„ ,Ï‰ âˆ’1
Ïƒ2
Îº = H2 .
ÏƒP

(5)

(6)

Fig. 2. Boxplot with SNR improvement (dB) of the P-components with vocals/drums versus the other instruments for the multitrack recordings used in
[7] as a function of the number of iterations (J) with and without applying the
BM from (9). The window length of the STFT used in this graph is 185 ms.

(7)
(8)

The tunable parameter Îº is optimized to maximize the separation of vocals and drums from the other instruments by using
(16) and the multitrack recordings (vocals, drums, bass, guitar,
piano) used in [7]. From the estimated spectrograms HÏ„ ,Ï‰ and
PÏ„ ,Ï‰ , a timeâ€“frequency mask is defined, which is then used to
estimate the corresponding waveforms h(t) and p(t). From the
considered timeâ€“frequency masks, the binary mask has been
found more effective to improve the separation performance
compared to the Wiener mask or not applying any mask [24].
The binary mask (BMÏ„ ,Ï‰ ) is defined as

1, PÏ„ ,Ï‰ > HÏ„ ,Ï‰
BMÏ„ ,Ï‰ =
(9)
0, PÏ„ ,Ï‰ â‰¤ HÏ„ ,Ï‰ .
Using this binary mask, the P- and H-components are computed from the input spectrogram as
p (t) = ISTFT (BMÏ„ ,Ï‰ .WÏ„ ,Ï‰ )

(10)

h (t) = ISTFT ((1 âˆ’ BMÏ„ ,Ï‰ ) .WÏ„ ,Ï‰ ) .

(11)

Fig. 2 shows the separation performance versus the number
of iterations with and without binary mask for the multitrack
recordings used in [7]. The benefit of the binary mask on the separation performance is clearly seen. The separation performance
is defined as the signal-to-noise ratio of the P-components p(t),
in which the signal (S) consists of vocals and drums and the noise
(N) represents the other instruments. Since multitrack recordings are available from the songs used in [7], the signal-to-noise
ratio can be calculated at both the input and the output as


rms (S)
.
(12)
SNR = 20 âˆ— log10
rms (N )

To indicate the presence of the different instruments in the
P-components, the energy ratio for the different tracks is used,
which is calculated as
rpi =

Epi
Ehi + Epi

(13)

where
Epi = fi (t) , p (t)

(14)

Ehi = fi (t), h(t)

(15)

in which  represents the cross-correlation operation with timelag zero and fi (t) the signal of track i. Therefore, the separation
of vocals and drums from the other instruments can also be
defined as the difference (Î´) in energy ratio of the P-components
for vocals/drums and bass/guitar/piano
Î´ = (rpvo cals/drum s ) âˆ’ (rpbass/guitar/piano ).

(16)

The separation results from the â€œvocals and drums extractionâ€
block show that in addition to the drums also the vocals can
indeed be extracted as P-components based on their representation in the input spectrogram. â€œTemporally variableâ€ sounds
are contrasted to â€œtemporally stableâ€ sounds. The â€œtemporally
variableâ€ sounds (such as vocal tones) contain 4â€“8-Hz quasiperiodic vibrations of the fundamental frequencies (F0s) and do
not sustain for a long time. On the other hand, the â€œtemporally
stableâ€ sounds (such as chord tones) contain very few fluctuations and are maintained stationary for a while [23]. Adjusting
the timeâ€“frequency resolution of the STFT results in a different classification for the temporally variable components. For
a STFT with long time window (100â€“500 ms), percussive and
temporally variable sounds appear â€œsmoothâ€ in the frequency
direction (P-components), whereas for a STFT with short-time
window (30 ms) temporally stable sounds as well as temporally variable sounds appear â€œsmoothâ€ in the temporal direction

BUYENS et al.: STEREO MUSIC PREPROCESSING SCHEME FOR COCHLEAR IMPLANT USERS

Fig. 3. Energy ratio of the P-components (j = 15) for the different tracks
of a typical pop song as a function of the STFT window length. The vocals
(straight line) are separated as P-components with high STFT window length
and as H-components with low STFT window length.

(H-components). This is illustrated in Fig. 3 for a typical pop
song with vocals, drums, bass, guitar, and piano. The energy
ratio of the P-components calculated with (13) is shown for every instrument as a function of the window length. Instruments
with high energy ratio (close to one) appear as P-components,
whereas instruments with low energy ratio (close to zero) appear as H-components. Fig. 3 shows that the vocals appear in
the H-components if a small STFT window length is adopted,
whereas with a large STFT window length, the vocals appear in
the P-components. A window length above 185 ms is good for
vocals/drums separation, but the longer the window the more
latency. Hence, we choose in the â€œvocals and drums extractionâ€ block (see Fig. 1), a window length of 185 ms resulting
in the classification of temporally variable components (such as
vocals) as P-components. The small distortions on the vocals
introduced by the algorithm are hardly noticeable for CI users.
In speech, similar distortions introduced by a binary mask have
been studied in [25], showing no degradation in quality for CI
users, as opposed to NH listeners, but a significant improvement
in speech understanding in noise.
The impact of the proposed settings of the â€œvocals and drums
extractionâ€ on other instruments is shown in Fig. 4. The energy
ratio of the P-components is computed for different instruments.
An energy ratio close to zero means the instrument has been removed from the P-components, whereas an energy ratio close
to one means the instrument remains in the P-components. Instrument samples are collected from the instrument recognition
tests MACarena [26] and UW-CAMP [27], [28]. The MACarena
samples consist of the first eight bars of a traditional Swedish
folksong played by professional musicians on different instruments. The UW-CAMP samples consist of a five-note melodic
sequence played on different instruments with the same detached articulation. The majority of the instruments show low
energy ratio, and are, thus, removed from the P-components
with the â€œvocals and drums extraction.â€ However, the flute and
the violin from the MACarena test show a high energy ratio
value due to the very strong vibrato in the music samples. This

2437

Fig. 4. Energy ratio of the P-components for different instrument samples
from the instrument recognition tests MACarena and UW-CAMP. (j = 15,
STFT 185 ms).

contrasts with the UW-CAMP flute and violin, which have a
low energy ratio value and no vibrato. The pitched percussion
instruments (guitar and piano) are not completely removed in
the P-components because of the clear percussive onset, except
for the MACarena piano which plays the sequence of notes in
legato.
B. Bass Frequency Extraction
The bass guitar is classified with the H-components in the
â€œvocals and drums extractionâ€ block, which means it is attenuated or removed from the P-components. Because the bass
guitar and the bass frequencies, in general, give more fullness to
the music, and the attenuation of bass frequencies may result in
chopped (bass) drum sounds, the bass frequencies should be preserved in the output signal of the music preprocessing scheme.
Moreover, the preference for bass sounds was also found in the
music mixing preference study with CI users in [7]. To preserve
the bass frequencies in the output of the music preprocessing
scheme, the binary mask for the frequency bins below a chosen cutoff frequency is uniformly set to 1, whereas the binary
mask for the higher frequency bins is calculated according to
their percussiveness (see Fig. 1). To set the cutoff frequency,
eight solo bass tracks from different pop songs were passed
through the music preprocessing scheme, and the energy ratio
of the P-components for these bass tracks was evaluated for
different cutoff frequencies (see Fig. 5). The cutoff frequency is
incorporated in the binary mask formula (9) as follows:

1, PÏ„ ,Ï‰ > HÏ„ ,Ï‰ or Ï‰ â‰¤ Ï‰cutoï¬€
(17)
BMÏ„ ,Ï‰ =
0, PÏ„ ,Ï‰ â‰¤ HÏ„ ,Ï‰ and Ï‰ > Ï‰cutoï¬€ .
With a cutoff frequency of 400 Hz, the bass guitar is mostly
present in the P-components; therefore, 400 Hz was chosen
for the cutoff frequency in the music preprocessing scheme of
Fig. 1.

2438

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

Fig. 6. SNR improvement for vocals/drums versus other instruments as a
function of the stereo parameter Î¸ from (18) for different stereo mixes with
panning Ï‡ ranging from 0 to 100 for piano (panned to the left) and guitar
(panned to the right).

Fig. 5. Mean energy ratio of the P-components for eight solo bass guitar
tracks processed with the music preprocessing scheme as a function of the
cutoff frequency. Error bars represent 95% confidence interval.

C. Stereo Binary Mask
Music is widely available in stereo recordings, which means
that all instruments are mixed together in two channels (left and
right). Stereo recordings aim to provide the listener with the experience of a live performance in which instrument sounds are
coming from the direction of the corresponding musicians on
stage [29]. A typical stereo recording consists of vocals, (parts
of the) drums, and bass in the center and other instruments, such
as guitar and piano, panned to the left or the right. This property is also used in karaoke systems to remove the vocals from
the instruments. In [22], a typical voice removal algorithm for
stereo recordings is described, in which the high-pass filtered
left and right channel are subtracted from each other and mixed
together with the low-pass filtered left and right channel. Consequently, only vocals are removed in the output, whereas the
low-frequency content, such as drums and bass, is preserved.
In contrast, the aim for the music preprocessing scheme is to
emphasize vocals, drums, and bass, which are typically mixed
in the center of the stereo image. In [29], the vocals in a stereo
signal are identified by comparing the spectrogram of the left
channel, the right channel, and the difference between left and
right channel. The frequency bins for which the spectrogram of
the difference is smaller than the spectrogram of the left and
the right channel are classified as belonging to the vocals component. A binary mask to extract the center part of the stereo
image can thus be defined as follows:
âŽ§




L
R
diï¬€
âŽ¨ 1, Î¸ âˆ— WÏ„diï¬€
,Ï‰ < WÏ„ ,Ï‰ and Î¸ âˆ— WÏ„ ,Ï‰ < WÏ„ ,Ï‰
BMstereo =
âŽ© 0, Î¸ âˆ— W diï¬€  â‰¥ W L or Î¸ âˆ— W diï¬€  â‰¥ W R
Ï„ ,Ï‰
Ï„ ,Ï‰
Ï„ ,Ï‰
Ï„ ,Ï‰
(18)
L

R

in which Î¸ is a tunable parameter and WÏ„ ,Ï‰ , WÏ„ ,Ï‰ , and WÏ„diï¬€
,Ï‰ are
the respective spectrogram of the left channel, the right channel,
and the difference between the left and the right channel.
Applying the binary mask on the input spectrogram results in

extracting the center part of the stereo recording, which
contains vocals, (part of the) drums, and bass, but separation
performance heavily depends on the broadness of the stereo
image in the recording.
The â€œvocals and drums extractionâ€ block described in
Section II-A separates vocals and drums from the other instruments based on a single-channel input signal. For stereo
recordings, the performance of the â€œvocals and drums extractionâ€ can be significantly improved by exploiting the inherent
spatial information of the instruments. To integrate this spatial
information in the optimization problem (1), an additional stereo
constraint (19) has been added next to constraints (2) and (3),
which limits the P-components to the center of the stereo image
(cfr. (18))




L
diï¬€
R
(19)
Î¸ âˆ— PÏ„diï¬€
,Ï‰ < PÏ„ ,Ï‰ and Î¸ âˆ— PÏ„ ,Ï‰ < PÏ„ ,Ï‰ .
Consequently, the update (4) and (5) are changed into
(j )

(j + 1)

PÏ„2,Ï‰

(j + 1)

PÏ„2,Ï‰

(j + 1)

HÏ„2,Ï‰

Î²Ï„ ,Ï‰ |WÏ„ ,Ï‰ |2

= 
(j )
(j )
Î±Ï„ ,Ï‰ + Î²Ï„ ,Ï‰
(j + 1)

= BMstereo âˆ— PÏ„2,Ï‰

(j + 1)

= |WÏ„ ,Ï‰ |2 âˆ’ PÏ„2,Ï‰

(20)

(21)
(22)

with BMstereo defined in (18) and Î±Ï„ ,Ï‰ and Î²Ï„ ,Ï‰ defined in (6)
and (7).
The SNR improvement for vocals/drums versus other instruments as a function of the stereo parameter Î¸ from (18) is shown
in Fig. 6 for five different audio mixes of a typical pop song
with vocals, drums, bass, guitar, and piano. The audio mixes
were artificially constructed with vocals, drums, and bass in the
center of the stereo image, and different stereo panning Ï‡ ranging from 0 to 100 for piano and guitar. Panning Ï‡ = 0 means no
panning for piano and guitar, resulting in a mono signal, whereas
panning Ï‡ = 100 means complete panning of piano and guitar,
respectively, to the left and to the right of the stereo image. It
is clear that there is no improvement in separation performance
for the audio mix with panning Ï‡ = 0 (mono), nor for the other
audio mixes with stereo parameter Î¸ equal to zero. Increasing
the stereo parameter Î¸ results in an improved SNR, but as shown
in Fig. 7 also results in distorted vocals/drums.

BUYENS et al.: STEREO MUSIC PREPROCESSING SCHEME FOR COCHLEAR IMPLANT USERS

2439

The corresponding output waveform is obtained from the
inverse STFT
output (t) = ISTFT



 j âˆ W 
Ï„ ,Ï‰
WÏ„out
,Ï‰ e

(24)

in which the phase information from the input (âˆ WÏ„ Ï‰ ) is reused.
With the attenuation parameter equal to 0 dB, the output signal of the stereo music preprocessing scheme remains unaltered
compared to the input signal. The final stage in the music preprocessing scheme applies a gain to the output signal as a function
of the attenuation parameter to compensate for the decrease in
output level due to the attenuated H-components.
Fig. 7. Vocals/drums distortion indicated as the energy ratio of the
P-components for the vocals/drums track as a function of the stereo parameter Î¸ from (18) for different stereo mixes with panning Ï‡ ranging from 0 to
100 for piano (panned to the left) and guitar (panned to the right).

III. METHODS
The stereo music preprocessing scheme with an adjustable attenuation parameter was evaluated with CI subjects. This paragraph includes a description of the sound material, the demographic and etiological information of the CI test subjects, and
the test procedure.
A. Sound Material

Fig. 8. Removal of the off-center vocals track (from 0 to 100) from the
P-components of the music preprocessing scheme visualized as the energy ratio
of the P-components for the vocals track as a function of the stereo parameter Î¸.

In the case that the vocals are not mixed in the center of
the stereo image but panned to the left or the right, e.g., for
background singers, the inclusion of the stereo parameter Î¸ in
(18) and (21) influences the vocals separation. Fig. 8 shows
the influence of the stereo parameter Î¸ on the separation of the
vocals represented by the energy ratio of the P-components for
the vocals track. Five different audio mixes of a typical pop
song were artificially constructed atypically with bass, guitar,
and piano in the center of the stereo image, and vocals and drums
panned to left and right with panning level Ï‡ ranging from 0
to 100. Increasing the stereo parameter Î¸ results in the removal
of the vocals from the P-components depending on its panning
level Ï‡.
The parameter Î¸ is determined to be 0.4, which is a
good compromise between vocal distortion and instrument
attenuation.
D. Stereo Music Preprocessing Output
As shown in Fig. 1, the obtained P-components with vocals,
drums, and bass are added to the H-components, which are attenuated with an adjustable parameter â€œAttenuation.â€ The output
spectrogram after addition becomes
WÏ„out
,Ï‰ = PÏ„ ,Ï‰ + Attenuation âˆ— HÏ„ ,Ï‰ .

(23)

For the perceptual evaluation of the stereo music preprocessing scheme, a selection of pop/rock songs was used from the top
50 songs in the all-time greatest hits list of a popular radio station in Belgium (Joe FM). Representative excerpts of the songs
with an average length of 27 s and an average dynamic range of
10.0 dB (SD = 1.5 dB) were selected. The song excerpts were
rms equalized, and stored as stereo wav files with sampling rate
of 44.1 kHz. A complexity rating experiment was performed
with 12 NH test subjects with no self-reported hearing deficit.
The subjects were recruited with an internal advertisement, had
diverse musical background and were familiar with most of
the music excerpts. The 50 song excerpts were played through
headphones (Beyerdynamic DT-770 pro) in a silent room, and
the test subjects were asked to rate the musical complexity of
the song on a scale from 1 to 100 with a slider in a graphical
user interface on a laptop. No further definition or information
was given to the test subjects in order not to prime them in the
experiment. This resulted in a ranking of the 50 songs from the
least complex to the most complex for every subject, which was
quite consistent across subjects. The average ranking over all
subjects was used to compose three groups of songs containing
the eight least complex songs, the eight most complex songs,
and the eight songs with medium complexity. These 24 excerpts
were used in the experiment described in Section III-C.
B. Subjects
Seven postlingually deafened CI subjects (all Cochlear Nucleus) participated in this study. A summary with demographic
and etiological information can be found in Table I. The speech
performance resultsâ€”as provided by the clinicâ€”are measured
with an adaptive speech reception threshold (SRT) test with
LIST material and speech-shaped noise [30]. The subjects
signed a consent form and were paid for their travel expenses.
Ethical committee approval was obtained.

2440

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

TABLE I
DEMOGRAPHIC AND ETIOLOGICAL INFORMATION OF SEVEN POSTLINGUALLY
DEAFENED CI TEST SUBJECTS
Subject

CI1
CI2
CI3
CI4
CI5
CI6
CI7

Age
Gender CI exp SRT
(years)
(years) (dB)
51
63
60
75
55
62
28

Male
Male
Male
Female
Female
Female
Male

âˆ’2
+1
+1
âˆ’1
âˆ’3
âˆ’3
+6

1
3
8
5
3
1
10

Etiology

Sound
processor

Implant
type

Colitis Ulcerosa
Progressive
Unknown
Otosclerosis
Unknown
Progressive
Meningitis

CP810
CP810
Freedom
Freedom
Freedom
CP810
CP810

CI24 RE(CA)
CI24 RE(CA)
CI24 R(CS)
CI24 RE(CA)
CI24 RE(CA)
CI24 RE(CA)
CI24 R(CS)

C. Perceptual Evaluation
For the evaluation of the stereo music preprocessing scheme,
the 24 pop/rock songs, which were divided in three groups of
eight songs with, respectively, low, mid, and high complexity,
were used. Test subjects were asked to select the preferred setting for the attenuation parameter in the range âˆ’6 to 30 dB referring to the attenuation of the H-components in the output signal
of the stereo music preprocessing scheme. A negative value
means an amplification of the H-components. The settings were
visualized on a graphical user interface with buttons numbered
from 1 to 7 representing the range of the attenuation parameter
from â€“6 to 30 dB (or reversed) in steps of 6 dB. The order was
randomized in every trial to prevent the test subject from using
the visual cue in the evaluation. The song excerpts were played
continuously in a loop in free-field in a sound-treated room at a
level of 60 dB(A). The experiment was performed on a laptop
connected to a loudspeaker (Genelec 8020 A). The songs were
presented in a random order and repeated three times. The actual instructions for the test subject were: 1) press the â€œplayâ€
button to start listening to the first music excerpt, 2) listen to the
seven modified music excerpts and identify the differences, and
3) select the music excerpt that is most enjoyable for you and
press â€œnextâ€ to store your preference and to move to the next
music excerpt. The test subjects were allowed to take a break
whenever necessary. The experiment took on average 2 to 2.5 h
per subject.
IV. RESULTS
Fig. 9 shows the individual settings for the preferred attenuation of H-components for the three different groups of songs
(low, mid, and high complexity). In the rightmost column, the
average scores from the seven subjects are indicated. All individual CI subjects preferred for every group of songs, a setting
with attenuated H-components which is significantly different
from zero (One-Sample Wilcoxon Signed-ranks Test, p < 0.05).
However, individual differences occur across subjects with on
the one hand subject CI1 with low preferred attenuation between 0 and 6 dB, and on the other hand, subject CI4 with
high preferred attenuation up to 24 dB for complex songs. Test
subjects CI2, CI4, CI6, and CI7 preferred higher attenuation of
H-components for songs with high complexity, whereas subjects CI1, CI3, and CI5 preferred the H-components to be at-

Fig. 9. Individual results for seven CI subjects with their preferred setting
for the attenuation of the H-components for 24 song excerpts with low, mid,
and high complexity. The average preferred setting from the seven subjects for
low, mid, and high complexity songs are in the rightmost column. Error bars
represent 95% confidence interval.

tenuated irrespective of the complexity of the songs. The average preferred attenuation for each group (8 songs Ã— 7 subjects
Ã— 3 repetitions) is analyzed with the Wilcoxon Signed-ranks
Test with Bonferroni correction and is significantly higher for
songs with high complexity compared to songs with low complexity (Z = 4.71, p < 0.001) or mid complexity (Z = 2.64,
p = 0.024). The difference in preferred attenuation between low
and mid complexity is not significant after Bonferroni correction
(Z = 2.08, p = 0.11).
The preferred average setting of the attenuation parameter for
the 24 song excerpts is positively correlated with the complexity
of the songs as rated by the NH subjects (Pearsonâ€™s r(24) = 0.67,
p < 0.001) (see Fig. 10). For more complex songs, the preferred
attenuation of the H-components is higher.
V. DISCUSSION
A stereo music preprocessing scheme for CI users has been
described, which has the potential to enhance music enjoyment
by emphasizing vocals, drums, and bass in complex music. The
signal processing techniques used are applicable in real time
(with latency around 500 ms), which makes them interesting
for music listening and/or music rehabilitation for CI users.
When listening to songs from a music library, the latency is
not an issue. However, for audiovisual music stimuli, the synchronization between audio and video is corrupted and should
be resolved (if possible) by delaying the video signal with the
(stable) latency. While attending a live performance, the latency
cannot be compensated, and the performance of the proposed
signal processing scheme is likely to be suboptimal, unless two
microphones are capturing the sound in the sweet spot of the
stereo image.
The â€œvocals and drums extractionâ€ block from Fig. 1 is based
on the separation of H- and P-components, which has been

BUYENS et al.: STEREO MUSIC PREPROCESSING SCHEME FOR COCHLEAR IMPLANT USERS

Fig. 10. Mean preferred attenuation of the H-components for the 24 song
excerpts with seven CI subjects as a function of the complexity rating given by
12 NH subjects. Error bars represent 95% confidence interval. Straight line is
the linear regression (R 2 = 0.43).

developed and used to improve the performance of music information retrieval tasks, such as chord detection and drum
detection, respectively. In this paper, it is used to enhance vocals and drums in an audio mix for CI users. Different approaches for drums extraction based on HPSS are described in
[20] and [31]. A comparative evaluation of various HPSS algorithms based on the anisotropic continuity in the spectrogram
is described in [24]. Five different methods are analyzed, and
the parameter sets for each method are tuned to maximize the
signal-to-interference ratio and the signal-to-distortion ratio for
H or P. In [23], the same HPSS techniques are used for melody
extraction by changing the STFT window length, which can
be combined with the drums extraction as in [21] and Fig. 3
to achieve the â€œvocals and drums extractionâ€ from Fig. 1. The
separation performance with mono signals, which is illustrated
in Fig. 2 for a selection of songs, can be significantly improved
for stereo signals by exploiting the spatial properties of a typical
stereo recording with vocals, drums, and bass in the center of the
stereo image and the other instruments panned to left or right.
By increasing the stereo parameter Î¸ from (18), an increase in
SNR for the vocals/drums track is achieved, which is visualized
in Fig. 6 for different artificially constructed audio mixes. The
increase in SNR is most apparent for audio mixes with instruments panned completely to left or right (panning Ï‡ = 100)1 .
However, increasing the stereo parameter Î¸ might also introduce distortions in vocals/drums, which is shown in Fig. 7 with
the energy ratio for the vocals/drums track. Moreover, if the
vocals are not panned in the center of the audio mix, they may
get distorted or completely removed in the output signal. Fig. 8
shows that in an artificially constructed audio mix the vocals
with, e.g., panning Ï‡ = 75 or Ï‡ = 100 are eliminated almost
entirely with stereo parameter Î¸ = 0.4. Background vocals are
typically panned off-center, and are, thus, likely to be attenuated
or removed. If, on the other hand, the panning of the instruments
1 e.g.,

in old recordings from The Beatles.

2441

in the stereo audio mix is not distinct or if the input signal is a
mono signal, the stereo music preprocessing scheme relies only
on the different representations of H- and P-components in the
input spectrogram.
Clear vocals and clear rhythm/beat are described to be top
factors contributing to music appreciation in CI users [3]. A
study with multitrack recordings in which CI subjects (N = 10)
are able to adjust the different instrument levels in pop music excerpts indicates a similar preference for vocals, drums,
and bass in complex music [7]. The modification of the relative instrument level settings in complex music is found to be
beneficial for music appreciation in CI users. The music preprocessing scheme described in this paper is able to emphasize
vocal, drum, and bass tracks based on mono or stereo recorded
music. Songs with different complexity rating are presented to
CI subjects who have been asked to determine their preferred
setting for the attenuation of the H-components. By attenuating
the H-components in complex music a reduction of structural
complexity is achieved. The preference for a high attenuation
of H-components for songs with high complexity, as found in
this study, is in agreement with the findings from [4], in which
a negative correlation is found between complexity and music
appreciation in CI users. The songs that are rated more complex are appreciated less and areâ€”in this studyâ€”preferred with
higher attenuation of H-components as opposed to songs with
low complexity that are already appreciated more, and, thus,
require lower attenuation. The individual differences among
subjects may relate to the experience of the CI user with listening to music. Subjects CI1 and CI7 are active music listeners,
whereas the other subjects only report a more passive music listening involvement. The experienced music listener CI1, who is
capable of distinguishing the different instruments in a complex
song, is missing out certain instruments when attenuating the Hcomponents, whereas a nonexperienced music listener (such as
subject CI4) benefits from the attenuated H-components to more
easily follow the lyrics and the song in general. The sound material excerpts used in this study are pop/rock. In a future study,
more different styles of music should be included together with
a thorough investigation on the musical habits and music experience of the subjects and their speech and pitch performance to
explain the individual differences among subjects.
VI. CONCLUSION
A stereo music preprocessing scheme aimed at improving
music perception and appreciation in CI users has been described and evaluated. The scheme is capable of modifying
the instrument level settings of music, while preserving vocals,
drums, and bass, which has been found to be beneficial for music
appreciation in CI users. The scheme has been evaluated subjectively using pop/rock song excerpts with low, medium, and
high complexity. On average, the preferred setting for the attenuation parameter has been found to be significantly different
for the group of songs with low and high complexity. The music
preprocessing scheme has the potential to improve music appreciation in CI users, in particular, for complex songs. Individual
differences have been observed across subjects.

2442

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

ACKNOWLEDGMENT
The authors would like to thank all CI and NH subjects for
participating in this music experiment, V. Looi and T. Stainsby
for the interesting suggestions in defining the experiments, H.
Buyens for collecting the music tracks, and G. Peeters for the
interesting discussions on music mixing.
REFERENCES
[1] P. Loizou, â€œMimicking the human ear,â€ IEEE Signal Process. Mag., vol.
15, no. 5, pp. 101â€“130, 1998.
[2] H. McDermott, â€œMusic perception with cochlear implants: A review,â€
Trends Amplification, vol. 8, pp. 49â€“82, 2004.
[3] K. Gfeller et al., â€œMusical backgrounds, listening habits, and aesthetic
enjoyment of adult cochlear implant recipients,â€ J. Amer. Acad. Audiol.,
vol. 11, pp. 390â€“406, 2000.
[4] K. Gfeller et al., â€œThe effects of familiarity and complexity on appraisal of
complex songs by cochlear implant recipients and normal hearing adults,â€
J. Music Ther., vol. XL, pp. 78â€“113, 2003.
[5] Y. Kong et al., â€œMusic perception with temporal cues in acoustic and
electric hearing,â€ Ear Hearing, vol. 25, pp. 173â€“185, 2004.
[6] V. Looi et al., â€œMusic perception of cochlear implant users compared
to that of hearing aid users,â€ Ear Hearing, vol. 29, pp. 421â€“434,
2008.
[7] W. Buyens et al., â€œMusic mixing preferences of cochlear implant recipients: A pilot study,â€ Int. J. Audiol., vol. 53, no. 5, pp. 294â€“301, 2014.
[8] T. Virtanen, â€œSound source separation in monaural music signals,â€ Ph.D.
dissertation, Dept. Signal Processing, Tampere University of Technology,
Tampere, Finland, 2006.
[9] J. Smith and X. Serra, â€œPARSHL: An analysis/synthesis program for
nonharmonic sounds based on a sinusoidal representation,â€ presented at
the Int. Comput. Music Conf., Urbana, IL, USA, 1987.
[10] X. Serra, â€œA system for sound analysis/transformation/synthesis based on
a deterministic plus stochastic decomposition,â€ Ph.D. dissertation, Dept.
Music, Stanford University, Stanford, CA, USA, 1989.
[11] X. Serra, â€œMusical sound modeling with sinusoids plus noise,â€ in Musical
Signal Processing, C. Roads, S. Pope, A. Picialli, and G. D. Poli, Eds.
Lisse, The Netherlands: Swets & Zeitlinger Publishers, 1997, pp. 91â€“122.
[12] C. Uhle et al., â€œExtraction of drum tracks from polyphonic music using
independent subspace analysis,â€ in Proc. 4th Int. Symp. Ind. Compon.
Anal. Blind Signal Separation, Apr. 2003, pp. 843â€“847.
[13] M. Helen and T. Virtanen, â€œSeparation of drums from polyphonic music using non-negative matrix factorization and support vector machine,â€
presented at the Eur. Signal Process. Conf., Antalya, Turkey, Sep.2005.
[14] B. A. Olshausen and D. F. Field, â€œSparse coding with an overcomplete
basis set: A strategy employed by V1?,â€ Vis. Res., vol. 37, pp. 3311â€“3325,
1997.
[15] S. A. Abdallah and M. D. Plumbley, â€œPolyphonic transcription by nonnegative sparse coding of power spectra,â€ in Proc Int. Conf. Music Inf.
Retrieval, Barcelona, Spain, Oct. 2004, pp. 318â€“325.

[16] T. Virtanen, â€œSound source separation using sparse coding with temporal continuity objective,â€ presented at the Int. Comput. Music Conf.,
Singapore, 2003.
[17] A. Liutkus et al., â€œKernel spectrogram models for source separation,â€
presented at the 4th Joint Workshop Hands-Free Speech Commun. Microphone Arrays, Nancy, France, 2014.
[18] A. Ozerov et al., â€œA general flexible framework for the handling of prior information in audio source separation,â€ IEEE Trans. Audio, Speech, Lang.,
vol. 20, no. 4, pp. 1118â€“1133, May 2012.
[19] A. Bregman, Auditory Scene Analysis. Cambridge, MA, USA: MIT Press,
1990.
[20] N. Ono et al., â€œSeparation of a monaural audio signal into harmonic/percussive components by complementary diffusion on spectrogram,â€ presented at the Eur. Signal Process. Conf., Lausanne, Switzerland,
2008.
[21] W. Buyens et al., â€œA harmonic/percussive sound separation based music
pre-processing scheme for cochlear implant users,â€ presented at the Eur.
Signal Process. Conf., Marrakech, Morocco, 2013.
[22] G. W. P. York et al., â€œTeaching real-time DSP applications (voice removal)
with the C6711 DSK and MATLAB,â€ in Proc. Amer. Soc. Eng. Edu. Annu.
Conf. Expo., 2004.
[23] H. Tachibana et al., â€œMelody line estimation in homophonic music audio
signals based on temporal-variability of melodic source,â€ in Proc. IEEE
Int. Conf. Acoustics, Speech Signal Process., 2010, pp. 425â€“428.
[24] H. Tachibana et al., â€œComparative evaluations of various harmonic/percussive sound separation algorithms based on anisotropic continuity of spectrogram,â€ in Proc. IEEE Int. Conf. Acoustics, Speech Signal
Process., 2012, pp. 465â€“468.
[25] O. Qazi et al., â€œSpeech understanding performance of cochlear implant
subjects using time-frequency masking based noise reduction,â€ IEEE
Trans. Biomed. Eng., vol. 59, no. 5, pp. 1364â€“1373, May 2012.
[26] S. Omran et al., â€œPitch ranking, melody contour and instrument recognition tests using two semitone frequency maps for nucleus cochlear implants,â€ EURASIP J. Audio, Speech, Music Process., 2010.
[27] R. Kang et al., â€œDevelopment and validation of the University of Washington clinical assessment of music perception test,â€ Ear Hearing, vol. 30,
no. 4, pp. 411â€“418, 2009.
[28] G. L. Nimmons et al., â€œClinical assessment of music perception in cochlear
implant listeners,â€ Otol. Neurotol., vol. 29, no. 2, pp. 149â€“55, 2008.
[29] H. Kim et al., â€œA real time singing voice removal system using DSP and
multichannel audio interface,â€ Int. J. Multimedia Ubiquitous Eng., vol. 7,
no. 2, Apr. 2012.
[30] A. Van Wieringen and J. Wouters, â€œLIST and LINT: Sentences and numbers for quantifying speech understanding in severely impaired listeners for Flanders and the Netherlands,â€ Int. J. Audiol., vol. 47, no. 6,
pp. 348â€“355, 2008.
[31] D. Fitzgerald, â€œHarmonic/percussive separation using median filtering,â€
presented at the Proc. 13th Int. Conf. Digit. Audio Effects, Graz, Austria,
2010.

Authorsâ€™ photographs and biographies not available at the time of publication.

