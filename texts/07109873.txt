2508

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

Accelerating Submovement Decomposition
With Search-Space Reduction Heuristics
Suraj Gowda, Simon A. Overduin, Mo Chen, Young-Hwan Chang, Claire J. Tomlin, Fellow, IEEE,
and Jose M. Carmena∗ , Senior Member, IEEE

Abstract—Objective: Movements made by healthy individuals
can be characterized as superpositions of smooth bell-shaped velocity curves. Decomposing complex movements into these simpler
“submovement” building blocks is useful for studying the neural
control of movement as well as measuring motor impairment due to
neurological injury. Approach: One prevalent strategy to submovement decomposition is to formulate it as an optimization problem.
This optimization problem is nonconvex and finding an exact solution is computationally burdensome. We build on previous literature that generated approximate solutions to the submovement
optimization problem. Results: First, we demonstrate broad conditions on the submovement building block functions that enable
the optimization variables to be partitioned into disjoint subsets,
allowing for a faster alternating minimization solution. Specifically, the amplitude parameters of a submovement can typically
be fit independently of its shape parameters. Second, we develop a
method to concentrate the search in regions of high error to make
more efficient use of optimization routine iterations. Conclusion:
Both innovations result in substantial reductions in computation
time across multiple nonhuman primate subjects and diverse task
conditions. Significance: These innovations may accelerate analysis of submovements for basic neuroscience and enable real-time
applications of submovement decomposition.
Index Terms—Motor control, optimization heuristics, sparse
coding, submovements.

I. INTRODUCTION
OVEMENT kinematics and smoothness are analyzed
when probing the nature of neurodegenerative diseases,
including Huntington’s disease [1], multiple sclerosis [2], and
Alzheimer’s disease [3]. One commonly employed technique
in movement smoothness analysis is to decompose movements
into one or more discrete canonical shapes, or “submovements.”
Superpositions of a few simple submovements, e.g., bell-shaped
velocity profiles, can generate complex kinematic trajectories.
Submovement decomposition aims to recover the presumed
underlying submovement components from the observation of
the overall trajectory. An example of such a decomposition is
shown in Fig. 1, where submovement decomposition enables
us to break up the movement into different components that

M

Manuscript received September 10, 2014; revised February 23, 2015 and April
8, 2015; accepted May 3, 2015. Date of publication May 18, 2015; date of current
version September 16, 2015. This work was supported by the National Science
Foundation under Grant EFRI-M3C 1137267. Asterisk indicates corresponding
author.
S. Gowda, S. A. Overduin, M. Chen, Y.-H. Chang, and C. J. Tomlin are with
the Department of Electrical Engineering and Computer Sciences, University
of California.
∗ J. M. Carmena is with the Department of Electrical Engineering and
Computer Sciences and the Helen Wills Neuroscience Institute, University of
California, Berkeley, CA 94720 USA (e-mail: jcarmena@berkeley.edu).
Digital Object Identifier 10.1109/TBME.2015.2434595

may reflect the points at which differing motor plans began
execution.
The neural origins of submovements remain only partially
understood. One might theorize based on the apparently sparse
nature of submovement initiation that motor control may involve intermittent execution of new feedforward submovements
in response to movement error [4]. Electroencephalographical
correlates of this intermittent control hypothesis have been observed in humans [5]. Alternatively, movement intermittencies
may be related to cortical oscillatory dynamics [6], or they
may simply reflect a continuous controller interacting with the
musculoskeletal system [7]. Nevertheless, quantitatively precise
submovement algorithms have many clinical and engineering
applications. For instance, analysis has shown that submovements become more overlapping or “blended” as patients recover from stroke [8].
Several methods of submovement analysis do not require assumptions about the functional form of the submovement basis
functions. These methods may simply split movements into their
“primary” and “corrective” components [9], or analyze movement acceleration, jerk, and higher order derivatives for zero
crossings or local extrema to find velocity inflection points [10].
However, higher order derivatives of movement can be hard to
acquire using sensors or to compute numerically without substantial noise. Alternatively, the use of an explicit prototype
submovement function (minimum-jerk [11], lognormal [12],
etc.) allows for the use of optimization-based methods to extract submovements from position or velocity time-series data.
This approach involves curve fitting by optimizing a nonconvex
error function. Hogan and colleagues demonstrated that the best
results arise from optimizing over all submovement parameters
jointly rather than sequentially [13]. For clinical applications
where many movements must be analyzed or decompositions
must operate in real time, finding an exact solution requires an
unsuitable amount of computation time. Instead, random-restart
“scattershot” (SS) search retains advantages of joint optimization with orders of magnitude less computation time [14].
Additional reductions in computation time may help enable
otherwise infeasible real-time applications of submovement decomposition. In this study, we extend the SS method [14] to accelerate computation time. First, we define shape pursuit (SP),a
method of iteratively fitting submovements when the basis function has shape parameters distinct from amplitude parameters.
This condition applies to many submovement function prototypes [12]. Second, we introduce the idea of error concentration (EC), which allows for tunable concentration of the iterative
search procedure in regions of high error. This heuristic makes it

0018-9294 © 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

GOWDA et al.: ACCELERATING SUBMOVEMENT DECOMPOSITION WITH SEARCH-SPACE REDUCTION HEURISTICS

2509

Fig. 1. Example of submovement decomposition applied to a movement under target perturbation conditions in which the instructed target location changes in
the middle of the movement (see Section II-E for behavioral details). These perturbation conditions are likely to necessitate a large secondary corrective movement
following the perturbation that manifests as a distinct segment in the velocity profile. In this movement (gray line), two separate overlapping submovements are
extracted (dotted black lines). More than 98% of the velocity variance can be explained by two appropriately shaped and scaled “minimum-jerk” curves (see
Section II).

possible to trade exploration of solution space for computation
speed as demanded by the application. Both of these innovations
yield substantial gains in computation time and improve the
suitability of submovement decomposition for real-time applications.
II. METHODS
A. Submovement Decomposition as an Optimization Problem
Suppose that we have a velocity trajectory v(t) that we wish
to decompose into N submovements, and that the submovement
function can be parameterized as f (t; θ, A), in which t is the
time variable and θ and A are fixed function parameters defining shape (defined precisely in Section II-B) and amplitude,
respectively. For example, the minimum-jerk submovement velocity prototype [11] has three parameters, the onset time t0 , the
duration D, and the amplitude A

4
3
2
1 30 tn − 60 tn + 30 tn , t ∈ [t0 , t0 + D]
f (t; θ, A) = A ·
D 0,
otherwise
(1)
0
in which θ = (t0 , D) and tn = t−t
is
time
normalized
to
the
D
duration of the movement. In general, A may be a column vector
(movements may extend through multiple spatial dimensions),
so f (t; θ, A) maps a scalar t to a vector of the same dimension
as A. Examples of this type of bell-shaped speed profile are
shown in Fig. 1 (dashed lines). This particular submovement
function was previously discovered to be a good characterization of human hand velocity in multijoint arm movements [11].
Numerous other functions also may produce similar bell shapes
[12]. The optimization problem to solve is

2
K 
N





v(t) −
f (t; θi , Ai )
(2)
min



{θ i ,A i }N
i= 1
t=1

i=1

2

in which K is the number of sample points in the trajectory
and || · ||2 is the 2 -norm of a vector. For a set of submovement
N
parameters {θi , Ai }N
i=1 , let r(t) =
i=1 f (t; θi , Ai ) be the reconstructed movement and let c(t) = v(t) − r(t)22 be the reconstruction error at time sample t. This optimization problem
has multiple local optima (and thus, is not convex) and finding
a globally optimal solution can be computationally expensive.
In the SS method, local minima of the cost function are found

from different initializations of the parameters {θi , Ai }N
i=1 [14].
With multiple random initializations, the probability of finding
the true global optimum increases with the number of initializations. The number of submovements N is found iteratively,
by incrementing N and resolving the entire optimization problem [13]. In contrast, a “greedier” algorithm might attempt to
increment N by solving the problem

2
K 
N
−1





f (t; θi , Ai ) − f (t; θN , AN ) . (3)
min
v(t) −


θ N ,A N
t=1

i=1

2

Rather than fitting N submovements to the velocity curve v(t),
(3) fits one submovement
residual error from fitting N − 1
to the
−1
submovements, v(t) − N
f
(t; θi , Ai ). In this study, we ali=1
ways jointly optimize over all submovement parameters, including the parameters resulting from previous optimizations,
as joint optimization has been shown to be less prone to being
trapped in local optima [13].
Note there is no restriction on the dimensionality of the velocity data, as each A can be of dimension k × 1 allowing the
optimization to be carried out over multiple kinematic dimensions jointly. In this study, we decompose 2-D movements and
use
T


v(t) = vx (t) vy (t) vx2 (t) + vy2 (t)

Ai =


Ax Ay

A2x + A2y

T

in which the third element of both vectors is the tangential speed.
The inclusion of the tangential speed term acts as a regularization term. Concretely, suppose that we have a movement where
the true vx (t) = f (t; θex , 1) and vy (t) = 0. Then, the tangential speed of this movement is vs (t) = vx (t). If we happen to
choose r(t) = f (t; θex , [2, 0, 2]T ) + f (t; θex , [−1, 0, 1]T ), then
r(t) perfectly fits vx (t) and vy (t) since the two submovements
partially cancel each other. However, this is clearly an example of overfitting and this particular type of overfitting results
in a bad fit for vs (t). Since the submovements cannot cancel
each other in the speed dimension, vs (t) is fit with an amplitude of 3 when it should have had an amplitude of 1. Thus, the
inclusion of the tangential speed dimension into the optimization penalizes fits of overlapping submovements with opposite
signs. Importantly, it does not preclude such destructive cancellation; an example is shown in Fig. 1 in the y-velocity, where

2510

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

the two submovements of opposite signs slightly overlap. This
type of destructive cancellation may reflect a new motor plan
being vectorially combined with the ongoing motor plan, e.g.,
to abruptly change reach direction [15]. Thus, we aim to penalize such movements to avoid overfitting but not eliminate them
altogether.
Although we selected an 2 -norm cost function in (2), there
is no inherent restriction to do so. Indeed, the choice of cost
function may be motivated by computational rather than physiological considerations. For example, Rohrer and Hogan [13]
used a branch-and-bound algorithm with an 1 -norm cost function for computational speed. Our choice of an 2 -norm cost
function was also motivated by computational considerations,
as described in Section II-B.
For a given movement segment, we incremented the number of submovements fit until 1) success, assessed when the
residual error fell below a mean squared error (MSE) threshold (e.g., 2%), 2) insufficient progress was being made, i.e.,
when incrementing the number of submovements resulted in
an error reduction less than a threshold (e.g., 0.1% MSE), or
3) the maximum number of submovements was hit, where the
maximum was set based on a restriction that the minimum time
between submovement start times be 100 ms (a conservative
minimum reaction time to generate a new submovement as a
visual feedback correction).
B. Innovation 1: Shape Pursuit (SP)
In (1), the shape parameters θ alter the submovement function
independently of the amplitude parameter A: f (t; θ, A) = A ·
f (t; θ, 1). In this case, which describes many submovement
functions [12], (2) becomes

2
K 
N





v(t) − Ai
f (t; θi , 1) .
(4)
min

N


{θ i ,A i }i = 1
t=1

Let

	
V = v(1)
⎡
⎢
⎢
F =⎢
⎣
	

i=1

···

v(K)

2


T

f (1; θ1 , 1)

···

f (1; θN , 1)

..
.

..
.

..
.

f (K; θ1 , 1)

···

T

f (K; θN , 1)

M = A1

···

AN

⎤
⎥
⎥
⎥
⎦

.

The optimization cost function can be rewritten in matrix form
as V − F M 2F , in which ·F is the Frobenius norm. Once
the shape parameters (F ) are fixed, we can optimize for the
amplitude parameters (M ). Let Vx , Vy , and Vs be the first,
second, and third columns of V , and define Mx , My , and Ms
similarly. We seek to solve the problem
min

Vx − F Mx 2 + Vy − F My 2 + Vs − F Ms 2

2
2
subject to Mx(i)
+ My2(i) − Ms(i)
=0

i ∈ {1, · · · , N }.

The constraints are necessary to ensure that the last column is
consistent with its interpretation as the tangential speed scal-

ing factor. A straightforward application of Lagrange multipliers yields a system of highly coupled equations. A similar variable coupling occurs in 1 regularized regressions [16].
For computational reasons, we attempt two relaxations. First,
we notice that the aforementioned N constraints imply that
MxT Mx + MyT My − MsT Ms = 0, though the converse is not
true. Hence, we can relax the problem by replacing the N original constraints with the single condition MxT Mx + MyT My −
MsT Ms = 0, and then, apply the Lagrange multiplier method.
L (M, r) = Vx − F Mx 2 + Vy − F My 2 + Vs − F Ms 2


+ r MxT Mx + MyT My − MsT Ms
∂
L = −2F T (Vx − F Mx ) + 2rMx = 0
∂Mx

−1 T
Mx = F T F + rI
F Vx .

(5)

Similar equations follow for Mx and Ms . The Lagrange multiplier method provides a method of selecting the optimal r, but
it again requires solving a complicated equation. However, we
note that (5) is a ridge regression in which the ridge parameter r acts to shrink the amplitudes of each movement [17]. We
selected r = 0.5, making an implicit prior assumption that the
submovement amplitudes (in meters) are twice as variable as
the observed velocity (in m/s). In reality, we expect that the
velocity variance will be greater than the submovement amplitude, since the peak velocity of a movement (in m/s) is typically
greater than its total displacement (in meters). By assuming submovement amplitudes more variable than observed velocity, we
apply a conservative shrinkage penalty to the submovement amplitudes. We refer the reader to [17] for an extended discussion
of this Bayesian interpretation of ridge regression. Importantly,
although we have made two simplifications to the optimization
problem to calculate the amplitude parameters, the cost function
we use to evaluate the quality of solutions remained the same
across all methods.
A simple 1-D illustration of the SP concept is shown in
Fig. 2(a), where we attempt to fit one submovement to the
black curve. For an estimate of the submovement shape parameters t̂0 and D̂, the cost varies quadratically as a function of
the amplitude (see Fig. 2(a), bottom). For these fixed shape parameters, we can jump directly to the optimal amplitude rather
than seeking the optimal amplitude by an iterative method. On
the next iteration, the shape parameters are refined, and subsequently, the amplitude parameters are again found using (5).
Our original optimization problem thus becomes an alternating
minimization problem in which we alternatively fit shape, and
then, amplitude parameters. SP solves the problem


(6)
min V − F V (F T F + rI)−1 F T F
{θ i }N
i= 1

and makes the search space smaller by reducing the number of
variables to search over. For other types of cost functions, e.g.,
1 -optimization, the same partitioning of the parameter space
can be applied, but different methods must be used to find M .
Importantly, the scale-shape independence property that we
exploit is applicable to submovement functions other than the

GOWDA et al.: ACCELERATING SUBMOVEMENT DECOMPOSITION WITH SEARCH-SPACE REDUCTION HEURISTICS

2511

Fig. 2. Submovement decomposition innovations. (a) Top: An example of a simulated bell-shaped speed profile. For the (incorrect) estimates of the shape
parameters t̂0 and D̂ shown, the submovement of “unit” norm and two amplitude-scaled submovements, including the submovement of best fit (i.e., best scale
parameter for the given shape parameters), are shown. Bottom: The cost function for the same fixed t̂0 and D̂ as a function of the movement amplitude A. The cost
function is quadratic (and therefore, convex) in A, enabling it to be found quickly once the shape parameters are fixed. (b) Simulated velocity profiles as well as
the results of fitting one submovement and fitting two submovements. (c) The point-by-point cost functions c0 (t), c1 (t), and c2 (t) from fitting zero, one and two
submovements. (d) Cost functions from (c) are normalized to sum to one to form c̃0 (t), c̃1 (t), c̃2 (t) with no reweighting [λ = 1, see Section (II-C)]. (e) Different
reweighting factors λ can be used to make the normalized cost more flat (top) or peaked (bottom), enabling control of the “greediness” of the search concentration.

minimum-jerk function considered previously. For instance,
Gaussian functions also produce a bell shape, and we could
use



2 
1
D
g(t; t0 , D, A) = A · exp − 2 t − t0 +
D
2
as the submovement function. In this case, the movement is
no longer finite length; t0 + D2 represents the midpoint of
the movement (peak velocity) and D represents the duration
(smaller D corresponds to more concentration around the midpoint). However, we still retain the property that g(t; t0 , D, A) =
A · g(t; t0 , D, 1), meaning that the SP method still applies. Since
the focus of this study is on the computational methods and not
the function type, we only present results for the minimum-jerk
function.
C. Innovation 2: Error Concentration (EC)
Recall from Section II-A that we solve the optimization problem (2) several times from various initialization points for a
fixed N . One approach to parameter initialization is to sample
independently from uniform distributions for each parameter
on each restart. This is the approach taken by SS [14] and the
approach that we apply to SP. However, the number of restarts
must be small in order to accrue computational savings. As the
number of submovements being fit increases, a small number of
random samples in an exponentially growing search space has
increasingly poor coverage. To combat this, EC initializes parameters greedily. In particular, if we have already found N − 1
submovement parameters but now we want to fit N submove−1
ments, we recycle the old parameters {θi }N
i=1 and append new

randomly sampled parameters θN . Furthermore, we concentrate
θN to represent a region of relatively high error by picking a
submovement shape that spans two adjacent
 local minima in the
reweighted normalized cost c̃(t) = c(t)λ / c(τ )λ dτ in which τ
spans the time samples. The exponent λ is a reweighting factor [18]: λ > 1 makes the function closer to a “delta” (more
“peaked”), λ < 1 makes the function more uniform, and λ = 1
results in no reweighting (only normalization). Fig. 2(c)–(e)
shows simulation examples of the cost, normalized cost, and
reweighted cost. As λ increases, there is a reduction in the
number of local minima from which to initialize θN ; thus the
greediness of the parameter initialization can be quantified and
controlled.
While we initialize parameters greedily, we still perform joint
optimization over all parameters. In other words, EC only deals
with parameter initialization; the actual optimization routine
runs the same as SP. The cost residual c(t) must be calculated and
new parameters must be sampled from some distribution with or
without EC. The predominant computational expense of EC is
to exponentiate the residual c(t), which is an expense similar to
a single evaluation of the cost function. Furthermore, this minor
expense is only incurred when the number of submovements is
incremented. Hence, EC is in practice free when compared to
the expense of the actual optimization routine. In our results, we
used λ = N so that the greediness of the initialization scaled
roughly with the expansion of the search space.
D. Optimization Methods
We primarily used MATLAB’s fmincon to find locally optimal solutions using sequential quadratic programming (SQP) to

2512

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

solve both (2) and (6). The complexity of evaluating the cost
function scales linearly as the number of submovements being
fit increases because (1) must be evaluated once for each submovement i and each time-point t. In standard notation big-O
notation, the complexity of evaluating (2) is O(N K), where N is
the number of submovements and K is the number of time samples in the movement segment. Averaged over 10,000 evaluations, the average computation time to evaluate the cost function
scaled linearly with the number of submovements (R2 > 0.99,
p < 10−19 , evaluated up to 20 submovements). Evaluating the
SP cost function required strictly more time than evaluating the
original cost function (2) in our implementation. We first found
the amplitude parameters by evaluating (5), and then, proceeded
to evaluate the original cost function (2).
We supplied an analytical gradient when using SQP. For the
minimum-jerk prototype function, the complexity of evaluating
the cost function and computing the gradient also scaled linearly
with the number of submovements (R2 > 0.99, p < 10−19 ,
evaluated up to 20 submovements). Computing both the gradient and cost required about 1.6× more time that computing
the cost alone, and did not change significantly as the number
of submovements increased (R2 = 0.13, p = 0.11). The same
gradient was used for solving both (2) and (6). In the case of
SP, it is only an approximate gradient since the amplitudes have
been removed from the optimization variables.
We compared the performance of 1) the SS algorithm [14],
2) SS combined with SP, and 3) SS combined with SP and EC
(SPEC). We also compared the performance of the gradientbased SQP to two nongradient metaheuristics. Specifically,
we assessed the performance of covariance matrix adaptation
evolution strategy (CMA-ES) [19] and simulated annealing
(SA). For CMA-ES, we used the implementation corresponding to [19]. For SA, we used the implementation built into
MATLAB’s optimization toolbox. These methods do not utilize gradient information and instead rely on random search for
exploration. We applied CMA-ES and SA to the original optimization problem (2), which we refer to as CMA-ES-SS and
SA-SS, respectively. We also applied them to the SP equivalent problem (6), which we refer to as CMA-ES-SP and SASP, respectively. A pseudocode outline of parameter initialization and optimization is provided in Algorithm 1. A MATLAB
implementation of the gradient-based methods is available at
http://github.com/sgowda/decompose_submovements.
E. Behavioral Data
Three adult male macaques (Macaca mulatta), J, P, and R,
were used in this study. All procedures were conducted in compliance with the National Institutes of Health Guide for Care
and Use of Laboratory Animals and were approved by the University of California, Berkeley Institutional Animal Care and
Use Committee.
All monkeys performed variants of a 2-D point-to-point reach
and hold task. The monkeys initiated trials by moving their
right hand inside a KINARM exoskeleton (BKIN Technologies, Kingston, Canada) to the visually instructed origin and
holding for 400–1500 ms. Upon entering the origin, the ter-

Algorithm 1 Submovement decomposition pseudocode
Input: v(t), Δ
Output: Submovement parameters ΘN
KΔ
K = # of time samples in v(t), Nm ax = 0.1
s
m ax
Dm in = 0.150 s, Dm ax = 1 s, t0 = KΔ − Dm in
while True do
for k in {1, · · · 10} do
// Initialize submovement parameters
for i in {1, · · · , N } do
if N > 1 and method is SPEC and i < N :
(i)
Get t0 , D(i) from ΘN −1
(i)
elif method is SPEC: Sample t0 and D(i)
as described in Section II-C
(i)
ax
],
elif method is SP: t0 ∼ uni[0, tm
0
(i)
D ∼ uni[Dm in , Dm ax ]
elif method is SS:
(i)
ax
], D(i) ∼ uni[Dm in , Dm ax ]
t0 ∼ uni[0, tm
0
(i)
Ax ∼ uni[min(vx ), max(vx )] · Dm ax
(i)
Ay ∼ uni[min(vy ), max(vy )] · Dm ax
(i)
θi = (t0 , D(i) )
//
// Jointly optimize over all parameters {θi , Ai }N
i=1
// using SQP/CMA-ES/SA
if method is SS:
Solve (6) to find ΘkN
elif method is SP or method is SPEC:
Solve (2) to find ΘkN
ΘN = mink error(ΘkN )
// Determine the relative MSE of 2the current best fit
V −F (Θ N )M (Θ N )F
error(ΘN ) =
V 2
F

ΔM SE = error(ΘN −1 ) − error(ΘN )
// Check termination conditions
if error(ΘN ) < 2% MSE orΔMSE < 0.1% MSE
or N > Nm ax : break
else N = N + 1

minus appeared. The monkeys were then required to reach to
the specified terminus within a 3–7 second time-limit and hold
for 400–1500 ms (constant within each dataset) to receive a
liquid reward. Depending on the dataset, the terminus was either 1 of 8 targets spaced uniformly about a circle or 1 of 18
targets at the corners of two concentric hexagons. During the
trial, only the origin, terminus, and cursor representing the subject’s hand were visible; the remaining targets were hidden. The
next trial then began with the origin appearing at the center of
the workspace (center out) or the previous terminus (point-topoint). Targets were block randomized to evenly distribute trials
to each target in pseudorandom order. In all data, trial initiation
was self-paced. Monkeys were well trained on the task before
the datasets analyzed here were collected. Before decomposing
submovements, we applied a Savitzky–Golay filter to remove
high-frequency recording noise (fifth-order filter, 151-ms window, similar to [1]) and decimated to 100 Hz. Dataset-specific

GOWDA et al.: ACCELERATING SUBMOVEMENT DECOMPOSITION WITH SEARCH-SPACE REDUCTION HEURISTICS

2513

TABLE I
SUMMARY OF DATASETS AND EXPERIMENTAL PARAMETERS
Subject
P
J
R
P
J

Data

Length

Task type

Target configuration

Target radius

Workspace diameter

Hold times

P1
J1
R1
P2
J2

843 s
502 s
611 s
967 s
892 s

Center out
Point-to-point
Center out
Center out and curl force field
Center out and midtrial target jumps

8 points on circle
18 corners of two hexagons
8 points on circle
8 points on circle
8 points on circle

0.75 cm
1 cm
0.75 cm
0.75 cm
1.2 cm

10 cm
15 cm
12 cm
10 cm
13 cm

500 ms
0.5–1.5 s
400 ms
600 ms
700 ms

Three monkeys (J, P, and R) performed point-to-point reach-to-hold movements under a variety of experimental conditions, including different target
configurations, dynamical perturbations (curl force field), and target perturbations (midflight target jumps).

Fig. 4. Average acceleration from dataset P1 at the (a) start and (b) end of
submovements extracted using SPEC. The prominent peaks in the acceleration
traces, marked by dots, correspond to zero crossings in the movement jerk.
These features resemble those extracted without optimization routines purely
from higher derivatives of kinematic data (e.g., [10] and [21]), indicating a relationship between our optimization-based method of submovement extraction
and alternative methods that identify submovement beginning/end by timeseries feature extraction.
Fig. 3. Runtime and quality of fit for decomposition algorithms. Submovement decomposition performance for all five datasets described in Section II-E.
The performance of the SS method [14] was compared to SP, as well as SP
combined with EC (SPEC). Performance was measured by mean and standard
error of runtime and square-root MSE (RMSE) over five attempts. For runtime
comparisons, the length of each dataset is marked with a horizontal dotted line.
*p < 0.05; **p < 0.01; Kruskal–Wallis test.

parameters (duration, perturbation conditions, etc.) for the five
datasets we examined in this study are listed in Table I.
Datasets were not decomposed all at once. Instead, a thresholding procedure was applied to the tangential speed to segment
the velocity profile. A movement segment started whenever tangential speed increased above a 0.005-m/s threshold, and ended
as soon as the speed fell below the same threshold. The average
segment length ranged from 0.7 to 1.1 s over the five datasets.
III. RESULTS
For each of the five datasets, we ran each algorithm five times
in alternating order. Averaging performance statistics from multiple runs over the same data was necessary due to the stochastic nature of the algorithms. Software was implemented in
MATLAB and executed on a personal computer (Intel Core
i7 processor, 16-GB RAM). The results of runtime and MSE
are shown in Fig. 3.
In all datasets, the runtime of SP was significantly lower than
that of SS and the runtime of SPEC was significantly lower than
that of SP (Kruskal–Wallis test, p < 0.01). Absolute runtime
varied across datasets, depending on task structure and any per-

turbations applied during the task. Notably, in all datasets SPEC
was able to complete faster than real time, though in general,
this will depend on the speed of the computing platform. Depending on the dataset, SS took 2–4 times longer than SPEC to
decompose the same movements. Fig. 3 shows that there was
a statistically significant difference in MSE between the three
algorithms, with SPEC outperforming SP, which in turn outperformed SS. Furthermore, SP and SPEC used only slightly more
submovements than SS to fit the same data: averaged over all
datasets and decompositions, 0.1% more for SP and 3.5% more
for SPEC. Using slightly more submovements, SP and SPEC
were able to decompose the same movements for less MSE and
in significantly less time than SS.
We assessed the relationship between our extracted submovements and other feature extraction methods previously proposed
that did not rely on explicit function fitting. Fig. 4 shows the
average acceleration of the hand at the start and end of submovements in dataset P1 extracted using the SPEC method.
Local optima in the acceleration correspond to zero crossings
in the movement jerk. We see clearly that the average submovement onset time is followed by a zero crossing in the jerk and
the average end of a submovement is preceded by another jerk
zero crossing. Other datasets had the same property. This is
consistent with the submovement definition utilized by [10],
which relies on finding inflection points based on local extrema
of the acceleration or zero crossings of the jerk. Similarly, we
also observed that for the fitted submovements, the movement
√
duration (D) scaled with tangential amplitude (As = A 2x +A 2y ) .

2514

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 10, OCTOBER 2015

Fig. 5. (a) Monkey J made a 7-s circular movement starting at the dot and
ending at the “x.” (b) The velocity had many peaks, and thus, required many
submovements to capture. (c) Percent MSE of fitting an increasing number of
submovements for SS, SP, and SPEC was similar for all algorithms. Error bars
indicate standard deviation over 20 independent attempts. (d) For all algorithms,
the mean runtime increases exponentially with the number of submovements
being fit, but the steepness of the exponential increase was much steeper for
SS than for SPEC. (e) Number of function evaluations for each of the three
methods. The number of function evaluations plateaus earlier for SP and SPEC
than for SS.

Averaged over all datasets, the correlation between the fitted
movement durations and the tangential amplitudes was significant (r = 0.26, p < 10−8 ). This relationship was extracted
without an explicit constraint, and similar properties have been
observed in other studies that extracted submovements without fitting an explicit function prototype [20]. These two implicit properties of the submovement statistics extracted in our
approach indicate that physiologically relevant parameters are
being extracted.
We examined in detail one complex movement made by monkey J, which was from a dataset not included in the preceding
performance analysis. Fig. 5(a) and (b) shows a 7-s circular
movement during which the hand never comes to rest. A 7-s
movement is considerably longer than the typical movement duration of the five datasets in the preceding analysis (see Section
II-E). In various studies of reaching movements from patients
with neurological disorders such as stroke, Parkinson’s disease,
multiple sclerosis, and Alzheimer’s disease, typical movements
shown in those studies took 3–10 s [2], [3], [8], [22]. Hence,
this 7-s multipeaked movement represents a clinically relevant
duration. We fit 0–16 submovements to this movement, finding
a relatively high number of submovements necessary due to the
presence of many peaks in the speed profile, similar to what
is observed in stroke patients. (In comparison, each movement
segment in the five datasets previously analyzed required an
average of only 2.3–3.4 submovements to obtain 2% MSE.) We
fit this same movement 20 times to get dispersion estimates of
percent MSE and runtime for SS, SP, and SPEC. The MSE curve
as a function of number of submovements was similar for all

algorithms [see Fig. 5(c)] and all algorithms show exponential
increases in runtime [see Fig. 5(d)] but the increase in runtime
is visibly less steep for SP than for SS and less steep for SPEC
than SP. Though the complexity of cost function evaluations is
higher for SP and SPEC, and the complexity of evaluating the
cost function grows with the number of submovements, Fig. 5(e)
shows that the number of cost function evaluations plateaus earlier for SP and SPEC than SS. SPEC remains subject to the
same exponentially growing search-space problems as SS but
the point at which that begins to impact applications is “pushed
back.”
SP is an analytical method to exploit structure in the submovement decomposition optimization problem. We also explored
the possibility that the same structure might be discoverable
automatically through the use of optimization metaheuristics.
For the same movement segment depicted in Fig. 5, we again
attempted to fit 16 submovements with either CMA-ES or SA
(see Section II-D). We limited CMA-ES to 500,000 cost function
evaluations, which was roughly 100× the computation required
for SS (2,500 cost function and gradient evaluations). On average, CMA-ES-SS achieved an average MSE of 14.6% versus
CMA-ES-SP with an average MSE of 2.4%. This difference
was statistically significant (Kruskal–Wallis test, p < 10−10 , 20
runs). In addition, CMA-ES-SS required an average of 168,000
function evaluations compared to 41,000 function evaluations
for CMA-ES-SP. In the case of simulated annealing, SA-SS
did not fare nearly as well, achieving 73% MSE compared to
9.2% MSE for SA-SP (Kruskal–Wallis test, p < 10−10 , 20 runs).
Again, SA-SS required many more function evaluations than
SA-SP: an average of 68,000 for SA-SS versus 34,000 for SASP. In both cases, the use of problem-specific structure aided
the quality of solution found. That is, the advantages of SP over
SS are not specific to gradient-based optimization solvers. Furthermore, given that the expense of calculating the gradient is
the approximately the same as evaluating the cost function (see
Section II-D), SQP is substantially more efficient than either
metaheuristic.
IV. DISCUSSION
Optimization-based submovement decompositions allow for
a parsimonious and quantitatively precise method of submovement decomposition, but can be computationally expensive. We
have described methods for accelerating such decompositions.
Further computational time savings might be acquired by performing computation on a more specialized hardware platform
(e.g., graphical processing units). Though we have described
this method in the context of the minimum-jerk submovement
prototype function, the same fundamental partitioning of scale
and shape parameters can be applied broadly to other typical
submovement functions, e.g., Gaussian functions, log-normal
functions, etc.
Iterative fitting algorithms, including the approach we use
here, may suffer from overfitting if only goodness of fit criteria are applied, since increasing the number of parameters fit
always provides the opportunity to decrease error. There are
many standard penalty methods that apply to these types of

GOWDA et al.: ACCELERATING SUBMOVEMENT DECOMPOSITION WITH SEARCH-SPACE REDUCTION HEURISTICS

optimizations, in particular, the Akaike information criterion
and the Bayesian information criterion. This issue of methods to
examine overfitting is deserving of further study. However, these
standard methods describe termination conditions for when the
number of submovements should stop being incremented, and
do not affect the execution time differences at the core of this
study.
The SP algorithm analytically finds submovement amplitudes
after the shape parameters are fixed. This algorithm is very similar to some approaches in sparse coding (e.g., [23]). In the
sparse coding problem, if the basis vectors are fixed, then the
coefficients can also be found quickly (see [23] for a more
detailed discussion). Though our approach exploits the same
mathematical properties, there are a few differences. Here, we
fit multidimensional data and enforce a consistency constraint
between the different data dimensions (speed coefficients are
required to be consistent with the Cartesian coordinate coefficients). More importantly, our dictionary is a continuous space
of functions, which we assume from the outset. An interesting
area of future study would be to learn the feature vectors (i.e., the
submovement shapes) using a sparse coding approach, which to
our knowledge has not been formally performed. Unlike sparse
coding approaches, the present approach strictly enforces sparsity constraints by limiting the number of submovements fit,
effectively attempting to minimize a nonconvex 0 norm. Interestingly, [24] reports that the procedure for optimizing the scale
coefficients is slower than the procedure for optimizing the basis
vectors. The reverse is true in this study; further study of the
differences between these two approaches may yield additional
computational benefits.
Our methods enable submovement decompositions from
optimization-based routines to be computed in real time in many
cases, which may possibly enable biofeedback applications. For
instance, since stroke recovery is characterized by increased submovement blending [8], real-time biofeedback of submovement
statistics during therapy sessions may have rehabilitation value.
Though stroke rehabilitation in many ways resembles motor
learning [25], the usefulness of this type of feedback during
therapy remains unclear. For example, when visual feedback is
used to augment natural feedback about postural stability and
center of gravity, some studies indicate that the feedback has
little therapeutic effect (e.g., [26]), while others report positive
gain in the initial stages of therapy (e.g., [27]). The benefits of
quantitative feedback of movement during rehabilitation require
additional testing.

V. CONCLUSION
In this paper, we developed two methodological innovations
to accelerate the process of fitting natural movements to a sum of
smooth submovements. Our innovations apply to a broad range
of submovement functions explored in the literature, and significantly, reduce the time required to complete submovement
decompositions. These innovations may accelerate analysis of
submovements for basic neuroscience and enable real-time applications of submovement decomposition.

2515

REFERENCES
[1] M. A. Smith et al., “Motor disorder in Huntington’s disease begins as a dysfunction in error feedback control,” Nature, 403(6769),
pp. 544–549, Feb. 2000.
[2] T. Schenk et al., “Closed-and open-loop handwriting performance
in patients with multiple sclerosis,” Eur. J. Neurol., vol. 7, no. 3,
pp. 269–279, 2000.
[3] J. H. Yan et al., “Alzheimer’s disease and mild cognitive impairment
deteriorate fine movement control,” J. Psychiat. Res, vol. 42, no. 14,
pp. 1203–1212, Oct. 2008.
[4] A. Fishbach et al., “ Deciding when and how to correct a movement:
Discrete submovements as a decision making process,” Exp. Brain Res,
vol. 177, no. 1, pp. 45–63, Jan. 2007
[5] L. Dipietro et al., “Spatiotemporal dynamics of online motor correction
processing revealed by high-density electroencephalography,” J. Cognitive Neurosci, vol. 26, no. 9, pp. 1966–1980, Sep. 2014.
[6] T. Hall et al., “A common structure underlies low-frequency cortical
dynamics in movement, sleep, and sedation,” Neuron, Aug. 2014.
[7] A. M. Krylow and W. Z. Rymer, “Role of intrinsic muscle properties in
producing smooth movements,” IEEE Trans. Biomed. Eng., vol. 44, no. 2,
pp. 165–176, Feb. 1997.
[8] B. Rohrer et al., “Submovements grow larger, fewer, and more blended
during stroke recovery,” Motor control, vol. 8, no. 4, 2004.
[9] D. E. Meyer et al., “Optimality in human motor performance: Ideal control of rapid aimed movements,” Psychol. Rev., vol. 95, pp. 340–370,
1988.
[10] A. Fishbach et al., “Kinematic properties of on-line error corrections in
the monkey,” Exp. Brain Res, vol. 164, no. 4, pp. 442–457, Aug. 2005.
[11] T. Flash and N. Hogan, “The coordination of arm movements: an experimentally confirmed mathematical model,” J. Neurosci, vol. 5, no. 7,
pp. 1688–1703, 1985.
[12] R. Plamondon et al., “Modelling velocity profiles of rapid movements: A
comparative study,” Biol. Cybern, vol. 69, no. 2, pp. 119–128, 1993.
[13] B. Rohrer and N. Hogan, “Avoiding spurious submovement decompositions: A globally optimal algorithm,” Biol. Cybern, vol. 89, no. 3,
pp. 190–199, Sep. 2003.
[14] B. Rohrer and N. Hogan, “Avoiding spurious submovement decompositions II: A scattershot algorithm,” Biol. Cybern, vol. 94, no. 5,
pp. 409–414, May 2006.
[15] T. Flash and E. Henis, “Arm trajectory modifications during reaching
towards visual targets,” J. Cognitive Neurosci, vol. 3, no. 3, pp. 220–230,
1991.
[16] J. Friedman et al., “Pathwise coordinate optimization,” Ann Appl Stat,
vol. 1, no. 2, pp. 302–332, Dec. 2007.
[17] T. Hastie et al., The Elements of Statistical Learning (Springer Series in
Statistics). New York, NY, USA: Springer, 2001.
[18] S. Thrun et al., Probabilistic Robotics, 2005.
[19] N. Hansen et al., “A method for handling uncertainty in evolutionary
optimization with an application to feedback control of combustion,” IEEE
Trans. Evol. Comput, vol. 13, no. 1, pp. 180–197, Feb. 2009.
[20] A. V. Roitman et al., “Kinematic analysis of manual tracking in monkeys:
Characterization of movement intermittencies during a circular tracking
task,” J. Neurophysiol., vol. 91, no. 2, pp. 901–911, 2004.
[21] K. Novak et al., “Kinematic properties of rapid hand movements in a knob
turning task,” Exp. Brain Res, vol. 132, no. 4, pp. 419–433, Jun. 2000
[22] T. Flash et al., “Kinematic analysis of upper limb trajectories in Parkinson’s disease,” Exp. Neurol, vol. 118, no. 2, pp. 215–226, 1992.
[23] H. Lee et al., “Efficient sparse coding algorithms,” presented at the Int.
Conf. Advanced Neural Information Processing Systems, pp. 801–808,
2006.
[24] R. Raina et al., “Large-scale deep unsupervised learning using graphics
processors,” presented at the 26th Annu. Int. Conf. Machine Learning ,
pp. 873–880, 2009.
[25] J. W. Krakauer, “Motor learning: Its relevance to stroke recovery and
neurorehabilitation,” Curr. Opin. Neurol., vol. 19, no. 1, pp. 84–90, 2006.
[26] C. Walker et al., “Use of visual feedback in retraining balance following
acute stroke,” Phys. Ther, vol. 80, no. 9, pp. 886–895, 2000.
[27] C. M. Sackley and N. B. Lincoln, “Single blind randomized controlled trial
of visual feedback after stroke: Effects on stance symmetry and function,”
Disabil. Rehabil, vol. 19, no. 12, pp. 536–546, 1997.

Authors’ photographs and biographies not available at the time of publication

