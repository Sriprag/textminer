1466

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

Training for Planning Tumour Resection: Augmented
Reality and Human Factors
Kamyar Abhari∗ , John S. H. Baxter, Elvis C. S. Chen, Ali R. Khan, Member, IEEE, Terry M. Peters, Fellow, IEEE,
Sandrine de Ribaupierre, and Roy Eagleson

Abstract—Planning surgical interventions is a complex task, demanding a high degree of perceptual, cognitive, and sensorimotor
skills to reduce intra- and post-operative complications. This process requires spatial reasoning to coordinate between the preoperatively acquired medical images and patient reference frames. In
the case of neurosurgical interventions, traditional approaches to
planning tend to focus on providing a means for visualizing medical images, but rarely support transformation between different
spatial reference frames. Thus, surgeons often rely on their previous experience and intuition as their sole guide is to perform
mental transformation. In case of junior residents, this may lead
to longer operation times or increased chance of error under additional cognitive demands. In this paper, we introduce a mixed
augmented-/virtual-reality system to facilitate training for planning a common neurosurgical procedure, brain tumour resection.
The proposed system is designed and evaluated with human factors
explicitly in mind, alleviating the difficulty of mental transformation. Our results indicate that, compared to conventional planning
environments, the proposed system greatly improves the nonclinicians’ performance, independent of the sensorimotor tasks performed (p < 0.01). Furthermore, the use of the proposed system
by clinicians resulted in a significant reduction in time to perform
clinically relevant tasks (p < 0.05). These results demonstrate
the role of mixed-reality systems in assisting residents to develop
necessary spatial reasoning skills needed for planning brain tumour resection, improving patient outcomes.
Index Terms—Augmented reality (AR), human factors, neurosurgical planning, neurosurgical training, tumour resection, user
study, virtual reality (VR).

I. INTRODUCTION
A. Clinical Motivation
N 2013 alone, 26 000 North Americans were diagnosed
with brain cancer, resulting in 16 000 deaths [1], [2]. It is
estimated that 680 000 Americans are affected by some form
of primary brain or central nervous system tumours (20% of
which are malignant) [3]. The five-year relative survival rate

I

Manuscript received June 7, 2014; revised November 2, 2014; accepted
November 7, 2014. Date of publication December 24, 2014; date of current
version May 18, 2015. This work was supported by the CIHR under Grant
MOP 74626, NSERC under Grant #R314GA01 and Grant #A2680A02, NCEGrand, ORDC Fund, and CFI and Ontario Innovation Trust. The work of K.
Abhari was supported by scholarships from the NSERC, the Government of
Ontario, and Western University Asterisk indicates corresponding author.
* K. Abhari is with the Robart Research Institute, Medical Imaging Laboratory, London, ON N6A 5B7, Canada (e-mail: kabhari@robarts.ca).
J. S. H. Baxter, E. C. S. Chen, A. R. Khan, T. M. Peters, and R. Eagleson are
with the Robart Research Institute, Medical Imaging Laboratory.
S. de Ribaupierre is with the Department of Clinical Neurological Sciences,
London Health Sciences Centre.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2385874

of brain cancer is only 25–35%, making it one of the least
survivable types of cancer in North America [1], [2]. Effective
treatment is therefore necessary to prevent complications, which
can develop with brain cancer, ranging from loss of vision and
speech to paralysis and death.
Among the different courses of treatment, surgical removal
of the tumour is often recommended, the resection constrained
to preserve the brain’s healthy tissues and functional status.
Surgery is also indicated to perform a biopsy or implant a radiation source or chemotherapeutic agent [4]. Furthermore, total
removal of the tumour by surgery may be the only option in
the case of many benign tumours [4]. The success of these interventions greatly depends on the accuracy of planning and
navigating to the target tumour. The goal of preoperative planning is to reduce intra- and post-operative complications by
minimizing damage to healthy tissues and eloquent brain structures, particularly, in the case of deep-seated tumours, where
planning is more critical. This is often accomplished by determining the optimal point of entry, the extent of the craniotomy,
and the surgical pathways through which to advance instruments
for debulking and removing the tumour.
Traditionally, in the absence of any three-dimensional (3-D)
imaging to guide them, neurosurgeons learned to form a mental
representation of the brain to understand the spatial relationship
between the lesion and surrounding structures and landmarks.
This representation is built up from their prior anatomical knowledge, while scrolling through a sequence of two-dimensional (2D) orthogonal slices [see Fig. 1(a)], or through interaction with
3-D images of preoperative magnetic resonance or computed
tomography (CT) scans presented on a computer display [see
Fig. 1(c)]. The ease of use of such perceptual environments—as
will be discussed in the following section—is heavily influenced
by their mode of visualization and interaction. This is even more
true in the case of junior residents, whose more limited experience affects their ability to quickly perform spatial reasoning.
In the current standard of care, residents gradually acquire these
skills over several years throughout their residency by observing expert neurosurgeons planning their approach. Meanwhile,
residents rely heavily on neuronavigation systems, which may
help them to decide the surgical approach, but are not designed
to improve their spatial reasoning abilities.
B. Background
Interaction within an environment involves movement in
some coordinate space that can be estimated relative to a frame
of reference. To interact with the system, one must first establish a frame of reference, and, then, perform a series of mental

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

1467

information augments the real view (AR) to those where real
information augments the virtual scene [augmented-virtuality
or (AV)].
C. Related Work

Fig. 1. Planning environments: (a) 2-D views of axial/coronal/sagittal slices,
(b) XP representation of 2-D slices, (c) 3-D volume rendering, (d) overlay of
virtual images on the real video in an AR environment.

transformations. These transformations correspond to mentally
rotating, translating, or scaling an entity to transform its spatial
location and orientation from one frame of reference to another.
Performing these transformations requires mental resources that
may cause cognitive overload and reduce performance if the capacity of the working memory is exceeded [5]. In the realm of
neurosurgery, the choice of display (and its underlying methods of visualization and interaction) is perhaps one of the most
important, yet underappreciated, factors in task performance.
This is of particular importance in neurosurgical planning, in
which the entire process can vary significantly, depending on the
modes of visualization and interaction. Conventionally, planning is performed by scrolling through 2-D orthogonal slices
(2-D), or interacting with 2-D crossed planes (XP) or 3-D volume or surface rendered images (3-D) [6] [see Fig. 1(a)–(c)].
In this paper, in addition to these conventional approaches, we
explore the possibility of using a mixture of virtual-reality (VR)
and augmented-reality (AR) environments in surgical planning
of tumour resection interventions, with a particular focus on AR.
Unlike VR, where the entire scene is computationally generated,
AR is often described as an environment, in which the view of
the real world is enhanced by overlaying computer-generated
information. One such approach is purposed to complement the
available information with computer generated images, providing a rich view of the operative field, and so facilitating the
performance of a surgical task. An example of an AR environment is illustrated in Fig. 1(d). AR is sometimes referred as
mixed reality, a concept described and popularized by Milgram
and Kishino [7]. In their proposed reality-virtuality continuum,
the mixed-reality spans from environments, in which the virtual

Since the introduction of immersive environments in 1957 [8],
a body of work has been devoted to the development of VR and
AR environments for surgical training, planning, and navigation
(refer to [9]–[13] for more comprehensive reviews). In the realm
of neurosurgery, a number of AR simulators have been developed to allow surgeons to plan and rehearse a surgical approach
prior to the actual operation, with the VR-workbench [14], Dextroscope [15], and ImmersiveTouch [16] platforms being a few
examples. In these simulators, the user must wear a pair of active stereo glasses, while holding a set of controllers positioned
beneath a half-silvered mirror. By doing so, it is then possible to
visualize and manipulate patient volumetric images simultaneously, where the visual and haptic environments are registered.
Although these systems provide a safe training and planning
environment, user interaction is limited due to restricted working space, small field of view, and sometimes absence of head
tracking. A more natural method of interaction was proposed
by Hinckley et al. [17], which involved holding a miniature size
head mannequin in one hand, and a plastic plate in the other hand
to virtually slice open the volumetric representation of patient
data presented on a computer display. Although more intuitive,
the interaction appeared unnatural, whenever the operator’s perspective and the virtual slice were misaligned. To reduce the
risk associated with the insertion of surgical tools, Shamir et al.
have proposed an AR planning environment described in [18]
and [19].
AR also has been shown to facilitate the intraoperative navigation in minimally-invasive neurosurgery. In [20], for instance,
preoperative images of brain landmarks and critical structures
were superimposed on a live video stream of the patient’s head
(or the surgical site in [21]), and displayed on a screen during
surgery. The microscope-assisted guided interventions system
[22] is another example, where 3-D preoperative images were
overlaid into both eyepieces of the binocular optics of a surgical microscope. Shahidi et al. [23] made use of AR to improve
the endoscopic view by registering real endoscopic and virtual
preoperative images together. Bichlmeier et al. [24] proposed a
so-called virtual mirror to display a desired perspective in endoscopic procedures when the virtual object cannot be viewed due
to physical restrictions, a concept similar to the use of mouth
mirrors in dentistry. In the reality augmentation for surgical procedure system [25], a tracked head-mounted display (HMD) was
employed to display the neuroanatomical structures on a head
phantom. In this system, a camera and an LED infrared flash is
incorporated into the HMD, preventing the lag between rendering virtual and real images, as well as the line of sight issue of
the optical trackers. Lerotic et al. made use of a novel nonphotorealistic technique for visualizing virtual object, while maintaining salient anatomical details of the exposed surface [26].
Paul et al. [27] augmented intraoperative views of the operative

1468

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

field over preoperative images, and projected the result onto
a computer display. This compensated for the limited field of
view, providing context to the intraoperative images.
D. Hypothesis and Objective
1) Hypothesis: We hypothesize that, compared to conventional planning environments, the proposed AR system offers a
more intuitive visualization and interaction approach, resulting
in improved task performance to optimize surgical planning of
tumour resection interventions.
2) Objective: In this paper, we propose an AR system to assist novice surgeons in developing the cognitive skills demonstrated by experts in planning tumour resection interventions.
Experiments are conducted to evaluate the proposed system by
measuring the user performance, in both AR and conventional
environments.
Fig. 2.

Underlying mental processes in planning environment.

II. MATERIALS AND METHODS
In this section, a general framework for understanding the
interaction and visualization aspects of planning environments
in brain tumour resection is presented. This framework is used
to guide the design, implementation, and evaluation of the AR
environment described subsequently.
A. Action and Perception in Planning Environments
Different aspects of interactive systems, specifically interaction and visualization, can support different levels of visuospatial actions and perception. Action involves interacting with the
system to update the visualization, which requires mental transformation from the self to the display frame of reference (e.g.,
moving the mouse to the right in order to rotate the image).
Perception, on the other hand, involves observing the effects of
the interaction and reconciling them, performing mental transformation to match the visualization with the desired 3-D representation of the image, or validating that information estimated
from one viewpoint is legitimate in another. The continual cycle
of interaction with the system to perform a task is extremely
flexible and adaptive, and aspects of action and perception are
present in each of the underlying mental processes associated
with interactive systems [28].
B. Mental Processes in Planning Environments
Using an interactive system often involves executing a collection of mental processes. The hypothetical model shown in
Fig. 2 represents three primary underlying mental processes in
determining the optimal point of entry and surgical path within a
given planning environment. Perceptual integration involves assembling a 3-D mental representation of the patient’s brain by
continuously obtaining and processing views. This representation is necessary for action planning, that is, mentally formulating and identifying the optimal surgical path and point of
entry. The specific goals to be addressed are based on a number
of criteria described in Section II-F, and may involve continuous mental processing of the information within a 3-D space.
Additionally, one must coordinate between reference frames to

transform the mental representation of the optimal path/point of
entry from the display frame of reference to the object frame of
reference, i.e., the patient’s skull in the operation room. These
processes may occur simultaneously or sequentially depending
on the preferences and experience of the user. Nevertheless,
minimizing the mental demand elicited by these processes is
necessary to design an intuitive planning system.
C. Mental Demands in Planning Environments
Perceptual integration in 2-D involves generating a 3-D mental representation of the brain by scrolling through 2-D images,
while recalling previous views. This process imposes a high
demand on working memory and can be facilitated by positioning and interacting with these 2-D images within a 3-D context
(e.g., the XP environment). Visualizing the brain in 3-D with
the help of volume rendering can further lower the demand on
spatial reasoning, reducing the need for perceptual integration
(e.g., the AR and 3-D environments) .
The coordination of reference frames required for transforming the formulated optimal path and point of entry into the
physical frame of reference (e.g., patient’s head or a head phantom) is almost negligible in the AR environment. This is because
preoperative images are often overlaid directly on the physical
object, eliminating the transformation between the two. In contrast, extensive mental work is needed in the case of 2-D, XP,
and 3-D environments, particularly for novice operators, as they
must transform the view from the display into the physical frame
of reference.
Furthermore, exploring data via interaction with the phantom
in AR environments mimics direct hand manipulation. Such a
natural everyday human behavior can further reduce the mental demand of relevant judgment tasks in action planning. It
also provides extraretinal signals of different sources (such as
proprioceptive information), enhancing the perception of depth
when paired with certain visualization techniques (details are
discussed in the next section). Although a more realistic representation of the brain in 3-D facilitates the process of action

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

Fig. 3.

Implementation diagram.

Fig. 4. AR system consists of a pair of AR goggles, a head phantom, a stylus,
and an optical tracker. The transformation matrices of T 1, T 2, and T 3 are given
using the optical tracker, while T 4 is computed using camera calibration.

planning compared to 2-D and XP, less intuitive modes of interaction in these conventional environments demand a high degree
of mental effort to formulate the surgical approach.
D. System Implementation
Our AR system comprises several modules and components
as shown in Figs. 3 and 4. These modules can be broken down
into five overarching categories:
1) The AR goggles and other tools,
2) Tracking and calibration components,
3) Medical image components,
4) AR visualization modules, and
5) VR visualization modules.
The AR goggles (Vuzix 920AR, Vuzix corporation,
Rochester, NY, USA) consist of a set of stereoscopic cameras
(480 × 640) that provide a video feed into the planning system.
In addition, the Vuzix goggles contain stereoscopic displays

1469

(600 × 800) on the opposite side on which the system’s renderings can be displayed. Other tools include a head phantom and
a stylus, giving the user physical analogs for virtual representations of the patient and aperture, respectively. Additionally, a
set of foot pedals was incorporated to allow the user to interact with the system, while maintaining their viewpoint of the
phantom. Tracking input to this system includes physical coordinates from an optical tracking system (Polaris, Northern Digital
Inc., Waterloo, Canada), which relate to the AR glasses, tracked
stylus, and head phantom through preacquired calibration matrices. The calibration process for the AR glasses is described in
[29]. The tracked transform was smoothed using a Kalman filter
framework to reduce jitter. The state vector is the pose information, encoded using a 4 × 1 quaternion for rotation and 3 × 1
for translation. The process model is the identity matrix, and the
measurement model is the actual measurement from the optical
tracking system. This tracking information is used to manage
a virtual coordinate system, in which the AR goggles and the
head phantom dictate the position of the renderer viewpoint and
the position of the medical image data, respectively.
The system is capable of rendering T1 weighted MRI images,
using volume rendering integrated with a 2-D transfer function
[30], and displaying associated fMRI activation clusters and
white matter tracts. T1 weighted acquisitions are clinically used
for detecting brain tumours and planning brain tumour resection.
In VR mode, there are two separate renderers. First, the isosurface renderer converts a polyhedral representation of the DTI
tracts, eloquent fMRI regions, and the stylus to a 2-D rendering
of these objects. The depth buffer (z-buffer) for this rendering is
then passed to the VR volume renderer, which uses ray casting,
and 2-D transfer functions to render the tumour and skin in the
T1 weighted MRI. This rendering style supports opacity values
and, thus, renders the background as transparent and the skin
translucent, allowing internal structures to be visualized. These
renderings are then merged together, overlaying the volume rendering on the isosurface rendering. The result is a 3-D rendering
of the phantom with a layer of translucent skin through which
the tumour, white matter tracts, and eloquent cortical regions can
be seen. A virtual representation of the stylus is incorporated
that corresponds to its location relative to the head phantom.
In AR mode, the volume rendering acts similarly, but is sensitive to a virtual aperture at the stylus tip. The same ray casting
technique is used, but rays can terminate prematurely if they do
not pass through the aperture. This conveys the appearance of a
window into the head phantom at the tip of the stylus, which is
sensitive to the position and orientation of the Vuzix glasses relative to the phantom. The occlusion handling system is designed
to identify regions of the video feeds that are occluded by the
participant’s hands (see Fig. 61 ). This is accomplished via huebased thresholding, which masks out the blue surgical gloves
worn by the participant (refer to Section II-E2 for more details).
The final step in the AR display is to merge the AR volume
rendering with the occlusion sensitive video achieved by masking out occluded areas in the AR volume rendering, marking
1 In this paper, the stereoscopic views are illustrated in left-right-left fashion,
corresponding to left-right-left AR cameras and displays.

1470

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

Fig. 6.

Example of hue-based occlusion handling.

Fig. 5. Example of contour enhancement using cel shading in volumetric
images (left: before enhancement, right: after enhancement).
Fig. 7. Example of use of a keyhole with grid lines to promote the sensation
of depth.

them as transparent. Subsequently, the AR volume rendering is
overlaid on the AR video feed to the goggles. Furthermore, as
part of our visualization approach, users have an option to substitute the virtual window with a set of grid lines (see Fig. 7) to
improve the perception of depth (refer to Section II-E for more
details).
The desired rendering system is selected by the user; a foot
pedal can be used to toggle between the AR and VR display
modes. This foot pedal also can be used to lock the position
of the stylus, allowing for the aperture to remain locked in
position, giving the user the opportunity to use both hands to
manipulate the head phantom. The tracking status is provided at
the bottom of the visualization, indicating when a tracked object
has valid line-of-sight with the tracker (green), has lost tracking
information (red), or has its position locked (blue).
E. Perceptual Cues and Considerations in AR
While effective visualizations can dramatically improve the
outcome and efficiency of interactive environments, poor visualization approaches may result in cognitive overload or misinterpretation [31]. This is even more profound in AR environments,
as subtle conflicts between real and virtual images result in incorrect depth perception, degrading performance [11]. Although
many studies have discussed the issue of depth perception in
AR systems [32], [33], problems still persist [11]. In HMDbased AR systems, for instance, accommodation-vergence conflict may cause depth distortion, hindering performance [34].
Accommodation is an involuntary movement of the eye to keep
objects in focus by changing the shape of the lens. Vergence is
the inward/outward movement of both eyes in order to fixate on
a single point. Normally, these two eye movements act in synchrony, providing coherent cues for the perception of depth. In
HMD-based systems, however, the observer’s eyes accommodate to the screens, while constantly diverging and converging to
maintain the binocular vision on a single object, providing incoherent depth cues. This has been shown to be a major drawback
of HMD-based AR systems [32]. Such conflicts, however, can
be mitigated if more dominant visual cues convey the desired

sense of depth. This section is therefore devoted to describe the
visualization techniques used in the AR system to improve the
perception through the most effective depth cues [35] including
stereopsis, motion parallax, and occlusion.
1) Cel Shading: In our previous studies [36], [37], we
showed that contour enhancement can facilitate the perception
of relative depth. This is even more profound in HMD-based
ARs as the effect of contour enhancement is accentuated when
paired with stereopsis [37]. Thus, in this paper, a nonphotorealistic technique called cel shading [37] is employed to enhance
the contours of the target tumour (see Fig. 5).
2) Occlusion Handling: Partial blockage of one object’s
view by another object or occlusion is the strongest cue in perceiving the relative proximity of objects [35]. One well-known
problem with the use of AR environments for medical applications is the occlusion of operators’ hands or medical devices by
virtual images, resulting in depth misperception. To resolve this
issue, hue-based thresholding was employed to detect and mask
the blue surgical gloves worn by the participant (see Fig. 6).
This occlusion handling technique was inspired by the work
described in [38].
3) Grid Lines: The apparent displacement of objects against
their background, or motion parallax, is an effective and unambiguous cue, providing rich information about relative depth. In
our AR environment, motion parallax occurs both when user
makes lateral head movement with respect to the phantom (subject motion), and when the phantom translates relative to the user
(object motion). Studies have shown that the former type of motion provides stronger sense of depth [39], [40], perhaps due to
the fact that interaction with the phantom provides nonvisual
information, such as proprioceptive, enhancing the perception
in concert with motion parallax [41], [42]. Regardless of source
of motion, a stronger sense of parallax can be evoked if the apparent displacement of the tumour is accentuated relative to its
background. To this aim, a set of grid lines was placed behind
the target tumour, amplifying the motional difference between
the tumour and its background (see Figs. 7 and 8).

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

Fig. 8. Visualizing grid lines behind the tumour evokes a strong sense of
motion parallax, while interacting with the phantom.

Fig. 9. Example of the VR mode to visualize DTI tracts, functional areas, and
virtual stylus.

4) Keyhole: Williams [43] has shown that performing tasks
that demand a high degree of visual attention lowers the accuracy of peripheral vision from 75% to 36%, shrinking the field
of view. Therefore, when a visuospatial task requires focused
attention (i.e., high cognitive consumption), it is undesirable to
present a large amount of data in the periphery. This issue is
addressed in the literature by presenting information through a
virtual window [44]–[46]. Similarly, in the proposed AR system, virtual images were placed inside the phantom to present
the inner view within a keyhole (see Fig. 7).
5) Virtual Reality: In AR environments, new information is
superimposed on the real scene. This can extend the information
to the user, but can also add clutter to an already complicated
view, causing information overload [47]. Additionally, clutter in
3-D can lead to visual occlusion of the target, degrading perceptual performance [48]. Accordingly, the amount of information
presented in our proposed AR system was restricted to the target tumour and the keyhole. To visualize additional information,
however, a VR display was employed, in which the real worldview is removed by turning OFF the video feed and halting the
early ray termination process. In the VR mode, the user can visualize DTI tracts, functional areas of the brain, and the stylus’
trajectory with no clutter (see Fig. 9). Users may benefit from
both the AR and VR by toggling between these two modes of
visualization.
F. Evaluation Studies
Linte et al. [13] identified the lack of clinical evaluation and
the difficulty of 3-D information visualization and manipulation
as two major barriers in introducing AR technology into clinical
environments. Conducting user studies is therefore necessary
to validate the benefit of AR environments compared to other
approaches, and to provide insight toward perceptually-correct
visualization methods [13].
As discussed previously, the process of planning usually requires spatial reasoning in order to interact with preoperative
images, and transform the mental representation of the chosen

1471

path and entry point into the patient and operation environment. While 2-D displays are currently considered the de
facto standard for visualization and interaction with preoperative data, AR environments are believed to provide a more
intuitive alternative. Although many research groups have proposed different AR environments for medical interventions (see
Section I-C), evaluation studies are necessary to understand
the system’s shortcomings and facilitate the translation into
clinical practice. This motivated us to assess the effectiveness
and efficiency of the AR system against conventional planning
environments2 , exploring their underlying mental load (preliminary results have been presented in [49] and [50]).
Our experiments involve three different phases. In phases 1
and 2, nonclinicians interacted with synthetic images, whereas
phase 3 involved clinicians and nonclinicians performing
clinically-relevant tasks by examining patient-specific data.
These experiments were designed to address three major planning criteria:
1) determining the shortest possible distance to the target
tumour,
2) aligning the surgical tool with the longest axis (LA) of the
tumour, and
3) specifying a trajectory that would avoid critical functional
areas and white matter DTI tracts.
These criteria are the main considerations in planning tumour
resection to minimize damage to healthy brain tissues.
1) Phase 1 and 2: Data: Stimuli consisted of the CT images of the head phantom with synthetic structures analogous
to patient anatomical data (e.g., an ellipsoid representing the
target tumour). The use of CT images with synthetically created anatomical structures allowed for more control over the
experimental design.
Methodology: Each experiment involved 12 trials per subject (3[tasks] × 4[environments]) through which participants
(8 male/2 female, no prior training) were presented with a randomized collection of synthetic phantom images. Each trial isolated one criterion by defining the task as either determining the
LA of the tumour or the shortest distance (SD) from the skull to
the tumour or the maximal distance from the tumour to a critical structure and so avoiding it. The ground truth for the LA,
SD, and avoidance (AV) tasks were determined, respectively, by
computing the primary eigenvector of the tumour using principle component analysis, segmenting the skull and computing its
distance to the tumour, and finding the maximum Hausdorff distance between the tumour and the critical structure (see Table I).
Subjects were asked, first, to investigate the data to estimate the
optimal point of entry and surgical path and, then, place and
orient the stylus on the head phantom, disclosing their chosen
location and orientation.
The rationale behind the second phase was to examine
whether AR environments can facilitate the planning process,
even if one makes use of a computer to assist with the estimation
of the optimal point of entry and surgical path. Therefore, unlike phase 1, the ground truth, whether the SD, LA, or maximal
2 Neither occlusion handling nor grid lines were incorporated into the AR
system for these evaluation studies.

1472

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

TABLE I
Middle Row: CT IMAGES OF THE PHANTOM WITH A SYNTHETIC ELLIPSOID
(LA), A SPHERE (SD), OR A SPHERE AND A TUBE (AV) WERE GENERATED TO
BE USED AS STIMULI IN PHASE 1; Bottom Row: THE TRUE LA OF THE
ELLIPSOID, SD FROM THE SPHERE TO THE SKULL, AND MAXIMAL HAUSDORFF
DISTANCE FROM THE SPHERE TO THE TUBE (AV) WERE SHOWN IN PHASE 2
(IMAGES ARE ONLY A FEW EXAMPLES FROM THE 2-D ENVIRONMENT)

Fig. 11. Visualization of patient-specific data in (a) 2-D, (b) XP, (c) 3-D, and
(d) AR (left and right images correspond to left and right views of the AR
cameras/displays) environments in phase 3.

Fig. 10. Rotational (left) and translational error (right) were used as metrics
to measure users’ performance.

distance, was illustrated via synthetic lines (see Table I). Such
visual assistance eliminated the need to formulate the optimal
path and entry point, reducing the cognitive load mainly to the
mental transformation from a display frame of reference (the
computer’s screen or HMD’s display) to the object frame of
reference (head phantom).
In both phases, the stimuli, the planning environment, and
the task were randomized and counterbalanced to minimize the
effects of learning and fatigue.
Analysis: As illustrated in Fig. 10, user performance was
measured based on the translational error, that is the Euclidean
distance between the optimal points of entry and those selected
by users (in millimeter), and the rotational error, that is, the deviation between the optimal surgical path and the angle selected
by participants (in degrees).
2) Phase 3: Data: Our medical image input comes from the
medical image computing and computer intervention (MICCAI)
DTI challenge workshop3 to generate a total of 112 different
patient-specific cases by resizing and relocating the segmented
tumour samples. All images were processed a priori to segment
the tumour, relevant eloquent areas, and white matter tracts.
3 MICCAI

2010–2011, permission is granted.

This process was performed by an expert to create clinically relevant scenarios, and also to control the proximity of the tumour
to surrounding eloquent areas. Such regions include functional
areas of visual cortex, hippocampi, and areas representing language and the peripheral limbs, as well as the tractography of
corticospinal, uncinate fasciculus, arcuate fasciculus, and the
Meyer’s loop (see Fig. 11). Including DTI tracts and pseudofMRI data gives context to the experiments, increasing the
ecological validity. For the purposes of this study, the MRI images were registered and visualized within a CT scan of a head
phantom.
Methodology: Each experiment involved 64 trials per subject4
(2[tasks] × 4[environments] × 8[repeated trials]), in which 21
participants were asked to either identify the LA of the tumour
and align the stylus with their chosen axis or determine the
SD to the tumour and place the stylus on their chosen location
over the head phantom. The pool of subjects included seven
clinicians (three neurosurgeons with >five years of practice,
three residents and one fellow with >two years of training) and
14 novice graduate students (i.e., nonclinicians).
Analysis: Rotational error for LA and translational error for
SD were measured after excluding inaccessible points of entry,
such as face, ears, and neck. In addition to accuracy metrics, the
response time (RT) was also recorded, indicating the overall time
one took to explore the images, formulate the optimal points of
entry or surgical paths, and transform the results to the phantom
frame of reference.
4 Forty

eight trials for one of the clinicians.

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

1473

Index of Performance: Because our task has a complicated
3-D structure, we redefined different aspects of Fitts’ Law [51].
First, we express the index of difficulty as
ID = H[P (x)] − H[P (x|x ∈ A)]

(1)

where H[·] is the entropy functional, P (x) is the uniform distribution over possible stylus tips/trajectories, and P (x|x ∈ A)
is the uniform distribution over possible stylus tips/trajectories
that meet the criterion for success. Moreover, our methodology was an extension of Fitts’ classical click in the box task
[51]. Subjects were asked to specify their position and angles as
quickly and accurately as possible, but there was no prespecified
region that would correspond to a hit or miss within a region.
Instead, the effective width of their targeting was derived from
the mean of their position and angular responses. We thereby
define a pseudocriterion for each participant, specifically that
x ∈ A if and only if the error associated with x is less than
or equal to the participant’s average error. Accordingly, we can
define the index of performance for each user as
IP =

1
1
ID = (H[P (x)] − H[P (x|x ∈ A)]) .
T
T

(2)

For the rotational error used in aligning the surgical trajectory
with the LA, the index of performance can be determined analytically from the average angular error, μ (refer to the appendix
for more details)


1
1
IP = log2
.
(3)
T
1 − cos(μ)
For translation, the value of H[P (x)] − H[P (x|x ∈ A)] is determined via a Monte Carlo simulation similar to that employed
to determine the gold standard entry point (refer to the appendix
for more details).
III. RESULTS AND DISCUSSIONS
A. Phase 1
The overall performance was calculated by computing the rotational and translational error (see Fig. 12). Since our hypothesis, i.e., the AR system lowers both rotational and translational
error, may be interpreted methodologically as two independent
hypotheses, the Šidàk correction was applied to control the type
I error. Let the significance threshold for each test be p = 0.01;
then, a stricter
p-value based on Šidàk correction would be:
√
β = 1 − 1 − 0.01 ≈ 0.005, leading to a combined level of
significance of 1%.
A two-way multivariate ANOVA indicated that the environment had a significant effect on accuracy (rotational/
translational error: p < 0.005). Post hoc analysis using Tukey
HSD test revealed that users performed significantly more accurately in AR and 3-D environments, indicating their lower rate of
mental processes compared to XP or 2-D environments. Furthermore, no interaction was observed (p > 0.05) between the task
and the environment. Furthermore, no significant difference was
observed between the visualization environments in terms of the
task completion time [overall time (s)]: 178.8 ± 131.1 [2-D],

Fig. 12. Overall rotational and translational errors observed in phase 1 (top)
and phase 2 (bottom).

204.1 ± 172.7 [XP], 143.6 ± 106.8 [3-D], 170.1 ± 104.6
[AR]).
B. Phase 2
In phase 2, the task was restricted to a response that was
dependent on coordinating between different reference frames
to transform images from the display to the physical context. In
this phase, AR was significantly superior to the other planning
environments, reducing user dependent error. Both translational
and rotational errors were significantly lower in AR compared
to other environments (p < 0.005, see Fig. 12). Unlike phase
1, interaction analysis revealed that the level of performance
within the 2-D and XP environments depended in part upon
the task performed. Moreover, no significant difference was
observed between the visualization environments in terms of
the task completion time [overall time (s)]: 111.5 ± 83.8 [2-D],
123.6 ± 74.8 [XP], 78.7 ± 46.6 [3-D], 77.2 ± 52.7 [AR]).
C. Phase 3
Phase 3 involved clinicians and nonclinicians performing
clinically-relevant tasks by examining patient-specific data. The
result of this phase is depicted in Fig. 13. A two-way multivariate ANOVA indicated that RT was significantly affected by both
expertise and environment (p < 0.005). The choice of environment also significantly affected the rotational error, as well as
Ip for both tasks (p < 0.05). Regardless of expertise, the mean
accuracy was higher in AR compared to other environments (see
Table II), however, no significant difference was observed between these environments, perhaps due to insufficient power as a
result of small sample size, or because of longer experimentation

1474

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

Fig. 13. Top: Translational (left) and rotational error (right); Middle: Time for
SD (left) and LA (right); Bottom: Ip for SD (left) and LA (right).
TABLE II
AVERAGE ROTATIONAL AND TRANSLATIONAL ERROR (PHASE 3)
Rotational error (o ), μ ± σ
AR
38.07 ± 24.22

3-D
41.19 ± 23.74

2-D
42.80 ± 23.25

XP
44.98 ± 23.74

AR
37.29 ± 31.48

Translational error (mm), μ ± σ
3-D
2-D
38.94 ± 27.73
39.48 ± 26.98

XP
40.63 ± 32.91

time, which when coupled with poor ergonomics of the AR goggles resulted in fatigue and higher rate of error.
Unlike SD experiments, significant interaction was observed
in LA (p < 0.05), indicating that users’ performance in selecting the LA within different environments depended in part on
their level of expertise. Post hoc analysis using Tukey HSD
test revealed that, regardless of the task performed, clinicians
performed significantly faster and better (i.e., higher Ip ) than
nonclinicians within the AR and 2-D environments (p < 0.05).
This improvement of Ip within the 2-D environment was not
surprising given the fact that clinicians are highly accustomed
to interpreting 2-D images. Furthermore, regardless of the
task performed, clinicians performed significantly faster within
the AR and 3-D environments compared to the XP, whereas

nonclinicians performed significantly faster within the 3-D environment compared to the XP and 2-D (p < 0.05). In addition,
for the restricted range of eccentricities displayed, no correlation was observed between the eccentricity of the tumours and
performance in any of the environments.
The translational and rotational errors reported in this study
appear to be relatively high for IGI applications. However, this
is due to the nature of the metrics defined. For instance, one
can imagine that selecting an entry point that corresponds to the
second S to the target could be very distant from the optimal
point of entry, resulting in a large translational error. To this aim,
we defined two other metrics as the followings. A translational
error as the difference (in milliliter) between the length of the
optimal path and the path chosen by the user, and a rotational
error as the difference (in milliliter) between the section of the
optimal and chosen path cutting through the tumour. Analyzing
data based on these two metrics led to very similar results,
illustrating the robustness of our initial metrics.
Overall, following observations can be made based on the
results.
1) Phase 1: The improvement of performance in 3-D and AR
indicates that appropriate visualization methods (i.e., AR,
3-D) can significantly facilitate the perception and mental
transformation of the target location and orientation and its
spatial relationship with surrounding anatomical context;
2) Phase 2: The improvement of performance in AR demonstrates that AR can significantly facilitate the process of
perceptual integration and coordination between frames.
In other words, AR is superior to the other planning environments in assisting nonclinicians to generate a mental
representation of the brain, and transform it from a display to the patient frame of reference. In addition, observing an interaction between the task and the environment
in 2-D and XP indicates their lack of generic usability.
In contrast, this suggests the use of AR for applications
that involve tasks with multiple and possibly conflicting
criteria;
3) Phase 3: The improvement of speed in AR and 3-D compared to the XP and 2-D regardless of expertise, and the
task performed illustrates the potential impact of such environments in increasing the efficiency of planning. One
unexpected result, however, was the relationship between
the 2-D and XP. Our conceptual models predicted that
XP would have improved performance compared to 2-D
afforded by presenting information in a 3-D context. Although the action mechanisms in XP were more expressive, they were also more complex, leading to lengthened
RT across all participant groups. This could be mitigated
in practice as users become more familiar with this mode
of interactive viewing.
IV. CONCLUSION
In this paper, we designed, developed, and evaluated an AR
environment, by which novice physicians can practice and improve their basic spatial skills, increasing their cognitive ability to perform neurosurgical planning. Design of the proposed
environment was accomplished by careful consideration of the

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

cognitive and perceptual capacities and limitations of human observers. To evaluate our system, a number of experiments were
conducted, in which subjects performed relevant spatial judgment tasks using the proposed system along with conventional
approaches. Our results indicate that AR environments could
facilitate the task of planning brain tumour resections according to clinically-relevant criteria. Nonintuitive environments, on
the other hand, were shown to be inefficient and less effective.
The proposed work is a preliminary step toward the clinical use
of AR environments, particularly, in facilitating brain tumour
resections.

Ip for LA task-based on the rotational error and RT Let A be
space of acceptable x

1
dx
P (x ∈ A) =
2π
 μ  2π
1
=
sin(θ)dφdθ
2π
0
0
 μ
2π sin(θ)
=
dθ
2π
0
 μ
=
sin(θ) dθ
0

= 1 − cos(μ)
which implies


A

ID = H[P (x)] − H[P (x|x ∈ A)]

= − P (x) log(P (x)) dx

+
A

P (x)
log
P (x ∈ A)



P (x)
P (x ∈ A)


dx



 

1
1
1
1
log
log
+
x∈X |X|
x∈A |A|
|X|
|A|


|X|
= log
|A|


|A| + |¬A|
= log
|A|


1
|¬A|
1
ID =
log 1 +
.
Ip =
RT
RT
|A|


ACKNOWLEDGMENT
The authors would like to thank S. Bakhshmand for assisting
in collecting data, J. Williams, J. Moore, and C. Wedlake for
technical support, and Dr. S. Pujol for granting permission to
use their dataset. They would also like to thank the neurosurgery
residents and consultants at LHCS.
REFERENCES

ID = H[P (x)] − H[P (x|x ∈ A)]

= − P (x) log(P (x)) dx
+

Ip for SD task using the translational error and RT Let |X|
= Total number of points uniformly sampled over the skull, |A|
= number of points sampled within the region with a radius of
average translational error

≈−

APPENDIX
DERIVATION OF Ip

1475

P (x)
log
P (x ∈ A)



P (x)
P (x ∈ A)


dx



=−

P (x) log(P (x)) dx+

1
P (x ∈ A)

=−
0

π


P (x) log(P (x)) dx − log(P (x ∈ A))
A



2π

1
log
2π
0
 μ  2π

1
1 − cos(μ)

0

0



1
2π



1
log
2π

sin(θ) dθ dφ+


1
2π


sin(θ)

dθ dφ − log(1 − cos(μ))
= log(2π) − log(2π) − log(1 − cos(μ))


1
= log
1 − cos(μ)


1
1
1
ID =
log
Ip =
.
RT
RT
1 − cos(μ)

[1] Canadian cancer statistics. (2013). Canadian Cancer Society, Public
Health Agency of Canada. [Online]. Available: http://goo.gl/hkiWep
[2] Cancer facts and figures 2013. (2013). American Cancer Society. [Online].
Available: http://goo.gl/z3vHVc
[3] Primary brain and central nervous system tumors diagnosed in the united
states in 2004–2008. (2012). Central Brain Tumor Registry of the United
States. [Online]. Available: http://goo.gl/mM3Pgd
[4] A. Quinones-Hinojosa, Schmidek and Sweet: Operative Neurosurgical
Techniques: Indications, Methods and Results (Expert Consult—Online
and Print). Philadelphia, PA, USA: Elsevier Health Sciences, 2012. [Online]. Available: http://books.google.ca/books?id=4sKQSY-zdvgC
[5] D. Reisberg, The Oxford Handbook of Cognitive Psychology. London,
U.K: Oxford Univ. Press, 2013.
[6] T. M. Peters, “Image-guidance for surgical procedures,” Phys. Med. Biol.,
vol. 51, no. 14, pp. R505–R540, 2006.
[7] P. Milgram and F. Kishino, “A taxonomy of mixed reality visual displays,”
IEICE Trans. Inf. Syst., vol. 77, no. 12, pp. 1321–1329, 1994.
[8] H. L. Layer, “Sensorama simulator,” U.S. Patent 3 050 870, Aug. 28,
1962. [Online]. Available: https://www.google.com/patents/US3050870
[9] J. H. Shuhaiber, “Augmented reality in surgery,” Arch. Surg., vol. 139,
no. 2, pp. 170–174, 2004.
[10] R. T. Azuma et al., “A survey of augmented reality,” Presence, vol. 6,
no. 4, pp. 355–385, 1997.
[11] T. Sielhorst et al., “Advanced medical displays: A literature review of
augmented reality,” Disp. Technol. J., vol. 4, no. 4, pp. 451–467, 2008.
[12] M. Kersten-Oertel et al., “DVV: A taxonomy for mixed reality visualization in image guided surgery,” IEEE Trans. Vis. Comput. Graph., vol. 18,
no. 2, pp. 332–352, Feb. 2012.
[13] C. A. Linte et al., “On mixed reality environments for minimally invasive
therapy guidance: Systems architecture, successes and challenges in their
implementation from laboratory to clinic,” Comput. Med. Imag. Graph.,
vol. 37, no. 2, pp. 83–97, 2013.
[14] T. Poston and L. Serra, “The virtual workbench: Dextrous VR,” in Proc.
Conf. Virtual Reality Softw. Technol., 1994, pp. 111–121.

1476

[15] A. T. Stadie et al., “Virtual reality system for planning minimally invasive
neurosurgery,” Neurosurgery, vol. 108, pp. 382–394, 2008.
[16] C. Luciano et al., “Design of the immersivetouch: A high-performance
haptic augmented virtual reality system,” presented at the 11th Int. Conf.
Human-Comput. Interact., Las Vegas, NV, USA, 2005.
[17] K. Hinckley et al., “The props-based interface for neurosurgical visualization,” Stud. Health Technol. Informat., vol. 39, pp. 552–562,
1997.
[18] R. R. Shamir et al., “Reduced risk trajectory planning in image-guided
keyhole neurosurgery,” Med. Phys., vol. 39, no. 5, pp. 2885–2895, 2012.
[19] R. R. Shamir et al., “Trajectory planning with augmented reality for improved risk assessment in image-guided keyhole neurosurgery,” in Proc.
IEEE Int. Symp. Biomed. Imag., Nano Macro, 2011, pp. 1873–1876.
[20] W. Grimson et al., “Image-guided surgery,” Sci. Amer., vol. 280, no. 6,
pp. 54–61, 1999.
[21] R. A. Kockro et al., “Dex-ray: Augmented reality neurosurgical navigation
with a handheld video probe,” Neurosurgery, vol. 65, no. 4, pp. 795–808,
2009.
[22] P. J. Edwards et al., “Design and evaluation of a system for microscopeassisted guided interventions (MAGI),” IEEE Trans. Med. Imag., vol. 19,
no. 11, pp. 1082–1093, Nov. 2000.
[23] R. Shahidi et al., “Implementation, calibration and accuracy testing of an
image-enhanced endoscopy system,” IEEE Trans. Med. Imag., vol. 21,
no. 12, pp. 1524–1535, Dec. 2002.
[24] C. Bichlmeier et al., “The virtual mirror: A new interaction paradigm for
augmented reality environments,” IEEE Trans. Med. Imag., vol. 28, no. 9,
pp. 1498–1510, Sep. 2009.
[25] C. R. Maurer, Jr., et al., “Augmented-reality visualization of brain structures with stereo and kinetic depth cues: System description and initial evaluation with head phantom,” Med. Imag. Int. Soc. Opt. Photon.,
vol. 445, pp. 445–456, 2001.
[26] M. Leroticet al., “Pq-space based non-photorealistic rendering for augmented reality,” in Proc. Med. Image Comput. Comput.-Assisted Intervention Conf., 2007, pp. 102–109.
[27] P. Paul, “Augmented virtuality based on stereoscopic reconstruction in
multimodal image-guided neurosurgery: Methods and performance evaluation,” IEEE Trans. Med. Imag., vol. 24, no. 11, pp. 1500–1511, Nov.
2005.
[28] R. Eagleson, “Perceptual capacitites and constraints in augmented reality
biomedical displays,” presented at the Innovations Patient Care, Eng.
Phys. Sci. Med. Aust. Biomed. Eng. Conf., Christchurch, New Zealand,
2008.
[29] E. C. Chen, “An augmented reality platform for planning of minimally
invasive cardiac surgeries,” Proc. SPIE, Med. Imag., Int. Soc. Opt. Photon.,
vol. 8316, pp. 831617–831617, 2012.
[30] J. S. Baxter et al., “A unified framework for voxel classification and
triangulation,” Proc. SPIE, Med. Imag., Int. Soc. Opt. Photon., vol. 7964,
pp. 796436–796436, 2011.
[31] C. Ware, Information Visualization: Perception for Design. New York,
NY, USA: Elsevier, 2012.
[32] E. Kruijff et al., “Perceptual issues in augmented reality revisited,”
in Proc. IEEE 9th Int. Symp. Mixed Augmented Reality, 2010, vol. 9,
pp. 3–12.
[33] D. Drascic and P. Milgram, “Perceptual issues in augmented reality,”
Proc. Electron. Imag, Sci. Technol., Int. Soc. Opt. Photon., vol. 2653, pp.
123–134, 1996.
[34] M. Mon-Williams and J. R. Tresilian, “Ordinal depth information from
accommodation?” Ergonomics, vol. 43, no. 3, pp. 391–404, 2000.
[35] S. Nagata, “How to reinforce perception of depth in single twodimensional pictures,” Proc. Soc. Inf. Disp., vol. 25, no. 3, pp. 239–246,
1984.
[36] K. Abhari et al., “Perceptual enhancement of arteriovenous malformation
in MRI angiography displays,” Proc. SPIE, Med. Imag., Int. Soc. Opt.
Photon., vol. 8318, pp. 831809–831809, 2012.
[37] K. Abhari et al., “Visual enhancement of MR angiography images to
facilitate planning of arteriovenous malformation interventions,” ACM
Trans. Appl. Percept., submitted for publication, 2014
[38] O. Kutter et al., “Real-time volume rendering for high quality visualization
in augmented reality,” presented at the Int. Workshop Augmented Environ.
Med. Imag., Augmented Reality Comput.-Aided Surg., New York, NY,
USA, 2008.
[39] B. Rogers et al., “Motion parallax as an independent cue for depth perception,” Perception, vol. 8, no. 2, pp. 125–134, 1979.
[40] H. Ono and M. J. Steinbach, “Monocular stereopsis with and without head
movement,” Percept. Psychophys., vol. 48, no. 2, pp. 179–187, 1990.

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 6, JUNE 2015

[41] V. Cornilleau-Peres and J. Droulez, “The visual perception of threedimensional shape from self-motion and object-motion,” Vis. Res.,
vol. 34, no. 18, pp. 2331–2336, 1994.
[42] M. R. Mine et al., “Moving objects in space: Exploiting proprioception
in virtual-environment interaction,” in Proc. 24th Annu. Conf. Comput.
Graph. Interacti. Techn., 1997, pp. 19–26.
[43] L. J. Williams, “Tunnel vision induced by a foveal load manipulation,” Human Factors, J. Human Factors Ergonom. Soc., vol. 27, no. 2,
pp. 221–227, 1985.
[44] M. Bajura et al., “Merging virtual objects with the real world: Seeing ultrasound imagery within the patient,” ACM SIGGRAPH Comput. Graph.,
vol. 26, no. 2, pp. 203–210, 1992.
[45] C. Bichlmeier et al., “Contextual anatomic mimesis hybrid in-situ visualization method for improving multi-sensory depth perception in medical
augmented reality,” in Proc. IEEE 6th Int. Symp. Mixed Augmented Reality, 2007, pp. 129–138.
[46] L. T. De Paolis et al., “Augmented visualization of the patient’s organs
through a slidingwindow,” presented at the Workshop Vis. Model. Vis.,
Braunschweig, Germany, 2009.
[47] S. Mann and S. M. Nnlf, “Mediated reality,” 1994.
[48] R. Rosenholtz et al., “Measuring visual clutter,” J. Vis., vol. 7, no. 2,
p. 17, 2007.
[49] K. Abhari et al., “Use of a mixed-reality system to improve the planning of brain tumour resections: Preliminary results,” in Augmented Environments for Computer-Assisted Interventions, New York, NY, USA:
Springer, 2013, pp. 55–66.
[50] K. Abhari et al., “The role of augmented reality in training the planning
of brain tumor resection,” in Augmented Reality Environments for Medical Imaging and Computer-Assisted Interventions, New York, NY, USA:
Springer, 2013, pp. 241–248.
[51] P. M. Fitts, “The information capacity of the human motor system in
controlling the amplitude of movement.” J. Exp. Psychol., vol. 47, no. 6,
pp. 381–391, 1954.

Kamyar Abhari received the Master’s of Science degree in biomedical engineering from the University
of Manitoba, Winnipeg, MB, Canada, in 2008. He is
currently working toward the Ph.D. degree at Western University, London, ON, Canada. He is a Member
of the Robarts Research Institute, London. His primary research interests include surgical visualization
and human–computer interaction in image-guided
surgeries.

John S. H. Baxter received the Bachelor’s degree in
software engineering from the University of Waterloo, Waterloo, ON, Canada, in 2012. He is currently
working toward the Ph.D. degree at the Biomedical Engineering Graduate Program, Western University, London, ON, Canada. He is a Member of
the Robarts Research Institute, London. His current research interests include biomedical-image processing, statistical-image representation, and image
segmentation.

Elvis C. S. Chen received the Ph.D. degree in computer science from Queens University, Kingston, ON,
Canada, in 2007.
He works in the field of image-guided interventions, by applying techniques in robotics and computer graphics to the field of surgery. His research
interests include joint kinematics, ultrasound-guided
needle interventions, tool calibration and tracking,
and vision-guided laparoscopy. He is currently a Research Associate at the Robarts Research Institute,
London, ON, with cross appointment with Western
University, London.

ABHARI et al.: TRAINING FOR PLANNING TUMOUR RESECTION: AUGMENTED REALITY AND HUMAN FACTORS

Ali R. Khan (M’14) received the Ph.D. degree in
engineering science from Simon Fraser University,
Burnaby, BC, Canada, in 2011.
His research is focused on the development of intelligent image processing and analysis technologies,
and their application in the diagnosis and treatment
of neurological disorders. His research interests include computational anatomy, quantitative magnetic
resonance imaging, diffusion tensor tractography, and
image registration. He is currently an Assistant Professor at the Department of Medical Biophysics,
Western University, London, ON, Canada, and is affiliated with the Robarts
Research Institute, London. He is an Associate Member of the IEEE Engineering in Medicine and Biology Society Technical Committee on Biomedical
Imaging and Image Processing.

Terry M. Peters (S’72–M’73–SM’97–F’09–LF’14)
is currently a Scientist at the Imaging Research Laboratories, Robarts Research Institute, London, ON,
Canada, and a Professor at the Departments of Medical Imaging, Medical Biophysics, and Biomedical
Engineering, Western University, ON. From the past
30 years, he has been working in the field of imageguided surgery and therapy within the Robarts Imaging Research Laboratories. His lab has expanded
more than the past 13 years to encompass imageguided procedures of the heart, brain, and abdomen.

1477

Sandrine de Ribaupierre received the MD degree
from the University of Geneva, Geneva, Switzerland.
After a Neurosurgery residency in Lausanne,
Switzerland, she completed an epilepsy fellowship
from the Fondation Rothschild, Paris, France, then a
pediatric neurosurgery fellowship from the Hospital
for Sick Children, Toronto, Canada. She is c]urrently
an Assistant Professor at the Division of Neurosurgery, University of Western Ontario, London, ON,
Canada, working as a Pediatric Neurosurgeon with
some involvement in pediatric and adult trauma and
adult epilepsy surgery. Her main research interests include virtual reality as an
educational tool, such as to teach neuroanatomy to medical students, as well
as surgical simulation for residents. She is collaborating with CSTAR, Robarts,
and the Centre for Brain and Mind.

Roy Eagleson received the Ph.D. degree supervised
by Zenon Pylyshyn from the University of Western
Ontario, Centre for Cognitive Science, London, ON,
Canada, in 1992. In 2014, he was an Associate Professor with the Faculty of Engineering, Western University, London, doing his research with the Robarts
Research Institute, London, a Scientist and a Principal
Investigator at Canadian Surgical Technologies and
Advanced Robotics Research Center, London Health
Sciences Centre, Lawson Health Research Institute,
London, and Collaborator with the UWO Brain and
Mind Institute. His research interest includes 3-D biomedical visualization
and surgical simulation, human–computer interface design for telerobotics and
telesurgery, haptic interfaces, and programmable graphical interfaces (GPU
programming).

