500

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

Local and Global Preserving Semisupervised
Dimensionality Reduction Based on Random
Subspace for Cancer Classification
Xianfa Cai, Jia Wei, Guihua Wen, Member, IEEE, and Zhiwen Yu, Member, IEEE

Abstract—Precise cancer classification is essential to the successful diagnosis and treatment of cancers. Although semisupervised
dimensionality reduction approaches perform very well on clean
datasets, the topology of the neighborhood constructed with most
existing approaches is unstable in the presence of high-dimensional
data with noise. In order to solve this problem, a novel local and
global preserving semisupervised dimensionality reduction based
on random subspace algorithm marked as RSLGSSDR, which
utilizes random subspace for semisupervised dimensionality reduction, is proposed. The algorithm first designs multiple diverse
graphs on different random subspace of datasets and then fuses
these graphs into a mixture graph on which dimensionality reduction is performed. As the mixture graph is constructed in lower
dimensionality, it can ease the issues on graph construction on highdimensional samples such that it can hold complicated geometric
distribution of datasets as the diversity of random subspaces. Experimental results on public gene expression datasets demonstrate
that the proposed RSLGSSDR not only has superior recognition
performance to competitive methods, but also is robust against a
wide range of values of input parameters.
Index Terms—Cancer classification, dimensionality reduction,
graph construction, random subspace, semisupervised learning.

I. INTRODUCTION
IMENSIONALITY reduction is widely recognized as one
of the key steps towards such areas as computer vision,
machine learning and pattern recognition. Data in these fields are
often represented by a point in a high-dimensional space, such
as digital photographs, gene expression profile, text document,
etc. With the development of biology technic, the molecular diagnosis offers a systematic and precise prospect for tumor classification. At present, one typical molecular diagnosis method
is the DNA microarray technology, which has been becoming a
powerful tool in functional genome studies [1], [2]. Microarray

D

Manuscript received April 10, 2013; revised August 14, 2013; accepted
September 5, 2013. Date of publication September 16, 2013; date of current version March 3, 2014. This work was supported by the National Natural Science
Foundation of China under Grant 61273363, Grant 60973083, Grant 61070090,
and Grant 61003174.
X. Cai is with the School of Medical Information Engineering, Guangdong
Pharmaceutical University, Guangzhou 510006, China, and also with the School
of Computer Science and Engineering, South China University of Technology,
Guangzhou 510006, China (e-mail: cxianfa@126.com).
J. Wei, G. Wen, and Z. Yu are with the School of Computer Science and Engineering, South China University of Technology, Guangzhou 510006, China.
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2281985

experiments may lead to a complete observation of the molecular variations among tumors by monitoring the expression levels
in cells for thousands of genes simultaneously, and hence result
in a reliable classification. Due to high dimensionality and a
small number of noisy samples, gene expression data poses
great challenges to the existing machine learning methods as
it usually results in the known problem of “curse of dimensionality” [3]. During past years, it has drawn a great deal of
attention from both biological and engineering fields. Different
classification methods have been applied to cancer classification, such as support vector machines (SVMs) [4], [5], neural
networks [6], and GA/KNN [7]. More recently, with the development of machine learning, feature extraction has been extensively used in cancer classification. The most common methods
involved in gene data analysis are principal component analysis
(PCA), partial least squares (PLS) [8], and independent component analysis (ICA) [9]. A systematic benchmarking of these
methods is reported in the literature [10]. As gene expression
data exists in a form of nonlinear high-dimensional vector, many
nonlinear feature extraction methods have been developed for
cancer classification, such as kernel principal component analysis and LLDE [11], [12]. In addition, many other nonlinear
feature extraction methods have been proposed, such as LLE,
LE [13], LPP [14], ISOMAP [15], and LTSA [16], to address
the problem.
At the same time, labeled samples are fairly expensive to obtain as labeling often requires expensive human labor and much
time. However, in many practical classification tasks, unlabeled
training samples are easy to get but not used. This leads to the
potential waste of valuable classification information buried in
unlabeled samples. Therefore “Semi-supervised Learning” with
both labeled and unlabeled data has recently attracted more and
more attention [17]. The goal of semisupervised classification is
to use unlabeled data to improve the generalization of the classifier. By using unlabeled samples, semisupervised approaches
usually have better generalization ability than its corresponding supervised ones. On the other hand, due to extra focus on
labeled samples, semisupervised approaches often have better
performance than its unsupervised ones [17]. As we all know,
pairwise constraints information also called side information is
more general than label information, since we can obtain side
information from label information but not vice verse [18]. In
the past years, significant progress has been made on semisupervised dimensionality reduction with pairwise constraints. Zhang
et al. [19] put forward a semisupervised dimensionality reduction (SSDR) technique which utilizes both the must-link and

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

CAI et al.: LOCAL AND GLOBAL PRESERVING SEMISUPERVISED DIMENSIONALITY REDUCTION BASED ON RANDOM SUBSPACE

cannot-link constraints effectively. However, the method fails
to preserve the local structure of the data except preserving
the global covariance structure. Even worse, SSDR is sensitive to noise, since it considers the similarity between samples
coarsely. Cevikalp et al. [20] extended LPP to semisupervised
version with pairwise constraints and constraints propagation
(termed as CLPP). However, CLPP’s constraints propagation is
destructive when there are noise and outliers of samples. Wei and
Peng [21] proposed a method termed the neighborhood preserving based semisupervised dimensionality reduction (NPSSDR).
NPSSDR makes full use of side information, not only preserves
the must-link and cannot-link constraints but also can preserve
the local structure of the input data. Yet it is still susceptible to noise and outliers. All these methods mentioned earlier
are variants of graph embedding [22] with different schemes
to construct graphs. Thus, we argue that constructing a faithful graph of samples for graph embedding is extremely crucial
to graph embedding. Unfortunately, as the dimensionality increases, the distance metric of samples becomes meaningless,
leading to the deteriorated graph structure of samples. Besides
these approaches with pairwise constraints, there are semisupervised dimensionality reduction methods without pairwise
constraints, but with partially labeled or some benchmark samples in recent literatures. Such as semisupervised discriminate
analysis (SDA) [23], semisupervised learning on Riemannian
manifolds [24], spectral methods for semisupervised manifold
learning [25], and so on.
Generally speaking, when dealing with high-dimensional
samples, however, it’s hard to construct a graph that faithfully reflects the relationships between samples [26]. This is
because the distance between samples becomes meaningless as
the dimensionality increases [27]. The similarity (or distance)
metric is in fact often distorted by noisy or redundant features
of high-dimensional data. Considering that past semisupervised
dimensionality reduction methods are sensitive to the selection
of neighborhood parameter and rely more on the construction of
graph, this paper puts forward a special way to address this problem by utilizing the random subspace to semisupervised dimensionality reduction. Under these motivations, a novel algorithm
of local and global preserving semisupervised dimensionality
reduction based on random subspace (RSLGSSDR) is proposed
in this paper. The algorithm first constructs multiple diverse
graphs on different random subspaces of datasets and then combines these graphs into a mixture graph on which dimensionality reduction is performed such that it can preserve the global
geometric structure of the data as well as preserving its local geometric structure. As the mixture graph is constructed in lower
dimensionality, it can ease the issues on graph construction on
high-dimensional samples such that it can hold complicated
geometric distribution of dataset as the diversity of random subspaces. The approach has significantly increased performance
and robustness. Experimental results on public datasets demonstrate that the proposed approach often gives the better results
in classification and robustness.
There are two main contributions in this paper. First, random
subspace is designed to form a mixture graph which can enhance

501

the performance of classification, as the mixture graph is constructed in the lower space where the distance metric is more
reliable than that in original space. Furthermore, this method
has the ability to hold more complicated geometric distribution
due to the diversity of random subspaces. Second, a semisupervised dimensionality reduction method on this mixture graph is
proposed. It is robust to noise and outliers, and less sensitive to
the choice of parameters. Consequently, it can be applicable to
many real world tasks.
The rest of the paper is organized as follows. In Section II,
we review the random subspace. In Section III, we introduce
the proposed RSLGSSDR method. The experimental results are
presented in Section IV. In Section V, we conclude our methods
and provide some suggestions for future work.
II. RELATED WORK-RANDOM SUBSPACE
Our paper focuses on determining a better neighborhood
graph. Generally speaking, there are two alternative ways applied in past work. The first is ε-neighbor method, but it’s hard
to determine an optimal ε so seldom used in practice. The other
general way is k-nearest neighbors approach, but it’s also usually hard to choose an appropriate neighborhood size k. This
approach meets a lot of problems in practice, such as they are
not suitable for unevenly distributed manifolds or when noise
arises, short-circuit edges will influence the performance.
Random subspace method (RSM) was first introduced by T.
K. Ho to construct multiple decision tree classifiers in different
random subspaces. These classifiers were then combined for
ensemble classification [28]. As a parallel learning algorithm,
the generation of each subspace is independent. The method
relies on an autonomous procedure to select a small number
of dimensions from a high-dimensional space. All samples are
projected to this subspace, and a classifier is trained by using the projected training samples. In classification a sample
of an unknown class is also projected to the same subspace
and classified using the corresponding classifier. Following this
method, various random subspace-based classification methods
were proposed [29]–[31]. However, whether random subspace
based semisupervised classification or random subspace based
evidence classifier, they all design multiple random subspaces
and train classifiers directly in those subspaces. Besides, the
RSM were often used to boost ensemble learning for semisupervised learning or subspace clustering for high-dimensional
data [31], cotraining [32], or for the missing feature problem
[33]. Different from them, in our experiment, multiple diverse
graphs are designed in different random subspaces of datasets
and are then combined to form a mixture graph on which dimensionality reduction is performed. So, we don’t train classifiers
directly in those subspaces.
III. PROPOSED METHOD-RSLGSSDR
A. Objective Function of RSLGSSDR
We define the semisupervised linear dimensionality reduction
problem based on the side information as follows. Suppose we

502

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

have a set of D-dimensional points X = {x1 , x2 , ..., xn }T , xi ∈
RD together with some pairwise must-link (M ) and cannotlink (C) constraints as side-information: M : (xi , xj ) ∈ M , if
xi and xj belong to the same class; C : (xi , xj ) ∈ C, ifxi and
xj belong to the different classes. Then the next step is to find a
transformation matrix W such that the transformed low dimensional projections Y (yi = W T xi ) can preserve the structure of
the dataset as well as the side information. That is, points in M
should be close to each other while points in C should be as
far as possible from each other. Since it’s easy to extend to high
dimensions, for the convenience of discussion, the one dimensional case is considered here. As for must-link constraints and
cannot-link constraints, Qm denotes the intraclass compactness
and Qc denotes the interclass separability. To describe the compactness of labeled samples belonging to the same classes, Qm
is measured as follows:

⎧
⎪
Qm =
(wT xi − wT xj )2
⎪
⎪
⎪
⎪
ij
⎪
⎪
(x i , x j ) ∈ M or (x j , x i ) ∈ M
⎪
⎪
⎪
⎪


⎪
⎪
m T
⎨ =2
(wT xi Diim xTi w) − 2
(wT xi Sij
xj w)
(1)
i
ij
⎪
⎪
⎪
⎪
⎪
= 2wT X(Dm − S m )X T w = 2wT XLm X T w
⎪
⎪
⎪
⎪

⎪
⎪
1, if (xi , xj ) ∈ M or (xj , xi ) ∈ M
⎪
m
⎪
⎩ Sij =
0, else
(or
where Dm is a diagonal matrix whose entries are
 column
m
, Lm =
row, since S m is symmetric) sums of S m , Diim = j Sij
Dm − S m is a graph Laplacian matrix that is positive
semidefinite.
Similarly, the interclass separability Qc on cannot-link constraints can be termed as follows:

⎧
Qc =
(wT xi − wT xj )2
⎪
⎪
⎪
⎪
ij
⎪
⎪
⎪
(x i , x j ) ∈ C or (x j , x i ) ∈ C
⎪
⎪
⎪
⎨
= 2wT X(Dc − S c )X T w
(2)
⎪
⎪
c T
T
⎪
=
2w
XL
X
w
⎪
⎪
⎪

⎪
⎪
1, if (xi , xj ) ∈ C or (xj , xi ) ∈ C
⎪
c
⎪
⎩ Sij =
0, else
 c c
c
, L = Dc − S c .
where D is a diagonal matrix, Diic = j Sij
If the goal is just to place the samples of the same class close
to each other while the samples of different classes as far as
possible from one another, we can define an objective function
similar to LDA criterion in terms of Qc and Qm as follows:
w∗ = arg max
w

Qc
wT XLc X T w
= arg max T
Qm
w XLm X T w
w

(3)

1) Integrating Local and Global Topological Structures Based
on Random Subspace: Except for considering side information,
the topological structures considering global and local structure
properties are neglected in the current work which are important for dimension reduction. To preserve the global topological
structure of the datasets and make the fullest use of information buried in the unlabeled samples, we make the following

two assumptions: local manifold smooth assumption and nonneighborhood hypothesis. They are based on the idea that data
samples close to each other in the high-dimensional space will
be close to each other too in the embedding subspace. For
example, if xi is close to xj in the high-dimensional space,
their low-dimensional projection should be close to each other.
On the other hand, those non-neighboring samples in the highdimensional space should be as far as possible from one another
in the projected subspace. Thus two terms Qrsn and Qrsf are
introduced as representations.
In such situation, it begins with selecting T random subspaces with dimensionality of P (P < D) as
F = {F1 , F2 , . . . , FT } , FT = {f1t , f2t , . . . , fnt } , fit ∈ RP ,
where FT is the random subspace projection of X and then
construct a k nearest neighborhood graph on FT . Finally,
T graphs from F can be constructed. For the tth graph, its
t
is defined as follows:
similarity matrix Sij

1, if fit ∈ knn (fjt ) or fjt ∈ knn (fit )
t
Sij
=
0, else
where fit ∈ knn(fjt ) represents fit as one of the k nearest neighborhoods of fjt or vice verse.
The graphs of F are then combined to form a mixture graph
rsn
is defined as
G whose corresponding similarity matrix Sij
follows:
rsn
Sij
=

T
1 t
S .
T t=1 ij

The compactness Qrsn is defined by

rsn
Qrsn =
(wT xi − wT xj )2 Sij
= 2wT X(Drsn −S rsn )X T w
ij

= 2wT XLrsn X T w.

(4)

In a similar way, the random subspace separability Qrsf can
be defined as follows:

T
rsf
(wT xi − wT xj )2 S rsf
− S rsf )X T w
Qrsf =
ij = 2w X(D
ij

= 2wT XLrsf X T w
where
t
Sij


=

(5)

1,

if fit ∈
/ knn(fjt ) and fjt ∈
/ knn(fit )

0,

else
rsf
=
Sij

,

T
1 t
S
T t=1 ij

fit ∈
/ knn(fjt ) represents fit is not one of fjt ’s k nearest neighborhoods, and vice verse. As the mixture graph G is constructed
in lower dimensionality, it can ease the issues on the graph
construction from high-dimensional samples. Besides, the mixture graph can hold more complicated geometric distribution of
dataset due to the diversity of random subspaces. Furthermore,
the mixture graph is less sensitive to k and robust to noise.
Following experiments will confirm these virtues empirically.

CAI et al.: LOCAL AND GLOBAL PRESERVING SEMISUPERVISED DIMENSIONALITY REDUCTION BASED ON RANDOM SUBSPACE

503

TABLE I
GENERAL INFORMATION OF THE GENE EXPRESSION DATASETS

Fig 1.

(a) Intrinsic graph and (b) penalty graph.

Based on the earlier discussions, the RSLGSSDR’s objective
function can be defined as follows:
Qc + αQrsf
w∗ = arg max
Qm + βQrsn
w
= arg max
w

wT X(Lc + αLrsf )X T w
.
wT X(Lm + βLrsn )X T w

(6)

To solve the earlier optimization problem, we use the Lagrangian multiplier and have
X(Lc + αLrsf )X T w = λX(Lm + βLrsn )X T w.

(7)

It is easy to know that the d eigenvectors corresponding to the
d largest eigenvalues can form the transformation matrix W .
Here, X(Lm + βLrsn )X T w might be singular, we often use
PCA as a preprocessing tool with the 98% principal components
preserved (also reduce noise), then we apply dimensionality
reduction techniques (CLPP, SSDR, NPSSDR and RSLGSSDR)
on the projected data.
B. RSLGSSDR Graph Embedding Theory
According to the graph embedding theory [22], both supervised and unsupervised dimensionality reduction algorithms can
be described as a geometric characteristics graph embedding
process of datasets, which goes the same with semisupervised
dimensionality reduction approaches. According to the graph
embedding theory, we define two graphs, one is the intrinsic
graph and the other is penalty graph. The intrinsic graph describes the intraclass compactness based on the positive constraints as well as the neighborhood assumptions, while the
penalty graph describes the interclass separability based on the
negative constraints as well as the non-neighborhood assumptions. As shown in Fig. 1, the intrinsic graph is equivalent to the
graph of positive constraints plus the adjacency matrix and the
penalty graph is equivalent to the graph of negative constraints
plus the adjacency matrix.
IV. EXPERIMENTS AND RESULTS
Four publicly available microarray datasets used to study the
tumor classification problem, they are Colon, Ovarian, DLBCL
and Prostate gene expression datasets respectively. In these four
datasets, the data samples have been assigned to a training set
and a test set. An overview of the characteristics of the four
datasets can be found in Table I.

To validate the proposed RSLGSSDR on the performance,
we compare it with the methods including baseline, PCA,
CLPP [20], SSDR [19] and NPSSDR [21]. In experiment, as
gene expression data from DNA microarray is characterized by
thousands of genes, we take PCA as a preprocessing step with
the 98% principal components preserved for these semisupervised methods. Besides, the pairwise constraints in the experiments are obtained by randomly selecting pairs of instances
from the training set and creating must-link or cannot-link constraints depending on whether the underlying classes of the
two instances are the same or not. If without extra explanation,
the parameters in RSLGSSDR are always fixed to α = 0.05
and β = 0.05. All the results of CLPP, SSDR, NPSSDR and
RSLGSSDR are listed over 10 runs with different generations
of constraints. Here, we make some notations for the sake of
simplicity and illustration: T r, the number of training samples;
T e, the number of testing samples; C, the number of classes;
D, the dimensionality of original data; d, the target dimensionality; NOC, the number of must-link constraints; Sub dim, the
random subspace dimensionality; and k, the neighborhood size.

A. Results on Different Target Dimensionalities
To validate the effectiveness of RSLGSSDR on different target dimensionalities, we design four experiments in which we
vary d while set NOC. All the results are the 10 independent
runs to balance the random effects.
The classification accuracies of all compared methods on
these four datasets are shown in Tables II–V. It can be observed
that RSLGSSDR is clearly superior to all the other algorithms,
surpassing the second best method NPSSDR or SSDR. Especially in Table III, RSLGSSDR achieves the most stable results
on almost all the target dimensionalities. It should also be noted
that RSLGSSDR achieves better and better and relatively stable
results than the other methods in Tables II, IV, and V, while the
accuracies of the PCA, CLPP, SSDR and NPSSDR are always
in relatively low accuracies and raising in low speed. This is
because when dealing with high-dimensional samples, the similarity metric is in fact often distorted by noisy or redundant
features of high-dimensional data. As a result, the performance
of these methods is not satisfactory in high-dimensional spaces.
While RSLGSSDR considers this property based on random
subspace, it is robust against a wide range of values of input
parameters. We think that the random subspace and the corresponding neighborhood graph can figure the intrinsic geometric
structure of samples much better than the original space does,
especially when there exists noise.

504

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

TABLE II
CLASSIFICATION ACCURACIES OF DLBCL ON THE DIFFERENT TARGET DIMENSIONALITIES WITH CONSTANT NUMBER OF CONSTRAINTS (NOC = 100)

TABLE III
CLASSIFICATION ACCURACIES OF OVARIAN ON THE DIFFERENT TARGET DIMENSIONALITIES WITH CONSTANT NUMBER OF CONSTRAINTS (NOC = 500)

TABLE IV
CLASSIFICATION ACCURACIES OF PROSTATE ON THE DIFFERENT TARGET DIMENSIONALITIES WITH CONSTANT NUMBER OF CONSTRAINTS (NOC = 600)

TABLE V
CLASSIFICATION ACCURACIES OF COLON ON THE DIFFERENT TARGET DIMENSIONALITIES WITH CONSTANT NUMBER OF CONSTRAINTS (NOC = 100)

That the dimensionality reduction can promote the learning
results is also demonstrated in all tables, where RSLGSSDR,
NPSSDR and SSDR have higher accuracies than Baseline does.
Besides, RSLGSSDR, NPSSDR and SSDR also perform better
than PCA although they take PCA as a preprocessing step.

B. Results on Different Number of Constraints
From Figs. 2–5, experimental results indicate that
RSLGSSDR can make better use of constraints. As NOC increases, RSLGSSDR always gain higher and higher accuracies,
on the contrary, CLPP, SSDR and NPSSDR are always in relatively low accuracies. In reality, CLPP gains little benefit from
pairwise constraints propagation scheme even when the number
of pairwise constraints is relatively large except in Fig. 2. In addition, RSLGSSDR always achieves most stable results, which
demonstrates RSLGSSDR can make use of pairwise constraints
more efficiently than the other comparing methods.

Fig. 2. Classification accuracies of DLBCL on the different NOC with constant target dimensionality.

CAI et al.: LOCAL AND GLOBAL PRESERVING SEMISUPERVISED DIMENSIONALITY REDUCTION BASED ON RANDOM SUBSPACE

Fig. 3. Classification accuracies of ovarian on the different NOC with constant
target dimensionality.

Fig. 5. Classification accuracies of colon on the different NOC with constant
target dimensionality.

Fig. 6.

Classification accuracies of DLBCL on the different ks.

Fig. 7.

Classification accuracies of ovarian on the different ks.

Fig. 4. Classification accuracies of prostate on the different NOC with constant
target dimensionality.

C. Influence of Neighborhood Size k
Neighborhood size k is another influential parameter on the
construction of graph which determines the topological structure on RSLGSSDR, NPSSDR and CLPP. For the purpose of
studying the impact of k on these methods, we conduct another
four experiments in which d and NOC are set. Figs. 6–9 plot the
classification accuracies of comparing methods on the different
k. It can be seen from all of the figures that the classification
accuracies of RSLGSSDR increases along with increasing k in
a proper range, which shows the random subspace can utilize
the neighborhood information more efficiently. All the results
demonstrate that our method RSLGSSDR can have comparatively good performances in a wide range of k, so it can be
applied to pattern recognition tasks easily.

505

506

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 2, MARCH 2014

REFERENCES

Fig. 8.

Classification accuracies of prostate on the different ks.

Fig. 9.

Classification accuracies of colon on the different ks.

V. CONCLUSION AND DISCUSSIONS
In this paper, we present a novel local and global preserving semisupervised dimensionality reduction based on RSM.
In comparison with CLPP, SSDR and NPSSDR, our method
RSLGSSDR not only exploits side information more efficiently,
but also is robust to noise. Besides, RSLGSSDR is more adaptive to parameters since it has comparatively high classification
accuracies in a wide range. In this way it has addressed parameter selection problem in some degree, just as the experimental
results demonstrated. However, RSLGSSDR still has some issues for future work. In this paper, RSLGSSDR is based on the
local and global assumptions and uses Euclidean distance to
define the neighborhood. If real-world data does not meet the
demands, in our future work, we will try our best to better our
method’s effect and expand its range of practical use.

[1] C. H. Zheng, D. S. Huang, L. Zhang, and X. Z. Kong, “Tumor clustering
using non-negative matrix factorization with gene selection,” IEEE Trans.
Inf. Technol. Biomed., vol. 13, no. 4, pp. 599–607, Jul. 2009.
[2] T. R. Golub, D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek,
J. P. Mesirov, H. Coller, M. L. Loh, J. R. Downing, M. A. Caligiuri,
C. D. Bloomfield, and E. S. Lander, “Molecular classification of cancer: Class discovery and class prediction by gene expression monitoring,”
Science, vol. 286, pp. 531–537, 1999.
[3] R. O. Duda, P. E. Hart, and D. G. Stork, Pattern Classification[M]. New
York, NY, USA: Wiley, 2001, pp. 566–581.
[4] M. Brown, W. Grundyand, D. Lin et al., “Knowledge-based analysis of
microarray gene expression data by using support vector machines,” Proc.
Nat. Acad. Sci. USA., vol. 97, no. 1, pp. 262–267, 2000.
[5] G. Isabelle, W. Jason, B. Stephen, and V. Vladimir, “Gene selection for
cancer classification using support vector machines,” Mach. Learning,
vol. 46, no. 1, pp. 389–422, 2002.
[6] T. Ah Hwee and P. Hong, “Predictive neural networks for gene expression
data analysis,” Neural Netw., vol. 18, no. 3, pp. 297–306, 2005.
[7] L. Li, C. R. Weinberg, T. A. Darden, and L. G. Pedersen, “Gene selection
for sample classification based on gene expression data: Study of sensitivity to choice of parameters of the GA/KNN method,” Bioinformatics,
vol. 17, no. 12, pp. 1131–1142, 2001.
[8] D. V. Nguyen and D. M. Rocke, “Tumor classification by partial least
squares using microarray gene expression data,” Bioinformatics, vol. 18,
no. 1, pp. 39–50, 2002.
[9] D. S. Huang and C. H. Zheng, “Independent component analysis based
penalized discriminant method for tumor classification using gene expression data,” Bioinformatics, vol. 22, no. 15, pp. 1855–1862, 2006.
[10] N. Pochet, F. De Smet, J. A. K. Suykens, and B. L. R. De Moor, “Systematic benchmarking of microarray data classification: Assessing the role
of non-linearity and dimensionality reduction,” Bioinformatics, vol. 20,
pp. 3185–3195, 2004.
[11] B. Li, C. H. Zheng, D. S. Huang et al., “Gene expression data classification using locally linear discriminant embedding[J],” Comput. Biol. Med.,
vol. 40, pp. 802–810, 2010.
[12] D. D. Ridder, O. Kouropteva, O. Okun et al., “Supervised locally linear
embedding[C],” Lecture Notes Comput. Sci., vol. 34, no. 10, pp. 333–341,
2003.
[13] M. Belkin and P. Niyogi, “Laplacian eigenmaps for dimensionality reduction and data representation,” Neural Comput., vol. 15, no. 6, pp. 1373–
1396, 2003.
[14] X. F. He and P. Niyogi, “Locality preserving projections,” in Advances in Neural Information Processing Systems, S. Thrun, K. Saul,
and B. Schölkopf, Eds. vol. 16, Cambridge, MA, USA, MIT Press,
2004, pp. 153–160.
[15] J. B. Tenenbaum, V. de Silva, and J. C. Langford, “A global geometric
framework for nonlinear dimensionality reduction,” Science, vol. 290,
no. 5500, pp. 2319–2323, 2000.
[16] Z. Zhang and H. Zha, “Principal manifolds and nonlinear dimensionality
reduction via tangent space alignment,” SIAM J. Sci. Comput., vol. 26,
no. 1, pp. 313–338, 2004.
[17] X. Zhu, (2008). Semi-supervised learning literature. Dept. Computer
Sci., Univ. Wisconsin-Madison, Tech. Rep. 1530 [Online]. Available:
http://www.cs.wisc.edu/∼jerryzhu/pub/ssl_survey.pdf
[18] D. Klein, S. D. Kamvar, and C. D. Manning, “From instance-level constraints to space-level constraints: Making the most of prior knowledge
in data clustering,” in Proc. 19th Int’l Conf. Mach. Learning, C. Sammut
and A. G. Hoffmann, Eds. San Francisco, CA, USA: Morgan Kaufmann,
2002, pp. 307–314.
[19] D. Q. Zhang, Z. H. Zhou, and S. C. Chen, “Semi-supervised dimensionality reduction[C],” in Proc. 7th SIAM Int. Conf. Data Mining, 2007,
pp. 629–634.
[20] H. Cevikalp, J. Verbeek, F. Jurie, and A. Klaser, “Semi-supervised dimensionality reduction using pairwise equivalence constraints,” in Proc. 3rd
Int. Conf. Comput. Vision Theory Appl., 2008.
[21] J. Wei and H. Peng, “Neighbourhood preserving based semi-supervised
dimensionality reduction[J],” Electron. Lett., vol. 44, no. 20, pp. 1190–
1192, 2008.
[22] S. C. Yan, D. Xu, B. Y. Zhang, H. J. Zhang, Q. Yang, and S. Lin, “Graph
embedding and extensions: A general framework for dimensionality reduction,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 28, no. 1, pp. 40–51,
Jan. 2007.

CAI et al.: LOCAL AND GLOBAL PRESERVING SEMISUPERVISED DIMENSIONALITY REDUCTION BASED ON RANDOM SUBSPACE

507

[23] D. Cai, X. F. He, and J. W. Han, “Semi-supervised discriminate analysis,”
in Proc. IEEE 11th Int. Conf. Comput. Vision, 2007.
[24] M. Belkin and P. Niyogi, “Semi-supervised learning on Riemannian manifolds,” Mach. Learning, vol. 56, no. 1, pp. 209–239, 2004.
[25] Z. Y. Zhang, H. Y. Zha, and M. Zhang, “Spectral methods for semisupervised manifold learning,” in Proc. IEEE 26th Conf. Comput. Vision
Pattern Recog., 2008.
[26] L. Parsons, E. Haque, and H. Liu, “Subspace clustering for high dimensional data: A review,” ACM SIGKDD Explorations Newsletter, vol. 6,
no. 1, pp. 90–105, 2004.
[27] T. K. Ho, “The random subspace method for constructing decision
forests,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 20, pp. 832–844,
Aug. 1998.
[28] G. Yu, G. Zhang, and Z. Yu, “Semi-supervised ensemble classification in
subspaces,” Appl. Soft Comput., vol. 12, pp. 1511–1522, 2012.
[29] G. Yu and G. Zhang, “Semi-supervised classification based on random
subspace dimensionality reduction pattern recognition,” vol. 45, pp. 1119–
1135, 2012.
[30] H. Li and G. Wen, “Random subspace evidence classifier,” Nurocomputing, vol. 110, pp. 62–69, 2013.
[31] B. J. Yan and C. Domeniconi, “Subspace metric ensembles for semisupervised clustering of high dimensional data,” in Proc. 17th Eur. Conf.
Mach. Learning (ECML), 2006.
[32] J. Wang, S. W. Luo, and X. H. Zeng, “A random subspace method for
co-training,” in Proc. IEEE Int. Joint Conf. Neural Netw. (IJCNN), 2008.
[33] R. Polikar, J. Depasquale, H. S. Mohammed, G. Brown, and
L. I. Kuncheva, “Learn++: A random subspace approach for the missing
feature problem,” Pattern Recog., vol. 43, no. 11, pp. 3817–3832, 2010.

Guihua Wen (M’10) was born in 1968. He is currently working toward the Ph.D. degree.
He is currently a Professor and Doctor Supervisor with the School of Computer Science and Engineering, South China University of Technology,
Guangzhou, China. In 2005–2006, he did Visiting
Research on machine learning and semantic web
in School of Electronics and Computer, University
of Southampton, UK. His current research interests
include computational creativity, data mining and
knowledge discovery, machine learning, and cognitive geometry. Since 2006, he proposed some original methods based on the
computation of cognitive laws, which can effectively solve difficult problems
in information science. The research results have been published in the international journals, including Pattern Recognition, Neurocomputing, Journal of
Software, and Journal of computer Research and Development. He has also
authored or coauthored some papers in the international conferences such as
IJCAI. Since 2006, he directed the projects from the China National Natural
Science Foundation, State Key Laboratory of Brain and Cognitive Science, the
Ministry of Education Scientific Research Foundation for returned overseas
students, Guangdong Provincial Science and Technology research project, the
Fundamental Research Funds for the Central Universities, SCUT. He also directed many projects from enterprises, with applications of his research results
to the practical problems.
Dr. Wen has been a Council Member of Chinese Association for Artificial
Intelligence and a program Committee Member of many international conferences. He is also a reviewer for China National Natural Science Foundation.

Xianfa Cai spent his undergraduate days at Jiangxi
Normal University from 1998 to 2002, went on to
get his master’s degree from SunYat-Sen University
in 2006, and a Ph.D. degree at the School of Computer Science and Engineering, South China University of Technology in 2013. He is now a lecturer at the
School of Medical Information Engineering, Guangdong Pharmaceutical University. His current research
interests include machine learning, pattern recognition, bioinformatics.

Zhiwen Yu (S’06–M’08) is a professor in the School
of Computer Science and Engineering, South China
University of Technology, Guangzhou, China. He
received the B.Sc. and M.Phil. degrees from the
SunYat-Sen University in China in 2001 and 2004
respectively, and the Ph.D. degree in Computer Science from the City University of Hong Kong, in
2008. His research interests include pattern recognition, bioinformatics, multimedia, intelligent computing and data mining. He has published more than 50
technical articles in referred journals and conference
proceedings in the areas of pattern recognition, bioinformatics and multimedia.

Jia Wei received the B.Sc. and M.Sc. degrees both in
computer science from Harbin Institute of Technology, Harbin, China, in 2003 and 2006, respectively,
and the Ph.D. degree in computer science from South
China University of Technology, Guangzhou, China
in 2009.
He is currently a Lecturer at the School of Computer Science and Engineering, South China University of Technology. His current research fields include
machine learning and images retrieval.

