2278

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Caenorhabditis Elegans Segmentation Using
Texture-Based Models for Motility Phenotyping
Ayala Greenblum, Raphael Sznitman, Pascal Fua, Fellow, IEEE, Paulo E. Arratia, and Josué Sznitman∗

Abstract—With widening interests in using model organisms for
reverse genetic approaches and biomimmetic microrobotics, motility phenotyping of the nematode Caenorhabditis elegans is expanding across a growing array of locomotive environments. One ongoing bottleneck lies in providing users with automatic nematode
segmentations of C. elegans in image sequences featuring complex
and dynamic visual cues, a first and necessary step prior to extracting motility phenotypes. Here, we propose to tackle such automatic
segmentation challenges by introducing a novel texture factor
model (TFM). Our approach revolves around the use of combined
intensity- and texture-based features integrated within a probabilistic framework. This strategy first provides a coarse nematode
segmentation from which a Markov random field model is used to
refine the segmentation by inferring pixels belonging to the nematode using an approximate inference technique. Finally, informative priors can then be estimated and integrated in our framework
to provide coherent segmentations across image sequences. We
validate our TFM method across a wide range of motility environments. Not only does TFM assure comparative performances to existing segmentation methods on traditional environments featuring
static backgrounds, it importantly provides state-of-the-art C. elegans segmentations for dynamic environments such as the recently
introduced wet granular media. We show how such segmentations
may be used to compute nematode “skeletons” from which motility phenotypes can then be extracted. Overall, our TFM method
provides users with a tangible solution to tackle the growing needs
of C. elegans segmentation in challenging motility environments.
Index Terms—Caenorhabditis elegans, computer vision, model
organism, motility, phenotyping, segmentation.

I. INTRODUCTION

D

UE to its short life cycle [1], knowledge of its complete cell
lineage [2], [3], the simplicity of its nervous system [4],

Manuscript received July 28, 2013; revised November 28, 2013; accepted
January 2, 2014. Date of publication January 9, 2014; date of current version
July 15, 2014. This work was supported in part by the European Commission
(FP7 Program) through a Career Integration Grant PCIG09-GA-2011-293604
and a US–Israel Binational Science Foundation Grant BSF Nr. 2011323. The
work of Dr. Raphael Sznitman was supported in part by the European Commission (FP7 Program) through an ERC Grant MicroNano. A. Greenblum and R.
Sznitman are first co-authors. Asterisk indicates corresponding author
A. Greenblum is with the Department of Biomedical Engineering,
Technion—Israel Institute of Technology, 32000 Haifa, Israel (e-mail: ayalag@
tx.technion.ac.il).
R. Sznitman and P. Fua are at the Computer Vision Laboratory, École
Polytechnique Fédérale de Lausanne, 1015 Lausanne, Switzerland (e-mail:
raphael.sznitman@epfl.ch; pascal.fua@epfl.ch).
P. E. Arratia is with the Department of Mechanical Engineering and Applied
Mechanics, University of Pennsylvania, Philadelphia, PA 19104 USA (e-mail:
parratia@seas.upenn.edu).
∗ J. Sznitman is with the Department of Biomedical Engineering, Technion—
Israel Institute of Technology, 32000 Haifa, Israel (e-mail: sznitman@bm.
technion.ac.il).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2298612

and a fully sequenced genome [5], the nematode Caenorhabditis
elegans has become a ubiquitous model organism to investigate
the genetics of development, neuroscience, and behavior. An
active area of research lies in behavioral motility phenotyping
for reverse genetic approaches, where specific genes of interest
are identified from whole-genome sequences [6].
Motility phenotyping of C. elegans generally aims at quantifying locomotive traits of individual (or groups of) nematodes,
including but not limited to speed [7], [8], wavelength, and frequency of body undulations [9], [10], body curvature [11], [12],
and posture reversals known as “omega turns” [13]. Most recently, nematode tissues properties have been estimated from
imaged nematode body postures [14]–[18] as well as locomotive propulsive forces [19]–[23]. Alternatively, nematode imaging has been used to reconstruct basic body postures, known as
“eigenworms” [24], [25], to define behavioral motifs that allow
clustering of mutants into related groups [26].
Unfortunately, linking locomotion phenotypes to genotypes
remains a cumbersome and time-consuming task. It requires first
imaging numerous assays of nematodes and then carefully annotating hundreds, if not thousands, of nematodes from image
sequences. Traditionally, this latter point has often been performed manually, imposing a labor-intensive and error-prone
process. To alleviate the burden of annotating large image sets,
automated analysis tools based on computer vision and machine learning techniques have emerged in view of designing
high-throughput behavioral assays [6], [28], [30], [31], including drug screening applications [32]–[34]. In effect, they provide a roadmap for a systematic, reliable, and (semi-) automatic
extraction of key phenotypic attributes.
Automated behavioral analysis tools almost unanimously require first extracting C. elegans postures from binary images,
or segmentations, so as to separate the nematode from its environment (background). This segmentation process has been
tackled using simple image intensity thresholds [8], [10], [12],
[35]–[37], adaptive intensity thresholds [11], [13], [38], and
more sophisticated statistical models [39], [40]. Central to all
these methods is their complete reliance on nematode and background raw image intensities to guide the segmentation process.
While these methods have been shown to be effective for automatic segmentation across a range of motility environments,
such as locomotion on substrates [41]–[44] [see Fig. 1(a)], in
fluids [19], [27], [43], [45], [46] [see Fig. 1(b)], or in patterned
maze environments [28], [47]–[55] [see Fig. 1(c)], they are illsuited, if not entirely compromised, when dealing with more
challenging backgrounds that contain complex textures and/or
are dynamic [e.g., Fig. 1(d)], where the background appearance changes significantly as the nematode moves within the
surrounding medium [29], [56]. In particular, these complex

0018-9294 © 2014 IEEE. Translations and content mining are permitted for academic research only. Personal use is also permitted, but republication/redistribution
requires IEEE permission. See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

2279

Fig. 1. Sample selection of environments for C. elegans’ motility assays. (a) Nematode crawling on an agar substrate (gene: dat-1, strain: RM2702, allele: ok157,
chromosome: III genotype: dat-1(ok157)III; see supplementary video in Brown et al. [6]). (b) Nematode swimming in an aqueous drop of M9 buffer solution (see
example in Ghosh and Sznitman [27]). (c) Nematode locomoting through a structured microfluidic maze of pillars (source: nmeth.1630-S2.avi, see supplementary
video in Albrecht and Bargmann [28]). (d) Nematode locomoting through an unstructured domain made of polydisperse granular media (source: supplementary
material in Juarez et al. [29]). C. elegans shown in examples (a)–(d) are all approximately 1-mm long and illustrate environments with variations in lighting
conditions, spatial resolution, field of view scale, and background complexity.

environments are increasingly relevant in view of designing artificial biomimmetic microrobots [57]–[59].
It is in this latter context [i.e., Fig. 1(d)] that we propose
a novel statistical strategy for semisupervised segmentation of
C. elegans. In order to initialize our algorithm, only a single image is required to be manually annotated by a user. Our method
first learns a number of distinct image features that capture texture and intensity information regarding the nematode and the
background, respectively. From the user-provided segmented
image, we determine those features that are informative and
then segment pixel-by-pixel each image of a sequence by using a probabilistic classification model and the learned features.
We show experimentally that our method provides comparable,
if not improved, segmentation results for traditional environments relative to intensity-based methods. Most importantly, our
method delivers for the first time usable phenotypic results in
challenging granular environments where state-of-the-art methods fail entirely.
Below, we begin by describing previous related works. In
Section III, we present our method in detail and outline both
the model assumptions and dependencies on the end-user. Next,
we present in Section IV in-depth experiments evaluating our
method and compare performances to existing state-of-the-art
methods. In Section V, we briefly illustrate how such nematode
segmentations are typically utilized to extract motility phenotypes, and conclude with some remarks in Section VI.
II. RELATED WORK
While a growing body of work on segmentation techniques
has surfaced in the last decade in the computer vision and machine learning communities, methods adapted for the specific
needs of nematode segmentation still remain somewhat scarce.
One bottleneck lies in the ongoing scarcity of extensive, openaccess datasets of annotated nematodes; these are pivotal in
using segmentation methods that rely on statistical learning [6],
[60], [61].
When considering nematode segmentation with little training
data, i.e., a handful of labeled images, the most widespread approach relies on a simple intensity-based threshold at any given
pixel location [8], [10], [12], [35]–[37], where a user manually

selects an appropriate range of intensities characterizing the imaged nematode. A variation to this approach has been suggested
with the use of an adaptive threshold, where nematode intensities or appearance, are assumed to significantly differ from
the mean background intensities [11], [13], [38]. One intrinsic
drawback with intensity-based thresholds lies in their limited
versatility across the growing selection of nematode motility
environments [40] and have thus been traditionally limited to
crawling assays on substrates [41]–[44] [see Fig. 1(a)]. The overarching bottleneck with intensity-based thresholding remains
that accurate segmentations require a significant effort on the
user-end to select appropriate threshold values along with other
noise canceling schemes (e.g., median filters, morphological
operators, background subtraction, etc.). These limiting factors
ultimately jeopardize the efficiency of such strategies for motility assays across diverse environments [see Fig. 1(b)–(d)], and
thus still impose a heavy burden on the user.
To overcome some of these limitations, statistical learning
techniques have been recently applied to nematode segmentation [62], [63], where the strategy lies in systematically learning
how background intensity pixels are distributed. Here, the learning process is done using a set of labeled images, or training
images, to statistically model the background appearance by
means of a mixture of Gaussians (MOG) model for each pixel,
where the model parameters are estimated from the complete
training set of images [39], [64], [65]. Unfortunately, the number of MOGs used corresponds to the total number of pixels
in an image, and consequently, the number of parameters required becomes rapidly overwhelming. Hence, a large number
of image frames are needed to estimate the MOG parameters accurately. Moreover, the entire background scene must be visible
to estimate these parameters, since each image is used to model
only the background and not the nematode. This latter condition
is problematic when image sequences contain a nematode at all
times, as is frequently the case in available data [17], [29], [43],
[45], [50], [52].
More recently, Sznitman et al. [40] proposed to decompose
the background into rectangular regions and learn a single MOG
per region. This decomposition requires a far smaller number of
MOGs to be learned compared to traditional background modeling techniques [39], leading to significantly reduced number

2280

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Fig. 2. Framework of the TFM. At training time, different features F1p and F2p for a given image pixel p are extracted from a single image with a manual
segmentation of the nematode. The feature vectors are concatenated into one F̄ p and Gaussian likelihood models are then estimated (shown here in 1-D). At test
time, F̄ p is then extracted at each pixel and the likelihood models are used to compute the posterior ratio (see text for details). The posterior ratio can be visually
depicted at each pixel (heat map where green corresponds to small values and red to high values). To construct a coarse nematode segmentation, i.e., C p for each
pixel p, the posterior ratio is thresholded at a single value.
TABLE I
SUMMARY OF NOTATION

of training images required. The authors then use the likelihood
of ratios to segment nematodes across various motility environments. While effective in a number of cases [40], the method
yet fails when faced with complex, texture-rich scenarios; indeed, the central feature in the approach of [40] revolves around
the local average pixel intensity and thus cannot disambiguate
nematodes from background when both have similar intensities
[e.g., Fig. 1(d)].
III. METHOD
Our goal is to provide users with nematode segmentations
across image sequences. To make our method as practical and
straightforward as possible for the end user, we restrict the
amount of training data, i.e., the number of manually segmented
nematodes, to be the first image in a sequence; namely, this restriction considerably alleviates the burden imposed on the expert user and delivers an attractive solution within the nematode
community.
Our method will be broken into two parts: a coarse segmentation process and a second refinement stage. In the coarse
segmentation stage, we use a single annotated image to compute a number of image texture features and learn those that are
informative for a given particular image sequence. At test time,
the informative features are treated as factors within a Bayesian
model to infer which pixels belong to the nematode and which
do not, providing first a coarse image segmentation. To provide smooth and consistent image segmentations, we perform
the refinement stage which first uses the coarse segmentation
as observations of a Markov random field (MRF) and then the
mean-field variational inference technique [66] to compute the
maximum posterior marginals (MPMs) of the pixel labels. To
segment the resulting images in a sequence, we then evaluate
informative priors from the computed MPMs, via Bayesian filtering, and provide coherent estimates to where the nematode
is located across the image sequence. The details of our methods, which we refer to as the texture factor model (TFM), are
described below.

A. Notation
Let It be the tth gray-scale image from an image sequence
I = (I1 , . . . , IT ). Each pixel p in an image is associated with
a discrete class label, denoted Lp ∈ {0, 1}, indicating whether
or not that pixel belongs to the nematode (Lp = 1) or the background (Lp = 0). In addition, we consider Lp to be a discrete
random variable with probability distribution P (Lp ). We denote
St as the segmentation image of It , where each pixel of St has
value 0 or 1 corresponding to the class label of the associated
pixel. We define our training data, D = {I1 , S1 }, as the first image and segmentation from the image sequence I, as depicted
in Fig. 2. The remaining images of the image sequence are set
to be our test data. As such, our goal is to infer S2 , . . . , ST
as accurately as possible. In order to determine if a pixel label
belongs to the nematode or background, we will compute a set
of K ≥ 1 image features, denoted F p = {F1p , . . . , FKp }, where
Fkp ∈ Rn k is a feature vector for pixel p; note that we also consider Fkp to be a random vector. A summary of the notation used
is presented in Table I.
B. Nematode Image Features and Model
In order to provide usable nematode segmentations across
different experimental setups, we will introduce a number of
image features. As with Ensemble Classifiers [67], this strategy
allows good coverage of the positive/negative image statistics

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

and typically leads to robust segmentation [60]. In practice, we
specify K = 2 defined as:
1) Average Pixel: we compute the feature F1p ∈ R as the
average intensity value of a small window of 7 × 7 pixels
centered on pixel p [8], [36], [37].
2) RFS Feature: we compute F2p ∈ R38 as the response of
a set of 38 filters centered on pixel p. The filters used
are the root filter set (RFS) that consists in a mixture
of bar, Gaussian, and Laplacian of Gaussian filters with
different parameters, respectively [68]. Historically, these
have been shown to be effective in characterizing textured
regions.
We describe in the following section how and when these
features are used to segment nematodes.
C. Coarse Image Segmentation
We are interested in computing at each pixel p the posterior distribution of the class when a subset of image features,
{Fmp }M
m =1 , has been observed. This is written as
1
p
P ({Fmp }M
m =1 |L = l)P (L = l).
Z
(1)
In effect, (1) describes the probability that a pixel has label l
p
when the features F1p , . . . , FM
have been observed for pixel
p in the image. The equation has three multiplicative terms.
First, the prior probability of the class label for a given pixel,
P (Lp = 1) which will be held at 1/2 to indicate an unbiased
prior on the pixel label. Second, the normalization constant
p
) that can be expressed as
Z = P (F1p , . . . , FM
P (Lp = l|{Fmp }M
k =1 ) =

1
 

p
P {Fmp }M
P (F1p , . . . , FM
|L = z)P (Lp = z). (2)
k =1 =
z =0

Third is the likelihood model P ({Fmp }M
m =1 |L = l) which describes the likelihood of feature scores when the underlying
pixel is either part of the nematode or the background. While
modeling this likelihood is challenging as it requires having sufficient data to characterize how each feature affects the others,
we propose to simplify the likelihood by concatenating all the
p
] and assuming
features together, F̄ p = [F1p , . . . , FM

G(f ; μ1 , Σ1 ), if l = 1
p
(3)
P (F̄ = f |L = l) =
G(f ; μ0 , Σ0 ), if l = 0,
where (μi , Σi ), i = 0, 1 are the parameters of a Gaussian distribution for when the pixel is part of the nematode and when
it is not. In the following section, we describe how to estimate
these parameters from the training image. Despite representing
the likelihood model with (3) which is a strong assumption on
the image dataset, we will show in our experiments that this
assumption yet allows accurate segmentations of nematodes.
Given the above, for any test image, we can then compute a
coarse estimate of whether or not a pixel belongs to the nematode
or the background by calculating
C p = 1{R p > θ } ,

(4)

2281

where θ ∈ R+ is a sensitivity threshold and where we define
Rp =

P (Lp = 1|{Fmp }M
k =1 )
p M
p
P (L = 0|{Fm }k =1 )

(5)

as the ratio of posterior probability. As such, nematode pixels
are those where the ratio of posterior probability is larger than
the sensitivity threshold. In Fig. 2, we visually depict for a
sample image the relation between Rp and C p using a particular
threshold value θ.
D. Feature Likelihood Models
To compute the likelihood models P (F̄ p |L), L = 0, 1, we
proceed in the following manner. For a given feature vector,
F̄ p ∈ Rn , we must estimate the class conditional likelihood
model, P (F̄ p |L = 1) and P (F̄ p |L = 0), which we consider to
be Gaussian in nature. That is, P (Fkp |L = 0) ∼ G(Fkp ; μ0k , Σ0k )
and P (Fkp |L = 1) ∼ G(Fkp ; μ1k , Σ1k ), where μ1k , μ0k ∈ Rnk are
mean vectors of dimension nk and Σ1k , Σ0k ∈ Rn k ×n k are covariance matrices of size nk × nk .
To estimate the parameters μ1k , μ0k , Σ1k , Σ0k for each feature,
we make use of the user-provided training data available, D =
{I1 , S1 }. Using the image I1 and its corresponding nematode
segmentation S1 , we randomly sample 40% of the pixels that
belong to the background and 40% of the pixels corresponding
to the nematode nematode. At each of the sampled locations, we
compute the feature set responses F1p , . . . , FKp . We then use the
maximum likelihood estimators [67] to estimate the parameters
μ1k , μ0k , Σ1k , Σ0k for each feature response.
E. Selecting Feature Subsets
While we have described a pool of image features, only a
p
subset {Fmp }M
m =1 ∈ F of such features are effectively used in
(5) when segmenting a particular image sequence.
To select this subset, we first estimate P (F̄ p |L = l), l = 0, 1
using the training image I1 , for each possible combination of
the image feature subsets. Then, we segment the same training
image, using (5) and select the subset that segments best. Written
more formally, we solve
 p M

{Fm }m =1 , θ = arg max F1 (C̄θq , S1 )
(6)
q ⊂F p ,θ

where C̄θq is the coarse segmentation achieved using the feature subset q and sensitivity threshold θ, and F1 is the F-score
segmentation accuracy measure [60] defined as
F1 (C̄θq , S1 ) = 2 ·

precision × recall
.
precision + recall

(7)

In effect, we evaluate each possible feature subset and sensitivity threshold to select the pair which maximizes the F1
accuracy measure. Both the subset and the threshold are then
used to segment the rest of the image sequence S2 , . . . , ST .
F. Refined Segmentation
As noted earlier, the majority of previous automatic nematode
segmentation algorithms perform some form of morphological
operations during postprocessing so as to remove small errors in

2282

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

segmentation. Here, rather, we attempt to rely on a more principled mechanism. In line with the probabilistic model introduced
previously, we use the coarse segmentation results within an
MRF.
For a given test image, we are interested in inferring the true
segmentation S for all pixels p in that image. To this end, we
use an MRF to describe the joint probability distribution of all
pixel labels S and the coarse image segmentation C. We write
the joint probability distribution for all pixels in the image as
P (C, S) =


1 
P (C p |S p )
P (S p S j ),
Z p

(8)

j ∈Np

where Np is a four-connected neighborhood of pixel p and Z
is a normalizing constant. This model allows one to describe
both the likelihood of a pixel label as well as its relation to
neighboring pixels and has been shown to be effective in a
number of related segmentation problems [60], [69]. Namely,
this approach is particularly useful to encode smoothness and
coherence.
Given the MRF model, we are interested in estimating the
MPM, P (S p |C), which describes the probability of a pixel
label, given the entire coarse image segmentation. Computing
the MPMs can be achieved in a number of ways and we opt to
do so with the well-established mean-field [66] approximation
method. This method iteratively refines estimates of the MPMs
and does so by first letting P (S p |C p ) = μp = 0 at initialization.
Then, for m = 1, . . . , M iterations, we update the P (S p |C p ) by
computing
⎛
⎞

1
μp+1 = μp α + (1 − α) tanh ⎝
μj + P (C p |S p )⎠ (9)
β
j ∈Np

where α is a damping rate, P (C p |S p ) ∝ (1 + exp(C p ))−1 , and
β is known as a so-called temperature. In our experiments,
we set β = 0.5, iterate approximately M = 100 times, and let
α = 0.5. A detailed derivation of (9) can be found in [66].
Note that the strategy formulated in (9) is intended to smooth
out incoherences that may have been produced during the coarse
segmentation step. As such, the amount of smoothing necessary
may be directly related to the relative size of the nematode in
the image (i.e., proportion of pixels belonging to the nematode);
this latter property is in part controlled by the MRF neighborhood size, i.e., 4-pixel neighborhood in the present framework.
Hence, while the selected parameter may be appropriate in some
cases, alternate values may be required for nematodes that appear larger.

Specifically, we estimate the new prior over the entire image
domain, P (L̂t ), by using traditional Bayesian filtering [70],
defined as

(10)
P (L̂t ) = P (Lt |Lt−1 )P (Lt−1 )dLt−1
where P (Lt−1 ) corresponds to the posterior distribution computed for frame I t−1 using (1), and P (Lt |Lt−1 ) is a dynamics
model which we assume to be Gaussian with mean zero and
covariance Σ. In other words, we assume no change from one
frame to the next, other than some intrinsic noise governed by
Σ. In our experiments, we treat Σ = 8I2 , where I2 is the identity
matrix of size 2 × 2. As noted in the previous section, Σ can
be modified to better model nematode motion across specific
sequences [70]. In the remainder of this paper, we will use the
term “Prior-TFM” to denote the use of informative priors, in
contrast to the TFM.
IV. EXPERIMENTS
In the following section, we provide both quantitative and
qualitative performance results regarding our (Prior-)TFM
method.1 We do so by evaluating the TFM on real image sequences originating from motility assays in various environments. We compare our approach with existing methods to show
and highlight the benefits of the TFM method.
A. Nematode Image Sequences
We evaluate our method across various motility environments. First, we make use of a number of representative backgrounds:2 1) crawling on an agar surface [see example in
Fig. 1(a)], 2) swimming in an aqueous drop [see Fig. 1(b)],
3) swimming in gelatin-based solutions, and 4) navigating
through microfluidic mazes constructed of micropillars [see
Fig. 1(c)]. For each image sequence, approximately 35 manually labeled nematode segmentations are provided (see details
in [40]).
In addition, we introduce two new image sequences featuring nematode locomotion in wet granular environments [29].
Briefly, the motility behavior of C. elegans is imaged in shallow channels filled with an aqueous buffer solution that contain
either monodisperse or polydisperse particles set to a fixed packing fraction φ (i.e., ratio of volume of particles to total volume).
Such environments are believed to mimic more accurately the
soil-like milieus where C. elegans is found [29]. The first sequence contains polydisperse particles (52 ± 10-μm diameter,
φ = 0.55), while the second sequence contains monodisperse
particles (60 ± 3-μm diameter, φ = 0.55); 36 manually labeled
frames are available for each sequence.

G. Nematode Tracking
In the initial coarse segmentation, we assumed an unbiased
prior, i.e., P (L = 1) = P (L = 0) = 1/2. We now show how an
informative prior can be estimated for the purposes of nematode
tracking throughout the remainder of the image sequences. In
particular, when segmenting an image It at time step t, we will
use the posterior distribution computed at the previous time step
t − 1 to estimate the new prior.

B. Evaluation
For each image sequence, we evaluated our method by training our TFM method on the first image and evaluating the remaining images in the corresponding sequence. We compare
1 Source

code and data will be made available online on the author webpage.
for download at: http://sites.google.com/site/sznitr/code-and-

2 Available

datasets

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

2283

Fig. 3. PR curves for the coarse segmentation stage across selected motility environments: (a) crawling, (b) swimming in drops, (c) swimming in gelatin-based
solutions (see supplementary material in Berri et al. [45]), (d) and (e) locomotion in microfluidic mazes (see supplementary material in Lockery et al. [50]), and
(f) and (g) locomotion in wet granular media (see supplementary material in Juarez et al. [29]). For each image sequence, we depict the PR curve for the TFM,
intensity-based, and MEME approaches, respectively. The TFM outperforms previous methods on the majority of sequences. In particular, the TFM provides
substantial improvements for (f) polydisperse and (g) monodisperse environments.

our approach against a traditional intensity-based thresholding
method [11], [13], the recent multienvironment model estimation (MEME) algorithm [40] and the segmentation strategy of
Lucchi et al. [71]. In particular, this latter method extracts intensity histograms and co-occurrence features from superpixels
to learn a radial basis function kernel support vector machine
classifier. The classifier output is then used within an MRF to
segment the object of interest. As with our TFM approach, this
strategy also captures intensity and some amount of texture information, via the cooccurrence features. In each comparison
method, we also only used the first image of the sequence to
train the respective algorithms. Note that in contrast to traditional intensity-based thresholding and MEME, the method of
Lucchi et al. has previously never been applied to nematode
segmentation.
1) Coarse Segmentation: For the selected motility environments, Fig. 3 presents the precision recall (PR) curves obtained
from each method. Any single point on a given curve corresponds to a potential segmentation threshold θ; curves that have
both higher values of precision and recall are considered better, positioned nearer the top right corner of each plot, i.e.,
Precision = Recall = 1.
We see that for the image sequences shown in Fig. 3(a), (b),
(f), and (g), the TFM outperforms all other methods. This is
particularly true for the poly- [see Fig. 3(f)] and monodisperse
environments [see Fig. 3(g)], where improvements are significant. In fact, both MEME and intensity methods are unable
to characterize the nematode whatsoever, i.e., Precision ≈ 0.
For the microfluidic environments [see Fig. 3(d) and (e)], all
methods perform similarly, while for the drop scenario [see

Fig. 3(b)], the intensity-based method exhibits only a slightly
more advantageous PR curve.
To render the results of Fig. 3 more quantitative, we report
in Table II the best F-score [as introduced in (7)] for each segmentation method applied. Namely, we show the best F-score
obtained for each environment over the entire test set (see the
first row in Table II) as well as on the single training image
(see the second row in Table II). For the TFM method, we also
show how different sets of features (i.e., Average pixel, RFS
feature, and combination of both) perform on different environments. We notice that when determining with the TFM which
feature sets to use from the first training image on an entire image sequence, our approach selects the best feature set possible
barring the “Drop” image sequence. That is, evaluating each
feature combination over the first training image and selecting
the best one correlates rather well with the best feature set over
the entire image sequence.
In general, we notice that either average pixel intensity or
RFS combined with average pixel intensity performs best. This
observation is reasonable as average intensity is a very strong
visual cue for nematode segmentation [13], [40]. It appears
that the TFM method augments in some instances this strong
cue with more texture information such as in scenarios where
environments are visually more complex (i.e., granular media).
When comparing the TFM against other methods, our approach outperforms significantly previous methods on the challenging poly- and monodispere environments. The best F-scores
obtained with the TFM over the image stacks are ∼0.7 and 0.6,
respectively. Comparatively, MEME and the intensity method
both score below 0.1 for each image set. In general, these

2284

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

TABLE II
SUMMARY TABLE OF BEST F-SCORES (7) FOR EACH SEGMENTATION METHOD AND CALCULATED FOR THE VARIOUS
MOTILITY ENVIRONMENTS OF FIG. 3

results indicate that the coarse segmentation produced by the
TFM provides significant improvements for segmenting nematodes in unstructured environments such as wet granular media,
where the background is both texture-rich and dynamic (i.e.,
particles are displaced as the nematode passes by). In other environments, the TFM performs similarly to the other methods,
i.e., in some cases slightly better and in others slightly worse
(see Table II). Below, we show how implementing the fine segmentation approach delivers state-of-the-art results on nematode
segmentation across all environments.
2) Final Segmentation: To quantitatively evaluate and compare the final segmentation using the TFM, we depict in Fig. 4
both the nematode segmentation yield and the overall image
segmentation error for the sequences tested in Fig. 3; these performance metrics have been recently defined [40] and are also
reported here for the MEME, the intensity-based method, and
the segmentation strategy of [71]. In addition, we also provide
quantitative results for Prior-TFM, which uses temporal information as described in Section III-G.
We show for each evaluated image sequence in Fig. 4(a)
the proportion of the nematode pixels that is correctly segmented in a given image (i.e., nematode yield). Correspondingly, we show in Fig. 4(b) the proportion of pixels that is
incorrectly labeled over the entire image (i.e., surface error).
In both plots, the results are produced from the best F-score
obtained over the training set. While values of the nematode
yield obtained from the TFM are generally similar to those
for MEME and the intensity-based method applied across traditional environments [see Fig. 4(a)], the TFM achieves such
results by maintaining a significantly smaller amount of labeling error over the rest of the image [see Fig. 4(b)]. This point
is further highlighted when using Prior-TFM, where values of
the nematode yield improve drastically for almost all image se-

quences, at the cost of a slightly higher surface error due to over
segmentations.
Furthermore and importantly, both the MEME and intensitybased methods are simply unable to recover the nematode in
the poly- and monodisperse environments, yielding high rates
of nematode error. Both the TFM and Prior-TFM, on the other
hand, maintain a reasonable surface error rate (<2%) and produce nematode segmentations with a yield above 40% and 50%
for the mono and polydisperse environments, respectively. In
particular, the Prior-TFM approach delivers scores nearing or
above the 80% mark, underlining the efficacy of implementing
a nematode tracking technique.
When comparing segmentation results to the method of Lucchi et al., both the TFM and Prior-TFM perform better across
the array of traditional motility environments [see Fig. 4(a)];
yet, the segmentation approach of [71] appears more competitive for the mono and polydisperse environments where margins
in nematode yield are much closer. In particular, Lucchi et al.
performs similarly to Prior-TFM for the polydisperse sequence
and only slightly worse for the monodisperse case.
In Fig. 5, we depict qualitative results of the final segmentation using the TFM and Prior-TFM on the poly- and
monodisperse environments. For comparison, we also show
the corresponding (human) manually labeled segmentations,
i.e., groundtruth, as well as results produced with MEME and
Lucchi et al. We immediately notice that MEME as well as
the intensity-based method (not shown here) fail altogether in
providing any reasonable segmentation of the nematode, further
underlining the poor F-scores observed earlier in the PR curves
(see Fig. 3 and Table II). In contrast, the TFM and Prior-TFM
provide far more suitable segmentations, visually capturing the
main characteristics of the nematode, i.e., shape and length. In
the case of Lucchi et al., the final segmentations on these two

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

Fig. 4. Performance evaluation of final nematode segmentation. For the environments shown in Fig. 3, the TFM and correspondingly the Prior-TFM are
compared to a traditional intensity-based thresholding approach [17], [18], as
well as the recent methods of MEME [40] and Lucchi et al. [71]. (a) Nematode
yield: proportion of the nematode region that is correctly segmented for a given
image. (b) Surface error: proportion of pixels misclassified by a given algorithm
over the entire image. For each environment shown, standard deviations (shown
as error bars) are obtained across (manually-labeled) sequences of n = 29 to 37
images. Note that for the intensity-based method, successful segmentation are
not available for both the mono- and polydisperse environments (see last two
columns).

challenging cases also capture the main characteristics of nematode but suffer from some jagged contours due to the superpixel
preprocessing step.
Upon closer inspection, we notice that the TFM and PriorTFM segmentations are far from perfect however. In fact, qualitative segmentation errors are apparent when particles are in
the vicinity of the nematode or touching it. Portions of the nematode tail and head are also incorrectly segmented and lost.
This latter problem has been previously noted and applies to
a broader range of (static) backgrounds, including swimming
and microfluidic environments [21], [40]. Namely, this problem
occurs when the nematode extremities are transparent against
the background. Finally, we note that for the polydisperse environment, the Prior-TFM delivers a disconnected nematode
segmentation as is the case for the method of [71].
V. DISCUSSION
From a user end, clean and reliable nematode segmentations
represent the first and necessary step prior to extracting motility
phenotypes of C. elegans. Indeed, the wide majority of quanti-

2285

Fig. 5. Nematode segmentation in unstructured environments featuring poly(a)–(d) and monodisperse (e)–(h) granular media; see supplementary material
in Juarez et al. [29]). (a) and (e) Snapshots of raw images. Comparisons between
nematode segmentations are, respectively, shown for 1) the ground truth (b) and
(f), i.e., hand-segmented nematodes, 2) the MEME approach [40] (c) and (i),
3) the method of Lucchi et al. [71] (d) and (j), 4) the TFM shown in (e) and (k),
and 5) the corresponding Prior TFM shown in (f) and (l).

tative motility traits characterizing C. elegans locomotion (e.g.,
speed, beating frequency, amplitude, body curvature, etc.) are
not directly extracted from whole-body segmentations but rather
from so-called “skeletons” that describe the nematode centerline data [10]–[12], [19], [41]. Here, we briefly discuss the
feasibility of using nematode skeletons obtained with the TFM
across complex environments such as wet granular media; to
date, these latter environments have required tedious manual
labeling to reliably extract usable skeletons [29].
In Fig. 6, tracking of nematode body postures are presented in
both space and time; sample snapshots of the nematode moving
through mono- [see Fig. 6(a)] and polydisperse [see Fig. 6(b)]
wet granular media are shown with the corresponding body
postures (i.e., skeletons). These skeletons are extracted using the
method of [40], where nematode ends points are first extracted,
and then a Bayesian marching algorithm is used to trace out the
centerline between end points of the nematode using the image
segmentation directly.
Changes in the motility behavior between the two environments can be qualitatively captured at a glance from the evolution of nematode skeletons over the image sequences. This is
shown in Fig. 6(c) and (d) where the time evolution of skeletons
is color-coded as a function of time; envelopes of body postures

2286

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

Fig. 6. Motility features using nematode skeletons. Micrographs of C. elegans moving through (a) mono- and (b) polydisperse wet granular media with a
packing fraction of φ = 0.55; raw image sequence available from supplementary material in [29]. For each snapshot, the corresponding nematode skeleton
is superimposed (red) as obtained after final segmentation using the TFM and
applying the skeletonizing algorithm of [40]. Corresponding color-coded temporal evolution of C. elegans skeletons over an image sequence for (c) monoand (d) polydisperse media, respectively; skeleton lines are evenly spaced with
intervals of five frames and centered at the nematode’s center-of-mass, with
head oriented to the right. Results reveal a distinct envelope of body postures
specific to the motility environment.

are constructed using a principal component analysis to find the
skeletons principal axis and orientation at each instant in time.
Here, we may simply note that the near-symmetrical waveform
seen during swimming in monodisperse medium [see Fig. 6(c)]
is transformed into a somewhat more slithering-like motion as
the nematode evolves in polydisperse medium [see Fig. 6(d)];
a motility trait that has been previously attributed to the differences in response and material properties between the two
granular media [29]. Note that further quantitative locomotive
phenotypes including the nematode wavelength and the amplitude of body undulations can be obtained in a seamless manner
from the construction of such skeleton envelopes [17], [18],
[20].
While our method provides a useable segmentation framework, some noticeable shortcomings are nevertheless apparent.
In particular, we see that nematode skeletons are somewhat
truncated in length and skeletons do not always sit along the
centerline of the actual body. In general, the first shortcoming is
a consequence of the difficult nature in accurately segmenting
the head and tails of nematodes, as they are often transparent and
are extremely similar to (or invisible against) the background
environment. The second shortcoming results from both errors

in the body segmentation (due to the complex background) and
from the skeletonization process, which assumes that the correct
centerline be equidistant from the boundaries of the segmented
body.
Both of these shortcomings could be partly addressed by having a stronger model of the overall shape, head and tail of the
nematode, as is the case in the work of Huang et al. [38]. Yet in
general, the method outlined in [38] first relies on obtaining successful binary nematode segmentations to determine the correct
body outline, from which a nematode skeleton is then extracted;
in particular, this method has been solely applied to traditional
crawling assays for the purpose of characterizing “foraging” behavior (i.e., rapid, side-to-side movement of the nematode head
as it explores its environment). Since our method fails to segment the head and tail in a number of complex environments,
integrating such strategies into our framework appears challenging when incorrect nematode contours and thus skeletons
are generated.
Finally, we note an important limitation to our approach as a
result of the static nature of the model. While we learn appropriate image features for given sequences, these are determined
from the first image only in an effort to limit the user’s manual
labor and increase the easiness of the method. Yet, since some
environments appear visually different as time lapses (i.e., as
the sequence evolves), the models estimated from the initial image can become less accurate. This property is apparent in the
discrepancies observed in the F-scores between the first image
and the remaining sequence in a number of motility environments (see Table II). In turn, this greatly affects the quality of
the produced final segmentations and potentially renders them
unusable. In addition, we note that simply increasing the number
of training images (i.e., using for instance the first ten images in
an image sequence) to estimate the texture models does not appear to provide improved or more stable results (data not shown
here). As such, our approach should not be expected to provide
good nematode segmentations for scenarios where illumination
of the fore- and/or background gradually changes across the
sequence.
VI. CONCLUSION AND OUTLOOK
In this paper, we have proposed a novel TFM for automatic segmentation of nematode C. elegans in complex and
dynamic visual environments. Our TFM method relies on the
use of both intensity- and texture-based image features integrated within a probabilistic framework. This strategy first provides a coarse nematode segmentation from which we use an
MRF model to refine our segmentation by inferring pixels belonging to the nematode by means of an approximate inference
technique. To segment subsequent images in a sequence, we
compute informative priors from the computed segmentations,
via Bayesian filtering, and provide coherent estimates of nematode locations across time. We validate the TFM on a number
of image sequences sampled across different visual environments and provide state-of-the-art results on challenging environments while assuring comparative performances to existing
methods on traditional environments. In addition, we show how

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

C. elegans segmentations delivered with the TFM can be used to
compute nematode skeletons from which motility phenotypes
can then be extracted. This later point is extremely promising
for high-throughput assays of motility phenotyping in complex
environments.
In the future, we plan to extend our method to dynamically
estimate image models as time proceeds in an effort to provide even better segmentations while improving strategies to
segment the head and tail of nematodes more accurately. In addition, we will investigate methods for large-scale, automatic
image feature learning that can adapt directly to specific environments and should provide overall improvements in nematode
segmentations.
ACKNOWLEDGMENT
The authors would like to thank Dr. G. Juarez for helpful
discussions.
REFERENCES
[1] S. Brenner, “The genetics of Caenorhabditis elegans,” Genetics, vol. 77,
pp. 71–94, 1974.
[2] J. E. Sulston and H. R. Horvitz, “Post-embryonic cell lineages of the
nematode Caenorhabditis elegans,” Dev. Biol., vol. 56, pp. 110–156, 1977.
[3] J. E. Sulston, E. Schierenberg, J. G. White, and J. N. Thomson, “The
embryonic cell lineage of the nematode Caenorhabditis elegans,” Dev.
Biol., vol. 100, pp. 64–119, 1983.
[4] J. G. White, E. Southgate, J. N. Thomson, and S. Brenner, “The structure
of the nervous system of the nematode C. elegans,” Phil. Trans. Roy. Soc.
Lond. B Biol. Sci., vol. 314, pp. 1–340, 1986.
[5] C. E. S. Consortium, “Genome sequence of the nematode C. elegans: A
platform for investigating biology,” Science, vol. 282, pp. 2012–2018,
1998.
[6] A. Brown and W. Schafer, “Automated behavioural fingerprinting of C.
elegans mutants,” in in Cambridge Series in Systems Genetics.. Cambridge, U.K.: Cambridge Univ. Press, 2013.
[7] D. Ramot, B. E. Johnson, T. L. Berry, L. Carnell, and M. B. Goodman,
“The parallel worm tracker: a platform for measuring average speed and
drug-induced paralysis in nematodes,” PLOS One, vol. 3, p. e2208, 2008.
[8] G. D. Tsibidis and N. Tavernarakis, “Nemo: A computational tool for
analyzing nematode locomotion,” BMC Neuroscience, vol. 8, 2007.
[9] S. D. Buckingham and D. B. Sattelle, “Fast, automated measurement of
nematode swimming (thrashing) without morphometry,” BMC Neuroscience, vol. 10, p. 84, 2009.
[10] C. J. Cronin, J. E. Mendel, S. Mukhtar, Y.-M. Kim, R. C. Stirb, J. Bruck,
and P. W. Sternberg, “An automated system for measuring parameters of
nematode sinusoidal movement,” BMC Genetics, vol. 6, p. 5, 2005.
[11] J.-H. Baek, P. Cosman, Z. Feng, J. Silver, and W. R. Schafer, “Using
machine vision to analyze and classify Caenorhabditis elegans behavioral
phenotypes quantitatively,” J. Neurosci. Meth., vol. 118, no. 1, pp. 9–21,
2002.
[12] Z. Feng, C. J. Cronin, J. H. Wittig, P. W. Sternberg, and W. R. Schafer,
“An imaging system for standardized quantitative analysis of C. elegans
behavior,” BMC Bioinformatics, vol. 5, p. 115, 2004.
[13] K.-M. Huang, P. Cosman, and W. R. Schafer, “Machine vision based
detection of omega bends and reversals in C. elegans,” J. Neurosci. Meth.,
vol. 158, pp. 323–336, 2006.
[14] C. Fang-Yen, M. Wyart, J. Xie, R. Kawai, T. Kodger, S. Chen, Q. Wen, and
A. Samuel, “Biomechanical analysis of gait adaptation in the nematode
Caenorhabditis elegans,” Proc. Nat. Academy Sci. United States America,
vol. 107, pp. 20 323–20 328, 2010.
[15] S.-J. Park, M. B. Goodman, and B. L. Pruitt, “Analysis of nematode mechanics by piezoresistive displacement clamp,” Proc. Nat. Acad. Sci. USA,
vol. 104, pp. 17 376–17 381, 2007.
[16] B. Petzold, S.-J. Park, P. Ponce, C. Roozeboom, C. Powell, M. Goodman,
and B. Pruitt, “Caenorhabditis elegans body mechanics are regulated by
body wall muscle tone,” Biophysical J., vol. 100, pp. 1977–1985, 2011.

2287

[17] J. Sznitman, P. K. Purohit, P. Krajacic, T. Lamitina, and P. E. Arratia, “Material properties of Caenorhabditis elegans swimming at low Reynolds
number,” Biophysical J., vol. 98, pp. 617–626, 2010.
[18] J. Sznitman, X. Shen, P. K. Purohit, and P. E. Arratia, “The effects of
fluid viscosity on the kinematics and material properties of C. elegans
swimming at low Reynolds number,” Exp. Mech., vol. 50, pp. 1313–1311,
2010.
[19] J. Korta, D. A. Clark, C. V. Gabel, L. Mahadevan, and A. D. T. Samuel,
“Mechanosensation and mechanical load modulate the locomotory gait
of swimming C. elegans,” J. Exp. Biol., vol. 210, pp. 2383–2389,
2007.
[20] P. Krajacic, X. Shen, P. Purohit, P. Arratia, and T. Lamitina, “Biomechanical profiling of Caenorhabditis elegans motility,” Genetics, vol. 191,
pp. 1015–1021, 2012.
[21] J. Sznitman, X. Shen, R. Sznitman, and P. Arratia, “Propulsive force
measurements and flow behavior of undulatory swimmers at low Reynolds
number,” Phys. Fluids, vol. 22, p. 121901, 2010.
[22] P. Sauvage, M. Argentina, J. Drappier, T. Senden, J. Siméon, and
J. DiMeglio, “An elasto-hydrodynamical model of friction for the locomotion of Caenorhabditis elegans,” J. Biomechanics, vol. 44, pp. 1117–1722,
2011.
[23] X. Shen, J. Sznitman, P. Krajacic, T. Lamitina, and P. Arratia, “Undulatory
locomotion of Caenorhabditis elegans on wet surfaces,” Biophysical J.,
vol. 102, pp. 2772–2781, 2012.
[24] G. Stephens, B. Johnson-Kerner, W. Bialek, and W. Ryu, “Dimensionality
and dynamics in the behavior of C. elegans,” PLOS Comput. Biol., vol. 4,
p. e1000028, 2008.
[25] G. Stephens, L. Osborne, W. Bialek, “Searching for simplicity in the
analysis of neurons and behavior,” Proc. Nat. Academy Sci. United States
America, vol. 108, pp. 15 565–15 571, 2011.
[26] A. Brown, E. Yemini, L. Grundy, T. Jucikas, and W. Schafer, “A dictionary
of behavioral motifs reveals clusters of genes affecting Caenorhabditis
elegans locomotion,” Proc. Nat. Academy Sci. United States America,
vol. 110, pp. 791–796, 2013.
[27] R. Ghosh and J. Sznitman, “Visualization of nematode Caenorhabditis
elegans swimming in a liquid drop,” J. Visualization, vol. 15, pp. 277–
279, 2012.
[28] D. Albrecht and C. Bargmann, “High-content behavioral analysis of
Caenorhabditis elegans in precise spatiotemporal chemical environments,” Nature Methods, vol. 8, pp. 599–605, 2011.
[29] G. Juarez, K. Lu, J. Sznitman, and P. Arratia, “Motility of small nematodes
in wet granular media,” Europhysics Lett., vol. 92, p. 44002, 2010.
[30] M. Mathew, N. Mathew, and P. Ebert, “Wormscan: A technique for highthroughput phenotypic analysis of Caenorhabditis elegans,” PLOS One,
vol. 7, p. e33483, 2012.
[31] N. Swierczek, A. Giles, C. Rankin, and R. Kerr, “High-throughput behavioral analysis in C. elegans,” Nature Methods, vol. 8, pp. 592–598,
2011.
[32] J. Carr, A. Parashar, R. Gibson, A. Robertson, R. Martin, and S. Pandey,
“A microfluidic platform for high-sensitivity, real-time drug screening on
C. elegans and parasitic nematodes,” Lab Chip, vol. 11, pp. 2385–2396,
2011.
[33] C. B. Rhode, F. Zeng, R. Gonzalesz-Rubio, M. Angel, and M. F. Yanik,
“Microfluidic system for on-chip high-throughput whole-animal sorting
and screening at subcellular resolution,” Proc. Nat. Acad. Sci. USA,
vol. 104, pp. 13 891–13 895, 2007.
[34] W. Shi, J. Qin, N. Ye, and B. Lin, “Droplet-based microfluidic system for
individual Caenorhabditis elegans assay,” Lab Chip, vol. 8, pp. 1432–
1435, 2008.
[35] W. Geng, P. Cosman, J.-H. Baek, C. C. Berry, and W. R. Schafer,
“Quantitative classification and natural clustering of Caenorhabditis elegans behavioral phenotypes,” Genetics, vol. 165, no. 3, pp. 1117–1126,
2003.
[36] K. Hoshi and R. Shingai, “Computer-driven automatic identification of
locomotion states in Caenorhabditis elegans,” J. Neuro. Meth., vol. 157,
pp. 355–363, 2006.
[37] K.-M. Huang, P. Cosman, and W. R. Schafer, “Using articulated models
for tracking multiple C. elegans in physical contact,” J. Sign. Process
Syst., vol. 55, pp. 113–126, 2009.
[38] K. M. Huang, P. Cosman, and W. R. Schafer, “Automated detection and
analysis of foraging behavior in Caenorhabditis elegans,” J. Neurosci.
Meth., vol. 171, pp. 153–164, 2008.
[39] C. Stauffer and W. Grimson, “Adaptive background mixture models for
real-time tracking,” in Proc. IEEE Conf. Comput. Vision Pattern Recog.,
1999, pp. 246–252.

2288

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 8, AUGUST 2014

[40] R. Sznitman, M. Gupta, G. Hager, P. Arratia, and J. Sznitman, “Multienvironment model estimation for motility analysis of Caenorhabditis
elegans,” PLOS One, vol. 5, p. e11631, 2010.
[41] J. Karbowski, C. J. Cronin, A. Seah, J. E. Mendel, D. Cleary, and
P. W. Sternberg, “Conservation rules, their breakdown, and optimality in
Caenorhabditis sinusoidal locomotion,” J. Theor. Biol., vol. 242, pp. 652–
669, 2006.
[42] J. T. Pierce-Shimomura, T. M. Morse, and S. R. Lockery, “The fundamental role of pirouettes in Caenorhabditis elegans chemotaxis,” J. Neurosci.,
vol. 19, pp. 9557–9569, 1999.
[43] J. T. Pierce-Shimomura, B. L. Chen, J. J. Mun, R. Ho, R. Sarkis, and
S. L. McIntire, “Genetic analysis of crawling and swimming locomotory
patterns in C. elegans,” Proc. Nat. Acad. Sci. USA, vol. 105, pp. 20982–
20987, 2008.
[44] N. Tavernarakis, W. Shreffler, S. Wang, and M. Driscoll, “unc-8, a
DEG/ENAC family member, encodes a subunit of a candidate mechanically gated channel that modulates C. elegans locomotion,” Neuron.,
vol. 18, pp. 107–119, 1997.
[45] S. Berri, J. H. Boyle, M. Tassieri, I. A. Hope, and N. Cohen, “Forward
locomotion of the nematode C. elegans is achieved through modulation of
a single gait,” Human Frontier Sci. Program J., vol. 3, pp. 186–193, 2009.
[46] R. Ghosh and S. W. Emmons, “Episodic swimming behavior in the nematode C. elegans,” J. Exp. Biol., vol. 211, pp. 3703–3711, 2008.
[47] N. Chronis, M. Zimmer, and C. I. Bargmann, “Microfluidics for in vivo
imaging of neuronal and behavioral activity in Caenorhabditis elegans,”
Nature Methods, vol. 4, pp. 727–731, 2007.
[48] A. Ghanbari, V. Nock, R. Blaikie, X. Chen, J. Chase, and W. Wang,
“Automated vision-based force measurement of moving C. elegans,” in
Proc. IEEE 6th Annu. Conf. Autom. Sci. Eng., 2010, pp. 198–203.
[49] S. Hulme, S. Shevkoplyas, J. Apfeld, W. Fontana, and G. Whitesides,
“A microfabricated array of clamps for immobilizing and imaging C.
elegans,” Lab Chip, vol. 7, pp. 1515–1523, 2007.
[50] S. R. Lockery, K. J. Lawton, J. C. Doll, S. Faumont, S. M. Couthard,
T. R. Thiele, N. Chronis, K. E. McCormick, M. B. Goodman, and
B. L. Pruitt, “Artificial dirt: microfluidic substrates for nematode neurobiology,” J. Neurophysiol., vol. 99, pp. 3136–3143, 2008.
[51] S. Johari, V. Nock, M. Alkaisi, and W. Wang, “On-chip analysis of C.
elegans muscular forces and locomotion patterns in microstructured environments,” Lab Chip, vol. 13, pp. 1699–1707, 2013.
[52] T. Majmudar, E. Keaveny, J. Zhang, and M. Shelley, “Experiments and
theory of undulatory locomotion in a simple structured medium,” J. Roy.
Soc. Interface, vol. 9, pp. 1809–1823, 2012.
[53] A. Parashar, R. Lycke, J. Carr, and S. Pandey, “Amplitude-modulated
sinusoidal microchannels for observing adaptability in C. elegans locomotion,” Biomicrofluidics, vol. 5, p. 024112, 2011.
[54] S. Park, H. Hwang, S.-W. Nam, F. Martinez, R. H. Austin, and W. S. Ryu,
“Enhanced Caenorhabditis elegans locomotion in a structured microfluidic environment,” PLOS One, vol. 3, p. e2550, 2008.
[55] J. Qin and A. R. Wheeler, “Maze exploration and learning in C. elegans,”
Lab Chip, vol. 7, pp. 186–192, 2007.
[56] S. Jung, “Caenorhabditis elegans swimming in a saturated particulate
system,” Phys. Fluids, vol. 22, p. 031903, 2010.
[57] J. Boyle, S. Johnson, and A. Dehghani-Sanij, “Adaptive undulatory locomotion of a C. elegans inspired robot,” IEEE/ASME Trans. Mechatronics,
vol. 18, no. 2, pp. 439–448, Apr. 2013.
[58] S. Kim, C. Laschi, and B. Trimmer, “Soft robotics: A bioinspired evolution
in robotics,” Trends Biotechnol., vol. 31, pp. 287–294, 2013.
[59] H. Yuk, D. Kim, H. Lee, S. Jo, and J. Shin, “Shape memory alloy-based
small crawling robots inspired by C. elegans,” Bioinspir. Biomim., vol. 6,
p. 0460002, 2011.
[60] A. Lucchi, K. Smith, R. Achanta, G. Knott, and P. Fua, “Supervoxelbased segmentation of mitochondria in EM image stacks with learned
shape features,” IEEE Trans. Med. Imag., vol. 31, no. 2, pp. 474–486,
Feb. 2012.
[61] K. S. Camilus and V. K. Govindan, “A review on graph based segmentation,” Int. J. Image, Graphics Signal Process., vol. 4, no. 5, pp. 1–13,
2012.
[62] N. Roussel, C. A. Morton, F. P. Finger, and B. Roysam, “A computational
model for C. elegans locomotory behavior: Application to multiworm
tracking,” IEEE Trans. Biomed. Eng., vol. 54, no. 10, pp. 1786–1797, Oct.
2007.
[63] G. Tsechpenakis, L. Bianchi, D. N. Metaxas, and M. Driscoll, “A novel
computational approach for simultaneous tracking and feature extraction
of C. elegans populations in fluid environments,” IEEE Trans. Biomed.
Eng., vol. 55, no. 5, pp. 1539–1549, May 2008.

[64] M. Piccardi, “Background subtraction techniques: A review,” in Proc.
IEEE Int. Conf. Syst., Man Cybern., 2004, pp. 3099–3104.
[65] R. Sznitman, H. Lin, M. Gupta, and G. Hager, “Active background modeling: Actors on a stage,” in Proc. IEEE 12th Int. Conf. Comput. Vision,
Workshop Vis. Surveillance, 2009, pp. 1222–1228.
[66] K. Murphy, Machine Learning: A Probabilistic Perspective. Cambridge,
MA, USA: MIT, 2012.
[67] T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical
Learning. New York, NY, USA: Springer, 2001.
[68] J. Geusebroek, A. Smeulders, and J. van de Weijer, “Fast anisotropic
Gauss filtering,” IEEE Trans. Image Process., vol. 12, no. 8, pp. 938–943,
Aug. 2003.
[69] R. Sznitman, A. Lucchi, M. Cantoni, G. Knott, and P. Fua, “Flash scanning
electron microscopy,” presented at the Med. Image Comput. Comput.Assist. Interv., Nagoya, Japan, Sep. 2013.
[70] S. Thrun, W. Burgard, and D. Fox, Probabilistic Robotics. Cambridge,
MA, USA: MIT Press, 2005.
[71] A. Lucchi, K. Smith, R. Achanta, V. Lepetit, and P. Fua, “A fully automated
approach to segmentation of irregularly shaped cellular structures in EM
images,” in Proc. Med. Image Comput. Comput. Assist. Interv., 2010,
pp. 463–471.

Ayala Greenblum received the B.Sc. degree in biomedical engineering from
the Technion—Israel Institute of Technology, Haifa, Israel, in 2011, where she
is currently working toward the M.Sc. degree in the Department of Biomedical
Engineering, Biofluids Laboratory.
Her research interests include image processing, computer vision, and statistical learning for biomedical applications.

Raphael Sznitman received the B.Sc. degree in
cognitive systems from the University of British
Columbia, Vancouver, BC, Canada, in 2007, and the
M.Sc. and Ph.D. degrees in computer science from
Johns Hopkins University, Baltimore, MD, USA, in
2011.
He is currently a Postdoctoral Fellow at the
Ecole Polytechnique Federale de Lausanne, Lausanne, Switzerland, where he works in the computer
vision laboratory. His research interests include computational vision, probabilistic methods, and statistical learning, applied to applications in biomedical imaging.

Pascal Fua (F’12) received the Engineering degree
from Ecole Polytechnique, Paris, France, in 1984,
and the Ph.D. degree in computer science from the
University of Orsay, Orsay, France, in 1989.
He joined Ecole Polytechnique Federale de Lausanne, the Swiss Federal Institute of Technology,
Lausanne, Switzerland, in 1996, where he is currently a Professor in the School of Computer and
Communication Science. Before that, he worked at
SRI International and at INRIA Sophia-Antipolis as
a Computer Scientist. His research interests include
shape modeling and motion recovery from images, analysis of microscopy images, and augmented reality. He has (co)authored more than 200 publications
in refereed journals and conferences.
Dr. Fua is an Associate Editor of the IEEE TRANSACTIONS FOR PATTERN
ANALYSIS AND MACHINE INTELLIGENCE. He often serves as a Program Committee Member, Area Chair, and Program Chair of major vision conferences.

GREENBLUM et al.: CA ENORHABDI TIS ELEGANS SEGMENTATION USING TEXTURE-BASED MODELS FOR MOTILITY PHENOTYPING

Paulo E. Arratia received the B.Sc. degree in chemical engineering from Hampton University, Hampton,
VA, USA, in 1997, and the M.Sc. and Ph.D. degrees
in chemical and biochemical engineering from Rutgers University, New Brunswick, NJ, USA, in 2001
and 2003, respectively.
He was a postdoctoral fellow at Haverford College (2003–2005) and the University of Pennsylvania (2005–2007). He is currently an Associate Professor in the Department of Mechanical Engineering
and Applied Mechanics, University of Pennsylvania,
Philadelphia, PA, USA. He is the Director of the Penn Complex Fluids Lab
where his research interests focus on interdisciplinary soft-condensed matter
and fluid dynamics, including swimming of microorganisms, blood flow in microfluidic devices, and viscoelasticity.

2289

Josué Sznitman received the B.Sc. degree in mechanical engineering from the Massachusetts Institute of Technology, Cambridge, MA, USA, in 2002,
and the Dipl.-Ing. and Dr. Sc. degrees in mechanical
engineering from the ETH Zurich, Zurich, Switzerland, in 2003 and 2007, respectively.
Prior to joining the Department of Biomedical Engineering at the Technion—Israel Institute of Technology, Haifa, Israel, as an Assistant Professor in
2010, he was a Postdoctoral Researcher at the University of Pennsylvania (2008–2009) and a Lecturer
and Research Associate at Princeton University appointed by the Council of
Science and Technology. He is the founder of the Technion Biofluids Laboratory where his research interests include biofluid mechanics, quantitative flow
visualization, and microfluidics.

