IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

2633

Brain Tumor Segmentation Based on Local
Independent Projection-Based Classification
Meiyan Huang, Wei Yang, Yao Wu, Jun Jiang, Wufan Chen, Senior Member, IEEE, and Qianjin Feng∗ , Member, IEEE

Abstract—Brain tumor segmentation is an important procedure
for early tumor diagnosis and radiotherapy planning. Although numerous brain tumor segmentation methods have been presented,
enhancing tumor segmentation methods is still challenging because
brain tumor MRI images exhibit complex characteristics, such as
high diversity in tumor appearance and ambiguous tumor boundaries. To address this problem, we propose a novel automatic tumor
segmentation method for MRI images. This method treats tumor
segmentation as a classification problem. Additionally, the local
independent projection-based classification (LIPC) method is used
to classify each voxel into different classes. A novel classification
framework is derived by introducing the local independent projection into the classical classification model. Locality is important in
the calculation of local independent projections for LIPC. Locality
is also considered in determining whether local anchor embedding
is more applicable in solving linear projection weights compared
with other coding methods. Moreover, LIPC considers the data
distribution of different classes by learning a softmax regression
model, which can further improve classification performance. In
this study, 80 brain tumor MRI images with ground truth data are
used as training data and 40 images without ground truth data
are used as testing data. The segmentation results of testing data
are evaluated by an online evaluation tool. The average dice similarities of the proposed method for segmenting complete tumor,
tumor core, and contrast-enhancing tumor on real patient data are
0.84, 0.685, and 0.585, respectively. These results are comparable
to other state-of-the-art methods.
Index Terms—Brain tumor segmentation, local anchor embedding, local independent projection-based classification, softmax regression.

I. INTRODUCTION
RAIN tumor segmentation is one of the crucial procedures
in surgical and treatment planning. However, at present,
brain tumor segmentation in brain tumor images is mostly performed manually in clinical practice. Apart from being time
consuming, manual brain tumor delineation is difficult and depends on the individual operator. Currently, multimodal MRI

B

Manuscript received January 21, 2014; revised May 12, 2014; accepted May
14, 2014. Date of publication May 19, 2014; date of current version September
16, 2014. This work was supported by the National Basic Research Program of
China (973 Program) under Grant 2010CB732505, National Science & Technology Pillar Program of China under Grant 2012BAI14B02, National Natural
Science Funds of China under Grants 81101109 and 31371009, National High
Technology Research and Development Program of China (863 Program) under
Grant 2012AA02A616, and Program of Pearl River Young Talents of Science
and Technology in Guangzhou under Grant 2013J2200065. Asterisk indicates
corresponding author.
M. Huang, W. Yang, Y. Wu, J. Jiang, and W. Chen are with the
School of Biomedical Engineering, Southern Medical University, Guangzhou
510515, China (e-mail: huangmeiyan11@gmail.com; weiyanggm@gmail.com;
wuyao198851@gmail.com; smujiang@gmail.com; wufanchen@gmail.com).
∗ Q. Feng is with the School of Biomedical Engineering, Southern Medical
University, Guangzhou 510515, China (e-mail: qianjinfeng08@gmail.com).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2325410

Fig. 1. Different modalities reveal different parts in the tumor area. The edge
of the tumor area is visually vague. In addition, the brain structure is deformed
because of the occurrence of edema. (a) T1C-weighted brain tumor MRI image.
(b) T2-weighted brain tumor MRI image. (c) Contour of the actual brain tumor.
“t” represents the combination of contrast-enhancing and necrotic parts, and “e”
represents the edema part.

images are used simultaneously by radiologists in segmenting brain tumor images because multimodal MRI images can
provide various data on tumors. In glioma, the tumor area is
usually divided into necrosis, contrast-enhancing tumor, nonenhancing tumor, and edema [1]. Different MRI image modalities can reveal different parts in the tumor area. For instance,
T1C (T1-weighted images with contrast enhancement) highlights contrast-enhancing regions, whereas T2 highlights edema
regions (see Fig. 1). Although multimodal MRI images can provide complementary information in the tumor area, brain tumor
segmentation is still a challenging and difficult task. Brain tumors can have various sizes and shapes and may appear at
different locations. In addition to tumor heterogeneity, tumor
edges can be complex and visually vague (see Fig. 1). Moreover, some tumors may deform surrounding structures in the
brain because of the mass effect or edema (see Fig. 1). Additionally, artifacts and noise in brain tumor images increase the
difficulty when segmenting tumors. Thus, designing of a semiautomatic or automatic brain tumor segmentation approach is
necessary to provide an acceptable performance.
Numerous algorithms have been developed to perform brain
tumor detection and segmentation. These methods include
thresholding and morphological techniques [2]–[4], watershed
method [5], region growing approach [4], [6], asymmetry analysis [7]–[10], atlas-based method [11]–[14], contour/surface evolution method [15]–[17], interactive algorithm [18]–[20], and
supervised [21]–[29] and unsupervised [6], [30]–[35] learning
methods.
A. Intensity-Based Method
Low-level operations, such as thresholding, edge detection,
and morphological techniques [2]–[4], are fast and can be

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

2634

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

easily adjusted. However, the tumor segmentation performance
of these methods highly depends on evident difference in the
intensities between tumor and nontumor regions. Watershed [5]
and region growing [4], [6] approaches are simple and consistently produce complete boundaries. However, these two methods are sensitive to noise, which is a common problem in the
intensity-based method. Moreover, most intensity-based methods tend to oversegment tumors because of the weak and diffused edges caused by edema [15].

B. Asymmetry Analysis
The healthy human brain is largely symmetric across the
mid-sagittal plane. The asymmetric analysis method [7]–[10]
for tumor segmentation is based on the principle that tumors,
which appear in one of the cerebral hemispheres, can cause
asymmetry between the left and right cerebral hemispheres.
This asymmetry can be detected, and tumors can be roughly
located in the corresponding cerebral hemisphere. The asymmetric analysis method can hasten the tumor detection and segmentation process because tumor segmentation is implemented
in one of the cerebral hemispheres. However, accurately finding
the mid-sagittal plane is a challenging and time-consuming task
[7]. More importantly, asymmetry analysis may not be useful
when a tumor is located across the mid-sagittal plane [9].

C. Atlas-Based Method
Atlas-based segmentation methods have been extensively investigated [36], [37]. Brain atlases can provide important data
prior to tumor segmentation enhancement by measuring the
difference between abnormal and normal brains. However, the
deformable registration of the brain atlas to brain images with
tumor is an extremely challenging task because of the intensity
variations around the tumor caused by edema and the deformations of healthy tissue morphology caused by the tumor mass
effect [38]. In a previous study [11], affine registration is used to
align the atlas to the tumor image data. When a large brain structure deformation appears, the misalignment issues are noted on
the aligned atlas, which may significantly decrease segmentation accuracy [11].

D. Contour/Surface Evolution Method
The contour/surface evolution method has been widely used
for the tumor segmentation of 2-D/3-D data [15]–[17]. This
method can be represented implicitly as a level set function or
explicitly as an active contour model/snake function. Compared
with the parametric active contour model, the level set method
can represent contours with complex topology and handle topological changes, such as splitting and merging in a natural and
efficient way [19]. Furthermore, the extension of the level set
method to 3D is straightforward and does not require additional
machinery [16]. However, the contour/surface evolution method
does not easily determine the initializations and tune the parameters even when 3-D level set surfaces are used.

E. Interactive Algorithm
Graph-based seeded segmentation framework is one of the
popular methods among interactive algorithms. Graph-based
seeded segmentation is a global optimization approach, which
showed outstanding performance for tumor segmentation in our
previous studies [19], [20]. However, this method needs manual
seed selection in different tissues, and distinguishing different
tissues in the tumor is difficult during the selection of initial
seeds for different tissues. In a previous study [18], a cellular
automata-based seeded method, called tumor-cut, has been presented for brain tumor segmentation. This method only requires
the user to draw a line over the largest visible tumor diameter.
Although this initial seed selection strategy can reduce manual
interaction and decrease the sensitivity of the method to initialization, this procedure may not include all tumor areas within the
volume of interest along the depth direction [18], thus leading
to tumor undersegmentation.
F. Supervised and Unsupervised Learning Methods
Unsupervised learning method, such as k-means and fuzzy
clustering, has become popular for brain tumor segmentation
in recent years [6], [30]–[35]. The fuzzy method considers that
medical images are inherently fuzzy, so it is a very strong tool
for medical image processing. Furthermore, the fuzzy method
can capture pixel proximity in the same objective region without a training step. However, most fuzzy methods work well
only for tumors that present hyper-intensity and exhibit poor
performance on segmenting nonenhanced tumors [7]. These
conditions are due to the fact that these fuzzy methods typically
use intensity-based method, such as thresholding and morphological operations, as pre- or postprocessing.
Supervised classification learning method is widely used
in tumor segmentation [21]–[29]. Well-trained classifiers can
extract discriminative information from the training data and
estimate the label of each voxel in a testing volume. However,
the traditional classification methods classify each voxel into
different classes without considering the spatial correlation between current and nearby voxels. This method may not obtain
a global optimized result. To address this problem, a classification method is generally combined with a regularization
step. The regularization step can be implemented by modeling
the boundary [25] or by applying a variant of a random field
spatial prior (MRF/CRF) [23], [24], [26], [29]. In the previous studies [21], [22], context-aware spatial features and the
probabilities obtained by tissue-specific Gaussian mixture models are used as inputs for classifiers, and satisfied segmentation results are achieved without using posthoc regularization.
Similar to the previously described methods [21], [22], we propose a novel classification method, named local independent
projection-based classification (LIPC), for brain tumor segmentation without using explicit regularization.
The contribution of this study can be summarized as follows:
First, an LIPC-based method is introduced to solve the tumor segmentation problem. Based on recent studies on sparse
representation [39]–[41], the sparse representation-based classification (SRC) method [42] was adopted in medical image

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

2635

TABLE I
DETAILS OF THE BRAIN TUMOR IMAGE DATA
Real patient data
High-grade
Training data
Testing data

2012
2013

Total

20
11
10
55

Low-grade
10
4
-

Synthetic data
High-grade
25
10
65

Low-grade
25
5
-

segmentation, thereby producing robust results. In the SRC
method, a sample can be sparsely represented in a specific dictionary, which contains samples from all classes. Contrary to
SRC, the proposed LIPC assumes that the training samples
from different classes are located on different submanifolds.
Then, the training samples are divided into different groups and
subsequently used to construct different dictionaries. The testing
sample is independently projected on these different dictionaries
using the local anchor embedding (LAE) method [43].
Second, a softmax model is used to determine the relationship
between data distribution and reconstruction error norm. When
the data distribution is uniform and the noise is low, classification
may be performed well. However, data distribution is complex
in brain tumor MRI images. In addition, the data distribution
of different classes (i.e., tumor, edema, and brain tissue) may
widely vary. Therefore, the data distribution of each class should
be considered when segmenting brain tumors. Our evaluation
on synthetic data and public available brain tumor image data
demonstrates that considering the data distribution of different
classes can further improve the segmentation performance.
This study is organized as follows. Section II introduces
methodology of the proposed algorithm. Section III presents
the experimental analysis and results. Section IV provides several discussions and conclusions on the proposed method. A
preliminary work of this study appeared in another study [44].
II. MATERIALS AND METHODS
A. Materials
The brain tumor image data used in this study were provided
by the MICCAI 2012 and 2013 Challenges on Multimodal
Brain Tumor Segmentation (http://www.imm.dtu.dk/projects/
BRATS2012 and http://martinos.org/qtim/miccai2013/data.html).
The challenge database contains fully anonymized images from
ETH Zurich, University of Bern, University of Debrecen, and
University of Utah [45].
The brain tumor image data contain information on 120 subjects with gliomas; 55 images are real patient data, and 65 images
are synthetic data (see Table I). A total of 80 images out of 120
images with ground truth data are treated as training data. In
the ground truth data, the complete tumor region is subdivided
into necrosis, contrast-enhancing tumor, nonenhancing tumor,
and edema parts for real patient data, whereas the complete tumor region is subdivided into tumor core and edema parts for
synthetic data. The remaining 40 images are testing data with
publicly unavailable ground truth data. T1, T2, FLAIR, and

Fig. 2.

Flowchart of the proposed method.

postGadolinium T1 MRI images are available for each subject.
All volumes are linearly co-registered to the T1 contrast image,
skull stripped, and interpolated to 1 mm isotropic resolution.
In addition to the brain tumor image data, a set of interleaving spirals was also used in elucidating the mechanism of the
proposed LIPC method.
B. Overview of the Proposed Method
The proposed method consists of four major steps, i.e., preprocessing, feature extraction, tumor segmentation using the
LIPC method, and postprocessing. To reduce computational
costs, we embedded the proposed method in a multiresolution
framework. The flowchart of the proposed method is illustrated
in Fig. 2.
C. The Basic Principle of LIPC
Brain tumor segmentation can be considered as a multiclass
classification problem. To solve this problem, a one-versus-all
(OvA) strategy can be used. In the OvA approach, a classifier is trained per class to distinguish a class from all other
classes. Therefore, N classifiers f = {fi }N
i=1 have to be determined in this study, where N represents the number of classes.
Given a testing sample x ∈ RM , N real classification scores
y = {yi }N
i=1 are computed using the learned classifiers f (x);
where the sample x stands for the image feature in the current
study (details in Section II-F) and yi ∈ [0, 1] stands for the probabilities that the sample belongs to the ith class. The label of
sample x can be defined as follows:
l = arg max fi (x) = arg max yi .
i∈1,···,N

(1)

i∈1,...,N

Before the proposed LIPC was introduced, the following assumption was considered as the base for LIPC:
Assumption I: Samples from different classes are located on
different nonlinear submanifolds, and a sample can be approximately represented as a linear combination of several nearest
neighbors from its corresponding submanifold.
For N-class classification, this assumption indicates that the
samples are found on a manifold {M i }N
i=1 , which consists of N
submanifolds; M i represents the submanifold associated with
the ith class. For a dictionary D = {D i }N
i=1 , which consists of N
i
subdictionaries, D i = [d1i , d2i , . . . , dN
i ] consists of Ni typical
samples from the ith submanifold.
Considering Assumption I, a testing sample x can be
projected into each submanifold using the following linear

2636

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

representation:
x = D i ai + εi =

Ni


aji dji + εi

j =1

s.t. εi  < τ, ∀dji ∈
/ Nix (k) , aji = 0

(2)

where Nix (k) denotes the set of k nearest neighbors of x in dictioi T
nary D i , εi is the reconstruction error, ai = (a1i , a2i , . . . , aN
i )
is the weight coefficient vector of the linear combination, and τ
is a small positive real number that ensures the reconstruction
accuracy. Many (at least Ni − k) elements of ai are zeroes for
an arbitrary sample x, which indicates that the described linear
representation is sparse. The L2-norm of εi noted as εi 2 can
be used in measuring the proximity between x and its projection on M i . Furthermore, a smaller εi 2 is associated with
increased probability that x belongs to the ith class. Therefore,
the classification scores of x have a negative correlation with
εi 2 and can be formulated as
⎛
⎞
Ni

aji dji + εi ⎠
yi = fi (x) = fi ⎝
j =1


= gi (εi ) = gi x −

N i
j =1


aji dji

(3)

where gi (·) is an inverse proportional function. Based on (3),
the label prediction in (1) can be rewritten as
l = arg max fi (x) = arg max yi = arg max gi (εi ) .
i∈1,...,N

i∈1,...,N

(4)

i∈1,...,N

The proposed LIPC is different from SRC [42]. Although the
reconstruction errors are both used as the classification measure
in these two methods, the testing sample is projected onto each
submanifold independently in the proposed method instead of
the whole manifold in SRC. For each submanifold, the projection calculation is independent. Therefore, if Assumption I is
considered, the proposed method is more applicable than SRC.
D. LIPC Implementation
We performed dictionary construction, locally linear representation, and classification score computation to implement
LIPC.
1) Dictionary Construction: The manually labeled original
samples in a training set are used to construct D [41]. However,
numerous original training samples possibly produce a large D,
which dramatically increases computational and memory costs.
In the present study, more than half a million samples for each
class are available for training. Thus, subsequent processes are
impractical when conducted traditionally. Applying a dictionary
learning method is necessary in learning a compact representation of the original training samples. The k-means [46] method
can obtain the typical structures of the original sample space;
and thus, this method is used in the current study to learn a
compact representation of the original training samples of each
class.
2) Locally Linear Representation: Several methods, such
as sparse coding (SC) [41], locality-constrained linear coding

(LLC) [47], and LAE, have been proposed to approximately represent a sample that is linearly based on training samples. Sparse
representation emphasizes the sparsity of the representation and
attempts to use the smallest number of training samples in reconstructing the testing sample with minimal error. LLC and LAE
belong to the special cases of sparse representation. These two
approaches focus on locality rather than sparsity by limiting the
linear coding within a local neighborhood. In particular, LAE
forces the weight coefficients to maintain nonnegativity and a
sum that is equal to one, which ensures that the reconstructed
sample is a convex combination of its closest neighbors. Therefore, the reconstructed sample is located in a convex region on
a hyperplane spanned by its closest neighbors. Actually, this
condition is a tough constraint of the locality. The locality of
the representation is highly preferable for classification tasks
[43], [47], [48]. Thus, LAE is appropriate in solving the linear
representation in the current study. For completeness, we briefly
introduce LAE.
Considering the concrete task in this paper, we formally reformulize the cost function of LAE as
	
	2


	
	
a∗ = arg min 	x − N
j =1 aj dj 	
(5)
a


s.t. ∀ dj ∈
/ Nx (k) , aj = 0, N
a
=
1,
a
≥
0.
j
j
j
Three steps were performed to obtain the solution of LAE.
First, the k nearest neighbors of the testing sample x was selected
from D and Nx (k) was constructed. Second, for dj s that do not
belong to Nx (k), the associated aj s were set to 0. Third, for the
remaining dj s that belong to Nx (k), their corresponding aj s
was calculated using the projected gradient method [43]. The
updating rule in the projected gradient method can formally be
expressed as the following iterative formula:



(t+1)
(t)
(t)
(6)
= Pχ ai − ηt ∇h ai
ai
where t is the number of iterations, ηt > 0 is the step size,
∇h (a) is the gradient of function h at a, and Pχ (a) is the simplex projection operator on any a ∈ Rp . The simplex projection
operator can be formulated as
Pχ (a) = arg min a − a .
a  ∈χ

(7)

As introduced in [43], (6) uses the simplex projection operator
in (7), is implemented efficiently in O (s log s), and can be
accelerated using Nesterov’s method [49].
3) Classification Score Computation: As described in
Section II-C, the classification scores of x have a negative correlation with εi 2 . Furthermore, to implement the classification
within a multiresolution framework (details in Section II-G),
the classification score can be easily represented as a probability form:
N

exp (− εi 2 ). (8)
yi = g (εi ) = exp (− εi 2 )
i=1

However, (8) considers only the relationship between the reconstruction error norm and the classification score, which ignores the relationship between the reconstruction error norm and
the data distribution on each submanifold. In general, the data

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

Fig. 3. (a) Example that shows how a testing sample may be wrongly classified if the data distributions on different submanifolds are not considered.
ε2 2 is smaller than ε1 2 , so the testing sample will be assigned to Class 2
based on (8). However, assigning the testing sample to be Class 1 is reasonable
after considering the distribution of all training data on different submanifolds.
(b) Learning a softmax regression model using the reconstruction error norms
to separate the data into different classes.

distributions on different submanifolds are different. Therefore,
the reconstruction error norms of the samples cover a wide range
and may sometimes violate the negative correlation as listed in
(8) [see Fig. 3(a)]. To address this problem, we propose to establish a softmax regression model using the reconstruction error
norms of the training data to achieve the classification score [see
Fig. 3(b)]. Thus, the classification score can be defined as
N
 

 T

(9)
exp wTk ε2 .
yi = exp wi ε2
k =1
j
Given the original training set T = {T i }N
i=1 = {{xi ,
N
i
lij }nj =1
}N
i=1 and the learned dictionary D = {D i }i=1 , the
(j )

weight coefficient vector ai that corresponds to each sample in T can be calculated using (5). Then, the reconstruction
(j )
(j )
error norm ε2 = {[|ε1 2 , . . . , εN 2 ]T }nj=1 of all training
samples in T can be computed using (2). With ε2 , the cost
function of the softmax regression model can be defined as
 n N

1    (j )
1 l =i
J (w) = −
n j =1 i=1

(j )
exp(wTi ε2 )
(10)
× log 
N
(j )
T
k =1 exp(w k ε2 )
where 1 {·} is an indicator function. Given that no closed-form
technique can be used to solve for the minimum of J (w), we
employ the gradient descent algorithm to iteratively optimize
(10).
E. Summary of LIPC
We provide the pseudo-code of the proposed method in
Algorithm I.
ALGORITHM I
j j ni
N
Input: Training set T = {T i }N
i=1 = {{xi , li }j =1 }i=1 ; A
testing sample x.
Output: The classification scores y = {yi }N
i=1 and the label l
of x.

2637

Stage 1: Construction of sub dictionary for each class
i
into Ni subsets and calculate the clusPartition {xji , lij }nj =1
i
ter centers denoted as D i = [d1i , d2i , . . . , dN
i ] using k-means
methods.
Stage 2: Calculation of locally linear representation coefficients:
Reconstruct each x(j ) in T based on dictionary D using the
LAE method and calculate the corresponding coefficient vector
(j )
{ai }N
i=1 .
Stage 3: Softmax regression model determination:
Calculate reconstruction error vector εi of all training samples
for each class based on dictionary D = {D i }N
i=1 and coefficient
vectors {a(j ) }nj=1 .
Use the calculated reconstruction error norms to determine a
softmax regression model by minimizing (10).
Stage 4: Core of the proposed method:
Reconstruct the input sample x based on dictionary D using
the LAE method and calculate the coefficient vectors {ai }N
i=1 .
Calculate the reconstruction error vector εi of x for each class
according to (2).
Calculate the classification scores y = {yi }N
i=1 of x based on
(9).
Achieve the label l of x based on (1).
F. Feature Extraction
Before extracting image features to represent a sample, image
inhomogeneity correction and intensity normalization should
be performed because the image intensities in MRI images do
not have a fixed meaning and widely vary within or between
subjects. In this study, all MRI image modalities are processed
as follows. First, the N3 algorithm [50] is applied to remove
the bias field artifacts from the images. Second, intensity values
at the 1% and 99% quantiles are computed for the brain region
(including tumors, edema, and brain tissues), and then these two
values are used to linearly scale the voxel intensities to the range
[0,100].
In the present paper, a patch-based technique was used in
extracting the image feature. The intensity values in a patch
around a voxel v were obtained and rearranged as a feature
vector. Four modalities of MRI images were used, so the feature
of v that used a cubic patch with a size of w × w × w was
(w3 × 4)-dimensional.
G. Multiresolution Framework
To improve robustness and reduce computational cost, a multiresolution framework was adopted in the proposed method.
For a multiresolution framework with P levels, classification
originated from the coarsest Lp−1 to the finest level L0 (original
resolution) to classify the voxels into different classes. The classification scores for all voxels in coarser levels were up-sampled
to initialize the classification for a finer level using the trilinear
interpolation method. At each level, a confidence threshold α
was defined to determine the specific voxels that can be directly
labeled using the propagated initial classification scores and the
specific voxels that required further processing. In this study, the

2638

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

voxels with initial classification scores yi > 1 − α were labeled
as the ith class. The remaining voxels with yi ∈ [0, 1 − α] were
fed into the proposed classifier for an accurate classification.
This procedure was repeated until the resolution of the original level L0 was reached. At level L0 , we directly used (1) to
achieve the final label.
Three levels (P = 3) were used with corresponding voxel sizes of 1 mm × 1 mm × 1 mm, 2 mm × 2 mm ×
2 mm, and 4 mm × mm × 4 mm. The voxel number for each
class in each training image was set to 8,000 to construct the
training set at each level, and the threshold value (α) was empirically set to 0.2 at L1 and L2 . In the coarser levels, the voxels
in the tumor or edema regions were very few. To increase the
number of training samples, we randomly transformed the MRI
images by smoothing and adding noise to the images at each
level.
H. Postprocessing With Spatial Constraint
In this study, we used the assumption that each edema region
is located near tumor core regions to postprocess the classified
edema region as introduced in [11]. According to this assumption, each classified edema region must have a voxel near the
classified tumor regions within some small distance. Therefore,
the connected component algorithm and mathematical morphology can be used to refine the classified edema regions. First, a binary image representing the classified edema regions is formed.
Second, the binary image is used as an input for the connected
component algorithm, and then some individual edema regions
are generated. Third, each individual edema region is dilated
with a small structuring element and compared against the classified tumor regions. Finally, the dilated edema regions that
share at least a voxel with the classified tumor regions are considered valid. The edema regions from these valid regions are
retained as the final edema classification results, whereas the
other edema regions are discarded.
III. EXPERIMENTAL RESULTS
A. Experiments on Synthetic Data
A two-spiral structure was generated to evaluate the classification performance of the proposed LIPC method. A comparison was also made between the proposed method and two
other popular classification methods [SRC and support vector
machine (SVM) [51]] using the generated two-spiral structures.
As described in Section II-C, SRC uses the whole dictionary to
represent the testing sample and classifies x based on the performance of the coefficients associated with the training samples.
In the proposed LIPC method, each subdictionary was used to
represent x, and the closest subspace was determined to accomplish the classification task. Moreover, a softmax model was
learned in LIPC to capture the information of data distribution
of different classes. SVM attempts to minimize the empirical
classification error and maximize the geometric margin simultaneously in the training set, which leads to high generalization
ability in the new samples.

Fig. 4. Error rates of different methods on different data. LIPC and LIPC-learn
represent the proposed method without and with the learned softmax regression
model, respectively.

In this section, the parameters in the two-spiral structures were
designed as follows. The number of training samples on one spiral was fixed to 500. Then, the number of training samples on
other spiral was set to 50, 250, and 500. The number of testing
samples on each spiral was set to 1000. Noises with different
levels (0.5 to 3) were also added to the generated interleaving
spirals. The coordinates of each point in the spirals were the features to be fed to different classification methods. For LIPC and
SRC, the training samples were used as dictionary because the
sample number was small. To evaluate the LIPC effectiveness,
both SRC and LIPC used LAE as the coding method. To make a
fair comparison, we carefully designed and selected the parameters with optimal performance in each classification method.
The classification results of different methods on different data
are shown in Fig. 4. When the number of training samples on
the two spirals was the same (e.g., 500 and 500) and the noise
was small, the difference in the classification accuracy of the
different methods was nonsignificant. However, the classification accuracy of the different methods varied significantly when
the training samples on one spiral were much sparser than the
training samples on the other spiral (e.g., 50 and 500). A higher
noise level led to a large variety of classification accuracy under
the different methods. In general, the classification accuracy decreased when the data distribution was nonuniform and the noise
level was high. However, the proposed method with a learned
softmax regression model achieved the lowest error rate when a
serious nonuniformity of data distribution and high noise were
observed in the training samples. The higher classification accuracy of the proposed method contributes in considering the data
distribution of different classes. The classification performance
of the proposed method without the learned softmax regression
model was comparable to that of SVM. The proposed method
with or without the learned softmax regression model also outperformed the SRC method, which indicates that the proposed
LIPC was more effective than SRC in this classification task.
The decision curves of the proposed method are shown in Fig. 5.

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

2639

TABLE II
SUMMARY OF THE PARAMETER SETTINGS USED IN THE PROPOSED METHOD
Parameter

Description

k
N
w
P
α

Number of the nearest neighbors of LAE
Dictionary size
Patch size (w × w × w)
Levels of Multiresolution
Confidence threshold described in Section 2.5

Setting
10
40 000
5
3
0.2 at levels L 1 and L 2

where A and B are the voxel sets of the result and ground truth,
respectively. The false positive rate (FPR) and false negative
rate (FNR) were also adopted to quantify the over-segmentation
and under-segmentation. These two metrics are computed as
follows:
Fig. 5. (a) Number of training samples in each of the two classes was 500, and
the noise level was 2.9. The decision curves learned by LIPC (b) without and
(c) with the learned softmax regression model on the testing samples. (d) The
number of training samples in the two classes was 50 and 500, and the noise
level was 2.9. The decision curves learned by LIPC (e) without and (f) with the
learned softmax regression model on the testing samples.

As shown in Fig. 5(b) and (c), the decision curves of the proposed method with and without the learned softmax regression
model were similar when the nonuniformity of the data distribution was not obvious. However, the decision curve of the
proposed method without the learned softmax regression model
was worse than that of the proposed method with the learned
softmax regression model when the training samples on one
spiral were much sparser than those on the other spiral [see
Fig. 5(e) and (f)].
B. Experiments on Brain Tumor Image Data
A series of experiments were performed on the training and
testing brain tumor image data. For the training data, a complete tumor is subdivided into tumor core (necrosis, contrastenhancing tumor, and nonenhancing tumor in real patient data)
and edema parts. The proposed method was evaluated using
a five-fold cross-validation fashion. All experiments were repeated five times, and the final results were reported as the
mean and standard deviation of the results from the individual
runs. For each run, a total of 64 images were used in training and
16 images were used for testing. Meanwhile, no joint set existed
between the training and testing datasets in all experiments. To
evaluate the proposed method for the testing data, the training
data was used to train classifiers for different tissues. The segmentation results of the testing data were uploaded to the online
evaluation platform and were evaluated automatically using the
online evaluation tool (http://virtualskeleton.ch/).
1) Quantitative Evaluation Metrics: The dice similarity
(DS) and jaccard similarity (JS) were used in this study to quantitatively assess the performance of the proposed method. These
two similarities are defined as follows:
DS = 2 |A ∩ B|/(|A| + |B|), JS = |A ∩ B|/|A ∪ B| (11)





F P R = A ∩ B̄  |A ∪ B|, F N R = Ā ∩ B  |A ∪ B|. (12)
2) Parameter Optimization: The parameter settings of the
proposed method were carefully considered to obtain the optimum performance during our experiments. A summary of the
parameter settings used in the proposed method is listed in Table II.
In this study, we employed LAE method to achieve the linear representation coefficients to linearly reconstruct a testing
sample and ensure the minimum reconstruction error. To obtain
the solution of LAE, we first selected the k nearest neighbors of
the testing sample from the dictionary. In this experiment, we
varied k from 5 to 100 to evaluate the effects of the different
numbers of k nearest neighbors on the reconstruction error. We
first randomly selected 10 000 samples from the training data
for each class at each level. For each class, we then computed
the reconstruction error norms of the selected samples using
the corresponding dictionary. We fixed the dictionary and patch
sizes of each class at each level to be 40 000 and 5, respectively.
The effect of the varied numbers of k nearest neighbors on the
reconstruction error is shown in Fig. 6. We found that the lowest
value for the reconstruction error was always obtained when
k was equal to 10 for each class at each level. This result is
significant in rapidly obtaining the solution of LAE.
The dictionary size N was a crucial parameter in the proposed LIPC. Therefore, we performed the proposed method on
the brain tumor image data to compare the effect of different
dictionary sizes Ni on the classification performance. We set
Ni for different classes to be of the same values at different
levels, and the dictionary size was denoted as N. We varied N
from 5,000 to 40 000. k was also set to 10 for each class at
each level for different values of N. The patch size for each
class at each level was fixed to 5. As shown in Fig. 7, increasing
N improved the classification accuracy on the different image
data groups. Meanwhile, the standard deviation decreased with
increasing N. in general, large dictionaries contain numerous
discriminative data, which result in a more accurate classification compared with small dictionaries. However, to obtain the
tradeoff between memory and computational costs and accuracy, we selected N = 40000 for the following experiments.

2640

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

Fig. 8. Classification performance of the proposed method with different
values of w on different data groups.

Fig. 6.
error.

Effect of different numbers of k nearest neighbors on the reconstruction

Fig. 7. Classification performance of the proposed method with different
values of N on different data groups.

Fig. 9. Qualitative results of the proposed method for different data groups.
Each row shows a typical result for each data group. (a) Slice of T2 images.
(b) Contours of the edema classification results (yellow) and ground truth (red).
(c) Contours of the tumor core classification results (yellow) and ground truth
(red). (d) Ground truth of the tumor core (red voxels represent the necrosis,
green voxels represent the nonenhancing tumor, and blue voxels represent the
contrast-enhancing tumor). (e) Classification results of the tumor core.

The patch size w is also an important parameter that should be
carefully determined. We set the same w for the different classes
at different levels and fixed w to 3, 5, and 7. Parameters k and
N were set to 10 and 40 000, respectively, for each class at each
level. The classification accuracy was improved by increasing
w from 3 to 5 (except for the tumor classification accuracy on
real patient data with high-grade gliomas and synthetic data
with high-grade gliomas), in which the average DS increased
from 0.742 to 0.782 (see Fig. 8). The average DS of w = 5
(0.782) was also higher than that of w = 7 (0.756). Therefore,
we selected w = 5 for the following experiments.
3) Qualitative Results: The qualitative results of the proposed method with the learned softmax regression model on
different data groups are shown in Fig. 9. Parameters k, N, and
w were set to 10, 40 000, and 5, respectively, for each class
at each level. The tumor boundaries of the real patient data

were more blurry than those of the synthetic data [see Fig. 9(a)].
Therefore, the tumor classification performance was better in the
synthetic data than that in the real patient data [see Fig. 9(c)].
The edema boundaries of both real patient data and synthetic
data were quite blurry [see Fig. 9(a)], which led to more inaccurately classified voxels in the edema regions than those in the
tumor regions [see Fig. 9(b)]. Two other typical segmentation
results using the proposed method are shown in Fig. 10. Fig. 10
shows that the tumor boundary of low-grade real patient data
was more blurry than that of high-grade real patient data. The
edema region was also blurry in low-grade real patient data.
Thus, the classification accuracy of the tumor and edema in the
low-grade patient data was lower than those in the high-grade
patient data.
4) Effectiveness of LAE Method: Numerous coding methods
can be used to solve the coefficients of the dictionary. In this

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

Fig. 10. Typical results using the proposed method on high-grade and lowgrade real patient data. (a) Slice of T2 images. (b) Ground truth of the complete
tumor (yellow voxels represent the edema, red voxels represent the necrosis,
green voxels represent the nonenhancing tumor, and blue voxels represent the
contrast-enhancing tumor). (c) Classification results of the complete tumor.

Fig. 11.

Tumor segmentation results of LLC, SC, and LAE.

section, we show the comparative results of the tumor segmentation with several coding methods. The SC, LLC, and LAE
coding methods were used for comparison. SC focuses on the
sparse property, and LLC concentrates on the local property.
In this study, LAE was used as the coding method, which emphasizes the local and nonnegative properties. To obtain a fair
comparison, these three coding methods were performed in the
LIPC framework, and the classification scores were computed
using (8). Parameters N and w were set to 40 000, and 5, respectively, for each class at each level. For LAE, parameter k was
set to 10 for each class at each level. For LLC and SC, we first
conducted an experiment, which was similar to the experiment
described in Section III-B, to determine the number of nonzero
values. We found that when the numbers of nonzero values were
90 and 15 for LLC and SC, respectively, the corresponding reconstruction error of the randomly selected samples reached the
minimum value. The tumor segmentation results with different
coding methods are shown in Fig. 11. LAE exhibited better robustness compared with SC and LLC. The mean DS of LAE was
2.6% and 3.1% higher than that of LLC and SC, respectively.
This result indicates that LAE was more applicable in address-

Fig. 12.

2641

Comparison of LIPC and SRC.

ing difficulties in brain tumor segmentation compared with SC
and LLC.
5) Comparison Between LIPC and SRC: To evaluate the
effectiveness of LIPC, both SRC and LIPC used LAE as the
coding method. Moreover, the classification scores were computed using (8). For LIPC, parameters k, N, and w were set to
10, 40 000, and 5, respectively, for each class at each level. For
SRC, a dictionary containing samples from three classes was
constructed for each level. This dictionary consisted of three
subdictionaries and each subdictionary corresponded to a class.
For a fair comparison, the size of each subdictionary was set
to 40 000. Therefore, the size of the dictionary for SRC was
120 000 at each level. The number of nonzero values in LAE
for SRC was determined as follows. We first randomly selected
10 000 samples from the training data for each class at each level
and computed the reconstruction error norms of all the selected
samples using the dictionary. The number of nonzero values in
LAE was varied from 5 to 1,200. Finally, the minimum reconstruction error was found when the number of nonzero values
was set to 1000. After we investigated the results of different
data groups, the mean DS of LIPC was 5.3% higher than that of
SRC. The classification results with LIPC and SRC on different data groups are displayed in Fig. 12, which shows that the
proposed LIPC could be effectively used in tumor segmentation.
6) Effectiveness of Softmax Regression Model Learning: To
assess the effect of learning a softmax regression model using
the reconstruction error norm on the classification performance,
we compared the classification results obtained from (8) and (9).
Parameters k, N, and w were set to 10, 40 000, and 5, respectively,
for each class at each level. As listed in Table III, the accuracy of
the proposed method with the learned softmax regression model
was higher than that of the proposed method without using the
learned softmax regression model. The case without the learned
softmax regression model had a higher FPR than that with the
learned softmax regression model, which indicates the former
tended to retain extra background.
7) Comparison With Other Methods: The segmentation results of the testing data were evaluated by the online evaluation
tool in the evaluation platform. This evaluation platform contains an archive of all uploaded results, which enables segmentation methods to be objectively benchmarked and compared

2642

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

TABLE III
EFFECT OF LEARNING A SOFTMAX REGRESSION MODEL USING THE RECONSTRUCTION ERROR NORM

DS
(%)
JS
(%)
FPR
(%)
FNR
(%)

Without
With
Without
With
Without
With
Without
With

High-grade (real)
Edema

Tumor

Low-grade (real)
Edema

Tumor

High-grade (synth)
Edema

Tumor

Low-grade (synth)
Edema

Tumor

71.9 ± 11.7
76.6 ± 8.5
58.6 ± 16.6
62.8 ± 10.7
32.8 ± 26.2
2.8 ± 1.7
8.6 ± 5.2
34.4 ± 11.1

79.8 ± 17.0
86.9 ± 4.9
70.1 ± 18.5
77.2 ± 7.6
11.8 ± 10.1
0.90 ± 0.1
18.1 ± 7.9
21.9 ± 7.6

50.5 ± 8.5
63.9 ± 14.7
38.8 ± 11.2
48.5 ± 15.8
50.9 ± 26.9
2.9 ± 2.8
10.3 ± 7.6
48.6 ± 15.3

72.8 ± 22.4
83.7 ± 6.7
64.3 ± 23.3
72.5 ± 9.3
28.9 ± 24.8
0.90 ± 0.84
6.8 ± 4.3
26.6 ± 9.5

65.8 ± 17.0
66.5 ± 20.8
52.6 ± 21.1
53.0 ± 21.4
9.1 ± 1.1
3.9 ± 2.6
38.3 ± 27.4
43.1 ± 20.6

89.5 ± 10.2
90.1 ± 2.2
80.8 ± 14.6
82.1 ± 3.6
1.0 ± 0.94
1.07 ± 0.1
18.2 ± 14.6
16.8 ± 4.1

66.8 ± 13.2
67.4 ± 13.9
51.1 ± 16.5
52.2 ± 14.3
3.9 ± 0.9
1.9 ± 1.5
45.0 ± 18.9
45.9 ± 13.7

90.0 ± 1.2
90.4 ± 2.3
80.9 ± 2.1
82.5 ± 3.8
0.2 ± 0.03
0.6 ± 0.1
18.9 ± 3.6
16.9 ± 4.2

“With” and “Without” represent the proposed method with and without using the learned softmax regression model, respectively.

TABLE IV
DS OF DIFFERENT METHODS ON THE TESTING DATA
2012

2013

Real patient data

Synthetic data

Real patient data

Position

complete

core

enhancing

complete

core

complete

core

enhancing

1
2
3
4
5

0.75
0.82
0.82
0.78
0.75

0.70
0.63
0.66
0.58
0.54

0.43
0.44
0.36
0.40
0.37

0.92
0.91
0.87
0.88
0.83

0.88
0.86
0.81
0.76
0.62

0.88
0.87
0.86
0.85
0.85

0.83
0.78
0.74
0.81
0.73

0.72
0.74
0.73
0.70
0.73

The results of the proposed method are shown in bold form.

with each other. The tumor region in the real patient data was
subdivided into four classes (necrosis, contrast-enhancing tumor, nonenhancing tumor, and edema), and the evaluation was
performed for three different tumor subregions: complete tumor
(necrosis, contrast-enhancing tumor, nonenhancing tumor, and
edema), tumor core (necrosis, contrast-enhancing tumor, and
nonenhancing tumor), and contrast-enhancing tumor. The tumor region in the synthetic data was subdivided into two classes
(tumor core and edema). For the real patient data, five dictionaries and five classifiers were learned from the real patient training
data. For the synthetic data, three dictionaries and three classifiers were learned from the synthetic training data. For all testing
data, parameters N, w, and k were set to 40 000, 5, and 10 for
each class at each level. Table IV shows the tumor segmentation
results with different methods evaluated by the online evaluation
tool. The top five results are listed. At the time of writing of this
study, the proposed method was in the second and third places
in the 2012 and 2013 real patient data, respectively. Moreover,
the proposed method is in the first place in 2012 synthetic data.
These results indicate that the proposed method is comparable
to other state-of-the-art methods.
8) Computation Time: In this study, all experiments were
implemented on a standard PC using a single thread on an Intel
Core i5–2400 processor at 3.10 GHz. In the testing step, the
processing time for tumor segmentation of a subject with N =
40000 was approximately 26 min, at which 2 min was used for
intensity normalization and 24 min was used for segmentation.
In the training step, the construction of a dictionary with N =
40000 from a training dataset with 64 images took 5 h.

IV. DISCUSSION AND CONCLUSION
An automatic method is proposed for brain tumor segmentation in MRI images. An LIPC-based method was introduced
to solve the tumor segmentation problem. The proposed LIPC
used local independent projection into the classical classification model, and a novel classification framework was derived.
Compared with other coding approaches, the LAE method was
more suitable in solving the linear reconstruction weights under
the locality constraint. The data distribution in each submanifold was important for the classification, and we used a softmax
model to learn the relationship between the data distribution and
reconstruction error norm. We evaluated the proposed method
using both synthetic data and public available brain tumor image data. In both problems, our method outperformed competing
methods.
A. Comparison With SRC Method
The proposed LIPC method applied each subdictionary to represent the testing sample instead of using the whole dictionary
to represent the testing sample in the SRC method. The classification strategy used in LIPC was more applicable than that used
in SRC, especially when the data distribution is nonuniform
and the noise level is high (see Fig. 4). When the data distribution of one class is sparser than that of other classes, SRC
may select few nearest neighbors in the sparse class. This selection strategy may result in the wrong classification if the testing
sample belongs to the sparse class. To prevent this problem,
SRC tended to select more nearest neighbors than LIPC (1000

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

2643

neighbors in SRC versus ten neighbors in LIPC as mentioned in
Section III-B-5), which leads to increased computation costs.

B. Comparison With the State-of-the-Art
in Brain Tumor Segmentation
The brain tumor segmentation problem has been widely investigated; putting our results in context helps to reveal their
significance. The proposed method was compared with other
methods by uploading the segmentation results to the online evaluation tool. In the proceedings of the BRATS2012,
Zikic et al. [21] and Bauer et al. [23] proposed an automatic classification method, respectively, and their methods
performed best on the image data. Zikic et al. [21] achieved
average DS values of 0.75, 0.47, and 0.41 for complete tumor, tumor core, and contrast-enhancing tumor for real patient
testing data, respectively, and 0.91 and 0.86 for complete tumor
and tumor core for synthetic testing data, respectively. Bauer
et al. [23] achieved average DS values of 0.75, 0.54, and 0.37 for
complete tumor, tumor core, and contrast-enhancing tumor for
real patient testing data, respectively, and 0.87 and 0.81 for complete tumor and tumor core for synthetic testing data, respectively. Our results shown average DS values of 0.82, 0.63, and
0.44 for complete tumor, tumor core, and contrast-enhancing
tumor for real patient testing data, respectively, and 0.92 and
0.88 for complete tumor and tumor core for synthetic testing
data, respectively, the average of which was 6% higher than
those obtained by Zikic et al. [21] and Bauer et al. [23]. In the
proceedings of BRATS2013, Cordier et al. [52] proposed a tumor segmentation method by choosing similar patches from the
training data and combining labels of chosen patches to be the
labels of testing data. The patches selection step in [52] is similar
to the locally linear representation step in our proposed method.
However, our proposed method focuses not only on similarity
and sparsity, but also on locality. The average DS values of
[52] reported in [45] were 0.84, 0.68, and 0.65 for complete
tumor, tumor core, and contrast-enhancing tumor, respectively.
Our results shown the average DS values of 0.86, 0.74, and 0.73
for complete tumor, tumor core, and contrast-enhancing tumor,
respectively, the average of which was 5.4% higher than those
obtained by Cordier et al. [52].

C. Computational Complexity
Compared with the classical classification model, our proposed method requires an additional step of constructing a dictionary for each class at each level in the training stage. This
dictionary construction step is an offline procedure; thus, this
step is performed only once and is used for all testing images.
In the testing stage, the k nearest neighbor searching and coding
algorithms, which were the most time-consuming steps, were
run in a single thread. However, the voxel-based implementation in this study can be parallelized and implemented to exploit
the multicore CPUs, thereby significantly decreasing processing
time.

Fig. 13. Intensity distribution of different classes in different data groups
using T2 MRI images. The voxel number of the intensity value of each class
was normalized and summed to one. “Background” represents the brain tissue
region without tumor and edema.

D. Estimation of the Proposed Method
In this study, we learned a softmax regression model using the reconstruction error norm to achieve the classification
scores. The experimental results displayed an improvement on
the classification performance using the learned softmax regression model (see Figs. 4, 5, and Table III). As shown in Table III,
the classification accuracy of the proposed method with the
learned softmax regression model was much higher than that
of the proposed method without the learned softmax regression model for the real data groups (p < 0.002). However, the
classification accuracy of the proposed method with the learned
softmax regression model was similar to that of the proposed
method without the learned softmax regression model for the
synthetic data groups (p > 0.6). This finding may be attributed
to the more complex intensity distribution in the real data groups
than that in the synthetic data groups (see Fig. 13). The intensities of the three classes could be easily separated in synthetic
data with high-grade gliomas, whereas the intensity distribution
of the three classes largely overlapped with one another in the
real data groups (see Fig. 13). We used a patch-based feature,
so the image intensity distribution had a strong correlation with
the sample distribution. The distribution of the training data in
each submanifold is an important clue for the classification task
and can bring discriminative information when classifying a
testing sample. Thus, the proposed method with a learned softmax regression model is more applicable for data with complex
distribution than data without the learned softmax regression
model.
The proposed classification method requires no explicit regularization because the patch feature contains the contextual information of a voxel in the image. The proposed method leads a
natural smoothness to the segmentation results without explicit
regularization by using this contextual information. However,
the patch feature may be insufficient to discriminate the brain
tumor segmentation task because of the complex characteristics

2644

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 61, NO. 10, OCTOBER 2014

of brain MRI images. Other contextual features may be added
in future studies to further improve the classification accuracy
obtained in this study.

REFERENCES
[1] D. A. Gutman, L. A. Cooper, S. N. Hwang, C. A. Holder, J. Gao, T. D.
Aurora, W. D. Dunn, Jr., L. Scarpace, T. Mikkelsen, R. Jain, M. Wintermark, M. Jilwan, P. Raghavan, E. Huang, R. J. Clifford, P. Mongkolwat,
V. Kleper, J. Freymann, J. Kirby, P. O. Zinn, C. S. Moreno, C. Jaffe,
R. Colen, D. L. Rubin, J. Saltz, A. Flanders, and D. J. Brat, “MR imaging
predictors of molecular profile and survival: multi-institutional study of
the TCGA glioblastoma data set,” Radiology, vol. 267, no. 2, pp. 560–569,
May 2013.
[2] D. Bhattacharyya and T. H. Kim, “Brain tumor detection using MRI image
analysis,” Commun. Comput. Inform. Sci., vol. 151, pp. 307–314, 2011.
[3] C. L. Biji, D. Selvathi, and A. Panicker, “Tumor detection in brain magnetic resonance images using modified thresholding techniques,” Commun. Comput. Inform. Sic., vol. 4, pp. 300–308, 2011.
[4] P. Gibbs, D. L. Buckley, S. J. Blackband, and A. Horsman, “Tumour
volume determination from MR images by morphological segmentation,”
Phys. Med. Biol., vol. 41, no. 11, pp. 2437–2446, Nov. 1996.
[5] R. Ratan, S. Sharma, and S. K. Sharma, “Brain tumor detection based
on multi-parameter MRI image analysis,” Int. J. Graphics, Vision Image
Process., vol. 9, no. 3, pp. 9–11, 2009.
[6] T. M. Hsieh, Y. M. Liu, C. C. Liao, F. Xiao, I. J. Chiang, and J. M. Wong,
“Automatic segmentation of meningioma from non-contrasted brain MRI
integrating fuzzy clustering and region growing,” BMC Med. Informat.
Decision Making, vol. 11, p. 54, 2011.
[7] B. N. Saha, N. Ray, R. Greiner, A. Murtha, and H. Zhang, “Quick detection
of brain tumors and edemas: A bounding box method using symmetry,”
Comput. Med. Imag. Graphics, vol. 36, no. 2, pp. 95–107, 2012.
[8] Z. Iscan, Z. Dokur, and T. Olmez, “Tumor detection by using Zernike
moments on segmented magnetic resonance brain images,” Expert Syst.
Appl., vol. 37, no. 3, pp. 2540–2549, 2010.
[9] H. Khotanlou, O. Colliot, J. Atif, and I. Bloch, “3D brain tumor segmentation in MRI using fuzzy classification, symmetry analysis and spatially
constrained deformable models,” Fuzzy Sets Syst., vol. 160, no. 10, pp.
1457–1473, 2009.
[10] Z. J. Wang, Q. M. Hu, K. F. Loe, A. Aziz, and W. L. Nowinski, “Rapid
and automatic detection of brain tumors in MR images,” in Proc. SPIE,
2004.
[11] M. Prastawa, E. Bullitt, S. Ho, and G. Gerig, “A brain tumor segmentation
framework based on outlier detection,” Med. Image Anal., vol. 8, no. 3,
pp. 275–283, Sep. 2004.
[12] M. B. Cuadra, C. Pollo, A. Bardera, O. Cuisenaire, J. G. Villemure, and
J. P. Thiran, “Atlas-based segmentation of pathological MR brain images
using a model of lesion growth,” IEEE Trans. Med. Imag., vol. 23, no. 10,
pp. 1301–1314, Oct. 2004.
[13] N. Moon, E. Bullitt, K. V. Leemput, and G. Gerig, “Model-based brain
and tumor segmentation,” in Proc. 16th Int. Conf. Pattern Recog., 2002,
pp. 528–531.
[14] M. R. Kaus, S. K. Warfield, A. Nabavi, P. M. Black, F. A. Jolesz, and
R. Kikinis, “Automated segmentation of MR images of brain tumors,”
Radiology, vol. 218, no. 2, pp. 586–591, Feb. 2001.
[15] J. Sachdeva, V. Kumar, I. Gupta, N. Khandelwal, and C. K. Ahuja, “A
novel content-based active contour model for brain tumor segmentation,”
Magn. Resonance Imag., vol. 30, no. 5, pp. 694–715, Jun. 2012.
[16] S. Taheri, S. H. Ong, and V. F. H. Chong, “Level-set segmentation of brain
tumors using a threshold-based speed function,” Image Vision Comput.,
vol. 28, no. 1, pp. 26–37, 2010.
[17] T. Wang, I. Cheng, and A. Basu, “Fluid vector flow and applications in
brain tumor segmentation,” IEEE Trans. Biomed. Eng., vol. 56, no. 3, pp.
781–789, Mar. 2009.
[18] A. Hamamci, N. Kucuk, K. Karaman, K. Engin, and G. Unal, “TumorCut: segmentation of brain tumors on contrast enhanced MR images for
radiosurgery applications,” IEEE Trans. Med. Imag., vol. 31, no. 3, pp.
790–804, Mar. 2012.
[19] J. Jiang, Y. Wu, M. Huang, W. Yang, W. F. Chen, and Q. J. Feng, “3D
brain tumor segmentation in multimodal MR images based on learning
population- and patient-specific feature sets,” Comput. Med. Imag. Graphics, vol. 37, no. 7–8, pp. 512–521, Jun. 28, 2013.

[20] Y. Wu, W. Yang, J. Jiang, S. Q. Li, Q. J. Feng, and W. F. Chen,
“Semi-automatic segmentation of brain tumors using population and
individual information,” J. Digital Imag., vol. 26, no. 4, pp. 786–796,
2013.
[21] D. Zikic, B. Glocker, E. Konukoglu, J. Shotton, A. Criminisi, D. H. Ye,
C. Demiralp, O. M. Thomas, T. Das, R. Jena, and S. J. Price, “Contextsensitive classification forests for segmentation of brain tumor tissues,”
presented at the Med. Image Comput. Comput.-Assisted Intervention
Conf.–Brain Tumor Segmentation Challenge, Nice, France, 2012.
[22] D. Zikic, B. Glocker, E. Konukoglu, A. Criminisi, C. Demiralp, J. Shotton,
O. M. Thomas, T. Das, R. Jena, and S. J. Price, “Decision forests for tissuespecific segmentation of high-grade gliomas in multi-channel MR,” in
Proc. Med. Image Comput. Comput. Assist. Interv. Conf., 2012, pp. 369–
376.
[23] S. Bauer, T. Fejes, J. Slotboom, R. Wiest, L.-P. Nolte, and M. Reyes,
“Segmentation of brain tumor images based on integrated hierarchical
classification and regularization,” presented at the Med. Image Comput.
Comput.-Assisted Intervention Conf.–Brain Tumor Segmentation Challenge, Nice, France, 2012.
[24] S. Bauer, L. P. Nolte, and M. Reyes, “Fully automatic segmentation of
brain tumor images using support vector machine classification in combination with hierarchical conditional random field regularization,” in Proc.
Med. Image Comput. Comput. Assist. Interv., 2011, pp. 354–361.
[25] M. Wels, G. Carneiro, A. Aplas, M. Huber, J. Hornegger, and D.
Comaniciu, “A discriminative model-constrained graph cuts approach
to fully automated pediatric brain tumor segmentation in 3-D MRI,”
in Proc. Med. Image Comput. Comput. Assist. Interv., 2008, pp. 66–
75.
[26] C. H. Lee, S. Wang, A. Murtha, M. R. Brown, and R. Greiner, “Segmenting brain tumors using pseudo-conditional random fields,” in Proc. Med.
Image Comput. Comput. Assist. Interv., 2008, pp. 359–366.
[27] J. J. Corso, E. Sharon, S. Dube, S. El-Saden, U. Sinha, and A. Yuille,
“Efficient multilevel brain tumor segmentation with integrated bayesian
model classification,” IEEE Trans. Med. Imag., vol. 27, no. 5, pp. 629–640,
May 2008.
[28] G. P. Beyer, R. P. Velthuizen, F. R. Murtagh, and J. L. Pearlman, “Technical
aspects and evaluation methodology for the application of two automated
brain MRI tumor segmentation methods in radiation therapy planning,”
Magn. Resonance Imag., vol. 24, no. 9, pp. 1167–1178, Nov. 2006.
[29] C. H. Lee, M. Schmidt, A. Murtha, A. Bistritz, J. Sander, and R. Greiner,
“Segmenting brain tumors with conditional random fields and support
vector machines,” in Proc. First Int. Conf. Comput. Vision Biomed. Image
Appl., 2005, pp. 469–478.
[30] V. Harati, R. Khayati, and A. Farzan, “Fully automated tumor segmentation based on improved fuzzy connectedness algorithm in brain MR
images,” Comput. Biol. Med., vol. 41, no. 7, pp. 483–492, Jul. 2011.
[31] T. Logeswari and M. Karnan, “An enhanced implementation of brain
tumor detection using segmentation based on soft computing,” presented
at the Int. Conf. Signal Acquisition Process., Bangalore, India, 2010.
[32] M. M. Ahmed and D. B. Mohammad, “Segmentation of brain MR images
for tumor extraction by combining kmeans clustering and Perona–Malik
anisotropic diffusion model,” Int. J. Image Process., vol. 2, no. 1, pp.
27–34, 2008.
[33] W. B. Dou, S. Ruan, Y. P. Chen, D. Bloyet, and J. M. Constans, “A
framework of fuzzy information fusion for the segmentation of brain
tumor tissues on MR images,” Image Vision Comput., vol. 25, no. 2, pp.
164–171, 2007.
[34] J. Liu, J. K. Udupa, D. Odhner, D. Hackney, and G. Moonis, “A system for
brain tumor volume estimation via MR imaging and fuzzy connectedness,”
Comput. Med. Imag. Graphics, vol. 29, no. 1, pp. 21–34, Jan. 2005.
[35] L. M. Fletcher-Heath, L. O. Hall, D. B. Goldgof, and F. R. Murtagh, “Automatic segmentation of non-enhancing brain tumors in magnetic resonance
images,” Artif. Intell. Med., vol. 21, no. 1–3, pp. 43–63, Jan.–Mar. 2001.
[36] J. E. Iglesias, M. R. Sabuncu, and K. Van Leemput, “A unified framework
for cross-modality multi-atlas segmentation of brain MRI,” Med. Image
Anal., vol. 17, no. 8, pp. 1181–1191, Dec. 2013.
[37] H. Wang, J. W. Suh, S. R. Das, J. Pluta, C. Craige, and P. A. Yushkevich,
“Multi-atlas segmentation with joint label fusion,” IEEE Trans. Pattern
Anal. Mach. Intell., vol. 35, no. 3, pp. 611–623, Jun. 22, 2013.
[38] A. Gooya, G. Biros, and C. Davatzikos, “Deformable registration of glioma
images using em algorithm and diffusion reaction modeling,” IEEE Trans.
Med. Imag., vol. 30, no. 2, pp. 375–390, Feb. 2011.
[39] S. Zhang, Y. Zhan, and D. N. Metaxas, “Deformable segmentation via
sparse representation and dictionary learning,” Med. Image Anal., vol. 16,
no. 7, pp. 1385–1396, Oct. 2012.

HUANG et al.: BRAIN TUMOR SEGMENTATION BASED ON LOCAL INDEPENDENT PROJECTION-BASED CLASSIFICATION

[40] H. Lee, D. S. Lee, H. Kang, B. N. Kim, and M. K. Chung, “Sparse brain
network recovery under compressed sensing,” IEEE Trans. Med. Imag.,
vol. 30, no. 5, pp. 1154–1165, May 2011.
[41] J. Wright, A. Y. Yang, A. Ganesh, S. S. Sastry, and Y. Ma, “Robust face
recognition via sparse representation,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 31, no. 2, pp. 210–227, Feb. 2009.
[42] Y. Gao, S. Liao, and D. Shen, “Prostate segmentation by sparse representation based classification,” Med. Phys., vol. 39, no. 10, pp. 6372–6387,
Oct. 2012.
[43] W. Liu, J. F. He, and S.-F. Chang, “Large graph construction for scalable semi-supervised learning,” in Proc. Int. Conf. Mach. Learning, 2010,
pp. 679–686.
[44] Y. Wu, G. Liu, M. Huang, J. Jiang, W. Yang, W. Chen, and Q. Feng,
“Prostate segmentation based on variant scale patch and local independent
projection,” IEEE Trans. Med. Imag.,, vol. 33, no. 6, pp. 1290–1303, Jun.
2014.
[45] B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J.
Kirby, Y. Burren, N. Porz, J. Slotboom, R. Wiest, L. Lanczi, E. Gerstner,
M. A. Weber, T. Arbel, B. Avants, N. Ayache, P. Buendia, L. Collins,
N. Cordier, J. J. Corso, A. Criminisi, T. Das, H. Delingette, C. Demiralp,
C. Durst, M. Dojat, S. Doyle, J. Festa, F. Forbes, E. Geremia, B. Glocker,
P. Golland, X. Guo, A. Hamamci, K. Iftekharuddin, R. Jena, N. John,
E. Konukoglu, D. Lashkari, J. A. Mariz, R. Meier, S. Pereira, D. Precup, S. J. Price, T. Riklin-Raviv, S. Reza, M. Ryan, L. Schwartz, H. C.
Shin, J. Shotton, C. Silva, N. Sousa, N. Subbanna, G. Szekely, T. Taylor,
O. M. Thomas, N. Tustison, G. Unal, F. Vasseur, M. Wintermark, D. H. Ye,
L. Zhao, B. Zhao, D. Zikic, M. Prastawa, M. Reyes, and K. V. Leemput,
“The multimodal brain tumor image segmentation benchmark (BRATS),”
IEEE Trans. Med. Imag., 2014, to be published.

2645

[46] J. Macqueen, “Some methods for classification and analysis of multivariate
observations,” in Proc. 5th Berkeley Symp. Math. Statist. Prob., 1967, pp.
281–297.
[47] J. J. Wang, J. C. Yang, K. Yu, F. J. Lv, T. Huang, and Y. H. Gong,
“Locality-constrained linear coding for image classification,” presented at
the Comput. Vision Pattern Recog., San Francisco, CA, USA, 2010.
[48] K. Yu, T. Zhang, and Y. H. Gong, “Nonlinear learning using local coordinate coding,” presented at the Neural Inf. Process. Syst., Vancouver, BC,
Canada, 2009.
[49] Y. Nesterov, Introductory Lectures on Convex Optimization: A Basic
Course, Norwell, MA: Kluwer Academic Publishers, 2003.
[50] J. G. Sled, A. P. Zijdenbos, and A. C. Evans, “A nonparametric method
for automatic correction of intensity nonuniformity in MRI data,” IEEE
Trans. Med. Imag., vol. 17, no. 1, pp. 87–97, Feb. 1998.
[51] C. Cortes and V. Vapnik, “Support-Vector Networks,” Mach. Learning,
vol. 20, no. 3, pp. 273–297, 1995.
[52] N. Cordier, B. Menze, H. Delingette, and N. Ayache, “Patch-based Segmentation of Brain Tissues,” presented at the Med. Image Comput.
Comput.-Assisted Intervention Conf.–Brain Tumor Segmentation Challenge, 2013, Nagoya, Japan.

Authors’ biographies and photographs not available at the time of publication.

