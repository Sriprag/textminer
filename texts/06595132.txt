1390

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

Local Object Patterns for the Representation and
Classification of Colon Tissue Images
Gulden Olgun, Cenk Sokmensuer, and Cigdem Gunduz-Demir, Member, IEEE

Abstract—This paper presents a new approach for the effective representation and classification of images of histopathological
colon tissues stained with hematoxylin and eosin. In this approach,
we propose to decompose a tissue image into its histological components and introduce a set of new texture descriptors, which we
call local object patterns, on these components to model their composition within a tissue. We define these descriptors using the idea
of local binary patterns, which quantify a pixel by constructing a
binary string based on relative intensities of its neighbors. However, as opposed to pixel-level local binary patterns, we define our
local object pattern descriptors at the component level to quantify
a component. To this end, we specify neighborhoods with different
locality ranges and encode spatial arrangements of the components
within the specified local neighborhoods by generating strings. We
then extract our texture descriptors from these strings to characterize histological components and construct the bag-of-words
representation of an image from the characterized components.
Working on microscopic images of colon tissues, our experiments
reveal that the use of these component-level texture descriptors results in higher classification accuracies than the previous textural
approaches.
Index Terms—Classification, colon cancer, digital pathology,
local patterns, texture, tissue image representation.

I. INTRODUCTION
ISTOPATHOLOGICAL examination of a tissue is the
routine practice to identify numerous neoplastic diseases
including cancer. In this practice, pathologists examine the tissue under a microscope to find histological manifestations of a
disease and then provide diagnostic information based on the
findings and their interpretations. However, this practice is subject to substantial amount of subjectivity because locating and
correctly interpreting the findings highly depend on expertise
and experience of the pathologists.
Digital pathology emerges as a need to help the pathologists lessen the subjectivity level of their decisions. To this end,

H

Manuscript received February 5, 2013; revised June 17, 2013 and August 19,
2013; accepted September 5, 2013. Date of publication September 10, 2013;
date of current version June 30, 2014. This work was supported by the Scientific and Technological Research Council of Turkey under the Project TÜBİTAK
110E232.
G. Olgun and C. Gunduz-Demir are with the Department of Computer Engineering, Bilkent University, Ankara TR-06800, Turkey (e-mail: gulden@cs.
bilkent.edu.tr; gunduz@cs.bilkent.edu.tr).
C. Sokmensuer is with the Department of Pathology, Hacettepe University Medical School, Ankara TR-06100, Turkey (e-mail: csokmens@hacettepe.
edu.tr).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/JBHI.2013.2281335

many studies have been working on developing automated diagnostic systems. These systems rely on representing a tissue
image with quantitative features and using this representation
for classifying the tissue. In the literature, there exist several
studies that use texture descriptors to define these features. The
most commonly used descriptors are those that are defined on
intensity/color histograms, which quantify the first-order statistics of image pixels [1]–[3], and cooccurrence matrices, which
quantify the second-order statistics among pixels [4], [5]. In
addition to these, many studies make use of wavelets to define
their features. Examples include the descriptors defined on multiwavelet coefficients [6] and Gabor filter responses [7]. Fractal
analysis is another method used for defining texture descriptors.
In this analysis, fractal dimensions are frequently used as features [8], [9]. More recent studies use local binary patterns to
define additional texture descriptors [10]–[12]. They are used to
quantify a pixel according to spatial arrangement of its neighbors’ intensities with respect to its intensity. All these texture
descriptors yield promising results. However, they are defined
on pixels, directly using pixels’ intensity/color values. Thus,
they are susceptible to pixel-level noise and variations that are
typically observed in histopathological images.
In this study, we propose a new algorithm for the effective
representation and classification of images of histopathological
colon tissues stained with hematoxylin and eosin. In the proposed algorithm, our main contributions are the introduction
of a set of new texture descriptors, which we call local object
patterns, to model composition of histological components in
a tissue image and the use of this descriptor set to define the
visual words of the bag-of-words representation of the image. In
this algorithm, we decompose the image into component objects
of multiple types and define texture of these objects using the
idea of local binary patterns [13]. However, as opposed to local
binary patterns defined at the pixel level, we define local object
patterns on the objects at the component level. Particularly, local
binary patterns are defined to quantify a pixel by constructing
a binary string from the spatial arrangement of its neighbors’
relative intensities. On the other hand, we define our local object
patterns to quantify an object by specifying a set of neighborhoods with different locality ranges and constructing a string
based on how the object’s neighbors arrange in an order in each
of these local neighborhoods. The motivation behind defining
the local object patterns is the following: A normal tissue has the
characteristic composition of histological components and cancer causes deviations from it. Therefore, components (objects)
belonging to similar regions in normal and cancerous tissues are
expected to have neighbors of different types in specified neighborhoods as a part of their composition. Thus, the difference

2168-2194 © 2013 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

OLGUN et al.: LOCAL OBJECT PATTERNS FOR THE REPRESENTATION AND CLASSIFICATION OF COLON TISSUE IMAGES

in neighbor distributions can be used to differentiate such components, the distribution of which can be used to differentiate
normal and cancerous tissues. Our experiments on 3236 microscopic images of colon tissues demonstrate that our proposed
texture descriptors are effective to obtain better classification
accuracies compared to previous texture definitions.
The proposed algorithm mainly differs from the previous
texture-based tissue classification studies in the following two
aspects: First, it defines its texture descriptors on higher level
component objects instead of defining them at the pixel level.
Second, our algorithm uses the component-level texture descriptors to quantize the objects and constructs the bag-of-words
representation of an image from its quantized objects, instead
of directly using texture descriptors for representing the image.
Our experiments show that these differences make our algorithm
less susceptible to pixel-level noise and variations observed in
the overall image.
In the literature, there exist structural approaches that also use
histological components to represent a tissue image. These approaches commonly construct a graph on these components and
use graph descriptors for image classification. Earlier studies
construct their graphs on only nucleus tissue components using
different techniques such as Delaunay triangulations [7], [14],
minimum spanning trees [15], and probabilistic graph generations [16]. In our more recent study [17], we construct a graph
on tissue components of different types and color graph edges
based on the types of their end nodes. Different than our proposed texture descriptors defined within objects’ local neighborhoods, these previous structural approaches usually use a global
graph representation for the entire image and extract global
graph descriptors for its quantification.
In our previous studies [18]–[20], we defined other objectbased descriptors. However, the definitions of these descriptors
are completely different. The currently proposed local object
patterns encode the object’s local neighborhood information
by constructing a set of strings and they are used for tissue
image classification. Different than this current work, we defined object-based cooccurrence and run-length matrices in [18]
and [19], respectively, and introduced a uniformity metric on
objects in [20]. Moreover, our previous studies did not make
any classification but focused on unsupervised tissue image
segmentation.

II. METHODOLOGY
Our algorithm decomposes a tissue image into its histological components, characterizes them with the newly introduced
local object pattern descriptors, and uses this characterization
for classification of tissue images. The details of these steps are
explained in the following sections.

A. Tissue Image Decomposition
We model a tissue image I by approximately representing
its histological components with a set of circular objects O

1391

Fig. 1. Examples of (a) normal and (b) cancerous tissue images. Objects
located on the (c) normal and (d) cancerous tissue images. Here, purple, pink,
and white objects are shown as purple, pink, and cyan circles, respectively.

(I) = {oi }. We represent each object oi by its coordinates
(xi , yi ) and its type ti ∈ {purple, pink, white}1 .
To define this object set, we first separate hematoxylin and
eosin channels of the image I by applying color deconvolution
[21]. We then use these channels to quantize pixels into three
groups (purple, pink, and white). Let hp and ep be the values
of pixel p in the hematoxylin and eosin channels, respectively,
and havg and eavg be the average pixel values in these channels.
We label pixel p as purple if hp ≤ havg , pink if hp > havg and
ep ≤ eavg , and white if hp > havg and ep > eavg . We finally
apply the circle-fit algorithm [20] on the pixels of each group
separately to locate a set of circular objects. This algorithm
iteratively locates objects starting from the largest one as long
as the radii of located circles are greater than threshold rm in .
The centroid of an object determines its coordinates (xi , yi ) and
the pixel group on which it is located determines its type ti .
Fig. 1 shows example tissue images and their located objects.
In our model, we use an approximate representation instead
of finding exact locations of histological components because
their exact localization gives rise to a quite difficult segmentation
problem. Thus, there may be one-to-one or many-to-one relation
between objects and components. For example, a purple object
usually corresponds to a single nucleus, whereas a group of
white objects that form a clique corresponds to a lumen region.
The proposed local binary patterns are also effective to model
such many-to-one relations.

1 These types correspond to the three main colors in a hematoxylin-and-eosin
stained tissue. Particularly, cell nuclei correspond to purple; stroma, stromal
cells’ cytoplasms, and mucin-poor epithelial cells’ cytoplasms correspond to
pink; and lumina and mucin-rich epithelial cells’ cytoplasms correspond to
white. Since there are multiple components corresponding to the same type,
we hereinafter refer to them as purple, pink, and white, to keep the manuscript
simpler and easier to read.

1392

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

Fig. 2. Extracting local object patterns for the objects with dashed borders. Here m is selected as 4, and thus S =
selected objects are indicated on the examples with their orders.

B. Local Object Patterns
For object oi , we define the nth local object pattern n-LOP(oi )
as follows: We select n-nearest neighbors of oi and order
them according to distances from their coordinates to (xi , yi ).
Let N (oi ) =< oin , . . . , oij , . . . , oi1 > be the ordered neighbor set of oi , where oin and oi1 are its farthest and closest
neighbors, respectively. We form a binary string B(oi ) =<
bin , . . . , bij , . . . , bi1 > considering types tij of the selected
neighbors. In this string

1 if tij ∈ {purple}
bij =
0 if tij ∈ {pink, white}.
Then, we define n-LOP(oi ) as the decimal equivalent of binary
string B(oi ). Note that this descriptor provides rotation invariance since objects are ordered based on their distances to object
oi and its value does not change with arbitrary rotations of the
image.
We define local object patterns for an object to quantify the
spatial arrangement of its neighbors’ types found in a local
neighborhood. In our model, we extract a set of m + 1 patterns
usingdifferent neighborhoods. Particularly, this set includes
j
S= m
j =0 2 -LOP. We will use this pattern set to label each
object with a new type and use the new types’ frequency in an
image for its classification. We will explain these steps in the
next section.

j=0

2 j -LOP. 16-nearest neighbors of the

images as shown in Fig. 1. Fig. 2 shows that although lower
order patterns are the same for the two selected objects, their
higher order patterns show differences, which can be used to
differentiate these objects.
We define the new object types as follows: For each original
type ti ∈ {purple, pink, white}, we separately cluster objects of
the corresponding type into k groups running the k-means algorithm on local object patterns of these objects. Thus, we learn k
clustering vectors Vpurple = {v1 , . . . , vk } for the purple type, k
clustering vectors Upink = {u1 , . . . , uk } for the pink type, and
k clustering vectors Wwhite = {w1 , . . . , wk } for the white type.
Then, for a given image, we relabel each object oi with a new
type ti based on its original type ti and the corresponding set
of the clustering vectors Vpurple , Upink , or Wwhite ; that is, we
take the clustering vector set Vpurple , Upink , or Wwhite according to the original type ti ∈ {purple, pink, white} and assign
the object oi to the closest cluster in this set. Since components
(objects) of normal and cancerous tissue images show different
neighbor distributions, they are expected to be relabeled with
different types ti . Thus, we use the distribution of these new
types to represent an image. To this end, we extract the bagof-words representation on the frequency of objects’ new types
and classify the image using a linear kernel support vector machine (SVM) classifier. Note that, in this study, we use the SVM
implementation provided by [22]; this implementation uses the
one-against-one strategy for multiclass classifications.

C. Bag-of-Words Representation and Classification
In this study, we use the observation that components belonging to similar regions in normal and cancerous tissue images
show different characteristics in their neighbor distributions. To
differentiate these components (objects), we define new object
types based on local object patterns, which quantify the neighbor distributions. For example, Fig. 2 illustrates the extraction
of local object patterns for the objects with dashed borders. We
select these objects such that they both belong to luminal regions; we crop these regions from normal and cancerous tissue

4

III. EXPERIMENTS
A. Dataset
We conduct our experiments on 3236 microscopic images
of hematoxylin and eosin-stained colon tissues of 258 patients.
Images are taken using a Nikon Coolscope Digital Microscope
with a 20× objective lens and at 640 × 480 pixel resolution. Images are divided into training and test sets such that they contain
images of different patients. Three different classes are used
to label each of these images: normal, low-grade cancerous,

OLGUN et al.: LOCAL OBJECT PATTERNS FOR THE REPRESENTATION AND CLASSIFICATION OF COLON TISSUE IMAGES

and high-grade cancerous.2 The training set contains 510 normal, 859 low-grade cancerous, and 275 high-grade cancerous
images of 129 patients. The test set contains 491 normal, 844
low-grade cancerous, and 257 high-grade cancerous images of
the remaining 129 patients.
B. Comparisons
We compare our results with the results of previous textural
and structural methods.
1) Textural Methods: We first use four texture descriptors,
all defined at the pixel level, to understand the effectiveness
of defining texture descriptors at the component level. These
pixel-level descriptors are extracted using intensity histograms,
gray-level cooccurrence matrices, Gabor filters, and local binary
patterns. Similar to ours, all these algorithms use linear kernel
SVM classifiers.
The IntensityHistogram [23] and CooccurrenceMatrix [4] descriptors are the first- and second-order statistics calculated on
gray-level intensities of image pixels. Particularly, the IntensityHistogram descriptors include mean, standard deviation, kurtosis, and skewness of a gray-level intensity histogram. The
CooccurrenceMatrix descriptors include energy, entropy, contrast, homogeneity, correlation, dissimilarity, inverse difference
moment, and maximum probability of gray-level cooccurrence
matrices extracted at eight orientations. In our experiments, we
first calculate these descriptors on entire images. However, it is
commonly difficult to find a constant texture over an entire image since the tissue image may contain subregions irrelevant to
classification. Thus, we also implement the grid-based variants
of these descriptors. In these grid-based variants, we divide the
image into fixed sized grids, extract an histogram (or a cooccurrence matrix) on each grid, calculate descriptors on the grid
histograms (or grid cooccurrence matrices), and average the grid
descriptors all over the image.
To extract the GaborFilter descriptors, we first convolve
an image with log-Gabor filters in six orientations and four
scales [24]. Then, for each scale, we average the responses of
different orientations to obtain rotation invariance, and calculate
average, standard deviation, minimum-to-maximum ratio, and
mode descriptors [7] on this average. Likewise, we implement
the grid-based variant of these descriptors.
The LocalBinaryPattern descriptors include histogram frequencies. We compute this histogram on the outputs of a uniform local binary pattern (LBP) operator [13] applied on image
pixels. For each pixel, the LBP operator outputs a binary string
by comparing the pixel’s gray-scale intensity with those of its
eight neighbors; it outputs 1 if its intensity is lower and 0 otherwise. It then assigns the pixel to an histogram bin based on
the number of consecutive 1’s in this binary string. This operator is called uniform if it constructs the histogram on only
the pixels whose binary strings contain at most two bitwise 0/1
transitions in their circular chain. We calculate an additional
2 The images were labeled by Prof. C. Sokmensuer, MD, who is specialized
in colorectal carcinomas and has been practicing pathology for nearly 20 years.
Each image was shown to him for at least two different times and at the end, he
decided on the image label considering his multiple decisions.

1393

bin for keeping frequencies of pixels with nonuniform strings.
In our experiments, we extract these descriptors from the histogram constructed on all pixels. Here, we did not implement
its grid-based variant because calculating histograms on pixels
of equal-sized grids and averaging their histogram frequencies
is equivalent to calculating an histogram on all pixels and using
its frequencies.
Instead, we make use of local binary patterns to implement
the pixel-based counterpart of our algorithm, which we call
PixelBasedAlgorithm. This algorithm follows exactly the same
steps of our algorithm except its descriptor definition step. Particularly, it decomposes a tissue image into a set of circular
objects, defines descriptors on the objects, clusters the objects based on their descriptors to find their new types, and
uses the new types’ frequency in a linear kernel SVM classifier. Here, different from our proposed algorithm, which uses
local object patterns as the descriptors, the PixelBasedAlgorithm uses local binary patterns. To this end, it locates a square
window at the center of each object and calculates local binary patterns of this window to find the descriptors of the
object. We use this comparison in our experiments to understand the effectiveness of defining component-level local object
patterns.
Additionally, we use the resampling-based Markovian model
(RMM) that we implemented in our previous work [25]. The
RMM obtains multiple samples of an image, labels each sample
using discrete Markov models, and votes the samples’ labels
to classify the image. To obtain an image sample, it generates
a sequence on the randomly selected points, which are characterized by texture descriptors and ordered based on proximity.
These descriptors include the histogram of quantized pixels and
the J-value texture measure [26].
All these textural methods except the PixelBasedAlgorithm
extract their features directly on image pixels. Thus, their complexity is polynomially bounded by the number Np of the pixels
in an image. On the other hand, the PixelBasedAlgorithm first
locates objects, complexity of which is also polynomial with
respect to Np , and then extracts its features for each object
considering its neighboring pixels. Likewise, our proposed algorithm locates the objects and extracts its features for each
object but this time considering the neighboring objects. Since
the object number is much lower than the pixel number, the feature extraction of both of these algorithms is also polynomially
bounded by Np .
2) Structural Methods: We first compare our algorithm with
two structural methods that use global graph features: DelaunayTriangulation and ColorGraph. The former constructs a Delaunay triangulation on purple circular objects and extracts features including average degree, average clustering coefficient,
and diameter, as well as average, standard deviation, minimumto-maximum ratio, and disorder of edge lengths and triangle
areas [7]. The latter also constructs a Delaunay triangulation,
but this time on all types of circular objects, and colors triangle edges based on the types of their end nodes. This method
extracts colored versions of average degree, average clustering
coefficient, and diameter features [17]. These two methods use
linear kernel SVM classifiers.

1394

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

The GraphWalk method is another sampling approach [27].
It represents an image generating a set of subgraphs, classifies
each subgraph based on its edge distribution, and votes the subgraphs’ classes to classify the entire image. This method obtains
the subgraphs by first constructing a graph on the attributed components in the entire image and then sampling this graph with
the breadth first search algorithm. It also uses a linear kernel
SVM to classify the subgraphs.
The last method is the HybridModel that we recently developed in our research group [28]. This model first represents
an image with an attributed graph and defines smaller query
graphs as a reference to normal gland structures. It then selects
the regions of the image whose subgraphs are most structurally
similar to the query graphs based on graph edit distances. Using the graph edit distances of the selected regions as well as
their texture descriptors, it classifies the image by a linear kernel
SVM.
These structural methods decompose tissue images into objects and define their features on the object representation. Thus,
after image decomposition, the complexity is polynomial with
respect to the number of the objects. However, since the objects
are located on image pixels, the overall time complexity of the
feature extraction is polynomially bounded by the number Np
of the pixels.

TABLE I
TEST SET RESULTS OF THE PROPOSED LocalObjectPattern ALGORITHM
AND THE COMPARISON METHODS

TABLE II
CONFUSION MATRIX OF THE PROPOSED LocalObjectPattern
ALGORITHM FOR THE TEST SET

C. Parameter Selection
The proposed algorithm has three model parameters: minimum circle radius rm in , highest degree m (highest order
2m ) of local object patterns, and cluster number k. Additionally, there is parameter C for the linear kernel SVM.
In our experiments, we consider all possible combinations
of rm in ∈ {3, 4, 5}, m ∈ {2, 3, 4, 5}, k ∈ {5, 10, 20, 30}, and
C ∈ {1, 2, . . . , 9, 10, 20, . . . , 90, 100, 150, . . . , 950, 1000} as
candidate sets and select the one that gives the highest accuracy when we use threefold cross validation on the training
set. The selected parameters are rm in = 4, m = 4, k = 20, and
C = 90. For the comparison methods, we also use threefold
cross validation to select their parameters.
D. Results
We report the test set results obtained by our proposed
LocalObjectPattern algorithm and the comparison methods in
Table I. This table shows that the proposed algorithm gives
high (> 90%) accuracies for all classes, leading to the highest
overall accuracy. We also provide the confusion matrix for the
proposed LocalObjectPattern algorithm in Table II. This table
depicts that most of the confusions occur in between low-grade
and high-grade cancerous tissues. This is indeed consistent with
the current practice, in which incorrect decisions are typically
observed in grading especially when tissues lie at the boundary
between low-grade and high-grade cancer. This table also shows
that confusions rarely occur in between normal- and high-grade
cancerous tissues.
The PixelBasedAlgorithm takes the same steps of our algorithm except the definition of its descriptors. It uses local binary
pattern descriptors, which are the pixel-based counterpart of

our proposed local object patterns. Comparison of these two
algorithms reveals that this new definition of local patterns on
objects (tissue components) provides more effective representation, resulting in better accuracies especially for classifying
high-grade cancerous tissue images.
The IntensityHistogram, CooccurrenceMatrix, GaborFilter,
and LocalBinaryPattern algorithms extract global texture descriptors on an entire image (using all image pixels) whereas
the RMM uses pixel-based texture descriptors locally defined
for the selected points. On the other hand, the proposed algorithm extracts texture descriptors for each object in a local
neighborhood defined by the distance from this object to its
2m -nearest neighbor. The results show that using object-based
textures is more effective to obtain higher accuracies. The gridbased variants improve results; however, they are still lower than
the results of the proposed algorithm.
The DelaunayTriangulation and ColorGraph methods also
employ circular objects obtained by tissue image decomposition
but use a structural representation. Table I shows that they result
in lower accuracies than the proposed algorithm. Both of these
methods use global properties of the graph defined for an entire
image in their classification. On the other hand, the GraphWalk
and HybridModel methods use characteristics of local graphs
defined for the selected regions of the image. The use of locality
improves the overall classification accuracy. However, the proposed LocalObjectPattern algorithm still yields better results.
As an interesting future work, it is possible to use the proposed
local object patterns as the texture descriptors of the HybridModel. This might further increase the classification accuracy.

OLGUN et al.: LOCAL OBJECT PATTERNS FOR THE REPRESENTATION AND CLASSIFICATION OF COLON TISSUE IMAGES

(a)

(b)

1395

(c)

Fig. 3. Test set accuracies as a function of the model parameters: (a) minimum circle radius rm in , (b) highest degree m of local object patterns, and (c) cluster
number k.

In our experiments, we also investigate the effects of dividing
images into training and test sets. To this end, we also apply
tenfold cross validation for our algorithm and obtain the average
results over the test sets of the ten folds. For our algorithm, the
percentages are 96.00 ± 1.88 for the normal class, 90.84 ± 1.28
for the low-grade cancerous class, 94.55 ± 1.88 for the highgrade cancerous class, and 93.05 ± 0.85 for the overall accuracy.
These results indicate that the overall accuracy obtained using
tenfold cross validation is quite similar to the overall accuracy
obtained on the test set. Moreover, the standard deviation of the
overall accuracy is low, indicating that this accuracy is not so
much dependent on the division of image samples into training
and test sets.

composition of its closer neighbors. This may result in defining
nondistinctive descriptors for objects belonging to images of
different classes. Both of these conditions lower classification
accuracies, as observed in Fig. 3(b).
After extracting the descriptors, we separately quantize objects of each type (each of the purple, pink, and white types) into
clusters. The last parameter is the number k of these clusters.
Smaller cluster numbers may not allow defining distinctive new
object types, which decreases accuracy. Increasing this parameter increases the number of words in the bag-of-words representation; this increases the number of features used in classification. Our analyses reveal that this increase only slightly affects
accuracy [see Fig. 3(c)]; we attribute this small change to the
curse-of-dimensionality phenomenon in classification.

E. Parameter Analyis
We investigate the effects of each parameter on the performance of our proposed LocalObjectPattern algorithm. To this
end, for each parameter, we fix the other two and analyze accuracy as a function of this parameter. Fig. 3 presents these
analyses.
The first parameter is the minimum circle radius rm in , which
is used as a threshold on objects’ radii in tissue image decomposition. Larger values of this parameter result in not defining
smaller objects, which may correspond to important histological components. For example, in a typical image, nuclei’s radii
are relatively small compared to other components. Using larger
thresholds may result in not defining purple objects corresponding to these nuclei. This lowers accuracy, as seen in Fig. 3(a).
On the other hand, smaller threshold values may lead to noisy
false objects, which, in turn, leads to defining false neighbors in
extracting local object pattern descriptors. This also decreases
classification accuracy.
In the proposed
algorithm, we use a set of local object pat
j
2
-LOP
to characterize an object. The second
terns S = m
j =0
parameter m is the highest degree, which determines the size of
this set (and also the local object pattern with the highest order).
As a consequence, it determines the size of neighborhood from
which descriptors are extracted. When larger values are used,
this neighborhood spans larger regions. This causes to lose locality in descriptor definition. On the other hand, when smaller
values are used, characterization of an object mainly relies on

IV. CONCLUSION
This study presents a new algorithm for representing and
classifying colon tissue images. In this algorithm, we introduce
a set of new high-level texture descriptors called local object
patterns. We define these descriptors on tissue objects, which
approximately represent histological tissue components. To this
end, we specify a set of neighborhoods with different locality
ranges and construct a binary string for each of these neighborhoods to encode spatial arrangements of the objects within
the specified local neighborhoods. We then characterize tissue
objects using the decimal equivalents of the binary strings as descriptors and construct bag-of-words representation of an image
from its characterized objects. We test our proposed algorithm
on 3236 microscopic images of colon tissues stained with hematoxylin and eosin. Our experiments demonstrate that our algorithm, which uses local object pattern descriptors, lead to higher
classification accuracies than its pixel-based counterparts.
The proposed algorithm constructs a binary string to encode
objects’ composition in a specified local neighborhood. In this
binary string, purple objects, which usually correspond to nucleus components, are represented with 1 and the others with 0.
Instead of this binary representation, one could consider constructing ternary strings where pink and white objects are represented with different values. Besides, the proposed algorithm
computes the local object pattern descriptors by converting the
binary strings to their decimal equivalents. It is also possible

1396

IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS, VOL. 18, NO. 4, JULY 2014

to obtain these descriptors directly from the strings. Exploring
these possibilities could be considered as future research directions for this study.
REFERENCES
[1] A. Tabesh, M. Teverovskiy, H. Y. Pang, V. P. Kumar, D. Verbel,
A. Kotsianti, and O. Saidi, “Multifeature prostate cancer diagnosis and
Gleason grading of histological images,” IEEE Trans. Med. Imag., vol. 26,
no. 10, pp. 1366–1378, Oct. 2007.
[2] R. Rahmadwati, G. Naghdy, M. Ros, and C.Todd, “Computer aided decision support system for cervical cancer classification,” in Proc. SPIE,
Applications of Digital Image Processing XXXV, Oct. 2012, vol. 8499, p.
849919.
[3] F. Bunyak, A. Hafiane, and K. Palaniappan, “Histopathology tissue segmentation by combining fuzzy clustering with multiphase vector level
sets,” Adv. Exp. Med. Biol., vol. 696, pp. 413–424, 2011.
[4] A. N. Esgiar, R. N. G. Naguib, B. S Sharif, M. K. Bennett, and A. Murray,
“Microscopic image analysis for quantitative measurement and feature
identification of normal and cancerous colonic mucosa,” IEEE Trans. Inf.
Technol. Biomed., vol. 2, no. 3, pp. 197–203, Sep. 1998.
[5] S. Doyle, M. Feldman, J. Tomaszewski, and A. Madabhushi, “A boosted
Bayesian multi-resolution classifier for prostate cancer detection from
digitized needle biopsies,” IEEE Trans. Biomed. Eng., vol. 59, no. 5,
pp. 1205–1218, May 2012.
[6] K. Jafari-Khouzani and H. Soltanian-Zadeh, “Multiwavelet grading of
pathological images of prostate,” IEEE Trans. Biomed. Eng., vol. 50,
no. 6, pp. 697–704, Jun. 2003.
[7] S. Doyle, S. Agner, A. Madabhushi, M. Feldman, and J. Tomaszewski,
“Automated grading of breast cancer histopathology using spectral clustering with textural and architectural image features,” in Proc. Biomed
Imag.: Nano Macro, 2008, pp. 496–499.
[8] A. N. Esgiar, R. N. G. Naguib, B. S. Sharif, M. K. Bennett, and A. Murray,
“Fractal analysis in the detection of colonic cancer images,” IEEE Trans.
Inf. Technol. Biomed., vol. 6, no. 1, pp. 54–58, Mar. 2002.
[9] P.-W. Huang and C.-H. Lee, “Automatic classification for pathological
prostate images based on fractal analysis,” IEEE Trans. Med. Imag.,
vol. 28, no. 7, pp. 1037–1050, Jul. 2009.
[10] H. Qureshi, O. Sertel, N. Rajpoot, R. Wilson, and M. N. Gurcan, “Adaptive
discriminant wavelet packet transform and local binary patterns for meningioma subtype classification,” in Proc. Med. Image Comput. Comput.Assisted Intervent., 2008, pp. 196–204.
[11] O. Sertel, J. Kong, H. Shimada, U. V. Catalyurek, J. H. Saltz, and
M.N. Gurcan, “Computer-aided prognosis of neuroblastoma on whole
slide images: Classification of stromal development,” Pattern Recogn.,
vol. 42, no. 6, pp. 1093–1103, 2009.
[12] Y. Zhang, B. Zhang, F. Coenen, and W. Lu, “Breast cancer diagnosis from
biopsy images with highly reliable random subspace classifier ensembles,”
Mach. Vis. Appl., pp. 1–16, 2012.
[13] T. Ojala, M. Pietikainen, and T. Maenpaa, “Multiresolution gray-scale and
rotation invariant texture classification with local binary patterns,” IEEE
Trans. Pattern Anal. Mach. Intell., vol. 24, no. 7, pp. 971–987, Jul. 2002.
[14] A. N. Basavanhally, S. Ganesan, S. Agner, J. P. Monaco, M. D. Feldman,
J. E. Tomaszewski, G. Bhanot, and A. Madabhushi, “Computerized imagebased detection and grading of lymphocytic infiltration in HER2+ breast
cancer histopathology,” IEEE Trans. Biomed. Eng., vol. 57, no. 3, pp. 642–
653, Mar. 2010.

[15] H.-K. Choi, T. Jarkrans, E. Bengtsson, J. Vasko, K. Wester,
P.-U. Malmstrom, and C. Busch, “Image analysis based grading of bladder
carcinoma. Comparison of object, texture, and graph based methods and
their reproducibility,” Anal. Cell. Pathol., vol. 15, pp. 1–18, 1997.
[16] C. Demir, S. H. Gultekin, and B. Yener, “Learning the topological properties of brain tumors,” IEEE ACM Trans. Comput. Biol., vol. 2, no. 3,
pp. 262–270, Jul./Sep. 2005.
[17] D. Altunbay, C. Cigir, C. Sokmensuer, and C. Gunduz-Demir, “Color
graphs for automated cancer diagnosis and grading,” IEEE Trans. Biomed.
Eng., vol. 57, no. 3, pp. 665–674, Mar. 2010.
[18] A. B. Tosun and C. Gunduz-Demir, “Graph run-length matrices for
histopathological image segmentation,” IEEE Trans. Med. Imag., vol. 30,
no. 3, pp. 721–732, Mar. 2011.
[19] A. C. Simsek, A. B. Tosun, C. Aykanat, C. Sokmensuer, and C. GunduzDemir, “Multilevel segmentation of histopathological images using cooccurrence of tissue objects,” IEEE Trans. Biomed. Eng., vol. 59, no. 6,
pp. 1681–1690, Jun. 2012.
[20] A. B. Tosun, M. Kandemir, C. Sokmensuer, and C. Gunduz-Demir,
“Object-oriented texture analysis for the unsupervised segmentation of
biopsy images for cancer detection,” Pattern Recognit., vol. 42, no. 6,
pp. 1104–1112, 2009.
[21] A. C. Ruifrok and D. A. Johnston. (2001). Quantification of histochemical staining by color deconvolution. Anal. Quant. Cytol. Histol.
[Online]. 23, pp. 291–299. Available: Source code is available at http://
www.dentistry.bham.ac.uk/landinig/software/cdeconv/cdeconv.html
[22] C.-C. Chang and C.-J. Lin. (2011). LIBSVM: A library for support vector machines. ACM Trans. Intell. Syst. Tech. [Online]. 2(3), pp. 1–27.
Available: http://www.csie.ntu.edu.tw/ cjlin/libsvm
[23] M. Wiltgen, A. Gerger, and J. Smolle, “Tissue counter analysis of benign
common nevi and malignant melanoma,” Int. J. Med. Inform., vol. 69,
pp. 17–28, 2003.
[24] P. Kovesi, Code for convolving an image with a bank of logGabor filters, [Online]. Available: http://www.csse.uwa.edu.au/∼pk/
Research/MatlabFns/PhaseCongruency/gaborconvolve.m
[25] E. Ozdemir, C. Sokmensuer, and C. Gunduz-Demir, “A resampling-based
Markovian model for automated colon cancer diagnosis,” IEEE Trans.
Biomed. Eng., vol. 59, no. 1, pp. 281–289, Jun. 2012.
[26] Y. Deng and B. S. Manjunath, “Unsupervised segmentation of colortexture regions in images and video,” IEEE Trans. Pattern Anal. Mach.
Intell., vol. 23, no. 8, pp. 800–810, Aug. 2001.
[27] G. Olgun, C. Sokmensuer, and C. Gunduz-Demir, “Graph walks for classification of histopathological images,” in Proc. Biomed. Imag.: Nano
Macro, 2013.
[28] E. Ozdemir and C. Gunduz-Demir, “A hybrid classification model for
digital pathology using structural and statistical pattern recognition,” IEEE
Trans. Med. Imag., vol. 32, no. 2, pp. 474–483, Feb. 2013.

Authors’ photographs and biographies not available at the time of publication.

