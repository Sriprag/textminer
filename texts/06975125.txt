972

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 3, MARCH 2015

Using a Noninvasive Decoding Method to Classify
Rhythmic Movement Imaginations of the Arm
in Two Planes
Patrick Ofner, Student Member, IEEE, and Gernot R. Müller-Putz∗ , Member, IEEE

Abstract—A brain–computer interface (BCI) can help to
overcome movement deficits in persons with spinal-cord injury.
Ideally, such a BCI detects detailed movement imaginations, i.e.,
trajectories, and transforms them into a control signal for a neuroprosthesis or a robotic arm restoring movement. Robotic arms have
already been controlled successfully by means of invasive recording techniques, and executed movements have been reconstructed
using noninvasive decoding techniques. However, it is unclear if
detailed imagined movements can be decoded noninvasively using
electroencephalography (EEG). We made progress toward imagined movement decoding and successfully classified horizontal and
vertical imagined rhythmic movements of the right arm in healthy
subjects using EEG. Notably, we used an experimental design
which avoided muscle and eye movements to prevent classification
results being affected. To classify imagined movements of the same
limb, we decoded the movement trajectories and correlated them
with assumed movement trajectories (horizontal and vertical). We
then assigned the decoded movements to the assumed movements
with the higher correlation. To train the decoder, we applied
partial least squares, which allowed us to interpret the classifier
weights although channels were highly correlated. To conclude, we
showed the classification of imagined movements of one limb in two
different movement planes in seven out of nine subjects. Furthermore, we found a strong involvement of the supplementary motor
area. Finally, as our classifier was based on the decoding approach,
we indirectly showed the decoding of imagined movements.
Index Terms—Brain–computer interface, electroencephalography (EEG), movement decoding, movement imagery.

I. INTRODUCTION
BRAIN–COMPUTER interface (BCI) [1] records brain
signals and transforms them into control signals for devices. One group of potential BCI users are persons with spinalcord injury (SCI) [2]. Such users will have lost control of body
parts and a BCI in combination with a neuroprosthesis, e.g.,
functional electrical stimulation, can be used to restore motor
functions. Ideally, a person with SCI imagines a certain movement, which is executed instantly by means of the system. The

A

Manuscript received June 24, 2014; revised November 12, 2014; accepted
November 25, 2014. Date of publication December 4, 2014; date of current
version February 16, 2015. This work was supported by the European ICT
Programme Project FP7-224631. Asterisk indicates corresponding author.
P. Ofner is with the Institute for Knowledge Discovery, Graz University of
Technology, and also with BioTechMed-Graz.
∗ G. R. Müller-Putz is with the Institute for Knowledge Discovery, Graz
University of Technology, 8010 Graz, Austria, and also with BioTechMedGraz, 8010 Graz, Austria (e-mail: gernot.mueller@tugraz.at).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/TBME.2014.2377023

BCI is used to decode the movement imagination (MI), and
a neuroprosthesis is used to generate the movement. An optimal BCI would decode the actual detailed MI, i.e., the user
imagines with, e.g., the hand a movement on a certain trajectory, and the exact trajectory should be decoded by the BCI.
Thus, a direct link between the MI and a paralyzed body part
would be reestablished, giving the user the possibility to control
movements in an entirely natural way. Of course, in addition to
a decoder, this also necessitates a highly sophisticated neuroprosthesis and compensation for the lacking sensory (especially
proprioceptive) feedback to execute precise movements. Today,
neuroprostheses can restore some movement functionality [3],
but are still not capable of performing natural human arm movements with similar degrees of freedom and precision. Also, sensory feedback compensation or restoration is still under research
[4], [5]. However, the work presented here is solely about the
MI decoding part of a neuroprothesis.
Promising results have been reported for invasive BCIs with
intracortically implanted arrays, also called brain–machine interfaces (BMIs). In [6], a person with SCI controlled a cursor
and a robotic arm; in [7], two persons with tetraplegia caused
by brainstem stroke controlled a robotic arm; a person with
a motor complete injury due to spinocerebellar degeneration,
but with generally intact sensation, controlled a seven-degreeof-freedom robotic arm in [8]. BMIs provide good brain signal qualities; however, the implantation requires the opening of
the skull and the penetration of the pia mater, which can cause
serious infections. Probably, one would feel more comfortable
with an alternative and that is where noninvasive BCIs come in.
Furthermore, experiments can be conducted with less effort and
costs.
Today’s noninvasive BCIs can detect the process of MI [9],
but not the detailed MI itself (i.e., imagined trajectories). They
are based on power changes in sensorimotor rhythms (SMR)
accompanying MI [10]. In the work of Pfurtscheller et al. [11],
foot MI was used to restore the lateral grasp in a tetraplegic
patient’s right hand. Müller-Putz et al. [2] showed the switching
between different lateral grasp phases with left hand MI, and
just recently, [12], [13] showed the restoration of hand and
elbow functions. The control of a 2-D and 3-D cursor, respectively, has been demonstrated in [14] and [15] by biofeedback
training. Potentially, instead of the cursor, a robotic arm or
neuroprosthesis could also have been controlled. Although
these BCIs restored substantial movement functionality, or
could be extended to that, they were not controlled in a familiar
and natural way by the user. SMR-based BCIs often require an

0018-9294 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications standards/publications/rights/index.html for more information.

OFNER AND MÜLLER-PUTZ: USING A NONINVASIVE DECODING METHOD TO CLASSIFY RHYTHMIC MOVEMENT IMAGINATIONS

artificial association between MI and movement functionality.
Classification performance is crucial, and scientists are,
therefore, forced to use the best classifiable MIs and not the
most similar MIs. Furthermore, as the user has to learn a new
mental strategy, SMR-based BCIs often require a long training
period of weeks or months. However, the learning period for
a decoder based BCI should be substantially reduced because
MIs are natural and have already been learned. Therefore, the
learning of a new mental strategy is no longer necessary.
Although Yuan et al. [16] found a correlation between mu and
beta band activity with the speed of imagery hand clenching,
the traditional mu and beta bands used in SMR-based BCIs are
rather associated with general movement activity but contain
only little information about movement trajectories [17]–[19].
Interestingly, low-frequency (<4 Hz) time-domain brain signals
measured with electroencephalography (EEG), electrocorticography (ECoG), or magnetoencephalography (MEG) seem to
provide proper decodable information about movement trajectories. Two-dimensional hand positions during arm movements
have been predicted in the work of Pistohl et al. [20] from
human ECoG low-frequency time-domain signals. Schalk et al.
[21] decoded 2-D movement trajectories during joystick control,
and Milekovic et al. [22] demonstrated an online classification
of 1-D joystick movement directions. The decoding of movement directions during a center-out task has been shown by Ball
et al. [18]. Acharya et al. [23] reported about finger position
decoding during slow hand grasps. Movement decoding has
also been investigated in human MEG signals. The prediction
of movement trajectories in a pentagon copying task with a 2-D
joystick has been reported by Georgopoulos et al. in [24], and
Bradberry et al. [25] decoded hand velocities in a center-out
drawing task. Toda et al. [26] reconstructed 2-D index fingertip trajectories during pointing movements. Also, 3-D velocity
decoding of movements in a center-out task has already been
reported by Yeom et al. [27]. Waldert et al. [17] decoded hand
movement directions on a single-trial basis. Wang et al. [28]
decoded four different wrist MIs in a 2-D center-out task. Jerbi
et al. [29] found phase locking between slow oscillatory brain
rhythms and time-varying hand speed. Finally, movements have
been decoded from EEG too. Bradberry et al. [30] decoded hand
movement velocities in a center-out out reaching task, and Lv
et al. [31] decoded hand movement velocities during a drawing
task in four directions. Our group showed the 3-D decoding of
hand velocities and positions in a continuous self-chosen movement task [32]. Notably, we used longer lasting movements,
about 1 min during each trial, instead of short lasting centerout reaching movements, which last usually around 1 s. The
often used correlation measure of short lasting movements can
be misinterpreted (see [33]). This is because a short-time window does not contain multiple periods of the movement position
signal at low frequencies. However, as we used longer lasting
movements, this was not an issue in our work.
The aforementioned EEG studies had in common that they all
used low-frequency and mostly time-domain signals to decode
or classify executed movements. However, our targeted subjects
are persons with SCI who are mainly restricted to MI; thus, the
next step is to try to decode MI from healthy subjects. Vučković

973

and Sepulveda already showed the classification of imagined
wrist movements using delta band features [34], [35]. Gu et al.
found in healthy subjects [36] and in amyotrophic lateral sclerosis patients [37] that the speed of imagined wrist movements
(fast and slow) is encoded in the movement-related cortical
potential. Thus, discrete imagined movement types of the same
limb (wrist) and velocities have been already decoded from low
EEG frequencies. However, finally, we would like to decode
imagined continuous movement trajectories of the arm and use
it as a control signal for neuroprostheses. Bradberry et al. [38]
made a first attempt to control a computer cursor in 2-D with
decoded imagined finger/arm movements. However, only one
target was presented on a screen in each trial, and Poli and Salvaris [39] showed that a random cursor also hits this target after
some time, and they reached similar decoding results. Thus, we
designed a new experiment where subjects imagined arm movements and decoded them (preliminary results have been shown
in [40] and [41]). However, there is one substantial issue. We
have to know the imagined movement trajectories—either the
hand velocity or position—to train and test the decoder. But the
imagined trajectories must be determined without inducing eye
movements, which excludes following a known cursor trajectory
on a computer screen. That would cause eye dipole movements
which would heavily influence the recorded EEG and the
decoder might be trained on eye movements instead of MI. Of
course, methods exist which can rid the EEG of eye movements.
They are also used in this study, but usually they just attenuate
the influence of eye movements. There is no guarantee that they
remove the influence completely. In addition to this influence
caused by the electrical fields of the eye dipoles, eye movements
could potentially modulate brain sources. In this case, eye
movements instead of MI would be decoded again. Indeed, Pesaran et al. [42] showed that eye movements modulate the neural
activity in the dorsal premotor area in monkeys. Therefore, we
avoided eye movements in our experimental design.
In our work, subjects imagined rhythmic arm movements
either in the horizontal or vertical plane. These movements were
synchronized to the beats of a metronome. Thus, we knew the
imagined movement trajectories. Afterward, we decoded the
MI trajectories for a few seconds from low-frequency timedomain signals and, then, classified the movement plane. In the
following, we analyze the performance of the decoder-based
classifier and show the involved brain regions in the sensor and
source space.
II. METHODS
A. Subjects
We recruited nine healthy and right-handed subjects (four
females and five males). Most of them had already participated
in BCI experiments. They were aged between 23 and 37 with a
mean value of 26.1 and a standard deviation of 4.3.
B. Paradigm
Subjects were comfortably seated in an arm chair with a computer screen in front of them displaying cues. They imagined

974

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 3, MARCH 2015

D. Preprocessing

Fig. 1. Subjects imagined rhythmic movements in the horizontal or vertical
plane. The gaze was fixated on a cross on the screen.

Fig. 2. Sequence of a trial. At second 0, a cue appeared (an arrow pointing
right or up) together with a beep to gain the subject’s attention, followed by a
gaze-fixation cross. Between 1.5 and 2.5 s started a metronome to tick for 20 s.

horizontal or vertical repetitive rhythmic arm movements from
left to right and back, or from bottom to top and back, respectively, for 20 s within each trial (see Fig. 1). The cue was an arrow
pointing to the right, indicating horizontal movements starting
from the left, or pointing upwards, indicating vertical movements starting from the bottom. Subjects were asked to imagine
natural, round, not jagged movements with the extended right
arm. During the trial, a cross was shown on the screen to fixate the gaze of the subject and to suppress eye movements. We
instructed subjects to synchronize the MIs to beep tones with a
rhythm of 1 Hz presented by a computer. As a beep tone corresponded to an end position of the imagined trajectories, MIs
were actually done with 0.5 Hz. See Fig. 2 for the exact sequence
of a trial. We recorded ten trials per run and eight runs, in total
80 trials with equally distributed classes. To remove the influence of eye movements from the EEG using a linear regression
method, we also recorded one run with 30-s-long deliberate eye
movements. Furthermore, we recorded four runs (each of 30 s)
of baseline EEG without any beep tones and MI. These runs
were used to calculate the noise covariance matrices needed for
source imaging.
C. Recording
We recorded the EEG with 68 electrodes covering frontal,
central, and parietal areas. Reference was placed on the left
mastoid, ground on the right mastoid. Additionally, we recorded
the electrooculogram with three electrodes placed above the nasion and below the outer canthi of the eyes. All signals were
recorded with five g.USBamp amplifiers (g.tec medical engineering GmbH, Schiedlberg, Austria). We applied an eighthorder Butterworth bandpass filter with cutoff frequencies at 0.01
and 100 Hz, a Notch filter at 50 Hz, and then sampled the signals
with 256 Hz. For subsequent source imaging, we also measured
the positions of the electrodes with a CMS 20 EP system (Zebris
Medical GmbH, Isny, Germany).

Using EEGLAB [43], we computed the independent components (ICs) with the extended infomax independent component
analysis [44] in the frequency range 0.2–70 Hz (fourth-order
zero-phase Butterworth filter) and searched for ICs suspected
to be muscle, eye, technical artifacts, or ICs obviously common to all electrodes. Then, we removed artifact contaminated
ICs from the original unfiltered data. Subsequently, we applied a
zero-phase antialiasing filter and downsampled data to 16 Hz for
computational convenience. To get the signal of interest—which
was expected to be at the movement frequency of 0.5 Hz—we
bandpass filtered the data with a second-order zero-phase Butterworth filter with cutoff frequencies at 0.4 and 0.6 Hz. We found
empirically the filter order and the bandwidth which worked
for us. Finally, we removed possible remaining eye artifacts
[45] and removed samples when their absolute value exceeds
a threshold of 4.4 times the median absolute deviation (MAD)
[46] of a channel. MAD is a robust deviation measure and corresponds to three times the standard deviation with normally
distributed data. This caused a removal of 6.2–19.8% of data.
E. Partial Least Squares
As multicollinearity [47] is a serious issue in our data and
prevents the interpretation of, e.g., multiple linear regression
weights, we used partial least squares (PLS) [48], [49]. PLS is a
method used to model the relation between observed variables.
It considers the internal structure of the data by using latent
(hidden, unobservable) variables. Furthermore, it can deal with
many noisy and multicollinear variables. PLS has its usefulness
in modeling, regression, classification, and dimension reduction applications. PLS is a key component in our classification
method and source contribution analysis, and hence, we give a
brief general overview of PLS modeling and PLS regression.
PLS regression is used for decoding in Section II-F; PLS modeling is used to find the sources involved in movement decoding
in Sections II-G and II-H.
1) Modelling: Let X be an n × N matrix of predictor variables (i.e., EEG data), and Y be an n × M matrix of response
variables (i.e., movement positions). The rows of these matrices
correspond to observations and the columns to predictor and
response variables, respectively. In PLS, X and Y are decomposed into:
X = TPT + E
T

Y = UQ + F.

(1)
(2)

T and U are the latent variables and are called scores. T and U
are n × P matrices with n observations and P components. In
general, T is an orthogonal matrix, and in our case, T was also
orthonormal, which makes a normalization step unnecessary. P
and Q are the so-called loadings and can be seen as regression
weights to approximate the original data (X and Y matrices)
with E and F as the residuals. P is of dimension N × P , and
Q is M × P . The basic idea in PLS is now to find scores T
and U with maximum covariance between them. In that way,
T multiplied by the loadings QT is also a good predictor of Y

OFNER AND MÜLLER-PUTZ: USING A NONINVASIVE DECODING METHOD TO CLASSIFY RHYTHMIC MOVEMENT IMAGINATIONS

975

with a residual matrix G:
Y = TQT + G.

(3)

Thus, T models the structure behind X and Y in the latent
variable space. T is calculated from X using the weight matrix
W with dimension N × P :
T = XW.

(4)

How W is calculated depends on the actual PLS algorithm; here,
we used the SIMPLS algorithm [50]. Now, the loadings are useful for interpreting the relations between predictor and response
variables, even when there are serious multicollinearities as the
scores T are orthogonal.
2) Regression: PLS can also be used as a regression method.
Inserting (4) in (3) gives
Y = XWQT + G.

(5)

Fig. 3. Classification method is based on the decoding approach. The EEG
signal is bandpass filtered and the horizontal and vertical positions are decoded.
Afterward, the decoded positions are correlated with the assumed sinusoidal
positions, and the trial is classified as horizontal or vertical MI depending on
which decoding model yielded the higher correlation.

V-model) show the two linear models used for position decoding. N is the number of channels, L the number of time lags, T
the interval between time lags, Sn [t] is the preprocessed EEG
signal at electrode n at time point t, h and v are the decoded
positions in the horizontal and vertical plane, respectively, and
a and b are the coefficients found with the two PLS regressions:

With the N × M dimensional matrix
T

B = WQ .

(6)

h[t] = ah +

(7)

where the columns of B contain the linear regression coefficients
for each dimension of the response variable. Please note that we
have shown the general case of a multidimensional response
variable, but in the remaining part of this study, we only deal
with 1-D response variables, i.e., Ŷ has the dimension n × 1.
F. Calculation of the Classification Accuracy
Our classification approach is based on the decoding principle [30], [32]. We decoded the horizontal and vertical component of the MI and correlated each one with an assumed
sinusoidal movement trajectory, and then assigned the MI to
the component with the higher correlation, this being a twoclass classification. The imagined movement positions in the
horizontal/vertical plane were assumed to follow a 0.5-Hz sinus
curve over time. The left/bottom end position corresponded to
−90◦ and the right/top end position to +90◦ . This is justified as
movements started from a position with maximum deflection,
accelerated, and reached their maximum speed in the middle,
then decelerated and stopped at the other end point, and so forth.
We extracted the EEG and the assumed movement positions
in a certain decoding window w within train trials and independently trained two linear models (the horizontal and the vertical
model) with PLS regression. The decoding window w ranged
from 2 to 18 s after the metronome began to tick; thus, we omitted the first and last 2 s of every MI phase to avoid any transient
response of the zero-phase filter. We trained each model using
the full-channel EEG from the current time step and from three
time lags at 62.5, 125, and 187.5 ms as the predictor variables,
and the assumed movement trajectory as the response variable.
Thus, the input dimension of each model was 273 (68 channels, four time points, one bias), and the output dimension was
1. Equations (8) (horizontal or H-model) and (9) (vertical or

bn k h Sn [t − kT ]

(8)

bn k v Sn [t − kT ].

(9)

n =1 k =0

Y can be approximated as Ŷ:
Ŷ = XB

N 
L


v[t] = av +

N 
L

n =1 k =0

To classify a trial, we used the same decoding window w as
in the training step to extract the EEG from the trial and applied the H- and V-models to the extracted EEG and decoded
the positions. Subsequently, we correlated the output of each
model with a 0.5-Hz sinusoidal oscillation (Pearson correlation
coefficient, r) and assigned the trial to the class (horizontal or
vertical) corresponding to the model yielding the higher correlation. See Fig. 3 for a visualization of the classification method.
To assess the classification method, we used a 10 × 10 fold
cross-validation, trained the regression models with the trials in
each train set, and classified the trials in the corresponding test
sets. Finally, we calculated the classification accuracy for each
cross-validation fold.
To test the validity of our approach, we assumed a 0.7-Hz sinusoidal trajectory (instead of the correct one with 0.5 Hz) and
shifted the 0.4–0.6-Hz bandpass applied in the preprocessing
step to 0.6–0.8 Hz. As the subjects did not imagined movements with this frequency, there should be no correlation with
this trajectory and no significant classification accuracies should
be reached. Furthermore, we calculated the classification accuracies when using electrodes only from the sensorimotor area
and the supplementary motor area (SMA). Here, we used 24
electrodes of our custom made electrode caps; the electrodes
covered positions close to FC1, FCz, FC2, C3, C1, Cz, C2, C4,
CP3, CP1, CPz, CP2, CP4 (10–20 system).
G. Sensor Space Contributions
It is of great interest to determine the electrodes contributing
the most to the classification, either to validate the method or to
gain more knowledge about the underlying processes. However,
the channels were highly correlated and multiple linear regression weights would not be interpretable (see Sections III and IV

976

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 3, MARCH 2015

for further information); therefore, we used PLS to analyze the
relations between EEG and MI.
In the contribution analysis, we used the whole EEG dataset as
training data for the PLS. Keeping the PLS model in Section II-E
in mind, the predictor variables in X correspond to the EEG data
including time lags, and the response variable in Y corresponds
to the assumed movement position. The n × P latent variables
T are the sources containing decodable movement information
and expand over the channels and time lags. The 1 × P loadings Q can be seen as regression weights, which show the MI
information contained in every single latent variable. The projection of the P = 272 latent variables (68 channels · 4 time
lags) back to the channel and time lag space is given through the
loadings P. We independently decoded the horizontal and vertical MI component and, therefore, used two models with their
own set of loadings and scores, i.e., the H- and V-models. For
these models, two interpretable (N · (L + 1) × 1)-dimensional
vectors can be obtained:
ch = Ph QT
h

(10)

cv = Pv QT
v.

(11)

ch and cv consist of the back projections of the latent variables into the channel/time lag space, summed and weighted
by their importance to the decoding procedure. In this study,
we are solely interested in the origin of the decoding sources,
and not, e.g., in the decoder internals. Thus, we averaged the
elements of ch and cv over the time lags, which changed the dimensions of ch and cv from N · (L + 1) × 1 (channel/time lag
space) to N × 1 (channel space). Then, we calculated the
2-norm of corresponding entries in ch and cv and call that the
contribution of a channel. The contributions are proportional to
the amplitudes of the latent variables projected to the channels
and the importance of the latent variables for the decoding.
H. Source Space Contributions
To analyze the contributions in the source space, we used
the same procedure as described in Section II-G, but with brain
sources instead of electrode channels. We used the software
Brainstorm [51], computed boundary element head models with
the Colin27 brain model included in Brainstorm and subject individual electrode positions, rereferenced the data to a common
average reference (a prerequisite of Brainstorm’s source imaging algorithms), and estimated the (full) noise covariance matrices based on the baseline runs with a shrinkage regularization
[52]. Subsequently, we computed 15 028 brain sources using
standardized low-resolution brain electromagnetic tomography
[53]. The source orientations were constrained normal to the
cortex. Unusual high loadings in the loading matrix Q indicated
overfitting. As a remedy and because of limited computing resources, we limited the number of latent variables to 150.
III. RESULTS
A. Classification Accuracies
We used the method described in Section II-F to calculate
the classification accuracies. Here, the decoding window w was

TABLE I
OBTAINED CLASSIFICATION ACCURACIES WITH THE MEAN VALUES AND
STANDARD DEVIATIONS OVER THE CROSS-VALIDATION FOLDS
subject

s1

s2

s3

s4

s5

s6

s7

s8

s9

avg.

mean value [%]
std. dev. [%]

65
17

61
17

51
16

75
14

65
16

48
18

68
15

75
16

74
14

64
10

Subjects with significant classification results are written in bold. The average values
were calculated using subjects’ mean classification. values.

Fig. 4. Dependency of the classification accuracy on the decoding window w
length. Averaged values over all subjects except s3 and s6 are shown as a blue
solid line. The solid black horizontal line marks the chance level; the dashed
black horizontal line represents the significance level (α = 0.05).

fixed to 2–18 s after the start of the metronome. Table I shows the
mean values and standard deviations over all cross-validation
folds for all subjects. The significance level is 59% with
α = 0.05 [54]. The grand average over all nine subjects is
64 ± 10%. Seven subjects reached a significant classification
accuracy.
To analyze the influence of the decoding window length, we
changed the window length in 1-s steps from 2 to 16 s. The
window always started 2 s after the cue presentation. The classification accuracies in dependence on the decoding window
length are shown in Fig. 4. An increase of the classification accuracy over the window length is observable for subjects s2, s4,
s5, s7, s8, and s9. Subjects s2 and s9 show initial peaks; s1 shows
a peak in the middle. Furthermore, Fig. 4 shows the averaged
accuracies over all subjects except those subjects which never
reached a significant classification accuracy (s3 and s6). A simple linear regression analysis shows a significant dependence
of the averaged classification accuracy on the window length
(p < 0.05). The fitted regression line (not shown) has a slope
of 0.66, indicating a positive dependence of the classification
accuracy on the window length, and the window length explains
R2 = 89% of the variance of the classification accuracy.
Table II shows the classification accuracies when assuming
a wrong movement trajectory, i.e., a sinusoidal trajectory with

OFNER AND MÜLLER-PUTZ: USING A NONINVASIVE DECODING METHOD TO CLASSIFY RHYTHMIC MOVEMENT IMAGINATIONS

TABLE II
OBTAINED CLASSIFICATION ACCURACIES WITH THE MEAN VALUES AND
STANDARD DEVIATIONS OVER THE CROSS-VALIDATION FOLDS WHEN
ASSUMING A 0.7-HZ MOVEMENT TRAJECTORY
subject

s1

s2

s3

s4

s5

s6

s7

s8

s9

avg.

mean value [%]
std. dev. [%]

43
18

50
16

46
17

42
18

43
16

53
17

44
16

53
17

54
16

48
5

977

TABLE V
SIGNIFICANT DIFFERENCES BETWEEN CORRELATIONS REACHED WITH THE
H- AND V-MODEL WHEN IMAGING HORIZONTAL OR VERTICAL MOVEMENTS,
TESTED WITH A BONFERRONI CORRECTED SIGN TEST
subject

s1

s2

horizontal MI
vertical MI

∗
∗

∗

s3

s4

s5

∗
∗

∗
∗

s6

s7

s8

s9

∗
∗

∗
∗

∗
∗

Significant differences (α = 0.05) are marked with an asterisk.

The average values were calculated using subjects’ mean classification values.

TABLE III
OBTAINED CLASSIFICATION ACCURACIES WITH THE MEAN VALUES AND
STANDARD DEVIATIONS OVER THE CROSS-VALIDATION FOLDS WHEN USING
ELECTRODES ONLY FROM THE SENSORIMOTOR AREA AND THE SMA

TABLE VI
BONFERRONI CORRECTED SIGN TEST IF THE CORRELATIONS REACHED WITH
AN H- OR V-MODEL AND HORIZONTAL OR VERTICAL MI HAD
A NONZERO MEDIAN

subject

s1

s2

s3

s4

s5

s6

s7

s8

s9

avg.

subject

mean value [%]
std. dev. [%]

63
15

65
17

48
18

77
13

56
15

57
20

57
17

71
15

84
13

64
11

model

H

V

H

V

H

V

H

hor MI
vert MI

∗
∗

∗
∗

∗
∗

∗
∗

∗

∗
∗

∗

Subjects with significant classification results are written in bold. The average values
were calculated using subjects’ mean classification. values.

TABLE IV
PEARSON CORRELATION COEFFICIENT AVERAGED OVER ALL TEST TRIALS
WHEN DECODING POSITIONS USING THE H/V MODEL AND IMAGINING
HORIZONTAL/VERTICAL MOVEMENTS

s1

subject

s2

s6

s3

s7

model

H

V

H

hor MI
vert MI

∗
∗

∗

∗

s4

s8
V
∗

H
∗

s5
V

∗

H

V

∗

∗
∗

s9
V

H

V

∗

∗
∗

∗
∗

Significant differences (α = 0.05) are marked with an asterisk.
subject

s1

s2

s3

s4

s5

model

H

V

H

V

H

V

H

V

H

V

hor MI
vert MI

0.55
0.47

0.46
0.71

0.23
0.20

0.18
0.43

0.02
0.07

0.05
0.10

0.52
0.06

0.09
0.49

0.11
−0.03

−0.06
0.18

subject

s6

s7

s8

s9

avg

model

H

V

H

V

H

V

H

V

H

V

hor MI
vert MI

0.21
0.22

0.22
0.07

0.19
−0.03

−0.04
0.11

0.41
0.01

0.05
0.35

0.49
0.25

0.23
0.63

0.30
0.14

0.13
0.34

0.7 Hz. Here, none of the subjects reached a significant classification accuracy above 59% with α = 0.05. The classification
accuracies are statistically significantly lower than the classification accuracies yielded by the correct movement trajectory, i.e.,
0.5 Hz, with respect to the median value (sign test, p < 0.05).
Table III shows the classification accuracies when using electrodes only from the sensorimotor area and the SMA. Subjects
s1, s2, s4, s8, and s9 reached again a significant classification accuracy, but not s5 and s7 as when using all electrodes. However,
the classification accuracies of both electrode configurations (all
versus sensorimotor/SMA electrodes) do not differ statistically
significantly with respect to the median value (sign test, p = 1).
The classification of a trial depended on whether the Vor H-model yield the higher correlation. Table IV shows
the mean value over all test trials of the Pearson correlation
coefficient, and the grand average over all subjects. The
subjects’ averaged standard deviations over the test trials are
0.35 (horizontal MI/H-model), 0.40 (horizontal MI/V-model),
0.38 (vertical MI/H-model), and 0.33 (vertical MI/V-model).
For subjects with a significant classification accuracy, correlations are always higher when decoding a MI with the correct

(= associated) model as opposed to with the wrong model.
We used a Bonferroni corrected sign test to test for statistical
significant differences between correlations obtained with the
H- and V-models while fixating the type of MI, i.e., we tested
if the differences between paired correlations had a nonzero
median distribution. Table V shows the significant differences
(α = 0.05) when imaging horizontal or vertical movements,
respectively. Subjects s3 and s6 showed no significant difference between correlations yielded by the H- and V-models and,
therefore, did not reach a significant classification accuracy.
Subject s2 only showed a significant difference when decoding
vertical MI, not when decoding horizontal MI. This is consistent
with the observation that s2 reached the lowest significant
classification accuracy. Furthermore, we used a Bonferroni
corrected sign test and α = 0.05 to test whether the correlations
had a nonzero median (see Table VI). All subjects which
reached a significant classification accuracy had a median correlation significantly different from 0 when decoding MI with
the associated decoding model. Subjects s4, s7, and s8 showed
no significant difference from a zero median when decoding an
MI with a notassociated decoding model, which was expected.
However, the other subjects have significant nonzero medians
when decoding an MI with a not-associated decoding model.
We selected the sign test because correlation coefficients are
not normally distributed (due to the range limitation).
B. Sensor Space Contributions
Due to high correlations between channels, we used PLS
regression. To assess the amount of correlation, we calculated the Pearson correlation coefficient between all channel

978

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 3, MARCH 2015

ally, the average over all subjects is shown. Before averaging,
we normalized all maps to the corresponding maximum contribution. Subjects s1, s5, s8, and s9 show strong contributions
from channels over the SMA. A contribution of the SMA and
the parietal area can be observed in the averaged plot.
C. Source Space Contributions
Analogous to the sensor level analysis, we calculated the decoding contribution of each voxel or brain source, respectively.
Fig. 6(b) shows the contributions of the voxels for all subjects
and the average based on normalized maps. All subjects except
s2 show an involvement of central motor regions. Subjects s1,
s5, s8, and s9 show focal contributions from the SMA. The
averaged plot indicates contributions from the SMA.
Fig. 5. Histogram showing a large number of high Pearson correlation coefficients between EEG channels. Included are all channel combinations and
subjects.

Fig. 6. Contribution of the channels and brain sources, respectively, to the
decoded positions for each subject and the average. (a) Contributions of the
channels. Red is mapped to the maximum value, blue corresponds to zero. (b)
Contribution of the brain sources. Contributions below 50% of each subjects
maximum contribution are not shown. Red is mapped to the maximum value,
white to 50% of the maximum value.

combinations for each subject. Here, we used preprocessed data
between 2 and 18 s after the start of the metronome. Fig. 5 shows
a histogram of the Pearson correlation coefficient including coefficients from all channel combinations and subjects. The histogram shows a substantial number of high channel correlations,
which make traditional regression weights uninterpretable.
We analyzed the contribution of each channel to the decoding
as described in Section II-G. Fig. 6(a) shows the contribution of
the channels to the decoded positions for all subjects. Addition-

IV. DISCUSSION
We showed the successful classification of rhythmic MI in
two orthogonal planes (horizontal versus vertical). Seven out
of nine subjects reached a significant classification accuracy.
Moreover, as our classification method is based on the decoder
presented in [30] and [32], we indirectly showed the decoding
of MI from EEG.
The correlations were lower than the correlations reached in
a decoding experiment performed by our group with executed
movements [32]. There, average correlation coefficients of 0.70
(X) and 0.78 (Y) were measured, but here we obtained 0.30 (X)
and 0.34 (Y). Reasons could be that executed movements are
better decodable than imagined movements, or that movement
execution is an easier task than MI. That is to say subjects
probably make more errors when imagining a given trajectory
because of the lack of direct feedback, which would be necessary
for movement corrections. Unfortunately, these correlations are
too low to control an arm neuroprosthesis in an efficient manner,
and the correlations’ standard deviations are relatively large
which would cause an unstable decoding performance.
All subjects with a significant classification accuracy reached
higher correlations with an MI associated decoding model than
with the not-associated decoding model. However, the correlations obtained with a not-associated model were often not close
to zero. A reason could be some 0.5-Hz oscillations common to
both MI classes, maybe due to the MI itself or induced by the
metronome beats.
A key point in our classification approach is the narrow-band
filter. The decoder algorithms in [30] and [32] used solely linear
operations. These linear operations could only amplify or damp
existing frequency components, but not add or delete frequency
components (that would only be possible with nonlinear operations). Therefore, the EEG has to contain decodable movement
information in frequencies corresponding to movement frequencies (we do not exclude that other EEG frequencies may
contain decodable movement information as well, but those
frequencies must be exploited with nonlinear methods). As our
movement frequency was fixed to 0.5 Hz, we were only interested in the EEG frequencies around 0.5 Hz and set a bandpass
filter appropriately to increase the signal-to-noise ratio. EEG
signals with frequencies below or above 0.5 Hz would have had

OFNER AND MÜLLER-PUTZ: USING A NONINVASIVE DECODING METHOD TO CLASSIFY RHYTHMIC MOVEMENT IMAGINATIONS

zero correlation with the assumed 0.5-Hz movement trajectories
(with respect to infinite long signals). We designed a stable
narrow-band filter, but with a low order. That way, the output
of the filter was still able to follow the input appropriately. In
order to validate our classifier, we tested our classifier also with
an assumed 0.7-Hz movement trajectory and obtained no statistically significant classification results. Therefore, our classifier
is expected to be indeed based on correlations between the EEG
and 0.5-Hz oscillations. However, we lack a neurophysiological
explanation why low EEG frequencies correspond to movement
frequencies, but observations and reasoning with respect to
linear operations. An understanding of the basic principles
behind movement decoding is desirable and should be a priority
task of all involved researchers to take forward decoding.
We assumed sinusoidal movement trajectories, which proved
to be sufficient to demonstrate the decoding. However, there
may be better assumptions which are closer to the imagined trajectories, e.g., triangular trajectories, which could yield better
correlations. A sinusoidal trajectory is the most canonical form
comprising of one frequency, other trajectories would comprise
more frequency components and, therefore, would typically necessitate a broader bandpass filter.
The decoded positions were correlated with a sinus. However, the positions were not perfectly decoded but with a certain
error. Therefore, the estimated correlations were expected to be
more accurate the longer the decoding window. Because of that,
classification accuracies were also expected to increase with a
longer decoding window. Indeed, an increase of the classification accuracy over the window length was apparent for six out of
seven subjects with a significant classification accuracy, and the
averaged classification accuracies depend statistically significant on the window length. Notably, three out of seven subjects
with a significant classification accuracy show an apparent classification peak in the beginning or middle, respectively. This
indicates a second opposing effect, and we suspect that some
subjects experienced a decrease in their concentration level over
the trial length.
We would like to point out that the decoding window was
necessary because of our experimental design. Our design goal
was to avoid eye movements by design and to decode imagined instead of executed movements. That way, we were able
to analyze MI decoding without provoking eye/muscle artifacts.
However, in a final end-user application, such a decoding window would be undesirable and imagined movement trajectories
should be decoded instantly.
The sources underlying the classification are of interest for
assessing the validity of the classification results. Our experimental design avoided eye, muscle, and electrode cable movements, and the metronome beats were exactly the same in both
classes. Furthermore, a classification based on electrodes only
from the sensorimotor area and SMA did not yield statistically
significant classification accuracies as when using all electrodes.
Therefore, the discriminative information must have originated
in brain sources and not in artifacts or external sources. However,
due to large channel correlations, it would not have been possible to interpret the weights of, e.g., a multiple linear regression,
a problem known in statistics as multicollinearity [47]. Intu-

979

itively, if two variables are correlated, weights have to be shared
in some ratio across them. Thus, a variable which correlates
with many other variables would tend to get a lower weight than
when not correlated with others. A multiple linear regression
still predicts the response variable, but the weights would not
be interpretable. Therefore, we used the PLS regression which
revealed common contribution patterns between subjects in the
sensor as well as in the source space. Strong contributions from
subareas of the SMA are observable, which become even more
clear in the averaged sensor and source space plots. Interestingly,
subjects s3 and s6 did not achieve a significant classification accuracy, but show a clear involvement of central motor areas.
An explanation could be the fact that s3 and s6 indeed partly
reached significant correlations, but with no exploitable difference between the classes allowing a classification. The SMA is
responsible for higher level motor tasks. Therefore, the brain
sources contributing to the decoding are neurophysiologically
plausible and indicate that we indeed decoded from brain signals. This is consistent with the MEG studies [17], [27] showing
central regions carrying movement trajectory/direction information. Notably, SMR power modulations triggered by MI occur
mainly on the sensorimotor cortex [10], [55], although Yuan
et al. [56] showed modulations also on the SMA. As the regions
are different, SMR modulations and low-frequency time-domain
signals are probably two different movement-related processes.
However, the literature about macroscale brain sources containing movement trajectory/direction information is not always
consistent and shows also involvements of other brain regions.
Toda et al. [26] showed the involvement of primary sensorimotor, higher motor, and parietal regions when decoding 2-D finger
trajectories from MEG. Lv et al. [31] reported larger weights
in motor, posterior parietal, and occipital areas when decoding
hand movement velocities during a drawing task. Jerbi et al. [29]
found phase locking between slow oscillatory MEG activity and
time-varying hand speed in the contralateral primary motor cortex. Wang et al. [28] revealed that the contralateral motor area
and the left inferior frontal gyrus encode intended movement
directions. Bradberry et al. [30] found involvements of the contralateral primary sensorimotor region and the inferior parietal
lobule when decoding 3-D movements trajectories from EEG.
As there is some inconsistence, the involved brain regions and
the exact movement conditions (e.g., self-chosen/target, repetitive, execution/imagery, trajectory/direction decoding) should
be investigated in future studies. A limitation of our source space
analysis is that we used a template head model instead of subject
individual models acquired from magnetic resonance imaging
scans. Individual models would increase the spatial accuracy of
the estimated sources.
We restricted the number of latent variables (scores) to 150 in
the source space analysis because of two reasons. First, it was not
computationally feasible to use all latent variables. Second, we
have observed exceptionally large loadings when using a high
number of latent variables. This is an indication of overfitting,
and probably because the PLS cannot find more orthonormal
latent variables than in the sensor space.
We do not know if metronome beats were a prerequisite for
the MI decoding presented in this study. They possibly caused

980

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 62, NO. 3, MARCH 2015

class-unspecific brain oscillations, which were subsequently
class-specifically modulated by the MI, and then decoded. In
functional magnetic resonance imaging studies, Grahn and Brett
[57] found that rhythm perception elicits higher activation in the
SMA and basal ganglia. Bengtsson et al. [58] reported a higher
activation of the dorsal premotor cortex, SMA, pre-SMA, and
lateral cerebellum when listening to rhythmic sequences. This
indicates a clear involvement of the SMA during beat perception, and according to our findings, the SMA is also involved
when decoding MI. However, the relation between the decoding
of rhythmic MI and the perception of rhythmic beats is yet unknown and has to be analyzed in further studies. Nevertheless,
the metronome beats could not have influenced (but probably
facilitated) the classification results as the beats were exactly the
same in both classes. In other words, the metronome beats could
at most be a necessary condition but not a sufficient condition
for decoding MI.
Our work shows the EEG-based classification of two different imagined movement trajectories with the same limb, and
we excluded an unintentional artifact-based classification. Furthermore, we showed that the main contributions originate in
the SMA. This is also an important finding for persons with
SCI as the SMA is involved in higher level motor control and,
therefore, does not rely on an intact sensorimotor feedback loop.
Thus, decoding should be applicable to persons with SCI. For a
practical neuroprosthesis control, the decoder needs to be substantially improved, and the generalization to nonrhythmic and
nonrestricted MI remains unknown. However, we have shown
that MI decoding from EEG basically works.
ACKNOWLEDGMENT
This paper only reflects the authors’ views and funding agencies are not liable for any use that may be made of the information contained herein.
REFERENCES
[1] J. R. Wolpaw et al., “Brain-computer interfaces for communication and
control,” Clin. Neurophysiol., vol. 113, no. 6, pp. 767–791, 2002.
[2] G. R. Müller-Putz et al., “EEG-based neuroprosthesis control: A step
towards clinical practice,” Neurosci. Lett., vol. 382, pp. 169–174, 2005.
[3] J. Hobby et al., “Restoration of tetraplegic hand function by use of
the neurocontrol freehand system,” J. Hand Surg. Br., vol. 26, no. 5,
pp. 459–464, 2001.
[4] R. R. Riso et al., “Cognitive feedback for use with FES upper extremity neuroprostheses,” IEEE Trans. Biomed. Eng., vol. 38, no. 1,
pp. 29–38, Jan. 1991.
[5] L. A. Johnson et al., “Direct electrical stimulation of the somatosensory
cortex in humans using electrocorticography electrodes: A qualitative and
quantitative report,” J. Neural Eng., vol. 10, no. 3, pp. 1–7, 2013.
[6] L. R. Hochberg et al., “Neuronal ensemble control of prosthetic devices
by a human with tetraplegia,” Nature, vol. 442, pp. 164–171, 2006.
[7] L. R. Hochberg et al., “Reach and grasp by people with tetraplegia using
a neurally controlled robotic arm,” Nature, vol. 485, pp. 372–375, 2012.
[8] J. L. Collinger et al., “High-performance neuroprosthetic control by an
individual with tetraplegia,” Lancet, vol. 381, no. 9866, pp. 557–564,
2013.
[9] G. Pfurtscheller et al., “Graz-BCI: State of the art and clinical applications,” IEEE Trans. Neural Syst. Rehabil. Eng., vol. 11, no. 2, pp. 177–180,
Jun. 2003.
[10] G. Pfurtscheller and F. H. L. D. Silva, “Event-related EEG/MEG synchronization and desynchronization: Basic principles,” Clin. Neurophysiol.,
vol. 110, no. 11, pp. 1842–1857, 1999.

[11] G. Pfurtscheller et al., ““Thought”-control of functional electrical stimulation to restore handgrasp in a patient with tetraplegia,” Neurosci. Lett.,
vol. 351, pp. 33–36, 2003.
[12] M. Rohm et al., “Hybrid brain-computer interfaces and hybrid neuroprostheses for restoration of upper limb functions in individuals with high-level
spinal cord injury,” Artif. Intell. Med., vol. 59, no. 2, pp. 133–142, 2013.
[13] A. Kreilinger et al., “Neuroprosthesis control via noninvasive hybrid braincomputer interface,” IEEE Intell. Syst., vol. 28, no. 5, pp. 40–43, 2013, to
be published.
[14] J. R. Wolpaw and D. J. McFarland, “Control of a two-dimensional movement signal by a noninvasive brain–computer interface in humans,” Proc.
Nat. Acad. Sci., vol. 101, no. 51, pp. 17849–17854, 2004.
[15] D. J. McFarland et al., “Electroencephalographic (EEG) control of threedimensional movement,” J. Neural Eng., vol. 7, no. 3, pp. 1–9, 2010.
[16] H. Yuan et al., “Relationship between speed and EEG activity during
imagined and executed hand movements,” J. Neural Eng., vol. 7, no. 2,
pp. 1–10, 2010.
[17] S. Waldert et al., “Hand movement direction decoded from MEG and
EEG,” J. Neurosci., vol. 28, no. 4, pp. 1000–1008, 2008.
[18] T. Ball et al., “Differential representation of arm movement direction in
relation to cortical anatomy and function,” J. Neural Eng., vol. 6, no. 1,
pp. 1–16, 2009.
[19] Y. Nakanishi et al., “Prediction of three-dimensional arm trajectories based
on ECoG signals recorded from human sensorimotor cortex,” PLoS One,
vol. 8, no. 8, pp. 1–9, 2013.
[20] T. Pistohl et al., “Prediction of arm movement trajectories from ECoGrecordings in humans,” J. Neurosci. Methods, vol. 167, no. 1, pp. 105–114,
2008.
[21] G. Schalk et al., “Decoding two-dimensional movement trajectories using
electrocorticographic signals in humans,” J. Neural Eng., vol. 4, no. 3,
pp. 264–275, 2007.
[22] T. Milekovic et al., “An online brain–machine interface using decoding of
movement direction from the human electrocorticogram,” J. Neural Eng.,
vol. 9, no. 4, pp. 1–14, 2012.
[23] S. Acharya et al., “Electrocorticographic amplitude predicts finger positions during slow grasping motions of the hand,” J. Neural Eng., vol. 7,
no. 4, pp. 1–13, 2010.
[24] A. P. Georgopoulos et al., “Magnetoencephalographic signals predict
movement trajectory in space,” Exp. Brain Res., vol. 167, no. 1,
pp. 132–135, 2005.
[25] T. J. Bradberry et al., “Decoding center-out hand velocity from meg
signals during visuomotor adaptation,” NeuroImage, vol. 47, no. 4,
pp. 1691–1700, 2009.
[26] A. Toda et al., “Reconstruction of two-dimensional movement trajectories
from selected magnetoencephalography cortical currents by combined
sparse Bayesian methods,” NeuroImage, vol. 54, no. 2, pp. 892–905,
2011.
[27] H. G. Yeom et al., “Estimation of the velocity and trajectory of threedimensional reaching movements from non-invasive magnetoencephalography signals,” J. Neural Eng., vol. 10, no. 2, pp. 1–9, 2013.
[28] W. Wang et al., “Decoding and cortical source localization for intended movement direction with meg,” J. Neurophysiol., vol. 104, no. 5,
pp. 2451–2461, 2010.
[29] K. Jerbi et al., “Coherent neural representation of hand speed in humans revealed by MEG imaging,” Proc. Nat. Acad. Sci., vol. 104, no. 18,
pp. 7676–7681, 2007.
[30] T. J. Bradberry et al., “Reconstructing three-dimensional hand movements
from noninvasive electroencephalographic signals,” J. Neurosci., vol. 30,
pp. 3432–3437, 2010.
[31] J. Lv et al., “Decoding hand movement velocity from electroencephalogram signals during a drawing task,” Biomed. Eng. Online, vol. 9, no. 64,
pp. 1–21, 2010.
[32] P. Ofner and G. R. ller-MüPutz, “Decoding of velocities and positions of
3D arm movement from EEG,” in Proc. Annu. Int. Conf. IEEE Eng. Med.
Biol. Soc., 2012, pp. 6406–6409.
[33] J. M. Antelis et al., “On the usage of linear regression models to reconstruct
limb kinematics from low frequency EEG signals,” PLoS One, vol. 8,
no. 4, pp. 1–14, 2013.
[34] A. Vučković and F. Sepulveda, “Delta band contribution in cue
based single trial classification of real and imaginary wrist movements,” Med. Biol. Eng. Comput., vol. 46, no. 6, pp. 529–539,
2008.
[35] A. Vučković and F. Sepulveda, “A two-stage four-class BCI based on
imaginary movements of the left and the right wrist,” Med. Eng. Phys.,
vol. 34, no. 7, pp. 964–971, 2012.

OFNER AND MÜLLER-PUTZ: USING A NONINVASIVE DECODING METHOD TO CLASSIFY RHYTHMIC MOVEMENT IMAGINATIONS

[36] Y. Gu et al., “Single-trial discrimination of type and speed of wrist
movements from EEG recordings,” Clin. Neurophysiol., vol. 120, no. 8,
pp. 1596–1600, 2009.
[37] Y. Gu et al., “Offline identification of imagined speed of wrist movements in paralyzed ALS patients from single-trial EEG,” Front Neurosci.,
vol. 3, no. 62, pp. 1–7, 2009.
[38] T. J. Bradberry et al., “Fast attainment of computer cursor control with
noninvasively acquired brain signals,” J. Neural Eng., vol. 8, no. 3, pp.
1–9, 2011.
[39] R. Poli, and M. Salvaris, “Comment on ‘fast attainment of computer
cursor control with noninvasively acquired brain signals’,” J. Neural Eng.,
vol. 8, no. 5, pp. 1–3, 2011.
[40] P. Ofner and G. R. Müller-Putz, “Towards a novel control paradigm based
on decoding imagined movements from EEG,” in Proc. 5th Int. BrainComput. Interface Meeting: Defining Future, 2013, pp. 62–63.
[41] P. Ofner and G. R. Müller-Putz, “EEG-based classification of imagined
arm trajectories,” in Proc. 2nd Int. Conf. NeuroRehabilitation, 2014,
pp. 611–620.
[42] B. Pesaran et al., “Dorsal premotor neurons encode the relative position
of the hand, eye, and goal during reach planning,” Neuron, vol. 51, no. 1,
pp. 125–134, 2006.
[43] A. Delorme and S. Makeig, “EEGLAB: An open source toolbox for analysis of single-trial EEG dynamics including independent component analysis,” J. Neurosci Methods, vol. 134, pp. 9–21, 2004.
[44] T. W. Lee et al., “Independent component analysis using an extended
infomax algorithm for mixed sub-Gaussian and super-Gaussian sources,”
Neural Comput., vol. 11, no. 2, pp. 417–441, 1999.
[45] A. Schlögl et al., “A fully automated correction method of EOG artifacts
in EEG recordings,” Clin. Neurophysiol., vol. 118, no. 1, pp. 98–104,
2007.
[46] F. Mosteller and J. W. Tukey, Data Analysis and Regression. Reading,
MA, USA: Addison-Wesley, 1977.
[47] D. E. Farrar and R. R. Glauber, “Multicollinearity in regression analysis:
The problem revisited,” Rev. Econ. Statist., vol. 49, no. 1, pp. 92–107,
1967.
[48] S. Wold et al., “PLS-regression: A basic tool of chemometrics,”
Chemometr. Intell. Lab. Syst., vol. 58, no. 2, pp. 109–130, 2001.
[49] R. Rosipal and N. Krämer, “Overview and recent advances in partial least
squares,” in Subspace, Latent Structure and Feature Selection (ser. Lecture
Notes in Computer Science, vol. 3940). Berlin, Germany, Springer, 2006,
pp. 34–51.
[50] S. de Jong, “Simpls: An alternative approach to partial least squares regression,” Chemometr. Intell. Lab. Syst., vol. 18, no. 3, pp. 251–263, 1993.
[51] F. Tadel et al., “Brainstorm: A user-friendly application for MEG/EEG
analysis,” Comput. Intell. Neurosci, vol. 2011, p. 879716, 2011.
[52] J. Schäfer and K. Strimmer, “A shrinkage approach to large-scale covariance matrix estimation and implications for functional genomics,” Statist.
Appl. Genet. Mol. Biol., vol. 4, no. 1, pp. 1–30, 2005.
[53] R. D. Pascual-Marqui, “Standardized low-resolution brain electromagnetic tomography (sLORETA): Technical details,” Methods Find Exp.
Clin. Pharmacol., vol. 24, pp. 5–12, 2002.
[54] M. Billinger et al., “Is it significant? Guidelines for reporting BCI performance,” in Towards Practical Brain-Computer Interfaces. Berlin, Germany: Springer, 2012, pp. 333–354.
[55] H. Yuan et al., “Cortical imaging of event-related (de)synchronization
during online control of brain-computer interface using minimum-norm
estimates in frequency domain,” IEEE Trans. Neural Syst. Rehabil. Eng.,
vol. 16, no. 5, pp. 425–431, Oct. 2008.

981

[56] H. Yuan et al., “Negative covariation between task-related responses in
alpha/beta-band activity and bold in human sensorimotor cortex: An EEG
and fMRI study of motor imagery and movement,” NeuroImage, vol. 49,
no. 3, pp. 2596–2606, 2010.
[57] J. A. Grahn and M. Brett, “Rhythm and beat perception in motor areas of
the brain,” J. Cogn. Neurosci., vol. 19, no. 5, pp. 893–906, 2007.
[58] S. L. Bengtsson et al., “Listening to rhythms activates motor and premotor
cortices,” Cortex, vol. 45, no. 1, pp. 62–71, 2009.

Patrick Ofner (S’14) received the B.Sc. and M.Sc.
degrees in information and communications technology from the Graz University of Technology, Graz,
Austria, in 2008 and 2011, respectively, where he is
working toward the Ph.D. degree under the mentorship of Dr. G. R. Müller-Putz.
He is currently a Scientific Project Assistant at the
BCI Lab of the Institute for Knowledge Discovery,
Graz University of Technology. His research interests
includes movement decoding, EEG source imaging,
biosignal processing, and machine learning.

Gernot R. Müller-Putz received the M.Sc. and Ph.D.
degrees from the Graz University of Technology,
Graz, Austria, in 2000 and 2004, respectively. He
also received the “venia docendi” for medical informatics from the Faculty of Computer Science, Graz
University of Technology.
He is currently the Head of the Institute for Knowledge Discovery, Graz University of Technology, and
its associated BCI Lab. Since October 2014, he has
been a full Professor for semantic data analysis at the
Graz University of Technology. He has gained extensive experience in the field of biosignal analysis, brain–computer interface
research, EEG-based neuroprosthesis control, hybrid BCI systems, the human
somatosensory system, and assistive technology over the past 15 years. He has
also managed several national and international projects and is currently partner
in 2 EU FP7 projects (BackHome, ABC) and coordinator of BNCI Horizon
2020. Recently, he has received a Horizon 2020 project, MoreGrasp, which will
be coordinated by him. Furthermore, he organized and hosted six international
Brain–Computer Interface Conferences over the last 13 years in Graz, the last
one in September 2014 in Graz. He is also steering board member for the International BCI Meeting, which takes place in the U.S. usually every three years
(last time in 2013).
Dr. M&uuml;ller-Putz is the Review Editor of Frontiers in Neuroprosthetics.
Since 2014, he has been an Associate Editor of the Brain-Computer Interface
Journal and of the IEEE TRANSACTIONS OF BIOMEDICAL ENGINEERING.

