2952

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

Head-Free, Remote Eye-Gaze Detection System
Based on Pupil-Corneal Reflection Method
With Easy Calibration Using Two
Stereo-Calibrated Video Cameras
Yoshinobu Ebisawa∗ , Member, IEEE, and Kiyotaka Fukumoto

Abstract—We have developed a pupil-corneal reflection methodbased gaze detection system, which allows large head movements
and achieves easy gaze calibration. This system contains two optical systems consisting of components such as a camera and a nearinfrared light source attached to the camera. The light source has
two concentric LED rings with different wavelengths. The inner
and outer rings generate bright and dark pupil images, respectively. The pupils are detected from a difference image created by
subtracting the bright and dark pupil images. The light source
also generates the corneal reflection. The 3-D coordinates of the
pupils are determined by the stereo matching method using two
optical systems. The vector from the corneal reflection center to
the pupil center in the camera image is determined as r. The angle
between the line of sight and the line passing through the pupil center and the camera (light source) is denoted as θ. The relationship
θ = k |r| is assumed, where k is a constant. The theory implies
that head movement of the user is allowed and facilitates the gaze
calibration procedure. In the automatic calibration method, k is
automatically determined while the user looks around on the PC
screen without fixating on any specific calibration target. In the
one-point calibration method, the user is asked to fixate on one calibration target at the PC screen in order to correct the difference
between the optical and visual axes. In the two-point calibration
method, in order to correct the nonlinear relationship between θ
and |r|, the user is asked to fixate on two targets. The experimental
results show that the three proposed calibration methods improve
the precision of gaze detection step by step. In addition, the average gaze error in the visual angle is less than 1◦ for the seven head
positions of the user.
Index Terms—Corneal reflection, gaze calibration, gaze detection, head-free, pupil.

I. INTRODUCTION
HUS far, various noncontact, remote, video-based eyegaze detection systems allowing head displacements have
been developed for use in human interface (input devices for

T

Manuscript received October 18, 2012; revised May 2, 2013; accepted May
29, 2013. Date of publication June 6, 2013; date of current version September
14, 2013. Asterisk indicates corresponding author.
∗ Y. Ebisawa is with the Graduate School of Engineering, Shizuoka University, Hamamatsu-shi 432–8561, Japan (e-mail: ebisawa@sys.eng.shizuoka.
ac.jp).
K. Fukumoto is with the Graduate School of Engineering, Shizuoka University, Hamamatsu-shi 432–8561, Japan (e-mail: fukumoto@sys.eng.shizuoka.
ac.jp).
Digital Object Identifier 10.1109/TBME.2013.2266478

the handicapped [1]–[3], entertainment [4], and security [5])
and human monitoring (monitoring the driver’s state [6] and
medical technologies [7], [8]). Gaze detection methods that are
based on iris detection include methods that work [9]–[11]. Such
iris-detection-based methods are generally implemented using
inexpensive hardware because no illumination is required. However, these methods cannot be used in darkness without invisible
near-infrared illumination of the eyes. Moreover, the iris tends
to be occluded by the eyelid. The pupil is only slightly hidden
by the eyelid because the pupil is smaller than the iris. Moreover, the pupil is easily found using near-infrared illumination.
Gaze detection systems based on the relative position between
the pupil and the corneal reflection in the video camera image
(pupil-corneal reflection method [1]–[4], [12]–[14]) have been
developed for use in various fields requiring high accuracy.
In general, in order to detect the pupils, it is necessary to
illuminate the user’s face using near-infrared light. If the source
of the light is located in or near an aperture of the camera, the
pupils tend to be bright due to the retroreflective light from
the retina (bright pupil effect [1]). Conversely, the light sources
positioned far from the aperture tend to make the pupils dark
(dark pupil effect [15]). In some previous studies, the pupils
were detected from a difference image created by subtracting
a dark pupil image from a bright pupil image [12]–[14]. In the
difference image, the pupils are easily detected under various
ambient lighting conditions because the background image, i.e.,
everything except the pupils, is canceled out. The light source
for pupil detection also produces a corneal reflection image
(Purkinje image), which works as a reference point to allow
user head movements [12], [14].
When the gaze detection system based on the pupil-corneal
reflection method is used for the general public or restless infants, the following two problems must be considered. The first
problem is related to gaze calibration. Generally, calibration is
performed by fixating on several calibration targets presented
on a PC screen [2]–[4]. Calibration methods that involve fixating on only a few targets have been proposed [16]–[18]. Model
and Eizenman [16] proposed a calibration method that does not
ask the user to fixate on any specific calibration target. In this
method, the user looks around a surface consisting of four observation planes, which have four different orientations. In order
to estimate the difference between the optical and visual axes,
the distance between the intersections of the left and right visual
axes on the plane surfaces is minimized. The differences were

0018-9294 © 2013 IEEE

EBISAWA AND FUKUMOTO: HEAD-FREE, REMOTE EYE-GAZE DETECTION SYSTEM BASED ON PUPIL-CORNEAL REFLECTION METHOD

estimated with a root mean square error of approximately 0.5◦ .
Since four specific planes are used, this method may be difficult
to use in field surveys. Hennessey and Lawrence [17] performed
point pattern matching between the corneal reflections and the
reference pattern depending on the relative spatial positions of
the four light sources. However, using four light sources requires
that the intensity of each light source is lower compared with the
case of only one light source. This is because a light source other
than the four light sources was used to obtain the bright pupil
image and it is also desirable that the intensities of the face in the
bright and dark pupil images are equal in order to easily obtain
the threshold of the pupil from the difference image. Thus, the
intensity of each of the corneal reflections also becomes low,
which makes detecting the corneal reflections difficult.
The second problem is related to the allowance of user head
movements [16]–[24]. Hennessey and Lawrence [17] determined the 3-D gaze point by computing the closest point between the left and right visual axes. Since both eyes were required to be in the captured image, large head movements were
not allowed. Villanueva and Cabeza [18] captured the image
around one eye of a user at a distance of 500–600 mm from
the camera. Accordingly, the range in which the user could
move his/her head was narrow. Zhu and Ji [19] proposed two
gaze detection methods allowing head movements, which exhibited large errors in the gaze detection when the subject’s
head was far from the camera. In addition, these studies did
not present detailed results for the precisions for lateral head
displacements. High image resolution is required in order to
obtain high-precision gaze detection because the coordinates of
the pupil and the corneal reflection image influence the precision. Moreover, in order to allow large head movements, the
camera view angle must be wide or the distance between the
camera and the user’s head must be great, which results in a low
image resolution. Thus, there is a tradeoff relationship between
the range of allowable head movements and the gaze detection
precision.
In this paper, we propose a novel remote, head-free gaze
detection system using a video-based pupil-corneal reflection
method and two stereo-calibrated optical systems. In order to
generate the bright and dark pupil images for which the intensity
exhibits a clear difference and to easily detect the pupils from the
difference image, an optical system including a small concentric
light source with two different wavelengths and a novel flashing
method are proposed. A new gaze detection theory is designed in
order to allow user head movements. The characteristics of the
eyeball are expressed as a simple relationship between the eye’s
angle and the size of the pupil-corneal reflection vector, which
reflects the eyeball’s symmetry around the visual axis. Three
gaze calibration methods based on the proposed gaze detection
theory complete the gaze calibration, while the user looks at or
fixates on zero to two calibration targets on a PC screen.
II. GAZE DETECTION SYSTEM
A. Pupil Detection Principle and Method
When a user’s face is illuminated by a light source, a portion
of the light from the light source enters the user’s pupil. The

Fig. 1.

2953

Lighting conditions for obtaining bright and dark pupil images.

light entering the pupil is reflected diffusely on the retina. A
portion of the reflected light comes out through the pupil and
returns toward the light source with strong directivity, as shown
in Fig. 1. When the light source is located near the aperture of
the camera, a portion of the returning light enters the aperture,
which results in a bright pupil image. On the other hand, when
the light source is far from the aperture, little of the returning
light enters the aperture, which results in a dark pupil image.
Thus, the distance between the light source and the aperture of
the camera is one of the factors that influence pupil brightness.
By differentiating consecutively obtained bright and dark pupil
images, the pupils become conspicuous in the difference image
due to cancellation of the background images, i.e., the area other
than the pupils.
In previous studies [12], [14], a light source consisting of
near-infrared LEDs that are arranged in two concentric rings
(inner and outer rings) was proposed in order to create the
bright and dark pupil images, respectively. Since the inner ring is
located near the aperture of the camera, the inner ring generates
a bright pupil image. Since the outer ring is far from the aperture,
the outer ring generates a dark pupil image. However, the size
of the light source was too large because the space between
the inner and outer rings was wide in order to make clarify
the difference in brightness between the bright and dark pupil
images. Therefore, in order to miniaturize the light source, we
propose a new light source. The structure of the light source is
described in the next section.
B. System Configuration
Fig. 2(a) shows an overview of the developed gaze detection
system. This system has two optical systems, each of which
consists of an NTSC B/W video camera having near-infrared
sensitivity, a 25-mm lens, an infrared filter (IR80), and a light
source [see Fig. 2(b) and (c)]. Each of the two optical systems
was placed under a 19-in liquid crystal display (screen size:
376.3 × 301.1 mm, 1280 × 1024 pixels). Light sources consisting of near-infrared LEDs of two different wavelengths that are
arranged in two concentric rings (inner: 850 nm, outer: 950 nm)
are attached to each camera. The pupil becomes brighter in the
850-nm ring than the 950-nm ring because the transmissivity of
the eyeball medium is different [25]. The distance between the
LEDs and the aperture of the camera also varies the pupil brightness. The combined effects of the distance and the difference in
transmissivity were applied to the light source. This resulted in
the outer ring of LEDs of the light source being closer to the
aperture of the camera than in previous studies [12], [14]. Thus,
the light source became smaller (diameter: approximately 4 cm)

2954

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

Fig. 4. Detection of pupils and corneal reflection. (a) Bright pupil image.
(b) Dark pupil image. (c) Difference image. (d) Around the pupil image.

Fig. 2. Overview of the developed gaze detection system and the near-infrared
LEDs light sources. (a) Overview of the gaze detection system. (b) Side view
of the optical system. (c) Front view of the light source.

imately 30 μW at a distance of approximately 80 cm from the
light source.
An 8-bit gray scale image of a user’s face was input by means
of an image grabbing board (Matrox, Morphis) connected to
a personal computer (PC). The captured image sizes of the
frame and field were 640 × 480 pixels and 640 × 240 pixels,
respectively. The image was processed using the PC to detect
the centers of the pupils and the corneal reflections, which were
used to determine the gaze points of both eyes on the display.
C. Image Processing for Detection of the Centers of Pupil
and the Corneal Reflection Image

Fig. 3. Flashing method of two optical systems for the generation of bright
and dark pupil images.

while maintaining the brightness difference between the bright
and dark pupil images.
In order to reduce the effect of ambient light, we wanted the
LEDs irradiation power on the user’s face to become as strong
as possible compared with the ambient light. Therefore, the
LEDs flashed for a short period of time (approximately 500 μs)
synchronously with the exposure time (500 μs) using an LED
driver. The current flow was approximately 1 A during LED
flashing. Note that the LEDs tolerate this excessive current flow
if the flow is instantaneous. The inner and outer rings flash alternately with the even/odd field of the camera. For this control,
the even/odd field signal derived from the NTSC video signal
via the LM1881 video sync separator was used. The inner LEDs
flash in the odd field, which produces the bright pupil image.
On the other hand, the outer LEDs flash in the even field, which
creates the dark pupil image. Furthermore, in order to avoid mutual light interference of the optical systems, the cameras used
in the optical systems are driven with a slight synchronization
difference (see Fig. 3). In order to realize this synchronization
difference, a vertical sync signal derived from a synchronous
signal generator (SG-101) was given to one camera. The signal
was delayed by 750 μs using a function synthesizer, and the
delayed signal was given to the other camera as a vertical sync
signal, and the horizontal sync signal derived from the generator
is given to both cameras. The average light power was approx-

The pupil is detected from the difference image generated
from the bright and dark pupil images [see Fig. 4(a)–(c)]. The
image is processed in the following order: binarization, removal
of isolated pixels, noise reduction using mathematical morphology operations, and labeling. The largest and second largest
labeled regions were detected as the two pupils. When a pupil
was detected in prior frames, the pupil position in the current
frame was estimated using a linear Kalman filter [26], and a
small window (70 × 70 pixels) was then applied around the
estimated pupil position. On the other hand, when the pupil was
not detected in the prior frames (e.g., effect of blinks and eyelashes), the pupils were again searched for in the entire image
of the user’s face.
The image within the small window is transformed into an image with twice the resolution (140 × 140 pixels) [see Fig. 4(d)].
In this image, the corneal reflection region was determined from
the bright and dark pupil images as an intense and tiny area. The
pupil region was again determined by binarizing the difference
image, which was obtained after shifting this double-resolution
dark pupil image so that the corneal reflection in this dark pupil
image may coincide with that in the double-resolution bright
pupil image [27]. This process helped to decrease the positional
deviation between the bright and dark pupil images while the
user’s head is moving. This maintains the robustness of the
pupil detection and the precision of the pupil center detection.
Ellipse fitting for the contour of the pupil was then performed.
The center of the ellipse was determined as the center of the
pupil. The center of gravity considering the values of the pixels
in the corneal reflection region in the bright pupil image was
determined as the center of the corneal reflection.

EBISAWA AND FUKUMOTO: HEAD-FREE, REMOTE EYE-GAZE DETECTION SYSTEM BASED ON PUPIL-CORNEAL REFLECTION METHOD

2955

we define virtual gaze plane H that passes camera O and is
perpendicular to line OP . Plane H rotates according to the
3-D pupil position. The X’-axis on plane H includes camera
O and is a line formed by the intersection of plane H and
a horizontal plane of the global coordinate system. Here, the
intersection between plane H and line P Q is denoted as T , and
the angle between line OT and the X’-axis is denoted as φ.
Moreover, θ is redefined as the angle between line OP and line
P Q. The coordinates of point T on plane H show a one-to-one
correspondence with a combination of φ and θ. At the same
time, (1) (involving (2)) and the following formula are satisfied
independently:
φ = φ
Fig. 5. Gaze detection theory in 3-D space. (a) Overall image of the system.
(b) Eye image.

D. Gaze Detection Theory
The proposed gaze detection theory assumes a simple eyeball
model in which the corneal sphere juts out from the sphere of the
eyeball. The pinhole camera model is applied to the two cameras.
Tsai’s camera calibration method [28] transforms the camera
position (pinhole) O into the global coordinates. By stereomatching using the two cameras, the 3-D pupil center position
P is detected for each eyeball. The line OP is calculated for
each camera. The explanation of the combination of one optical
system and one eyeball is given as follows.
The light source is assumed to be located at the same position
as the camera position O. This allows the corneal reflection
image to appear on a line that passes through O and the corneal
ball center C. When P moves from one position to another
while maintaining constant angle θ and distance |OP | , |r| is
invariable because ∠P OC is constant. Here, θ indicates the
angle between the optical axis of the eyeball and the line OP ,
and r is the vector from the corneal reflection center to the pupil
center in the captured image. Finally, |r| in the image is replaced
by its actual vector in millimeters based on the pinhole camera
model.
On the other hand, as θ becomes larger (up to 30◦ to 40◦ ),
∠P OC and |r| increase. Therefore, the relationship between θ
and |r| is expressed as a monotonically increasing function f as
follows:
θ = f (|r|)

(1)

where the function depends on the shape and size of the eyeball but not on the 3-D position of the pupil. In this paper, we
basically assume that f is a linear function. In this case, (1) is
replaced by (2):
θ = k |r|

(2)

where k is a proportional constant that differs in the individual
eyeballs. The value of k is determined by the gaze calibration
procedure described in the next section.
Fig. 5 shows the gaze detection theory extended to 3-D space.
Here, we define the intersection between the optical axis and the
PC screen as the gaze point Q. In order to clarify this theory,

(3)

where φ is the declination of vector r in the plane, which passes
through P parallel to the camera image sensor [see Fig. 5(b)],
and φ has been compensated using the camera pose determined
by the camera calibration. Accordingly, (1)–(3) are established
in the 3-D space and do not depend on the 3-D position of
the pupil. Therefore, the gaze detection theory allows head
movement.
When k in (2) is known, the coordinates of point T in plane
H are calculated from |r| and φ using (2) and (3). The optical
axis of the eyeball is determined by connecting the pupil center
P and T on the global coordinate system. The gaze point Q is
detected as the intersection of the optical axis and the PC screen.
E. Gaze Calibration Methods
The gaze calibration procedure should be completed before
the gaze detection is performed. We investigated the following
three calibration methods: the automatic calibration method,
the one-point calibration method, and the two-point calibration
method. In the automatic calibration method, the user does not
need to look at any specific calibration target on the PC screen.
In the one-point calibration method, the difference between the
visual axis and the optical axis is corrected by looking at one
specific calibration target. In the two-point calibration method,
the user needs to look at an additional calibration target in
addition to that in the one-point calibration method in order to
correct the nonlinear relationship between θ and |r|. The details
are as follows:
1) Automatic Calibration Method: An explanation of the
combination of two optical systems and one eyeball is shown
in Fig. 6. Two virtual gaze planes, HL and HR , of the left OL
and right cameras OR , respectively, are found. Vectors O L TL
and O R TR are determined by k, |r L | , |r R | , φL , and φR using
(2) and (3). Here, the new plane G is referred to as the gaze
projection plane, which intersects perpendicularly a bisector of
∠OL P OR . Let the distance between P and plane G be the
sum of the distances from P to planes HL and HR . The origin
OG of plane G is the intersection of plane G and the bisector
of ∠OL P OR , and the X-axis of plane G coincides with the
line formed by the intersection between plane G and the horizontal plane of the global coordinate system. Points TL and
TR on plane HL and HR are projected onto positions TL and

2956

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

denoted as ΔQn and is obtained as follows:
ΔQn = ΔQ/P OG
= (S − T )/P OG .

(4)

Equation (4) is transformed into (5). Point T is corrected by this
formula when the gaze detection procedure is performed, and
Point S is obtained as the final gaze detection point:
S = T + P OG · ΔQn .

Fig. 6. Gaze detection theory expanded to two cameras (optical systems).
Here, P indicates one of the two pupils, and O L and O R indicate the positions
of the two cameras. If the calibration value k is determined, then two optical
axes can be calculated for one eye. Two optical axes can be also determined for
the other eye.

(5)

ΔQn is the new calibration value in the one-point calibration
method.
3) Two-Point Calibration Method: In the present gaze detection theory, the relationship between θ and r is basically
assumed to be linear. However, when the user looks at a target far from the cameras (optical systems) on the PC screen, θ
becomes large. Under such a condition, some individual users
exhibit a nonlinear relationship between θ and |r|. The following
equations are used:

k |r| ,
when k |r| ≤ θnear
(6)
θ=
θnear + k  (|r| − |rnear |), when k |r| > θnear
where θnear and |rnear | indicate the averages of θ and |r|,
respectively, while the user fixates on the near calibration target,
θfar and |rfar | indicate the averages of θ and |r|, respectively,
while fixating on a calibration target far from the cameras, and
k  is obtained using the following equation:
k  = θfar − θnear
|rfar | − |rnear |

(7)

where k  is the new calibration value obtained using the twopoint calibration method.
Fig. 7. Relationship between the detected gaze point and the calibration target
projected onto gaze projection plane G.

TR , respectively, on plane G. In order to minimize the distance
between the two projected points (TL and TR ) on plane G, the
value of k was determined. The values of k, which are obtained
while the user is looking around the PC screen, are averaged
in order to determine the final value of k. The coordinates of
the point at which the user is looking are not used for gaze
calibration.
2) One-Point Calibration Method: The automatic calibration method causes errors in gaze detection for the following
reason. We assume a straight line passing through gaze point Q
and pupil center P as the visual axis (see Fig. 5). In general,
there is a difference of several degrees between the optical and
visual axes [29]. In the one-point calibration method, when the
user looks at the target on the PC screen, the gaze point T is detected on the plane G, as shown in Fig. 7. The coordinates of the
target on the PC screen are projected onto plane G as point S. A
vector from point T to point S is defined as vector ΔQ. Since
the magnitude of ΔQ varies with distance P OG , ΔQ must be
normalized by distance P OG . The normalized value of ΔQ is

III. EXPERIMENTS
A. Experiment 1: Comparison of the Precision of Gaze
Detection Among the Three Calibration Methods
1) Methods: Three university students without eyeglasses
served as the subjects of this experiment. The distance between
the subject’s face and the PC screen was approximately 80 cm.
The illumination on the subject’s face was 150–450 lx.
In the calibration procedure, the subject fixated on two calibration targets on the PC screen. We presented a near calibration
target at the center of the PC screen after careful consideration
of the symmetry of the proposed system, the position at which
the subjects could easily look, and the nonlinear characteristics
mentioned in Section II-E1. The far calibration target was arranged at the top of the PC screen because the cameras were
placed at the bottom of the PC screen in the proposed system.
The subject was asked to first look at the center calibration target for approximately 2 s, and to then look at the top calibration
target for approximately 2 s. The gaze points (from 20 to 30
points) were obtained when fixating on each target.
After the calibration procedure, the subject fixated one by one
on nine (three by three) targets that were evenly arranged on the
screen. The images of the subject’s face were recorded, and
the gaze points were later calculated by methods corresponding

EBISAWA AND FUKUMOTO: HEAD-FREE, REMOTE EYE-GAZE DETECTION SYSTEM BASED ON PUPIL-CORNEAL REFLECTION METHOD

Fig. 8.

2957

Comparison of the average gaze point distributions of both eyes among the three calibration methods for three subjects.

to each calibration method. The gaze points obtained at the
moment when the subject moved his/her line of sight to the next
target were excluded from the analysis. Finally, the remaining
points from 25 to 34 (30.6 ± 2.4 points) for each target were
shown as the detected gaze points in the next section.
2) Results: Fig. 8 compared the detected gaze point distributions among the three calibration methods for the three subjects.
The dots indicate the average of the gaze points of the left and
right eyes. The intersections of the vertical and horizontal dotted
lines indicate the XY coordinates of the nine target positions on
the PC screen. The gaze points for subject A for the nine targets
globally exhibit small errors and a similar distribution among
the three calibration methods. Fig. 9 compares the average gaze
detection errors for the visual angle among the three calibration
methods. The results for subject A indicate that the errors in visual angle are less than 1◦ in all of the methods. In other words,
the gaze point distributions of subject A imply a linear relationship between θ and |r| and would demonstrate that correction
by ΔQ (the difference between the optical and visual axes) is
not needed.
For subject B, in the one-point calibration method, the detected gaze points of each of the target positions shift globally
upward in comparison with those of in the automatic calibration method (see Fig. 8). Furthermore, in the two-point calibration method, the gaze points tend to approach the targets more
closely than the one-point calibration method, especially for the
upper three targets on the PC screen. Significant improvements
in the gaze errors in visual angle appear for the three calibration
methods, as shown in Fig. 9. The two-point calibration method

Fig. 9. Comparison of the errors (visual angle) from eyes among three easy
calibration methods.

provides the best precision. The ratio of the decrease of the error
from the automatic calibration method to the one-point calibration method was 39.5% and that of the error from the onepoint calibration method to the two-point calibration method
was 52.6%. In the upper three targets, ratio of the decrease of
the error from the automatic calibration method to the two-point
calibration method was 85.1%. The aforementioned improvements were effected first by correcting the difference between
the optical and visual axes in the one-point calibration method,
and, second, by correcting the nonlinear relationship between
θ and |r| in the two-point calibration method. Therefore, the

2958

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

Fig. 10. Comparison of the left and right gaze points obtained using the
automatic and one-point calibration results for subject C.

three calibration methods contribute to the improvement of the
precision of gaze point detection step by step.
For subject C, the gaze points indicated the improvement
provided by the two-point calibration method over the one-point
calibration method, as in the case of subject B (see Figs. 8 and
9). However, the gaze points between the automatic calibration
method and the one-point calibration method do not need to be
corrected by ΔQ, as in the case of subject A.
In Fig. 10, the detected gaze points of the left and right eyes
are separately presented in the automatic calibration method
and one-point calibration method for subject C. In the automatic
calibration method, the gaze points of both eyes deviated from
each other because the difference between the optical and visual
axes, namely, ΔQ, differed between both eyes. Using the onepoint calibration method, the gaze points of both eyes tend to
converge. These results indicate that the one-point calibration
method plays an important role in detecting the gaze points of
each eye, rather than detecting the average gaze points of both
eyes.
B. Experiment 2: Evaluation of Precision by Difference
of Head Positions
1) Methods: The subjects were the same as in experiment
1. In the calibration procedure, the subjects were asked to fixate on the same two calibration targets as in experiment 1 at
approximately 80 cm from the PC screen.
After the calibration procedure, the subjects fixated on the
same nine targets as in experiment 1 for the following seven
head positions: approximately 70, 75, 80, 85, and 90 cm from
the PC screen, and 5 cm to the left and 5 cm to the right at 80 cm.
The subjects’ heads were positioned using a chinrest stand. The
calibration values that were obtained at the head position of
80 cm were commonly used for gaze detection at all seven head
positions.

Fig. 11. Gaze errors in visual angle and on the PC screen between the target
and the gaze point among seven head positions. The higher precision results are
chosen from the one-point and two-point calibration results for each subject.

2) Results: Fig. 11 shows the relationship between the head
position and the gaze detection errors in visual angle (white
circles) and the gaze position errors on the PC screen (black
triangles). The averages and SDs for the automatic calibration
method for the three subjects were a gaze error in visual angle
of 1.30 ± 1.18◦ and an error on the screen of 19.5 ± 17.3 mm,
whereas those of the one-point calibration method or two-point
calibration method were 0.91 ± 1.08◦ and 13.6 ± 15.7 mm,
respectively. Neither of these errors show a large difference at
any head position or exhibit a similar trend among the three subjects. Therefore, it is suggested that head movement is allowed
in the proposed gaze detection system.
IV. DISCUSSION
We proposed a gaze detection system that allows for movement of the user’s head. The monotonically increasing relationship between θ and |r| “θ = f (|r|)” is assumed in the gaze
detection theory. This function is replaced by the simple linear relationship “θ = k |r|.” In order to correct the nonlinear
relationship between θ and |r| , f is modified as shown in (6).
Thus, f has high flexibility and can be easily modified according to the characteristics of the individual eyeballs. This is also
because two relationships (“θ = f (|r|)” and “φ = φ ”) used to
determine the gaze point are independent of each other. More
complex functions than (6) may be applied to correct the nonlinearity because the gaze detection theory is very simple.
The proposed system can obtain two lines of sight from one
eyeball by using the two optical systems, as shown in Fig. 6. For
two eyeballs, the proposed system can detect four lines of sight.
The precision of the gaze detection is increased by averaging
the four lines of sight. In addition, the coordinates of the gaze
points in 3-D space may be calculated because the lines of sight
from the left and right eyes are obtained separately [17].
In experiment 1, we compared the gaze detection precision
among three gaze calibration methods, namely, the automatic,
one-point, and two-point calibration methods. For subject A,

EBISAWA AND FUKUMOTO: HEAD-FREE, REMOTE EYE-GAZE DETECTION SYSTEM BASED ON PUPIL-CORNEAL REFLECTION METHOD

TABLE I
GAZE ERRORS OF GAZE DETECTION IN THE PREVIOUS STUDIES
AND IN THE PROPOSED SYSTEM

2959

bration values that were obtained at 80 cm from the PC screen
were used for all seven head positions, the errors in the visual
angle and the errors on the PC screen exhibit almost the same
values at any position (see Fig. 11). The experimental results
confirmed that the present gaze detection theory allows head
movements.
V. CONCLUSION

precise gaze detection was achieved by only the automatic calibration method (see Figs. 8 and 9). In subject B, the three
calibration methods improved the precision of the gaze point
detection step by step, as shown in Figs. 8 and 9. Subject C
showed that the gaze points can be exactly obtained by only
one eye (see Fig. 10). Nagamatsu et al. [30] also proposed a
calibration-free gaze estimation method. They assumed that the
human body is symmetric at the median plane, and the horizontal angles between the optical and visual axes of both eyes are
similar. The gaze point was determined as the midpoint of the
intersection between the PC screen and the optical axis of both
eyes. However, the detected gaze points tended to be detected at
positions that deviated toward the bottom of each target. Moreover, when detecting the 3-D gaze points, the gaze points of
each eye must be exactly detected.
Table I shows the errors of the gaze detection in the previous
studies and in the proposed system. However, since the system
configurations, experimental environment, and subjects are different, rigorous comparison is difficult. In the table, we have
recalculated the means and SDs of Model and Eizenman [16]
based on the results of individual subjects reported in their paper. They obtained their results through simulation. Villanueva
and Cabeza [18] proposed a gaze estimation system using a
single camera and from two to four light sources. In their experiment, they asked their subjects to fixate on two or three
calibration targets. Although not shown in the table, Hennessey
and Lawrence [17] showed the accuracy as the difference in
distance between the visual target and the detected gaze point
in 3-D space. The head position was not shown. The accuracy
was 5.24 ± 2.75 cm and 5.00 ± 3.61 cm when the number of
calibration targets was one and five, respectively. The proposed
system exhibited accuracy of 2.51 ± 1.98 cm, 1.84 ± 1.73 cm,
and 1.06 ± 0.66 cm for the automatic, one-point, and twopoint calibration methods, respectively. The proposed system
achieved high accuracy compared to the method of Hennessey
and Lawrence.
In all of the gaze calibration procedures, in our experiment,
the subjects were asked to fixate on the calibration target at the
center of the PC screen. Actually, however, the gaze calibration
is completed in the automatic calibration method, even if the
user looks around anywhere on the PC screen area.
In experiment 2, the precision of gaze detection was evaluated using the one-point calibration method or the two-point
calibration method for seven head positions. Gaze calibration
was performed at 80 cm from the PC screen. Although the cali-

This paper described a novel head-free video-based gaze detection system using the pupil-corneal reflection method. The
bright and dark pupil images were generated using unique optical systems that consist of a camera and a light source with double concentric LED rings having two different wavelengths. The
pupil was detected from the difference image, which was created by differentiating the bright and dark pupil images. A new
gaze detection theory was proposed. In addition, we proposed
three novel calibration methods, which changed the number of
calibration target on which the subject must fixate, namely, the
automatic, one-point, and two-point calibration methods. The
experimental results revealed a tendency for the precision of the
gaze detection to increase whenever the number of targets on
which to fixate is incremented one by one. Moreover, the developed gaze detection system experimentally exhibited an average
gaze error of less than 1◦ for 20 cm back/forth head movements
and 10 cm right/left head movements. These experimental results proved the correctness of the gaze detection theory. We
consider that this system can be used for the general public or
restless infants. In the future, we hope to develop a system that
solves the problem of reflection from eyeglasses while the user
is moving his/her head.
REFERENCES
[1] T. E. Hutchinson, K. P. White, Jr, W. N. Martin, K. C. Reichert, and
L. A. Frey, “Human–computer interaction using eye-gaze input,” IEEE
Trans. Syst., Man, Cybern., vol. 19, no. 6, pp. 1527–1533, Nov./Dec.
1989.
[2] Eye Tech Digital Systems. (2012). [Online]. Available: http://www.
eyetechds.com/
[3] SmartEye. (2012). [Online]. Available: http://www.smarteye.se/
[4] Tobii Technology. (2012). [Online]. Available: http://www.tobii.com/
[5] A. D. Luca, R. Weiss, and H. Drewes, “Evaluation of eye-gaze interaction
methods for security enhanced PIN-entry,” in Proc. 19th Australas. Conf.
Comput.–Human Interact., 2007, pp. 199–202.
[6] C. C. Chiang and C. L. Huang, “A neuro-based surveillance system for
detecting dangerous non-frontal gazes for car drivers,” IEICE Trans. Inf.
Syst., vol. E87-D, no. 9, pp. 2229–2238, 2004.
[7] U. Lahiri, Z. Warren, and N. Sarkar, “Design of a gaze-sensitive virtual
social interactive system for children with autism,” IEEE Trans. Neural
Syst. Rehabil. Eng., vol. 19, no. 4, pp. 443–452, Aug. 2011.
[8] T. D. Gedeon, D. Zhu, and B. S. U. Mendis, “Eye gaze assistance for a
game-like interactive task,” Int. J. Comput. Games Technol., vol. 2008,
no. 3, 2008.
[9] Y. Matsumoto, T. Ogasawara, and A. Zelinsky, “Behavior recognition
based on head pose and gaze direction measurement,” in Proc. IEEE/RSJ
Int. Conf. Intell. Robot Syst., 2000, pp. 2127–2132.
[10] K. N. Kim and R. S. Ramakrishna, “Vision-based eye-gaze tracking for
human computer interface,” in Proc. Conf. IEEE Syst., Man, Cybern.,
1999, vol. 2, pp. 324–329.
[11] J. G. Wang and E. Sung, “Study on eye gaze estimation,” IEEE Trans.
Syst., Man Cybern. B, Cybern., vol. 32, no. 3, pp. 332–350, Jun. 2002.
[12] Q. Ji and X. Yang, “Real-time eye, gaze, and face pose tracking for
monitoring driver vigilance,” Real-Time Imag., vol. 8, no. 5, pp. 357–377,
2002.

2960

IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING, VOL. 60, NO. 10, OCTOBER 2013

[13] Y. Ebisawa, “Improved video-based eye-gaze detection method,” IEEE
Trans. Instrum. Meas., vol. 47, no. 4, pp. 948–955, Aug. 1998.
[14] C. H. Morimoto, D. Koons, A. Amir, and M. Flickner, “Pupil detection
and tracking using multiple light sources,” Image Vis. Comput., vol. 18,
no. 4, pp. 331–335, 2000.
[15] A. M. Meyers, K. R. Sherman, and L. Stark, “Eye monitor,
microcomputer-based instrument uses an internal model to track the eye,”
Computer, pp. 14–21, 1991.
[16] D. Model and M. Eizenman, “An automatic personal calibration procedure
for advanced gaze estimation systems,” IEEE Trans. Biomed. Eng., vol. 57,
no. 5, pp. 1031–1039, May 2010.
[17] C. Hennessey and P. Lawrence, “Noncontact binocular eye-gaze tracking
for point-of-gaze estimation in three dimensions,” IEEE Trans. Biomed.
Eng., vol. 56, no. 3, pp. 790–799, Mar. 2009.
[18] A. Villanueva and R. Cabeza, “A novel gaze estimation system with one
calibration point,” IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 38,
no. 4, pp. 1123–1138, Aug. 2008.
[19] Z. Zhu and Q. Ji, “Novel eye gaze tracking techniques under natural head
movement,” IEEE Trans. Biomed. Eng., vol. 54, no. 12, pp. 2246–2260,
Dec. 2007.
[20] S. W. Shih and J. Liu, “A novel approach to 3-D gaze tracking using
stereo cameras,” IEEE Trans. Syst., Man, Cybern. B, Cybern., vol. 34,
no. 1, pp. 234–245, Feb. 2004.
[21] Y. Kondou and Y. Ebisawa, “Easy eye-gaze calibration using a moving visual target in the head-free remote eye-gaze detection system,”
in Proc. Conf. IEEE Virtual Environ., Human-Comput. Interfaces Meas.
Syst., 2008, pp. 145–150.
[22] D. Beymer and M. Flickner, “Eye gaze tracking using an active stereo
head,” in Proc. Conf. Comput. Vis. Pattern Recognit., 2003, pp. 451–459.
[23] E. D. Guestrin and M. Eizenman, “General theory of remote gaze estimation using the pupil center and corneal reflections,” IEEE Trans. Biomed.
Eng., vol. 53, no. 6, pp. 1124–1133, Jun. 2006.
[24] T. Ohno and N. Mukawa, “A free–head, simple calibration, gaze tracking
system that enables gaze-based interaction,” in Proc. Symp. Eye Track.
Res. Appl., 2004, pp. 115–122.
[25] T. J. T. P van den Berg and H. Spekreijse, “Near infrared light absorption
in the human eye media,” Vis. Res., vol. 37, no. 2, pp. 249–253, 1997.
[26] A. Blake, R. Curwen, and A. Zisserman, “A framework for spatio-temporal
control in the tracking of visual contours,” Int. J. Comput. Vis., vol. 11,
pp. 127–145, 1993.
[27] Y. Ebisawa and A. Nakashima, “Increasing precision of pupil position detection using the corneal reflection,” Inst. Image Inf. Televis. Eng., vol. 62,
no. 7, pp. 1122–1126, 2008.

[28] R. Y. Tsai, “A versatile camera calibration technique for high-accuracy
3D machine vision metrology using off-the-shelf TV cameras and lenses,”
IEEE J. Robot. Autom., vol. RA–3, no. 4, pp. 323–344, Aug. 1987.
[29] R. Carpenter, Movements of the Eyes. London, U.K.: Pion Limited,
1988.
[30] T. Nagamatsu, R. Sugano, Y. Iwamoto, J. Kamahara, and N. Tanaka,
“User-calibration-free gaze estimation method using a binocular 3D eye
model,” IEICE Trans. Inf. Syst., vol. E94–D, no. 9, pp. 1817–1829, 2011.

Yoshinobu Ebisawa (M’93) was born in Hiratsuka,
Japan, in February 1961. He received the B.Sc.,
M.Sc., and Ph.D. degrees in electrical engineering
from Keio University, Yokohama, Japan, in 1984,
1986 and 1989, respectively.
He is currently a Professor with the Department
of Mechanical Engineering, Graduate School of Engineering, Shizuoka University, Hamamatsu, Japan.
From 1992 to 2004, he was an Associate Professor
with the Department of Optoelectric and Mechanical Engineering and then the Department of Systems
Engineering, Faculty of Engineering, Shizuoka University. His current major
research interests include development of pupil detection technique and its applications for human-computer interaction, and human visuo-oculomotor system.

Kiyotaka Fukumoto received the Ph.D. degree in
engineering from Kyushu University, Fukuoka-shi,
Japan, in 2010.
From April to November 2010, he was a Postdoctoral Researcher at Kyushu University. From December 2011 to 2012, he was an Assistant Professor at the
Faculty of Engineering, Shizuoka University, where
he is currently an Assistant Professor at the Graduate School of Engineering, Shizuoka University. His
research interests include ergonomics and ultrasound
imaging.

