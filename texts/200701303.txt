Journal of Informetrics 1 (2007) 193–203

The h-index: Advantages, limitations and its relation with
other bibliometric indicators at the micro level
Rodrigo Costas ∗ , Marı́a Bordons
Centro de Información y Documentación Cientı́ﬁca (CINDOC), CSIC, Joaquı́n Costa, 22, 28002 Madrid, Spain
Received 17 November 2006; received in revised form 7 February 2007; accepted 8 February 2007

Abstract
The relationship of the h-index with other bibliometric indicators at the micro level is analysed for Spanish CSIC scientists in
Natural Resources, using publications downloaded from the Web of Science (1994–2004). Different activity and impact indicators
were obtained to describe the research performance of scientists in different dimensions, being the h-index located through factor
analysis in a quantitative dimension highly correlated with the absolute number of publications and citations. The need to include
the remaining dimensions in the analysis of research performance of scientists and the risks of relying only on the h-index are
stressed. The hypothesis that the achievement of some highly visible but intermediate-productive authors might be underestimated
when compared with other scientists by means of the h-index is tested.
© 2007 Elsevier Ltd. All rights reserved.
Keywords: h-index; Bibliometric indicators; Micro-level studies; Individual scientific performance; Individual scientific assessment; Research
evaluation

1. Introduction
Bibliometric studies at the micro level are increasingly requested by science managers and policy makers to support
research assessment decisions. Different indicators are frequently developed at this level of analysis, generally based
on both the production of scientists as well as the impact of their documents, such as the number of citations, number
of citations per document or the number of highly cited papers. The combined use of several indicators that give
information on different aspects of scientific output is generally recommended (i.e., Van Leeuwen, Visser, Moed,
Nederhof, & Van Raan, 2003). However, the h-index was introduced in 2005 (Hirsch, 2005), comprising in a single
indicator a measure of quantity and impact of the scientific output of a researcher. According to Hirsch, “a scientist
has index h if h of his or her Np papers have at least h citations each and the other (Np-h) papers have ≤h citations
each”.
The scientific community has shown a huge interest for this indicator, as shown by the high number of publications
on the topic (Ball, 2005; Cho, 2005; Dume, 2005a, 2005b; Glanzel, 2006; Monastersky, 2005; Nazaroff, 2005; Nature,
2005; Popov, 2005). The main advantage of h-index is that it combines a measure of quantity and impact in a single
indicator. It has been calculated in different fields such as physics (Hirsch, 2005), biomedicine (Bornmann & Daniel,
∗

Corresponding author. Tel.: +34 915635482; fax: +34 915642644.
E-mail addresses: rodrigo.costas@cindoc.csic.es (R. Costas), mbordons@cindoc.csic.es (M. Bordons).

1751-1577/$ – see front matter © 2007 Elsevier Ltd. All rights reserved.
doi:10.1016/j.joi.2007.02.001

194

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

2005), information science (Cronin & Meho, 2006), and business (Saad, 2006). It can be useful for journal assessment
(Braun, Glanzel, & Schubert, 2006; Rousseau, 2006b), for comparative description of scientific topics (Banks, 2006)
and also for awarding scientific prizes (Glanzel & Persson, 2005).
Among the advantages of h-index the following have been pointed out by Hirsch (2005):
• It combines a measure of quantity (publications) and impact (citations).
• It allows us to characterize the scientific output of a researcher with objectivity, and therefore may play an important
role when making decisions about promotions, fund allocation and awarding prizes.
• It performs better than other single-number criteria commonly used to evaluate the scientific output of a researcher
(impact factor, total number of documents, total number of citations, citation per paper rate and number of highly
cited papers).
• The h-index can be easily obtained by anyone with access to the Thomson ISI Web of Science and in addition it is
easy to understand.
However, several limitations of the h-index have also been remarked:
• There are inter-field differences in typical h values due to differences among fields in productivity and citation
practices (Hirsch, 2005), so the h-index should not be used to compare scientists from different disciplines.
• The h-index depends on the duration of each scientist’s career because the pool of publications and citations increases
over time (Hirsch, 2005; Kelly & Jennions, 2006). In order to compare scientists at different stages of their career,
Hirsch (2005) presented the “m parameter”, which is the result of dividing h by the scientific age of a scientist
(number of years since the author’s first publication).
• Highly cited papers are important for the determination of the h-index, but once they are selected to belong to the
top h papers, it is unimportant the number of citations they receive. This is a disadvantage of the h-index which
Egghe has tried to overcome through a new index, called g-index (Egghe, 2006b).
• Since the h-index is easy to obtain, we run the risk of indiscriminate use, such as relying only on it for the assessment
of scientists. Research performance is a complex multifaceted endeavour that cannot be assessed adequately by
means of a single indicator (Martin, 1996).
• The use of the h-index could provoke changes in the publishing behaviour of scientists, such an artificial increase
in the number of self-citations distributed among the documents on the edge of the h-index (Van Raan, 2006).
• There are also technical limitations, such as the difficulty to obtain the complete output of scientists with very
common names, or whether self-citations should be removed or not. Self-citations can increase a scientist’s h, but
their effect on h is much smaller than on the total citation count since only self-citations with a number of citations
just >h are relevant (Hirsch, 2005).
To overcome the limitations of the h-index different modifications have been suggested in the literature (Batista,
Campiteli, Kinouchi, & Martinez, 2005, 2006; Bollen, Rodriguez, & van de Sompel, 2006; Egghe, 2006a, 2006b;
Imperial & Rodrı́guez-Navarro, 2005; Rousseau, 2006a).
We think it is essential to continue analysing this indicator carefully, in order to establish clearly its drawbacks
and limitations with the same critical and strict approach that the more traditional indicators received. It is especially
relevant to determine in which cases this index could be biased, since it could have serious consequences on the
assessment of individual scientists.
2. Objectives
The objective of this paper is to analyse the relationship of the h-index with other bibliometric indicators at the micro
level in order to identify some of its advantages and limitations. Differences between h-index and several traditional
indicators in their ability to assess research performance of scientists are given special attention. Our hypothesis is
that h-index is heavily influenced by the absolute number of documents and citations and that it fails to identify those
researchers who are very selective when choosing journals and who have intermediate levels of production but with a
high international impact.

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

195

3. Methodology
Scientific publications of scientists at the Spanish Research Council (CSIC) in the area of Natural Resources
published during 1994–2004 were obtained from the Web of Science (WoS). Natural Resources is one of the eight
scientific areas at CSIC, comprising mainly research on earth sciences, geology, marine sciences, biology, environmental
sciences, and zoology.
A total of 348 permanent scientists were working at CSIC in the area of Natural Resources in 2004. A search
strategy was developed including all authors’ names and their possible variants (Costas & Bordons, 2005). Documents
were searched and downloaded from the Science Citation Index, Social Science Citation Index and Arts & Humanities
Citation Index in June 2005. Documents were checked to verify if they corresponded to the studied scientists. Publications from homonym authors were identified and removed. A steering scientific committee that included several CSIC
scientists in Natural Resources checked the results and supervised the whole process. Only 11 scientists did not have
any publication in the period, so the study focuses on the remaining 337 scientists.
The research performance of every scientist was described by means of the following indicators:
(a) Activity indicators, based on the number of documents published by every scientist.
(b) Observed impact indicators, based on the citations received by publications during 1994–2004. It includes:
• No. of citations (excluding self-citations).
• No. of citations per document.
• Percentage of highly cited papers (HCP ≥ 15 citations). For the purpose of this study the 20 percent of most
cited papers among the total papers published by CSIC scientists in Natural Resources were considered as Highly
Cited Papers. To identify them, the 80th percentile in the distribution of citations per document was used, which
corresponded to the score of 15 citations per document.
• Relative citation rate (RCR), that is, citations of documents as compared with their publication journal. An RCR
higher than 1 means that the article has been cited more often than the average document in its publication journal.
The citation window is the period ranging from 1994 until 2004.
• Percentage of documents with an RCR above 1, that is, the percentage of a scientist’s production which is cited
more often than its publication journals.
(c) Expected impact indicators, based on the impact factor of publication journals. We assume that documents published
in high impact factor journals will probably attain higher visibility than those published in low impact factor journals.
The following indicators were used:
• Median impact factor: the median of the Impact Factor of the publication journals of all documents published by each
scientist. We consider it is more robust than the average value, due to the skewness of the Impact Factor distribution.
Impact factors were obtained from the Journal Citation Reports (JCR). Yearly data were used from 1997 to 2004,
while documents published during 1994–1996 received 1997-JCRs Impact Factors.
• Normalized position of publication journal (NPJ): calculated according to the location of the publication journals
in the ranking of journals in decreasing order of impact factor within each JCR subject category (annual JCR was
used for documents from 1997 to 2004, while the 1997 JCR was consider for 1994–1996 documents) (Bordons &
Barrigon, 1992). For journals classified in more than one subject category, the best NPJ was selected.

NPJ = 1 −

position of the publication journal
total number of journals in the category

It ranges from 0 (low expected impact factor) to almost 1 (high expected impact factor).
The average NPJ for all the documents of every author was calculated.
(d) h-index, as described by Hirsch (2005), to quantify scientists’ achievements through a single number based on
both the number of publications and the number of citations.

196

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

Table 1
Research performance of CSIC scientists in Natural Resources (1994–2004) (average values per scientist)
X ± SD
No. documents
No. citations
No. citations/document
Percentage of highly cited papers
Percentage of Doc.RCR ≥ 1
Impact factor (median)
NJP
h-Index

25
199.22
7.25
0.18
45.14
1.27
0.65
7.98

Range (min–max)
±
±
±
±
±
±
±
±

19.50
230.17
5.08
0.16
18.89
0.53
0.14
4.51

1–162
0–2,201
0–40.96
0–1
7.14–100
0.20–3.69
0.05–0.96
1–29

The present paper comprises two different sections. First, the relationship of the h-index with other indicators of
activity and impact at the level of individual scientists is analysed by means of factor analysis (SPSS, Version 13).
Variables were square-root transformed to normalize their distributions. Logarithmic transformation was used for HCP.
Secondly, we tested the hypothesis according to which highly visible but intermediate-productive authors (those who
do not publish large number of documents but attain high impact, hereafter referred to as “selective scientists”) could
be underestimated when compared to other scientists if the only tool employed is the h-index. Differences between
means were explored through the Mann–Whitney test (p < .05).
4. Results
The production of Natural Resources scientists amounted to 6,093 documents (all types of documents considered)
in the Web of Science during 1994–2004. Productivity ranged from 1 to 162 documents, while the number of citations
ranged from 0 to 2201 and the number of citations per document from 0 to 40.96. The h-index ranged between 1 and
29 (see Table 1).
4.1. Relationship between h-index and other activity and impact indicators
The relationship between h-index and the other indicators has been studied through factor analysis. All the authors
(337 authors) were considered. Four factors were obtained which accounted for 93 percent of the explained variance
(Table 2). The contribution of the variables to the different factors is shown in Table 3.
The first factor is associated with the number of documents, number of citations and h-index. The second factor has
high loadings for RCR related variables, while the third one groups the number of citations per document and the HCP
value. Measures of expected impact appear in the fourth factor.
We have found a remarkably positive association between the h-index and the absolute indicators of activity and
impact, as was previously observed by Van Raan (2006) and Saad (2006) (see Fig. 1).
4.2. Does h-index undervalue scientists with a selective publication strategy?
Differences in the publication behaviour of scientists have been described elsewhere (Cole & Cole, 1967; Costas &
Bordons, 2005; Moed, 2000). Here, we would rather focus on those scientists with a selective publication strategy, that
Table 2
Factor analysis
Component

1
2
3
4

Rotation sums of squared loadings
Total

Percentage of variance

Cumulative percent

2.614
1.923
1.911
1.876

29.040
21.364
21.234
20.842

29.040
50.404
71.638
92.481

Total variance explained. Extraction method: principal component analysis.

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

197

Table 3
Rotated component matrix
Component

No. documents
h-Index
No. citations
RCR median
Percentage of RCR ≥ 1
Rate HCP
No. citations/document
NJP
IF median

1

2

3

4

.977
.915
.856
.108
.107
.161
.112
.063
.153

.001
.164
.173
.926
.920
.245
.295
.118
.033

−.131
.229
.411
.209
.218
.869
.846
.204
.263

−.033
.184
.201
.063
.091
.253
.333
.910
.886

Extraction method: principal component analysis. Rotation method: varimax with Kaiser normalization. Loadings above 0.800 are in bold.

is; those who do not publish a high number of documents but who nevertheless achieve a very important international
impact, receiving a large number of citations.
Since the h-index is size-dependent (Van Raan, 2006), it could underestimate the accomplishment of this type of
“selective researchers”. A theoretical example of the limitations of the h-index in such cases would be the following:
scientist “A” with 10 documents cited 10 times each would have an h-index of 10; whereas scientist “B”, with 5
documents which were cited 200 times each, would only achieve an h-index of 5. Scientist “B” publishes fewer
documents, but their impact is much higher than the other’s (i.e., a higher citation per document rate). Scientist “A”
publishes many more documents albeit with a lower impact. Despite this, according to the h-index, scientist A would
be regarded as much more successful than “B”. Is it true? Or are we rewarding quantity over impact when we follow
this indicator?
Another situation which also questions the use of the h-index is that concerning those scientists who attain a
similar h-index result and yet have different total citation counts (Roediger, 2006). For example, two authors who have
published five documents each may have the same h-index value of 5, although one has 10 citations per document
while the other has 20. Both obtain the same h-index value, but the second scientist does have a higher citation rate
per document as well as a higher total citation count. Are these scientists really comparable in terms of their overall
scientific impact?
Our hypothesis is that the h-index may undervalue the performance of those scientists with an intermediate productivity level but who nevertheless have a high impact and a great international visibility. In order to determine the
incidence of this underestimation, we have compared the classification of scientists in quartiles by h-index and by

Fig. 1. Relationship between h-index and total number of documents and citations (normalized values).

198

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

Table 4
Distribution of impact indicators for the 253 most productive scientists
Indicator

P25

P50

P75

No. citations
No. citations/document
h-Index

107
4.85
7

188
7.16
9

323
9.68
12

observed impact: to what extent do scientists obtain a lower position by h-index than by observed impact? Observed
impact was analysed according to the number of citations per document (which is very sensitive for detecting authors
with a selective publication strategy), and to total number of citations (very sensitive for identifying high h-index
cases).
Only those scientists having an intermediate to high production level (number of documents higher than 25th
percentile that is 11 documents) were included in this part of the study (N = 253). This way we were able to obtain
more reliable results, since the combination of a high impact with a very low production level was avoided (i.e., those
authors with 100 percent of highly cited documents, but who published only two or three papers during the period
under study).
First of all, the 25th, 50th and 75th percentiles were obtained for each of the following variables: total citations,
citations per document rate and h-index (Table 4).
These percentiles allow us to classify scientists into four classes:
•
•
•
•

Class 1. Low values: equal to or below the 25th percentile (P25).
Class 2. Intermediate-low values: above P25 and below or equal to P50.
Class 3. Intermediate-high values: above P50 and below or equal to P75.
Class 4. High values: above the 75th percentile (P75).

The detailed classification of scientists according to their position in the impact and the h-index classes is shown
in Table 5. A simplified classification is shown in Table 6, in which the information provided by the two impact
indicators (no. citations and no. citations/document) is finally summarised in a single one (“Final class by impact” in
Tables 5 and 6.).
As we can see, in half the cases there was concordance between the two impact indicators for a given author, that
is, half the scientists were assigned to the same class either by number of citations or by number of citations per
document, and that was the final class considered. In the remaining cases, the scientist was assigned to the lower of
the two possible classes. For example, if an author was in class 1 by number of citations but in class 2 by number of
citations per document, he/she would finally be included in class 1 (Tables 5 and 6).
In fact, a strong positive relationship was found between number of citations and number of citations per document,
maybe because low productivity scientists (who are more likely to obtain a high number of citations per document with
a low absolute number of citations) were not considered in this part of the study. As it can be seen, very few authors
score high in one impact indicator and low in the other one: for example, scientists falling into class 1 according to
number of citations are very rarely placed in class 4 when using the citation per document value.
Scientists classified in the same class for both Impact and h-index are neither positively nor negatively affected by
the h-index determination (these are found at the main diagonal of Table 6). Scientists with a lower scoring for Impact
than for h-index are considered to be favoured by the latter index, so we called them “overvalued scientists”. However,
those with lower scores for the h-index than for the Impact are deemed as negatively affected by the h-index and so
we refer to them as “undervalued scientists”.
A total of 41 scientists showed a lower score in h-index than in Impact (bottom left triangle in Table 6), while
the opposite held for 63 scientists whose Impact-based scores were lower than those based on their h-index (upper
right triangle in Table 6). It should be mentioned that 21 scientists showed a low h-index (≤P25), with intermediate
or high values of Impact (>P25), while 18 scientists showed low Impact with intermediate or high h-index values. So
the research activity of 41 scientists (16 percent) might be undervalued if we rely only on the h-index, while that of 63
scientists (25 percent) might be overvalued if compared with impact measures.

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

199

Table 5
Classification of scientists according to no. citations, no. citations/document and h-index

In summary, three different groups of scientists were identified according to their location in Table 6: undervalued
scientists (lower position by h-index than by Impact), overvalued scientists (higher position by h-index than by Impact)
and neutrally valued scientists (matching h-index and Impact positions) (main diagonal in Table 6). The scientific
behaviour of the three groups was compared by means of different indicators (Figs. 2 and 3).
As it can be seen in Fig. 2, overvalued scientists present a higher number of documents (p < .001) and citations
(p < .05) than undervalued scientists.
In Fig. 3, relative indicators are analysed in order to determine whether undervalued scientists might present a better
performance in said relative indicators. As it can be seen, undervalued scientists present a higher number of citations
per document and a higher rate of highly cited papers than overvalued scientists (p < .001). Moreover, undervalued
scientists have a lower percentage of documents with zero citations than overvalued and neutrally valued ones (p < .05)
(Fig. 4). The numerical values of these variables are shown in Table 7.
Table 6
Simplified classification of scientists

Overvalued scientists (upper-right triangle) as compared with undervalued scientists (bottom-left triangle).

200

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

Fig. 2. Total number of documents and citations for different types of scientists.

Fig. 3. Relative indicators for different types of scientists.

In Table 8, the bibliometric profiles of a few selected pairs of scientists are compared as a real sample of the risk of
underestimating the scientific achievement of scientists with a selective publication strategy. In the first three cases, the
highest h-index corresponds to the author A, who shows the highest number of documents, although A and B present
a similar number of citations and B has a higher number of citations per document. In relation to the fourth pair of
scientists, A4 and B4, they show the same h-index value, although B4 – who is three times less productive than A4 –
shows a higher number of citations and citations per document.
Scientists B1–B3 obtain lower h-indexes than their corresponding colleagues A1–A3 in Table 8, mainly due to their
lower productivity. The argument that some scientists obtain high impact values by publishing only one or two hot
Table 7
Bibliometric profile of scientists (1994–2004)
Type of scientist

N

No. documents

Neutral value
Overvalued
Undervalued
Total

149
63
41
253

27.99
45.38
21.83
31.32

±
±
±
±

18.25
16.81
6.52
18.51

No. citations
252.18
285.00
215.98
254.49

±
±
±
±

291.47
152.44
107.74
240.69

No. citations/
documents
7.98
6.09
9.95
7.83

±
±
±
±

5.44
1.78
4.50
4.78

RateHCP
0.20
0.17
0.24
0.20

±
±
±
±

0.16
0.09
0.11
0.14

Percentage of
non-cited documents
14.90
14.60
10.21
14.06

±
±
±
±

9.99
6.55
7.08
8.95

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

201

Fig. 4. Percentage of non-cited documents for different types of scientists.
Table 8
Bibliometric profile of selected pairs of scientists to show the risk of underestimating selective scientists by means of h-index
Scientists

No. documents

No. citations

No. citations/documents

Rate HCP

h-index

A1
B1

75
15

458
502

6.1
33.5

0.2
0.7

15
12

A2
B2

60
28

359
369

6.0
13.2

0.3
0.4

15
11

A3
B3

39
23

222
255

5.7
11.1

0.2
0.3

12
8

A4
B4

61
23

504
942

8.3
41.0

0.3
0.7

15
15

papers does not apply in this case, since the aforementioned scientists show intermediate-high levels of productivity
within their area and a good rate of highly cited papers (documents with 15 or more citations).
For selective scientists it is difficult to obtain a high h-index value, since the maximum h-index that a scientist
can obtain is that of his/her total number of documents. Note the case of scientist B1, who shows a higher number
of citations and more citations per document than A1, but a lower h-index. Independently of the number of citations
received it will be impossible for him/her to surpass the threshold of h-index = 15, because that is his/her total number
of documents.
5. Conclusions
Both quantity and impact of publications are taken into account when calculating the h-index, but the number of
publications plays a very important role, since it is the maximum h-index an author can obtain. The h-index tends to
underestimate the achievement of scientists with a “selective publication strategy”, that is, those who do not publish a
high number of documents but who achieve a very important international impact. A good correlation is found between
the h-index and other bibliometric indicators, especially the number of documents and citations received by scientists,

202

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

that is; the best correlation is found with absolute indicators of quantity. As previously stated by Van Raan (2006), the
h-index is size-dependent.
The use of h-index as a single indicator for the assessment of the scientific career of a researcher is not adequate.
The need to use diverse indicators that measure different aspects of the research performance has been advocated by
different authors (see e.g., Martin, 1996; Van Raan, 2006) and even by Hirsch himself (Hirsch, 2005). We certainly
think that the h-index should be complemented with other indicators. To support this statement we would like to
underscore the following:
• The first dimension of the factor analysis here shown includes number of documents, number of citations and h-index,
but it only explains 29 percent of the variance, so there is much valuable information in the remainder dimensions
that should not be discarded.
• We believe that the information related to the expected impact factor of the publications ought to be included in the
assessment process, since it encourages scientists to strive for acceptance in the most prestigious journals within
each discipline.
• Information related to relative citation measures, such as the number of citations per document or the ratio of HCP
is relevant to identify scientists with a selective publication strategy. If the h-index were widely adopted as the only
measure of scientific performance, these selective scientists could be unfairly treated.
• Relative citation rate indicators provide a measure of the impact of a given scientist’s publications as compared
to an international reference (the publication journal or the subfield), although unfortunately this dimension is not
considered by the h-index. These measures are very useful to identify outstanding performers within their journals
or subfields.
The widespread use of the h-index in the assessment of scientists’ careers might influence their publication behaviour,
as has been described previously for other indicators (Butler, 2003; Jiménez Contreras, Moya Anegón, & Delgado
López-Cózar, 2003; Weingart, 2005). It could foster productivity instead of promoting quality, and it may be increasing
the presence of least publishable units or salami publications, since the maximum h-index an author can obtain is that
of his/her total number of publications. It is true that publishing more is not enough to improve the h-index, but it is
also true that the total number of documents is crucial. In fact, for the purpose of obtaining a high h-index, publishing
10 medium-impact documents with 10 citations each one is better than having 5 high-impact documents with 200
citations each one.
More analyses and discussions on the h-index are needed in order to gain a deeper understanding of its applicability
and to determine the best role it can play as part of a comprehensive scientific performance assessment. Several attempts
have already pointed in this direction (Batista et al., 2005, 2006; Egghe, 2006a, 2006b; Rousseau, 2006a). For the time
being, it is still true that science is multidimensional and therefore multidimensional approaches are necessary to assess
properly the scientific performance of scientists.
Acknowledgements
This research was supported by the project SEJ2004-08052-C02-02 financed by the Spanish Plan Nacional de
I+D+I, and a predoctoral I3P grant conceded by the Spanish Research Council (CSIC).
References
Ball, P. (2005). Index aims for fair ranking of scientists. Nature, 436(7053), 900.
Banks, M. G. (2006). An extension of the Hirsch index: Indexing scientific topics and compounds. Scientometrics, 69(1), 161–168.
Batista, P. D., Campiteli, M. G., Kinouchi, O. & Martinez, A. S. (2005). Universal behaviour of a research productivity Index. ArXiv:physics/0510142.
http://arxiv.org/ftp/physics/papers/0509/0509048.pdf Accessed 20.12.2005.
Batista, P. D., Campiteli, M. G., Kinouchi, O., & Martinez, A. S. (2006). It is possible to compare researchers with different scientific interests?
Scientometrics, 68(1), 179–189.
Bollen, J., Rodriguez, M. & van de Sompel, H. (2006). Journal Status. Arxiv:physics. http://arxiv.org/PS Cache/cs/pdf/0601/0601030.pdf Accessed
19.03.2000.
Bordons, M., & Barrigon, S. (1992). Bibliometric analysis of publication of Spanish pharmacologists in the SCI (1984–1989). 2. Contribution to
subfields other than pharmacology and pharmacy (ISI). Scientometrics, 25(3), 425–446.

R. Costas, M. Bordons / Journal of Informetrics 1 (2007) 193–203

203

Bornmann, L., & Daniel, H. D. (2005). Does the h-index for ranking of scientists really work? Scientometrics, 65(3), 391–392.
Braun, T., Glanzel, W., & Schubert, A. (2006). A Hirsch-type index for journals. Scientometrics, 69(1), 169–173.
Butler, L. (2003). Modifying publication practices in response to funding formulas. Research Evaluation, 12(1), 39–46.
Cho, A. (2005). Your career in a number. Science Now. http://sciencenow.sciencemag.org/cgi/content/full/2005/812/1 Accessed 20.12.2005.
Cole, S., & Cole, J. R. (1967). Scientific output and recognition: A study in the operation of the reward system in science. American Sociological
Review, 32(3), 377–390.
Costas, R., & Bordons, M. (2005). Bibliometric indicators at the micro-level: Some results in the area of natural resources at the Spanish CSIC.
Research Evaluation, 14(2), 110–120.
Cronin, B., & Meho, L. (2006). Using the h-index to Rank influential Information Scientists. Journal of the American Society for Information
Science and Technology, 57(9), 1275–1278.
Dume, B. (2005a). Number theory. Physicsweb. http://physicsweb.org/articles/news/9/8/9/1 Accessed 20.12.2005.
Dume, B. (2005). How high is your h-index? Physics World, 18(9), 7.
Egghe, L. (2006a). How to improve the h-index. The Scientist, 20(3), 14.
Egghe, L. (2006b). Theory and practice of the g-index. Scientometrics, 69(1), 131–152.
Glanzel, W. (2006). On the h-index—A mathematical approach to a new measure of publication activity and citation impact. Scientometrics, 67(2),
315–321.
Glanzel, W., & Persson, O. (2005). H-index for Price Medallists. ISSI Newsletter, 1(4), 15–18.
Hirsch, J. E. (2005). An index to quantify and individual’s scientific research output. Proceedings of the National Academy of Sciences of the United
States of America, 102(46), 16569–16572.
Imperial, J., & Rodrı́guez-Navarro, A. (2005). Utilidad del Índice H de Hirsch para evaluar la investigación en España. http://www.bit.
etsia.upm.es/Imperial Rodriguez-Navarro.pdf Accessed 20.12.2005.
Jiménez Contreras, E., Moya Anegón, F. de, & Delgado López-Cózar, E. (2003). The evolution of research activity in Spain: The impact of the
National Commission for the Evaluation Research Activity (CNEAI). Research Policy, 32(1), 123–142.
Kelly, C. D., & Jennions, M. D. (2006). The h-index and career assessment by numbers. Trends in Ecology and Evolution, 21(4), 167–170.
Martin, B. R. (1996). The use of multiple indicators in the assessment of basic research. Scientometrics, 36(3), 343–362.
Moed, H. F. (2000). Bibliometric indicators reflect publication and management strategies. Scientometrics, 47(2), 323–346.
Monastersky, R. (2005). Impact factors run into competition. The Chronicle of Higher Education, 52(8), A17. http://chronicle.com/free/
v52/i08/08a01701.html Accessed 20.12.2005.
Nature (2005). Rating games: Researchers have two rare opportunities to influence the ways in which they may be assessed in future (editorial).
Nature, 436(7053), 889–890.
Nazaroff, W. W. (2005). Measuring research productivity. Indoor Air, 15, 382.
Popov, S. B. (2005). A parameter to quantity dynamics of a researcher’s scientific activity. ArXiv:physics/0508113. http://arxiv.
org/PS cache/physics/pdf/0508/0508113.pdf Accessed 20.12.2005.
Roediger, H. L. (2006). The h-index in science: A new measure of scholarly contribution. The Academic Observer, 19(4). http://www.
psychologicalscience.org/observer/19/4/academic observer Accessed 26.7.2006.
Rousseau, R. (2006a). New developments related to the Hirsch-index. http://eprints.rclis.org/archive/00006376/01/Hirsch new developments.pdf
Accessed 23.6.2006.
Rousseau, R. (2006b). A case study: Evolution of JASIS’ Hirsch-index. Science Focus, 1(1), 16–17. http://eprints.rclis.org/archive/
00005430/01/Evolution of h JASIS rev.pdf Accessed 12.2.2006.
Saad, G. (2006). Exploring the h-index at the author and journal levels using bibliometric data of productive consumer scholars and business-related
journals respectively. Scientometrics, 69(1), 117–120.
Van Leeuwen, T. N., Visser, M. S., Moed, H. F., Nederhof, T. J., & Van Raan, A. F. J. (2003). The holy grail of science policy: Exploring and
combining bibliometric tools in search of scientific excellence. Scientometrics, 57(2), 257–280.
Van Raan, A. F. J. (2006). Comparisons of the Hirsch-index with standard bibliometric indicators and with peer judgment for 147 chemistry research
groups. Scientometrics, 67(3), 491–502.
Weingart, P. (2005). Impact of bibliometrics upon the science system: Inadvertent consequences? Scientometrics, 62(1), 117–131.

